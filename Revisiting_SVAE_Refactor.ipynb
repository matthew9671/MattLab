{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthew9671/MattLab/blob/main/Revisiting_SVAE_Refactor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFyEe0Xi_84F"
      },
      "source": [
        "- [x] Implement and test Kalman filtering and smoothing with parallel scan\n",
        "- [ ] Make parallel scan KF work with non-zero biases\n",
        "- [x] Write analysis code for pendulum\n",
        "  - [x] Evaluate predictive accuracy\n",
        "  - [x] Do linear regression from latents to angle and velocity "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6u_EWGy0phmX"
      },
      "outputs": [],
      "source": [
        "# This reloads files not modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BJs02uuMM79",
        "outputId": "4a9847a4-5e97-467b-cef9-ff7cdbdc5038",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.4/238.4 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# @title Download stuff \n",
        "import os\n",
        "# Download and install the relevant libraries\n",
        "!pip install -q ml-collections git+https://github.com/google/flax\n",
        "!pip install -q wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import dynamax\n",
        "except ModuleNotFoundError:\n",
        "    print('installing dynamax')\n",
        "    !pip install git+https://github.com/probml/dynamax.git#egg=dynamax\n",
        "    import dynamax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbqX5CKc11Uc",
        "outputId": "bc1a0f1e-a184-4146-91d9-cd0e426e5a29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing dynamax\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dynamax\n",
            "  Cloning https://github.com/probml/dynamax.git to /tmp/pip-install-kcw9cluu/dynamax_3deb1beb4cd14572b9b5b5937fb1677d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/probml/dynamax.git /tmp/pip-install-kcw9cluu/dynamax_3deb1beb4cd14572b9b5b5937fb1677d\n",
            "  Resolved https://github.com/probml/dynamax.git to commit aada5c1495c6d4b88cf95e052600efb290f2c59c\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from dynamax) (1.0.2)\n",
            "Collecting jaxtyping\n",
            "  Downloading jaxtyping-0.2.11-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.8/dist-packages (from dynamax) (1.0.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from dynamax) (4.4.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.8/dist-packages (from dynamax) (0.1.4)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.8/dist-packages (from dynamax) (0.17.0)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.8/dist-packages (from dynamax) (0.3.25+cuda11.cudnn805)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.8/dist-packages (from dynamax) (0.3.25)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.15->dynamax) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.15->dynamax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.15->dynamax) (1.7.3)\n",
            "Collecting typeguard>=2.13.3\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax->dynamax) (1.3.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from optax->dynamax) (0.1.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->dynamax) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->dynamax) (1.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->dynamax) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->dynamax) (0.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->dynamax) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->dynamax) (2.2.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->dynamax) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax->dynamax) (0.12.0)\n",
            "Building wheels for collected packages: dynamax\n",
            "  Building wheel for dynamax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dynamax: filename=dynamax-0.1.0+131.gaada5c1-py3-none-any.whl size=147523 sha256=037c98baf125ee6e22f6b90bec170ae6edea0d79620edb497fca9b53ae27f927\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g40wstes/wheels/52/e1/7c/4664778646c92bb31957a1dac9e17ef231020b69e0b2e4d113\n",
            "Successfully built dynamax\n",
            "Installing collected packages: typeguard, jaxtyping, dynamax\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 2.7.1\n",
            "    Uninstalling typeguard-2.7.1:\n",
            "      Successfully uninstalled typeguard-2.7.1\n",
            "Successfully installed dynamax-0.1.0+131.gaada5c1 jaxtyping-0.2.11 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njW9wRLEPG7T"
      },
      "source": [
        "# Set everything up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "b1ikkl1ULTEB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "# Misc\n",
        "import os\n",
        "from importlib import reload\n",
        "import numpy as onp\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "from tqdm import trange\n",
        "import copy, traceback\n",
        "from pprint import pprint\n",
        "from copy import deepcopy\n",
        "import pickle as pkl\n",
        "\n",
        "# for logging\n",
        "import wandb\n",
        "# Debug\n",
        "import pdb\n",
        "# Jax\n",
        "import jax\n",
        "from jax import vmap, lax, jit, value_and_grad\n",
        "import jax.numpy as np\n",
        "import jax.scipy as scipy\n",
        "import jax.random as jr\n",
        "key_0 = jr.PRNGKey(0) # Convenience\n",
        "from jax.lax import scan, stop_gradient\n",
        "from jax.tree_util import tree_map\n",
        "# optax\n",
        "import optax as opt\n",
        "# Flax\n",
        "import flax.linen as nn\n",
        "from flax.linen import Conv, ConvTranspose\n",
        "from flax.core import frozen_dict as fd\n",
        "\n",
        "# Tensorflow probability\n",
        "import tensorflow_probability.substrates.jax as tfp\n",
        "import tensorflow_probability.substrates.jax.distributions as tfd\n",
        "from tensorflow_probability.python.internal import reparameterization\n",
        "MVN = tfd.MultivariateNormalFullCovariance\n",
        "\n",
        "# Dynamax (central to our implementation)\n",
        "from dynamax.linear_gaussian_ssm.inference import make_lgssm_params, lgssm_smoother\n",
        "# from dynamax.linear_gaussian_ssm.parallel_inference import lgssm_smoother as parallel_lgssm_smoother\n",
        "from dynamax.utils.utils import psd_solve\n",
        "\n",
        "# Common math functions\n",
        "from flax.linen import softplus, sigmoid\n",
        "from jax.scipy.special import logsumexp\n",
        "from jax.scipy.linalg import solve_triangular\n",
        "from jax.numpy.linalg import eigh, cholesky, svd, inv, solve\n",
        "\n",
        "# For typing in neural network utils\n",
        "from typing import (NamedTuple, Any, Callable, Sequence, Iterable, List, Optional, Tuple,\n",
        "                    Set, Type, Union, TypeVar, Generic, Dict)\n",
        "\n",
        "# For making the pendulum dataset\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "\n",
        "# For making nice visualizations\n",
        "from sklearn.decomposition import PCA\n",
        "from IPython.display import clear_output, HTML\n",
        "from matplotlib import animation, rc\n",
        "import seaborn as sns\n",
        "color_names = [\"windows blue\",\n",
        "                \"red\",\n",
        "                \"amber\",\n",
        "                \"faded green\",\n",
        "                \"dusty purple\",\n",
        "                \"orange\",\n",
        "                \"clay\",\n",
        "                \"pink\",\n",
        "                \"greyish\",\n",
        "                \"mint\",\n",
        "                \"light cyan\",\n",
        "                \"steel blue\",\n",
        "                \"forest green\",\n",
        "                \"pastel purple\",\n",
        "                \"salmon\",\n",
        "                \"dark brown\",\n",
        "               \"violet\",\n",
        "               \"mauve\",\n",
        "               \"ocean\",\n",
        "               \"ugly yellow\"]\n",
        "colors = sns.xkcd_palette(color_names)\n",
        "\n",
        "# Get rid of the check types warning\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "class CheckTypesFilter(logging.Filter):\n",
        "    def filter(self, record):\n",
        "        return \"check_types\" not in record.getMessage()\n",
        "logger.addFilter(CheckTypesFilter())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "cellView": "form",
        "id": "2MjuUxHxR5O0"
      },
      "outputs": [],
      "source": [
        "# @title Misc helpers\n",
        "def get_value(x):\n",
        "    try:\n",
        "        return x.val.val.primal\n",
        "    except:\n",
        "        try:\n",
        "            return x.val.val\n",
        "        except:\n",
        "            try:\n",
        "                return x.val\n",
        "            except:\n",
        "                return x  # Oh well.\n",
        "\n",
        "def plot_img_grid(recon):\n",
        "    plt.figure(figsize=(8,8))\n",
        "    # Show the sequence as a block of images\n",
        "    stacked = recon.reshape(10, 24 * 10, 24)\n",
        "    imgrid = stacked.swapaxes(0, 1).reshape(24 * 10, 24 * 10)\n",
        "    plt.imshow(imgrid, vmin=0, vmax=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "FGleKPUALeEd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Math helpers\n",
        "def softplus(x):\n",
        "    return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0)\n",
        "\n",
        "def inv_softplus(x, eps=1e-4):\n",
        "    return np.log(np.exp(x - eps) - 1)\n",
        "\n",
        "def vectorize_pytree(*args):\n",
        "    \"\"\"\n",
        "    Flatten an arbitrary PyTree into a vector.\n",
        "    :param args:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    flat_tree, _ = jax.tree_util.tree_flatten(args)\n",
        "    flat_vs = [x.flatten() for x in flat_tree]\n",
        "    return np.concatenate(flat_vs, axis=0)\n",
        "\n",
        "# converts an (n(n+1)/2,) vector of Lie parameters\n",
        "# to an (n, n) matrix\n",
        "def lie_params_to_constrained(out_flat, dim, eps=1e-4):\n",
        "    D, A = out_flat[:dim], out_flat[dim:]\n",
        "    # ATTENTION: we changed this!\n",
        "    # D = np.maximum(softplus(D), eps)\n",
        "    D = softplus(D) + eps\n",
        "    # Build a skew-symmetric matrix\n",
        "    S = np.zeros((dim, dim))\n",
        "    i1, i2 = np.tril_indices(dim - 1)\n",
        "    S = S.at[i1+1, i2].set(A)\n",
        "    S = S.T\n",
        "    S = S.at[i1+1, i2].set(-A)\n",
        "\n",
        "    O = scipy.linalg.expm(S)\n",
        "    J = O.T @ np.diag(D) @ O\n",
        "    return J\n",
        "\n",
        "# converts an (n, n) matrix \n",
        "# to an (n, n) matrix with singular values in (0, 1)\n",
        "def get_constrained_dynamics(A):\n",
        "    dim = A.shape[0]\n",
        "    diag = np.diag(A)\n",
        "    diag = sigmoid(diag)\n",
        "    # Build a skew-symmetric matrix\n",
        "    S = np.zeros((dim, dim))\n",
        "    i1, i2 = np.tril_indices(dim - 1)\n",
        "    S = S.at[i1+1, i2].set(A[i1+1, i2])\n",
        "    S = S.T\n",
        "    S = S.at[i1+1, i2].set(-A[i1+1, i2])\n",
        "    U = scipy.linalg.expm(S)\n",
        "\n",
        "    S = np.zeros((dim, dim))\n",
        "    i1, i2 = np.tril_indices(dim - 1)\n",
        "    S = S.at[i1+1, i2].set(A.T[i1+1, i2])\n",
        "    S = S.T\n",
        "    S = S.at[i1+1, i2].set(-A.T[i1+1, i2])\n",
        "    V = scipy.linalg.expm(S)\n",
        "\n",
        "    A = U @ np.diag(diag) @ V\n",
        "    return A, U, V\n",
        "\n",
        "def scale_singular_values(A):\n",
        "    _, s, _ = svd(A)\n",
        "    return A / (np.maximum(1, np.max(s)))\n",
        "\n",
        "def truncate_singular_values(A):\n",
        "    eps = 1e-3\n",
        "    u, s, vt = svd(A)\n",
        "    return u @ np.diag(np.clip(s, eps, 1)) @ vt\n",
        "\n",
        "# Assume that h has a batch shape here\n",
        "def sample_info_gaussian(seed, J, h):\n",
        "    # Avoid inversion.\n",
        "    # see https://github.com/mattjj/pybasicbayes/blob/master/pybasicbayes/util/stats.py#L117-L122\n",
        "    L = np.linalg.cholesky(J)\n",
        "    x = jr.normal(key=seed, shape=h.shape)\n",
        "    return solve_triangular(L,x.T,lower=True,trans='T').T \\\n",
        "        + np.linalg.solve(J,h.T).T\n",
        "\n",
        "def sample_info_gaussian_old(seed, J, h):\n",
        "    cov = np.linalg.inv(J)\n",
        "    loc = np.einsum(\"...ij,...j->...i\", cov, h)\n",
        "    return tfp.distributions.MultivariateNormalFullCovariance(\n",
        "        loc=loc, covariance_matrix=cov).sample(sample_shape=(), seed=seed)\n",
        "\n",
        "def random_rotation(seed, n, theta=None):\n",
        "    key1, key2 = jr.split(seed)\n",
        "\n",
        "    if theta is None:\n",
        "        # Sample a random, slow rotation\n",
        "        theta = 0.5 * np.pi * jr.uniform(key1)\n",
        "\n",
        "    if n == 1:\n",
        "        return jr.uniform(key1) * np.eye(1)\n",
        "\n",
        "    rot = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
        "    out = np.eye(n)\n",
        "    out = out.at[:2, :2].set(rot)\n",
        "    q = np.linalg.qr(jr.uniform(key2, shape=(n, n)))[0]\n",
        "    return q.dot(out).dot(q.T)\n",
        "\n",
        "# Computes ATQ-1A in a way that's guaranteed to be symmetric\n",
        "def inv_quad_form(Q, A):\n",
        "    sqrt_Q = np.linalg.cholesky(Q)\n",
        "    trm = solve_triangular(sqrt_Q, A, lower=True, check_finite=False)\n",
        "    return trm.T @ trm\n",
        "\n",
        "def inv_symmetric(Q):\n",
        "    sqrt_Q = np.linalg.cholesky(Q)\n",
        "    sqrt_Q_inv = np.linalg.inv(sqrt_Q)\n",
        "    return sqrt_Q_inv.T @ sqrt_Q_inv\n",
        "\n",
        "# Converts from (A, b, Q) to (J, L, h)\n",
        "def dynamics_to_tridiag(dynamics_params, T, D):\n",
        "    Q1, m1, A, Q, b = dynamics_params[\"Q1\"], \\\n",
        "        dynamics_params[\"m1\"], dynamics_params[\"A\"], \\\n",
        "        dynamics_params[\"Q\"], dynamics_params[\"b\"]\n",
        "    # diagonal blocks of precision matrix\n",
        "    J = np.zeros((T, D, D))\n",
        "    J = J.at[0].add(inv_symmetric(Q1))\n",
        "\n",
        "    J = J.at[:-1].add(inv_quad_form(Q, A))\n",
        "    J = J.at[1:].add(inv_symmetric(Q))\n",
        "    # lower diagonal blocks of precision matrix\n",
        "    L = -np.linalg.solve(Q, A)\n",
        "    L = np.tile(L[None, :, :], (T - 1, 1, 1))\n",
        "    # linear potential\n",
        "    h = np.zeros((T, D)) \n",
        "    h = h.at[0].add(np.linalg.solve(Q1, m1))\n",
        "    h = h.at[:-1].add(-np.dot(A.T, np.linalg.solve(Q, b)))\n",
        "    h = h.at[1:].add(np.linalg.solve(Q, b))\n",
        "    return { \"J\": J, \"L\": L, \"h\": h }\n",
        "\n",
        "# Helper function: solve a linear regression given expected sufficient statistics\n",
        "def fit_linear_regression(Ex, Ey, ExxT, EyxT, EyyT, En):\n",
        "    big_ExxT = np.row_stack([np.column_stack([ExxT, Ex]),\n",
        "                            np.concatenate( [Ex.T, np.array([En])])])\n",
        "    big_EyxT = np.column_stack([EyxT, Ey])\n",
        "    Cd = np.linalg.solve(big_ExxT, big_EyxT.T).T\n",
        "    C, d = Cd[:, :-1], Cd[:, -1]\n",
        "    R = (EyyT - 2 * Cd @ big_EyxT.T + Cd @ big_ExxT @ Cd.T) / En\n",
        "\n",
        "    # Manually symmetrize R\n",
        "    R = (R + R.T) / 2\n",
        "    return C, d, R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "cellView": "form",
        "id": "NeeYgySA0hPT"
      },
      "outputs": [],
      "source": [
        "# @title Experiment scheduler\n",
        "LINE_SEP = \"#\" * 42\n",
        "\n",
        "def dict_len(d):\n",
        "    if (type(d) == list):\n",
        "        return len(d)\n",
        "    else:\n",
        "        return dict_len(d[list(d.keys())[0]])\n",
        "\n",
        "def dict_map(d, func):\n",
        "    if type(d) == list:\n",
        "        return func(d)\n",
        "    elif type(d) == dict:\n",
        "        r = copy.deepcopy(d)\n",
        "        for key in d.keys():\n",
        "            r[key] = dict_map(r[key], func)\n",
        "            # Ignore all the Nones\n",
        "            if r[key] is None:\n",
        "                r.pop(key)\n",
        "        if len(r.keys()) == 0:\n",
        "            # There's no content\n",
        "            return None\n",
        "        else:\n",
        "            return r\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def dict_product(d1, d2):\n",
        "    l1, l2 = dict_len(d1), dict_len(d2)\n",
        "    def expand_list(d):\n",
        "        result = []\n",
        "        for item in d:\n",
        "            result.append(item)\n",
        "            result.extend([None] * (l2-1))\n",
        "        return result\n",
        "    def multiply_list(d):\n",
        "        return d * l1\n",
        "    result = dict_map(d1, expand_list)\n",
        "    additions = dict_map(d2, multiply_list)\n",
        "    return dict_update(result, additions)\n",
        "\n",
        "def dict_get(d, id):\n",
        "    return dict_map(d, lambda l: l[id])\n",
        "\n",
        "def dict_update(d, u):\n",
        "    if d is None:\n",
        "        d = dict()\n",
        "    for key in u.keys():\n",
        "        if type(u[key]) == dict:\n",
        "            d.update({\n",
        "                key: dict_update(d.get(key), u[key])\n",
        "            })\n",
        "        else:\n",
        "            d.update({key: u[key]})\n",
        "    return d\n",
        "\n",
        "# A standardized function that structures and schedules experiments\n",
        "# Can chain multiple variations of experiment parameters together\n",
        "def experiment_scheduler(run_params, dataset_getter, model_getter, train_func, \n",
        "                         logger_func=None, err_logger_func=None, \n",
        "                         run_variations=None, params_expander=None,\n",
        "                         on_error=None, continue_on_error=True, use_wandb=True):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        run_params: dict{\"dataset_params\"} \n",
        "            A large dictionary containing all relevant parameters to the run \n",
        "        dataset_getter: run_params -> dict{\"train_data\", [\"generative_model\"]}\n",
        "            A function that loads/samples a dataset\n",
        "        model_getter: run_params, data_dict -> model\n",
        "            A function that creates a model given parameters. Note that the model\n",
        "            could depend on the specifics of the dataset/generative model as well\n",
        "        train_func: model, data, run_params -> results\n",
        "            A function that contains the training loop. \n",
        "            TODO: later we might wanna open up this pipeline and customize further!\n",
        "        (optional) logger_func: results, run_params -> ()\n",
        "            A function that logs the current run.\n",
        "        (optional) err_logger_func: message, run_params -> ()\n",
        "            A function that is called when the run fails.\n",
        "        (optional) run_variations: dict{}\n",
        "            A nested dictionary where the leaves are lists of different parameters.\n",
        "            None means no change from parameters of the last run.\n",
        "        (optional) params_expander: dict{} -> dict{}\n",
        "            Turns high level parameters into specific low level parameters.\n",
        "    returns:\n",
        "        all_results: List<result>\n",
        "            A list containing results from all runs. Failed runs are indicated\n",
        "            with a None value.\n",
        "    \"\"\"\n",
        "    params_expander = params_expander or (lambda d: d)\n",
        "\n",
        "    num_runs = dict_len(run_variations) if run_variations else 1\n",
        "    params = copy.deepcopy(run_params)\n",
        "    print(\"Total number of runs: {}\".format(num_runs))\n",
        "    print(\"Base paramerters:\")\n",
        "    pprint(params)\n",
        "\n",
        "    global data_dict\n",
        "    all_results = []\n",
        "    all_models = []\n",
        "\n",
        "    def _single_run(data_out, model_out):\n",
        "        print(\"Loading dataset!\")\n",
        "        data_dict = dataset_getter(curr_params)\n",
        "        data_out.append(data_dict)\n",
        "        # Make a new model\n",
        "        model_dict = model_getter(curr_params, data_dict)\n",
        "        model_out.append(model_dict)\n",
        "        all_models.append(model_dict)\n",
        "        results = train_func(model_dict, data_dict, curr_params)\n",
        "        all_results.append(results)\n",
        "        if logger_func:\n",
        "            logger_func(results, curr_params, data_dict)\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        print(LINE_SEP)\n",
        "        print(\"Starting run #{}\".format(run))\n",
        "        print(LINE_SEP)\n",
        "        curr_variation = dict_get(run_variations, run)\n",
        "        if curr_variation is None:\n",
        "            if (run != 0):\n",
        "                print(\"Variation #{} is a duplicate, skipping run.\".format(run))\n",
        "                continue\n",
        "            curr_params = params_expander(params)\n",
        "        else:\n",
        "            print(\"Current parameter variation:\")\n",
        "            pprint(curr_variation)\n",
        "            curr_params = dict_update(params, curr_variation)\n",
        "            curr_params = params_expander(curr_params)\n",
        "            print(\"Current full parameters:\")\n",
        "            pprint(curr_params)\n",
        "            if curr_variation.get(\"dataset_params\"):\n",
        "                reload_data = True\n",
        "        # Hack to get the values even when they err out\n",
        "        data_out = []\n",
        "        model_out = []\n",
        "        if not continue_on_error:\n",
        "            _single_run(data_out, model_out)\n",
        "        else:\n",
        "            try:\n",
        "                _single_run(data_out, model_out)\n",
        "                if use_wandb: wandb.finish()\n",
        "            except:\n",
        "                all_results.append(None)\n",
        "                if (on_error): \n",
        "                    try:\n",
        "                        on_error(data_out[0], model_out[0])\n",
        "                    except:\n",
        "                        pass # Oh well...\n",
        "                print(\"Run errored out due to some the following reason:\")\n",
        "                traceback.print_exc()\n",
        "                if use_wandb: wandb.finish(exit_code=1)\n",
        "    return all_results, all_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OPHhcbZObuu"
      },
      "source": [
        "## Define the base SVAE object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "9Pj8Xn9jNBz1"
      },
      "outputs": [],
      "source": [
        "class SVAE:\n",
        "    def __init__(self,\n",
        "                 recognition=None, decoder=None, prior=None, posterior=None,\n",
        "                 input_dummy=None, latent_dummy=None):\n",
        "        \"\"\"\n",
        "        rec_net, dec_net, prior are all objects that take in parameters\n",
        "        rec_net.apply(params, data) returns Gaussian potentials (parameters)\n",
        "        dec_net.apply(params, latents) returns probability distributions\n",
        "        prior : SVAEPrior\n",
        "        \"\"\"\n",
        "        self.recognition = recognition\n",
        "        self.decoder = decoder\n",
        "        self.prior = prior\n",
        "        self.posterior = posterior\n",
        "        self.input_dummy = input_dummy\n",
        "        self.latent_dummy = latent_dummy\n",
        "\n",
        "    def init(self, key=None):\n",
        "        if key is None:\n",
        "            key = jr.PRNGKey(0)\n",
        "\n",
        "        rec_key, dec_key, prior_key, post_key = jr.split(key, 4)\n",
        "\n",
        "        return {\n",
        "            \"rec_params\": self.recognition.init(rec_key, self.input_dummy),\n",
        "            \"dec_params\": self.decoder.init(dec_key, self.latent_dummy),\n",
        "            \"prior_params\": self.prior.init(prior_key),\n",
        "            \"post_params\": self.posterior.init(post_key)\n",
        "        }\n",
        "\n",
        "    def kl_posterior_prior(self, posterior_params, prior_params, \n",
        "                           samples=None):\n",
        "        posterior = self.posterior.distribution(posterior_params)\n",
        "        prior = self.prior.distribution(prior_params)\n",
        "        if samples is None:\n",
        "            return posterior.kl_divergence(prior)\n",
        "        else:\n",
        "            return np.mean(posterior.log_prob(samples) - prior.log_prob(samples))\n",
        "\n",
        "    def elbo(self, key, data, model_params, sample_kl=False, **params):\n",
        "        rec_params = model_params[\"rec_params\"]\n",
        "        dec_params = model_params[\"dec_params\"]\n",
        "        prior_params = self.prior.get_constrained_params(model_params[\"prior_params\"])\n",
        "\n",
        "        # Mask out a large window of states\n",
        "        mask_size = params.get(\"mask_size\")\n",
        "        T = data.shape[0]\n",
        "        mask = onp.ones((T,))\n",
        "        key, dropout_key = jr.split(key)\n",
        "        if mask_size:\n",
        "            # Potential dropout...!\n",
        "            # Use a trick to generate the mask without indexing with a tracer\n",
        "            start_id = jr.choice(dropout_key, T - mask_size + 1)\n",
        "            mask = np.array(np.arange(T) >= start_id) \\\n",
        "                 * np.array(np.arange(T) < start_id + mask_size)\n",
        "            mask = 1 - mask\n",
        "            if params.get(\"mask_type\") == \"potential\":\n",
        "                # This only works with svaes\n",
        "                potential = self.recognition.apply(rec_params, data)\n",
        "                potential = tree_map(\n",
        "                    lambda t: np.einsum(\"i,i...->i...\", mask[:t.shape[0]], t), potential)\n",
        "            else:\n",
        "                potential = self.recognition.apply(rec_params, \n",
        "                                                   np.einsum(\"t...,t->t...\", data, mask))\n",
        "        else:\n",
        "            # Don't do any masking\n",
        "            potential = self.recognition.apply(rec_params, data)\n",
        "\n",
        "        # Update: it makes more sense that inference is done in the posterior object\n",
        "        posterior_params = self.posterior.infer(prior_params, potential)\n",
        "        \n",
        "        # Take samples under the posterior\n",
        "        num_samples = params.get(\"obj_samples\") or 1\n",
        "        samples = self.posterior.sample(posterior_params, (num_samples,), key)\n",
        "        # and compute average ll\n",
        "\n",
        "        def likelihood_outputs(latent):\n",
        "            likelihood_dist = self.decoder.apply(dec_params, latent)\n",
        "            return likelihood_dist.mean(), likelihood_dist.log_prob(data)\n",
        "\n",
        "        mean, ells = vmap(likelihood_outputs)(samples)\n",
        "        # Take average over samples then sum the rest\n",
        "        ell = np.sum(np.mean(ells, axis=0))\n",
        "        # Compute kl from posterior to prior\n",
        "        if sample_kl:\n",
        "            kl = self.kl_posterior_prior(posterior_params, prior_params, \n",
        "                                         samples=samples)\n",
        "        else:\n",
        "            kl = self.kl_posterior_prior(posterior_params, prior_params)\n",
        "\n",
        "        elbo = ell - kl\n",
        "\n",
        "        return {\n",
        "            \"elbo\": elbo,\n",
        "            \"ell\": ell,\n",
        "            \"kl\": kl,\n",
        "            \"posterior_params\": posterior_params,\n",
        "            \"posterior_samples\": samples,\n",
        "            \"reconstruction\": mean,\n",
        "            \"mask\": mask\n",
        "        }\n",
        "\n",
        "    def compute_objective(self, key, data, model_params, **params):\n",
        "        results = self.elbo(key, data, model_params, **params)\n",
        "        results[\"objective\"] = results[\"elbo\"]\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "7ckaLRUL1QVb"
      },
      "outputs": [],
      "source": [
        "# @title The DeepLDS object (implements custom kl function)\n",
        "class DeepLDS(SVAE):\n",
        "    def kl_posterior_prior(self, posterior_params, prior_params, \n",
        "                           samples=None):\n",
        "        posterior = self.posterior.distribution(posterior_params)\n",
        "        prior = self.prior.distribution(prior_params)\n",
        "        if samples is None:\n",
        "            Ex = posterior.expected_states\n",
        "            ExxT = posterior.expected_states_squared\n",
        "            ExnxT = posterior.expected_states_next_states\n",
        "            Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "            Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "            J, L = prior_params[\"J\"], prior_params[\"L\"]\n",
        "\n",
        "            cross_entropy = -prior.log_prob(Ex)\n",
        "            cross_entropy += 0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "            cross_entropy += np.einsum(\"tij,tij->\", L, Sigmatnt)\n",
        "            return cross_entropy - posterior.entropy()\n",
        "            \n",
        "        else:\n",
        "            return np.mean(posterior.log_prob(samples) - prior.log_prob(samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "Nre5PK2MPt2N",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title SVAE Prior object\n",
        "class SVAEPrior:\n",
        "    def init(self, key):\n",
        "        \"\"\"\n",
        "        Returns the initial prior parameters.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def distribution(self, prior_params):\n",
        "        \"\"\"\n",
        "        Returns a tfp distribution object\n",
        "        Takes constrained params\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def m_step(self, prior_params, posterior, post_params):\n",
        "        \"\"\"\n",
        "        Returns updated prior parameters.\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def sample(self, params, shape, key):\n",
        "        return self.distribution(\n",
        "            self.get_constrained_params(params)).sample(sample_shape=shape, seed=key)\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        return deepcopy(params)\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information form (deprecated)"
      ],
      "metadata": {
        "id": "pOPpeMzjj42H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title MVN tridiag object (taken from ssm)\n",
        "from tensorflow_probability.python.internal import reparameterization\n",
        "\n",
        "def block_tridiag_mvn_log_normalizer(J_diag, J_lower_diag, h):\n",
        "    \"\"\" TODO\n",
        "    \"\"\"\n",
        "    # extract dimensions\n",
        "    num_timesteps, dim = J_diag.shape[:2]\n",
        "\n",
        "    # Pad the L's with one extra set of zeros for the last predict step\n",
        "    J_lower_diag_pad = np.concatenate((J_lower_diag, np.zeros((1, dim, dim))), axis=0)\n",
        "\n",
        "    def marginalize(carry, t):\n",
        "        Jp, hp, lp = carry\n",
        "\n",
        "        # Condition\n",
        "        Jc = J_diag[t] + Jp\n",
        "        hc = h[t] + hp\n",
        "\n",
        "        # Predict -- Cholesky approach seems unstable!\n",
        "        # sqrt_Jc = np.linalg.cholesky(Jc)\n",
        "        # trm1 = solve_triangular(sqrt_Jc, hc, lower=True)\n",
        "        # trm2 = solve_triangular(sqrt_Jc, J_lower_diag_pad[t].T, lower=True)\n",
        "        # log_Z = 0.5 * dim * np.log(2 * np.pi)\n",
        "        # log_Z += -np.sum(np.log(np.diag(sqrt_Jc)))  # sum these terms only to get approx log|J|\n",
        "        # log_Z += 0.5 * np.dot(trm1.T, trm1)\n",
        "        # Jp = -np.dot(trm2.T, trm2)\n",
        "        # hp = -np.dot(trm2.T, trm1)\n",
        "\n",
        "        # Alternative predict step:\n",
        "        log_Z = 0.5 * dim * np.log(2 * np.pi)\n",
        "        log_Z += -0.5 * np.linalg.slogdet(Jc)[1]\n",
        "        log_Z += 0.5 * np.dot(hc, np.linalg.solve(Jc, hc))\n",
        "        Jp = -np.dot(J_lower_diag_pad[t], np.linalg.solve(Jc, J_lower_diag_pad[t].T))\n",
        "        # Jp = (Jp + Jp.T) * .5   # Manual symmetrization\n",
        "        hp = -np.dot(J_lower_diag_pad[t], np.linalg.solve(Jc, hc))\n",
        "\n",
        "        new_carry = Jp, hp, lp + log_Z\n",
        "        return new_carry, (Jc, hc)\n",
        "\n",
        "    # Initialize\n",
        "    Jp0 = np.zeros((dim, dim))\n",
        "    hp0 = np.zeros((dim,))\n",
        "    (_, _, log_Z), (filtered_Js, filtered_hs) = lax.scan(marginalize, (Jp0, hp0, 0), np.arange(num_timesteps))\n",
        "    return log_Z, (filtered_Js, filtered_hs)\n",
        "\n",
        "class MultivariateNormalBlockTridiag(tfd.Distribution):\n",
        "    \"\"\"\n",
        "    The Gaussian linear dynamical system's posterior distribution over latent states\n",
        "    is a multivariate normal distribution whose _precision_ matrix is\n",
        "    block tridiagonal.\n",
        "\n",
        "        x | y ~ N(\\mu, \\Sigma)\n",
        "\n",
        "    where\n",
        "\n",
        "        \\Sigma^{-1} = J = [[J_{0,0},   J_{0,1},   0,       0,      0],\n",
        "                           [J_{1,0},   J_{1,1},   J_{1,2}, 0,      0],\n",
        "                           [0,         J_{2,1},   J_{2,2}, \\ddots, 0],\n",
        "                           [0,         0,         \\ddots,  \\ddots,  ],\n",
        "\n",
        "    is block tridiagonal, and J_{t, t+1} = J_{t+1, t}^T.\n",
        "\n",
        "    The pdf is\n",
        "\n",
        "        p(x) = exp \\{-1/2 x^T J x + x^T h - \\log Z(J, h) \\}\n",
        "             = exp \\{- 1/2 \\sum_{t=1}^T x_t^T J_{t,t} x_t\n",
        "                     - \\sum_{t=1}^{T-1} x_{t+1}^T J_{t+1,t} x_t\n",
        "                     + \\sum_{t=1}^T x_t^T h_t\n",
        "                     -\\log Z(J, h)\\}\n",
        "\n",
        "    where J = \\Sigma^{-1} and h = \\Sigma^{-1} \\mu = J \\mu.\n",
        "\n",
        "    Using exponential family tricks we know that\n",
        "\n",
        "        E[x_t] = \\grad_{h_t} \\log Z(J, h)\n",
        "        E[x_t x_t^T] = -2 \\grad_{J_{t,t}} \\log Z(J, h)\n",
        "        E[x_{t+1} x_t^T] = -\\grad_{J_{t+1,t}} \\log Z(J, h)\n",
        "\n",
        "    These are the expectations we need for EM.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 precision_diag_blocks,\n",
        "                 precision_lower_diag_blocks,\n",
        "                 linear_potential,\n",
        "                 log_normalizer,\n",
        "                 filtered_precisions,\n",
        "                 filtered_linear_potentials,\n",
        "                 expected_states,\n",
        "                 expected_states_squared,\n",
        "                 expected_states_next_states,\n",
        "                 validate_args=False,\n",
        "                 allow_nan_stats=True,\n",
        "                 name=\"MultivariateNormalBlockTridiag\",\n",
        "             ) -> None:\n",
        "\n",
        "        self._precision_diag_blocks = precision_diag_blocks\n",
        "        self._precision_lower_diag_blocks = precision_lower_diag_blocks\n",
        "        self._linear_potential = linear_potential\n",
        "        self._log_normalizer = log_normalizer\n",
        "        self._filtered_precisions = filtered_precisions\n",
        "        self._filtered_linear_potentials = filtered_linear_potentials\n",
        "        self._expected_states = expected_states\n",
        "        self._expected_states_squared = expected_states_squared\n",
        "        self._expected_states_next_states = expected_states_next_states\n",
        "\n",
        "        # We would detect the dtype dynamically but that would break vmap\n",
        "        # see https://github.com/tensorflow/probability/issues/1271\n",
        "        dtype = np.float32\n",
        "        super(MultivariateNormalBlockTridiag, self).__init__(\n",
        "            dtype=dtype,\n",
        "            validate_args=validate_args,\n",
        "            allow_nan_stats=allow_nan_stats,\n",
        "            reparameterization_type=reparameterization.NOT_REPARAMETERIZED,\n",
        "            parameters=dict(precision_diag_blocks=self._precision_diag_blocks,\n",
        "                            precision_lower_diag_blocks=self._precision_lower_diag_blocks,\n",
        "                            linear_potential=self._linear_potential,\n",
        "                            log_normalizer=self._log_normalizer,\n",
        "                            filtered_precisions=self._filtered_precisions,\n",
        "                            filtered_linear_potentials=self._filtered_linear_potentials,\n",
        "                            expected_states=self._expected_states,\n",
        "                            expected_states_squared=self._expected_states_squared,\n",
        "                            expected_states_next_states=self._expected_states_next_states),\n",
        "            name=name,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _parameter_properties(cls, dtype, num_classes=None):\n",
        "        # pylint: disable=g-long-lambda\n",
        "        return dict(\n",
        "            precision_diag_blocks=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            precision_lower_diag_blocks=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            linear_potential=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "            log_normalizer=tfp.internal.parameter_properties.ParameterProperties(event_ndims=0),\n",
        "            filtered_precisions=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            filtered_linear_potentials=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "            expected_states=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "            expected_states_squared=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            expected_states_next_states=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def infer(cls,\n",
        "              precision_diag_blocks,\n",
        "              precision_lower_diag_blocks,\n",
        "              linear_potential):\n",
        "        assert precision_diag_blocks.ndim == 3\n",
        "        num_timesteps, dim = precision_diag_blocks.shape[:2]\n",
        "        assert precision_diag_blocks.shape[2] == dim\n",
        "        assert precision_lower_diag_blocks.shape == (num_timesteps - 1, dim, dim)\n",
        "        assert linear_potential.shape == (num_timesteps, dim)\n",
        "\n",
        "        # Run message passing code to get the log normalizer, the filtering potentials,\n",
        "        # and the expected values of x. Technically, the natural parameters are -1/2 J\n",
        "        # so we need to do a little correction of the gradients to get the expectations.\n",
        "        f = value_and_grad(block_tridiag_mvn_log_normalizer, argnums=(0, 1, 2), has_aux=True)\n",
        "        (log_normalizer, (filtered_precisions, filtered_linear_potentials)), grads = \\\n",
        "            f(precision_diag_blocks, precision_lower_diag_blocks, linear_potential)\n",
        "\n",
        "        # Manually symmetrize ExxT due to numerical issues...!!!\n",
        "        # Correct for the -1/2 J -> J implementation\n",
        "        expected_states_squared = - grads[0] - np.swapaxes(grads[0], -2, -1)\n",
        "        expected_states_next_states = -grads[1]\n",
        "        expected_states = grads[2]\n",
        "\n",
        "        return cls(precision_diag_blocks,\n",
        "                   precision_lower_diag_blocks,\n",
        "                   linear_potential,\n",
        "                   log_normalizer,\n",
        "                   filtered_precisions,\n",
        "                   filtered_linear_potentials,\n",
        "                   expected_states,\n",
        "                   expected_states_squared,\n",
        "                   expected_states_next_states)\n",
        "\n",
        "    @classmethod\n",
        "    def infer_from_precision_and_mean(cls,\n",
        "                                      precision_diag_blocks,\n",
        "                                      precision_lower_diag_blocks,\n",
        "                                      mean):\n",
        "        assert precision_diag_blocks.ndim == 3\n",
        "        num_timesteps, dim = precision_diag_blocks.shape[:2]\n",
        "        assert precision_diag_blocks.shape[2] == dim\n",
        "        assert precision_lower_diag_blocks.shape == (num_timesteps - 1, dim, dim)\n",
        "        assert mean.shape == (num_timesteps, dim)\n",
        "\n",
        "        # Convert the mean to the linear potential\n",
        "        linear_potential = np.einsum('tij,tj->ti', precision_diag_blocks, mean)\n",
        "        linear_potential = linear_potential.at[:-1].add(\n",
        "            np.einsum('tji,tj->ti', precision_lower_diag_blocks, mean[1:]))\n",
        "        linear_potential = linear_potential.at[1:].add(\n",
        "            np.einsum('tij,tj->ti', precision_lower_diag_blocks, mean[:-1]))\n",
        "\n",
        "        # Call the constructor above\n",
        "        return cls.infer(precision_diag_blocks,\n",
        "                         precision_lower_diag_blocks,\n",
        "                         linear_potential)\n",
        "\n",
        "    # Properties to get private class variables\n",
        "    @property\n",
        "    def precision_diag_blocks(self):\n",
        "        return self._precision_diag_blocks\n",
        "\n",
        "    @property\n",
        "    def precision_lower_diag_blocks(self):\n",
        "        return self._precision_lower_diag_blocks\n",
        "\n",
        "    @property\n",
        "    def linear_potential(self):\n",
        "        return self._linear_potential\n",
        "\n",
        "    @property\n",
        "    def log_normalizer(self):\n",
        "        return self._log_normalizer\n",
        "\n",
        "    @property\n",
        "    def filtered_precisions(self):\n",
        "        return self._filtered_precisions\n",
        "\n",
        "    @property\n",
        "    def filtered_linear_potentials(self):\n",
        "        return self._filtered_linear_potentials\n",
        "\n",
        "    @property\n",
        "    def filtered_covariances(self):\n",
        "        return inv(self._filtered_precisions)\n",
        "\n",
        "    @property\n",
        "    def filtered_means(self):\n",
        "        # TODO: this is bad numerically\n",
        "        return np.einsum(\"...ij,...j->...i\", self.filtered_covariances, \n",
        "                         self.filtered_linear_potentials)\n",
        "\n",
        "    @property\n",
        "    def expected_states(self):\n",
        "        return self._expected_states\n",
        "\n",
        "    @property\n",
        "    def expected_states_squared(self):\n",
        "        return self._expected_states_squared\n",
        "\n",
        "    @property\n",
        "    def expected_states_next_states(self):\n",
        "        return self._expected_states_next_states\n",
        "\n",
        "    def _log_prob(self, data, **kwargs):\n",
        "        lp = -0.5 * np.einsum('...ti,...tij,...tj->...', data, self._precision_diag_blocks, data)\n",
        "        lp += -np.einsum('...ti,...tij,...tj->...', data[...,1:,:], self._precision_lower_diag_blocks, data[...,:-1,:])\n",
        "        lp += np.einsum('...ti,...ti->...', data, self._linear_potential)\n",
        "        lp -= self.log_normalizer\n",
        "        return lp\n",
        "\n",
        "    def _mean(self):\n",
        "        return self.expected_states\n",
        "\n",
        "    def _covariance(self):\n",
        "        \"\"\"\n",
        "        NOTE: This computes the _marginal_ covariance Cov[x_t] for each t\n",
        "        \"\"\"\n",
        "        Ex = self._expected_states\n",
        "        ExxT = self._expected_states_squared\n",
        "        return ExxT - np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "\n",
        "    def _sample_n(self, n, seed=None):\n",
        "        filtered_Js = self._filtered_precisions\n",
        "        filtered_hs = self._filtered_linear_potentials\n",
        "        J_lower_diag = self._precision_lower_diag_blocks\n",
        "\n",
        "        def sample_single(seed, filtered_Js, filtered_hs, J_lower_diag):\n",
        "\n",
        "            def _sample_info_gaussian(seed, J, h, sample_shape=()):\n",
        "                # TODO: avoid inversion.\n",
        "                # see https://github.com/mattjj/pybasicbayes/blob/master/pybasicbayes/util/stats.py#L117-L122\n",
        "                # L = np.linalg.cholesky(J)\n",
        "                # x = np.random.randn(h.shape[0])\n",
        "                # return scipy.linalg.solve_triangular(L,x,lower=True,trans='T') \\\n",
        "                #     + dpotrs(L,h,lower=True)[0]\n",
        "                cov = np.linalg.inv(J)\n",
        "                loc = np.einsum(\"...ij,...j->...i\", cov, h)\n",
        "                return tfp.distributions.MultivariateNormalFullCovariance(\n",
        "                    loc=loc, covariance_matrix=cov).sample(sample_shape=sample_shape, seed=seed)\n",
        "\n",
        "            def _step(carry, inpt):\n",
        "                x_next, seed = carry\n",
        "                Jf, hf, L = inpt\n",
        "\n",
        "                # Condition on the next observation\n",
        "                Jc = Jf\n",
        "                hc = hf - np.einsum('ni,ij->nj', x_next, L)\n",
        "\n",
        "                # Split the seed\n",
        "                seed, this_seed = jr.split(seed)\n",
        "                x = _sample_info_gaussian(this_seed, Jc, hc)\n",
        "                return (x, seed), x\n",
        "\n",
        "            # Initialize with sample of last timestep and sample in reverse\n",
        "            seed_T, seed = jr.split(seed)\n",
        "            x_T = _sample_info_gaussian(seed_T, filtered_Js[-1], filtered_hs[-1], sample_shape=(n,))\n",
        "            inputs = (filtered_Js[:-1][::-1], filtered_hs[:-1][::-1], J_lower_diag[::-1])\n",
        "            _, x_rev = lax.scan(_step, (x_T, seed), inputs)\n",
        "\n",
        "            # Reverse and concatenate the last time-step's sample\n",
        "            x = np.concatenate((x_rev[::-1], x_T[None, ...]), axis=0)\n",
        "\n",
        "            # Transpose to be (num_samples, num_timesteps, dim)\n",
        "            return np.transpose(x, (1, 0, 2))\n",
        "\n",
        "        # TODO: Handle arbitrary batch shapes\n",
        "        if filtered_Js.ndim == 4:\n",
        "            # batch mode\n",
        "            samples = vmap(sample_single)(seed, filtered_Js, filtered_hs, J_lower_diag)\n",
        "            # Transpose to be (num_samples, num_batches, num_timesteps, dim)\n",
        "            samples = np.transpose(samples, (1, 0, 2, 3))\n",
        "        else:\n",
        "            # non-batch mode\n",
        "            samples = sample_single(seed, filtered_Js, filtered_hs, J_lower_diag)\n",
        "        return samples\n",
        "\n",
        "    def _entropy(self):\n",
        "        \"\"\"\n",
        "        Compute the entropy\n",
        "\n",
        "            H[X] = -E[\\log p(x)]\n",
        "                 = -E[-1/2 x^T J x + x^T h - log Z(J, h)]\n",
        "                 = 1/2 <J, E[x x^T] - <h, E[x]> + log Z(J, h)\n",
        "        \"\"\"\n",
        "        Ex = self._expected_states\n",
        "        ExxT = self._expected_states_squared\n",
        "        ExnxT = self._expected_states_next_states\n",
        "        J_diag = self._precision_diag_blocks\n",
        "        J_lower_diag = self._precision_lower_diag_blocks\n",
        "        h = self._linear_potential\n",
        "\n",
        "        entropy = 0.5 * np.sum(J_diag * ExxT)\n",
        "        entropy += np.sum(J_lower_diag * ExnxT)\n",
        "        entropy -= np.sum(h * Ex)\n",
        "        entropy += self.log_normalizer\n",
        "        return entropy\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        children = (self._precision_diag_blocks,\n",
        "                    self._precision_lower_diag_blocks,\n",
        "                    self._linear_potential,\n",
        "                    self._log_normalizer,\n",
        "                    self._filtered_precisions,\n",
        "                    self._filtered_linear_potentials,\n",
        "                    self._expected_states,\n",
        "                    self._expected_states_squared,\n",
        "                    self._expected_states_next_states)\n",
        "        aux_data = None\n",
        "        return children, aux_data\n",
        "\n",
        "    @classmethod\n",
        "    def tree_unflatten(cls, aux_data, children):\n",
        "        return cls(*children)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NHkdKpcxOi1S"
      },
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Prior object\n",
        "# class InfoLinearGaussianChain(SVAEPrior):\n",
        "\n",
        "#     # We're not using this at the moment\n",
        "#     # COVARIANCE_REGULARIZATION = 1e-3\n",
        "\n",
        "#     def __init__(self, latent_dims, seq_len):\n",
        "#         self.latent_dims = latent_dims\n",
        "#         # The only annoying thing is that we have to specify the sequence length\n",
        "#         # ahead of time\n",
        "#         self.seq_len = seq_len\n",
        "\n",
        "#     def shape(self):\n",
        "#         return (self.seq_len, self.latent_dims)\n",
        "\n",
        "#     # Must be the full set of constrained parameters!\n",
        "#     def distribution(self, params):\n",
        "#         J, L, h = params[\"J\"], params[\"L\"], params[\"h\"]\n",
        "#         log_Z, J_filtered, h_filtered = params[\"log_Z\"], params[\"J_filtered\"], params[\"h_filtered\"]\n",
        "#         Ex, ExxT, ExnxT = params[\"Ex\"], params[\"ExxT\"], params[\"ExnxT\"]\n",
        "#         return MultivariateNormalBlockTridiag(J, L, h, \n",
        "#             log_Z, J_filtered, h_filtered, Ex, ExxT, ExnxT)\n",
        "\n",
        "#     def init(self, key):\n",
        "#         T, D = self.seq_len, self.latent_dims\n",
        "#         key_A, key = jr.split(key, 2)\n",
        "#         params = {\n",
        "#             \"m1\": np.zeros(D),\n",
        "#             \"Q1\": np.eye(D),\n",
        "#             \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "#             \"b\": np.zeros(D),\n",
        "#             \"Q\": np.eye(D)\n",
        "#         }\n",
        "#         constrained = self.get_constrained_params(params)\n",
        "#         params[\"avg_suff_stats\"] = { \"Ex\": constrained[\"Ex\"], \n",
        "#                                     \"ExxT\": constrained[\"ExxT\"], \n",
        "#                                     \"ExnxT\": constrained[\"ExnxT\"] }\n",
        "#         return params\n",
        "\n",
        "#     def get_dynamics_params(self, params):\n",
        "#         return params\n",
        "\n",
        "#     def get_constrained_params(self, params):\n",
        "#         p = dynamics_to_tridiag(params, self.seq_len, self.latent_dims)\n",
        "#         J, L, h = p[\"J\"], p[\"L\"], p[\"h\"]\n",
        "#         dist = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "#         p.update({\n",
        "#             \"log_Z\": dist.log_normalizer,\n",
        "#             \"J_filtered\": dist.filtered_precisions,\n",
        "#             \"h_filtered\": dist.filtered_linear_potentials,\n",
        "#             \"Ex\": dist.expected_states,\n",
        "#             \"ExxT\": dist.expected_states_squared,\n",
        "#             \"ExnxT\": dist.expected_states_next_states\n",
        "#         })\n",
        "#         return p\n",
        "\n",
        "#     # This is pretty much deprecated since we're using sgd\n",
        "#     def m_step(self, prior_params):\n",
        "#         suff_stats = prior_params[\"avg_suff_stats\"]\n",
        "#         ExxT = suff_stats[\"ExxT\"]\n",
        "#         ExnxT = suff_stats[\"ExnxT\"]\n",
        "#         Ex = suff_stats[\"Ex\"]\n",
        "#         seq_len = Ex.shape[0]\n",
        "#         # Update the initials\n",
        "#         m1 = Ex[0]\n",
        "#         Q1 = ExxT[0] - np.outer(m1, m1)\n",
        "#         D = self.latent_dims\n",
        "#         A, b, Q = fit_linear_regression(Ex[:-1].sum(axis=0), \n",
        "#                                         Ex[1:].sum(axis=0), \n",
        "#                                         ExxT[:-1].sum(axis=0), \n",
        "#                                         ExnxT.sum(axis=0), \n",
        "#                                         ExxT[1:].sum(axis=0), \n",
        "#                                         seq_len - 1)\n",
        "#         out = { \"m1\": m1, \"Q1\": Q1, \"A\": A, \"b\": b, \"Q\": Q }\n",
        "#         out[\"avg_suff_stats\"] = deepcopy(suff_stats)\n",
        "#         return out"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aVUzQvdIj70F"
      },
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bss7PlIgQRDp"
      },
      "source": [
        "## Important distributions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Linear Gaussian chain distribution object\n",
        "# As a prior distribution, we only need to be able to 1) Evaluate log prob 2) sample\n",
        "# As a posterior distribution, we also need to figure out the sufficient stats (Ex, ExxT, ExnxT)\n",
        "class LinearGaussianChain:\n",
        "    def __init__(self, dynamics_matrix, dynamics_bias, noise_covariance,\n",
        "                 expected_states, expected_states_squared, expected_states_next_states):\n",
        "        \"\"\"\n",
        "        params: dictionary containing the following keys:\n",
        "            A:  (seq_len, dim, dim)\n",
        "            Q:  (seq_len, dim, dim)\n",
        "            b:  (seq_len, dim)\n",
        "        \"\"\"\n",
        "        self._dynamics_matrix = dynamics_matrix\n",
        "        self._dynamics_bias = dynamics_bias\n",
        "        self._noise_covariance = noise_covariance\n",
        "        self._expected_states = expected_states\n",
        "        self._expected_states_squared = expected_states_squared\n",
        "        self._expected_states_next_states = expected_states_next_states\n",
        "\n",
        "    @classmethod\n",
        "    def from_stationary_dynamics(cls, m1, Q1, A, b, Q, T):\n",
        "        dynamics_matrix = np.tile(A[None], (T, 1, 1))\n",
        "        dynamics_bias = np.concatenate([m1[None], \n",
        "                                        np.tile(b[None], (T-1, 1))])\n",
        "        noise_covariance = np.concatenate([Q1[None], \n",
        "                                           np.tile(Q[None], (T-1, 1, 1))])\n",
        "        return cls.from_nonstationary_dynamics(dynamics_matrix, dynamics_bias, noise_covariance)\n",
        "    \n",
        "    @classmethod\n",
        "    def from_nonstationary_dynamics(cls, dynamics_matrix, dynamics_bias, noise_covariance):\n",
        "        # Compute the means and covariances via parallel scan\n",
        "        init_elems = (dynamics_matrix, dynamics_bias, noise_covariance)\n",
        "\n",
        "        @vmap\n",
        "        def assoc_op(elem1, elem2):\n",
        "            A1, b1, Q1 = elem1\n",
        "            A2, b2, Q2 = elem2\n",
        "            return A2 @ A1, A2 @ b1 + b2, A2 @ Q1 @ A2.T + Q2\n",
        "\n",
        "        _, Ex, covariances = lax.associative_scan(assoc_op, init_elems)\n",
        "        expected_states = Ex\n",
        "        expected_states_squared = covariances + np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "        expected_states_next_states = np.einsum(\"...ij,...jk->...ik\", \n",
        "            covariances[:-1], dynamics_matrix[1:]) + np.einsum(\"...i,...j->...ji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "        return cls(dynamics_matrix, dynamics_bias, noise_covariance,\n",
        "                   expected_states, expected_states_squared, expected_states_next_states)\n",
        "\n",
        "    @property\n",
        "    def mean(self):\n",
        "        return self._expected_states\n",
        "\n",
        "    @property\n",
        "    def covariance(self):\n",
        "        Ex = self._expected_states\n",
        "        ExxT = self._expected_states_squared\n",
        "        return ExxT - np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "        \n",
        "    @property\n",
        "    def expected_states(self):\n",
        "        return self._expected_states\n",
        "\n",
        "    @property\n",
        "    def expected_states_squared(self):\n",
        "        return self._expected_states_squared\n",
        "\n",
        "    @property\n",
        "    def expected_states_next_states(self):\n",
        "        return self._expected_states_next_states\n",
        "\n",
        "    # Works with batched distributions and arguments...!\n",
        "    def log_prob(self, xs):\n",
        "\n",
        "        @partial(np.vectorize, signature=\"(t,d,d),(t,d),(t,d,d),(t,d)->()\")\n",
        "        def log_prob_single(A, b, Q, x):\n",
        "            ll = MVN(loc=b[0], covariance_matrix=Q[0]).log_prob(x[0])\n",
        "            ll += MVN(loc=np.einsum(\"tij,tj->ti\", A[1:], x[:-1]) + b[1:], \n",
        "                      covariance_matrix=Q[1:]).log_prob(x[1:]).sum()\n",
        "            return ll\n",
        "\n",
        "        return log_prob_single(self._dynamics_matrix,\n",
        "                               self._dynamics_bias, \n",
        "                               self._noise_covariance, xs)\n",
        "        \n",
        "    # Only supports 0d and 1d sample shapes\n",
        "    # Does not support sampling with batched object\n",
        "    def sample(self, seed, sample_shape=()):\n",
        "\n",
        "        @partial(np.vectorize, signature=\"(n),(t,d,d),(t,d),(t,d,d)->(t,d)\")\n",
        "        def sample_single(key, A, b, Q):\n",
        "\n",
        "            biases = MVN(loc=b, covariance_matrix=Q).sample(seed=key)\n",
        "            init_elems = (A, biases)\n",
        "\n",
        "            @vmap\n",
        "            def assoc_op(elem1, elem2):\n",
        "                A1, b1 = elem1\n",
        "                A2, b2 = elem2\n",
        "                return A2 @ A1, A2 @ b1 + b2\n",
        "\n",
        "            _, sample = lax.associative_scan(assoc_op, init_elems)\n",
        "            return sample\n",
        "        \n",
        "        if (len(sample_shape) == 0):\n",
        "            return sample_single(seed, self._dynamics_matrix,\n",
        "                                 self._dynamics_bias, \n",
        "                                 self._noise_covariance)\n",
        "        elif (len(sample_shape) == 1):\n",
        "            return sample_single(jr.split(seed, sample_shape[0]),\n",
        "                                 self._dynamics_matrix[None],\n",
        "                                 self._dynamics_bias[None],\n",
        "                                 self._noise_covariance[None]) \n",
        "        else:\n",
        "            raise Exception(\"More than one sample dimensions are not supported!\")\n",
        "\n",
        "    def entropy(self):\n",
        "        \"\"\"\n",
        "        Compute the entropy\n",
        "\n",
        "            H[X] = -E[\\log p(x)]\n",
        "                 = -E[-1/2 x^T J x + x^T h - log Z(J, h)]\n",
        "                 = 1/2 <J, E[x x^T] - <h, E[x]> + log Z(J, h)\n",
        "        \"\"\"\n",
        "        Ex = self.expected_states\n",
        "        ExxT = self.expected_states_squared\n",
        "        ExnxT = self.expected_states_next_states\n",
        "        \n",
        "        dim = Ex.shape[-1]        \n",
        "        Q_inv = solve(self._noise_covariance, np.eye(dim)[None])\n",
        "        A = self._dynamics_matrix\n",
        "\n",
        "        J_lower_diag = np.einsum(\"til,tlj->tij\", -Q_inv[1:], A[1:])\n",
        "        ATQinvA = np.einsum(\"tji,tjl,tlk->tik\", A[1:], Q_inv[1:], A[1:])\n",
        "        J_diag = Q_inv.at[:-1].add(ATQinvA)\n",
        "\n",
        "        Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "        Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "        trm1 = 0.5 * np.sum(J_diag * Sigmatt)\n",
        "        trm2 = np.sum(J_lower_diag * Sigmatnt)\n",
        "\n",
        "        return trm1 + trm2 - self.log_prob(Ex)"
      ],
      "metadata": {
        "id": "KZ0Avt4Ad8G-",
        "cellView": "form"
      },
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Testing the correctness of the linear Gaussian chain\n",
        "# jax.config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "# D = 3\n",
        "# T = 200\n",
        "\n",
        "# mu1 = np.zeros(D)\n",
        "# Q1 = np.eye(D)\n",
        "# # TODO: A could be a sensitive parameter\n",
        "# # A = jnp.linspace(0.001, 1.0, D)\n",
        "# A = random_rotation(key_0, D, np.pi / 20)\n",
        "# b = jr.normal(key_0, shape=(D,))#np.zeros(D)\n",
        "# Q = np.eye(D)\n",
        "\n",
        "# C = np.eye(D)\n",
        "# d = np.zeros(D)\n",
        "\n",
        "# dist = LinearGaussianChain.from_stationary_dynamics(mu1, Q1, A, b, Q, T)\n",
        "# dist.entropy()\n",
        "# # Sigmas = np.tile(0.01 * np.eye(D), (T, 1, 1))\n",
        "\n",
        "# p = dynamics_to_tridiag({\"m1\": mu1, \"Q1\": Q1, \"A\": A, \"b\": b, \"Q\": Q}, T, D)\n",
        "# dist_ = MultivariateNormalBlockTridiag.infer(p[\"J\"], p[\"L\"], p[\"h\"])\n",
        "\n",
        "# ExnxT_ = dist_.expected_states_next_states\n",
        "# ExnxT = dist.expected_states_next_states\n",
        "# print(ExnxT - ExnxT_)\n",
        "# print(dist_.entropy() - dist.entropy())\n",
        "# print(dist.expected_states_squared - dist_.expected_states_squared)\n",
        "# posterior = LinearGaussianChain.from_stationary_dynamics(mu1, Q1, A, b, Q, T)\n",
        "# key_1 = jr.split(key_0)[0]\n",
        "# A1 = random_rotation(key_1, D, np.pi / 20)\n",
        "# b1 = jr.normal(key_1, shape=(D,))#np.zeros(D)\n",
        "# prior_params = dynamics_to_tridiag({\"m1\": mu1, \"Q1\": Q1, \"A\": A1, \"b\": b1, \"Q\": Q}, T, D)\n",
        "# prior = LinearGaussianChain.from_stationary_dynamics(mu1, Q1, A1, b1, Q, T)\n",
        "# Ex = posterior.expected_states\n",
        "# ExxT = posterior.expected_states_squared\n",
        "# ExnxT = posterior.expected_states_next_states\n",
        "# Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "# Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "# J, L = prior_params[\"J\"], prior_params[\"L\"]\n",
        "# cross_entropy = -prior.log_prob(Ex)\n",
        "# cross_entropy += 0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "# cross_entropy += np.einsum(\"tij,tij->\", L, Sigmatnt)\n",
        "# print(\"Closed form KL:\", cross_entropy - posterior.entropy())\n",
        "# samples = posterior.sample(key_1, (100,))\n",
        "# print(\"Sampled KL:\", np.mean(posterior.log_prob(samples) - prior.log_prob(samples)))\n",
        "# # return np.mean(posterior.log_prob(samples) - prior.log_prob(samples))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lBVe4-YTOQLp"
      },
      "execution_count": 344,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "6D7qIBFpRyH8"
      },
      "outputs": [],
      "source": [
        "# @title Linear Gaussian chain prior\n",
        "# This is a linear Gaussian chain\n",
        "class LinearGaussianChainPrior(SVAEPrior):\n",
        "\n",
        "    def __init__(self, latent_dims, seq_len):\n",
        "        self.latent_dims = latent_dims\n",
        "        # The only annoying thing is that we have to specify the sequence length\n",
        "        # ahead of time\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return (self.seq_len, self.latent_dims)\n",
        "\n",
        "    # Must be the full set of constrained parameters!\n",
        "    def distribution(self, params):\n",
        "        As, bs, Qs = params[\"As\"], params[\"bs\"], params[\"Qs\"]\n",
        "        Ex, ExxT, ExnxT = params[\"Ex\"], params[\"ExxT\"], params[\"ExnxT\"]\n",
        "        return LinearGaussianChain(As, bs, Qs, Ex, ExxT, ExnxT)\n",
        "\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        params = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"Q1\": np.eye(D),\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": np.eye(D)\n",
        "        }\n",
        "        constrained = self.get_constrained_params(params)\n",
        "        return params\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        return params\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        tridiag = dynamics_to_tridiag(params, self.seq_len, self.latent_dims)\n",
        "        p.update(tridiag)\n",
        "        dist = LinearGaussianChain.from_stationary_dynamics(p[\"m1\"], p[\"Q1\"], \n",
        "                                         p[\"A\"], p[\"b\"], p[\"Q\"], self.seq_len)\n",
        "        p.update({\n",
        "            \"As\": dist._dynamics_matrix,\n",
        "            \"bs\": dist._dynamics_bias,\n",
        "            \"Qs\": dist._noise_covariance,\n",
        "            \"Ex\": dist.expected_states,\n",
        "            \"ExxT\": dist.expected_states_squared,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    # This is pretty much deprecated since we're using sgd\n",
        "    def m_step(self, prior_params):\n",
        "        suff_stats = prior_params[\"avg_suff_stats\"]\n",
        "        ExxT = suff_stats[\"ExxT\"]\n",
        "        ExnxT = suff_stats[\"ExnxT\"]\n",
        "        Ex = suff_stats[\"Ex\"]\n",
        "        seq_len = Ex.shape[0]\n",
        "        # Update the initials\n",
        "        m1 = Ex[0]\n",
        "        Q1 = ExxT[0] - np.outer(m1, m1)\n",
        "        D = self.latent_dims\n",
        "        A, b, Q = fit_linear_regression(Ex[:-1].sum(axis=0), \n",
        "                                        Ex[1:].sum(axis=0), \n",
        "                                        ExxT[:-1].sum(axis=0), \n",
        "                                        ExnxT.sum(axis=0), \n",
        "                                        ExxT[1:].sum(axis=0), \n",
        "                                        seq_len - 1)\n",
        "        out = { \"m1\": m1, \"Q1\": Q1, \"A\": A, \"b\": b, \"Q\": Q }\n",
        "        out[\"avg_suff_stats\"] = deepcopy(suff_stats)\n",
        "        return out\n",
        "\n",
        "# This is a bit clumsy but it's the best we can do without using some sophisticated way\n",
        "# Of marking the constrained/optimized parameters vs. unconstrained parameters\n",
        "class LieParameterizedLinearGaussianChainPrior(LinearGaussianChainPrior):\n",
        "    def init(self, key):\n",
        "        D = self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        # Equivalent to the unit matrix\n",
        "        Q_flat = np.concatenate([np.ones(D) * inv_softplus(1), np.zeros((D*(D-1)//2))])\n",
        "        params = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"Q1\": Q_flat,\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": Q_flat\n",
        "        }\n",
        "        return params\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        return {\n",
        "            \"m1\": params[\"m1\"],\n",
        "            \"Q1\": lie_params_to_constrained(params[\"Q1\"], self.latent_dims),\n",
        "            \"A\": params[\"A\"],\n",
        "            \"b\": params[\"b\"],\n",
        "            \"Q\": lie_params_to_constrained(params[\"Q\"], self.latent_dims)   \n",
        "        }\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        D = self.latent_dims\n",
        "        p = self.get_dynamics_params(params)\n",
        "        return super().get_constrained_params(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "6c7K9dJWzyG8"
      },
      "outputs": [],
      "source": [
        "# @title Linear Gaussian chain posteriors\n",
        "\n",
        "# The infer function for the DKF version just uses the posterior params \n",
        "class CDKFPosterior(LinearGaussianChainPrior):\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        params = {\n",
        "            \"As\": np.zeros((T, D, D)), \n",
        "            \"bs\": np.zeros((T, D)),\n",
        "            \"Qs\": np.tile(np.eye(D)[None], (T, 1, 1))\n",
        "        }\n",
        "        return self.get_constrained_params(params)\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        dist = LinearGaussianChain.from_nonstationary_dynamics(p[\"As\"], p[\"bs\"], p[\"Qs\"])\n",
        "        p.update({\n",
        "            \"Ex\": dist.expected_states,\n",
        "            \"ExxT\": dist.expected_states_squared,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    def sufficient_statistics(self, params):\n",
        "        return {\n",
        "            \"Ex\": params[\"Ex\"],\n",
        "            \"ExxT\": params[\"ExxT\"],\n",
        "            \"ExnxT\": params[\"ExnxT\"]\n",
        "        }\n",
        "\n",
        "    def infer(self, prior_params, posterior_params):\n",
        "        return self.get_constrained_params(posterior_params)\n",
        "\n",
        "class DKFPosterior(CDKFPosterior):\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        # The DKF produces a factored posterior\n",
        "        # So the dynamics matrix is zeroed out\n",
        "        p[\"As\"] *= 0\n",
        "        dist = LinearGaussianChain.from_nonstationary_dynamics(p[\"As\"], p[\"bs\"], p[\"Qs\"])\n",
        "        p.update({\n",
        "            \"Ex\": dist.expected_states,\n",
        "            \"ExxT\": dist.expected_states_squared,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "cellView": "form",
        "id": "Rbe3PsAIHliY"
      },
      "outputs": [],
      "source": [
        "# @title PlaNet type posterior\n",
        "# The infer function for the DKF version just uses the posterior params \n",
        "# TODO: Put the dummies in the params dictionary as well\n",
        "class PlaNetPosterior(DKFPosterior):\n",
        "    def __init__(self, network_params, latent_dims, seq_len):\n",
        "        super().__init__(latent_dims, seq_len)\n",
        "        self.network = StochasticRNNCell.from_params(**network_params)\n",
        "        self.input_dim = network_params[\"input_dim\"]      # u\n",
        "        self.latent_dim = network_params[\"rnn_dim\"]       # h\n",
        "        self.output_dim = network_params[\"output_dim\"]    # x\n",
        "\n",
        "    def init(self, key):\n",
        "        input_dummy = np.zeros((self.input_dim,))\n",
        "        latent_dummy = np.zeros((self.latent_dim,))\n",
        "        output_dummy = np.zeros((self.output_dim,))\n",
        "        rnn_params = self.network.init(key, input_dummy, latent_dummy, output_dummy)\n",
        "        return {\n",
        "            \"rnn_params\": rnn_params,\n",
        "            \"input_dummy\": input_dummy,\n",
        "            \"latent_dummy\": latent_dummy,\n",
        "            \"output_dummy\": output_dummy,\n",
        "        }\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        # All of the information is stored in the second argument already\n",
        "        return params\n",
        "\n",
        "    def distribution(self, params):\n",
        "        return DeepAutoregressiveDynamics(self.network, params)\n",
        "        \n",
        "    # These are just dummies\n",
        "    def sufficient_statistics(self, params):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        return {\n",
        "            \"Ex\": np.zeros((T, D)),\n",
        "            \"ExxT\": np.zeros((T, D, D)),\n",
        "            \"ExnxT\": np.zeros((T-1, D, D))\n",
        "        }\n",
        "\n",
        "# We only need to be able to 1) Evaluate log prob 2) sample\n",
        "# The tricky thing here is evaluating the \n",
        "class DeepAutoregressiveDynamics:\n",
        "\n",
        "    def __init__(self, network, params):\n",
        "        self.cell = network\n",
        "        self.params = params[\"network_params\"]\n",
        "        self.inputs = params[\"network_input\"]\n",
        "        # self.input_dummy = params[\"network_params\"][\"input_dummy\"]\n",
        "        # self.latent_dummy = params[\"network_params\"][\"latent_dummy\"]\n",
        "        # self.output_dummy = params[\"network_params\"][\"output_dummy\"]\n",
        "        self._mean = None\n",
        "        self._covariance = None\n",
        "\n",
        "    def mean(self):\n",
        "        if (self._mean is None):\n",
        "            self.compute_mean_and_cov()\n",
        "        return self._mean\n",
        "\n",
        "    def covariance(self):\n",
        "        if (self._covariance is None):\n",
        "            self.compute_mean_and_cov()\n",
        "        return self._covariance\n",
        "\n",
        "    @property\n",
        "    def filtered_covariances(self):\n",
        "        return self.covariance()\n",
        "\n",
        "    @property\n",
        "    def filtered_means(self):\n",
        "        return self.mean()\n",
        "        \n",
        "    def compute_mean_and_cov(self):\n",
        "        num_samples = 25\n",
        "        samples = self.sample((num_samples,), key_0)\n",
        "        Ex = np.mean(samples, axis=0)\n",
        "        self._mean = Ex\n",
        "        ExxT = np.einsum(\"s...ti,s...tj->s...tij\", samples, samples).mean(axis=0)\n",
        "        self._covariance = ExxT - np.einsum(\"...ti,...tj->...tij\", Ex, Ex)\n",
        "\n",
        "    # TODO: make this work properly with a batched distribution object\n",
        "    def log_prob(self, xs):\n",
        "        params = self.params\n",
        "        def log_prob_single(x_):\n",
        "            def _log_prob_step(carry, i):\n",
        "                h, prev_x = carry\n",
        "                x, u = x_[i], self.inputs[i]\n",
        "                h, (cov, mean) = self.cell.apply(params[\"rnn_params\"], h, prev_x, u)\n",
        "                pred_dist = tfd.MultivariateNormalFullCovariance(loc=mean, \n",
        "                                                            covariance_matrix=cov)\n",
        "                log_prob = pred_dist.log_prob(x)\n",
        "                carry = h, x\n",
        "                return carry, log_prob\n",
        "            # Assuming these are zero arrays already\n",
        "            init = (params[\"latent_dummy\"], params[\"output_dummy\"])\n",
        "            _, log_probs = scan(_log_prob_step, init, np.arange(x_.shape[0]))\n",
        "            return np.sum(log_probs, axis=0)\n",
        "        return vmap(log_prob_single)(xs)\n",
        "\n",
        "    # TODO: make this work with a batched distribution object\n",
        "    # Only supports rank 0 and 1 sample shapes\n",
        "    # Output: ([num_samples,] [batch_size,] seq_len, event_dim)\n",
        "    def sample(self, sample_shape, seed):\n",
        "        def _sample_single(key, params, inputs):\n",
        "            def _sample_step(carry, u):\n",
        "                key, h, x = carry\n",
        "                key, new_key = jr.split(key)\n",
        "                h, (cov, mean) = self.cell.apply(params[\"rnn_params\"], h, x, u)\n",
        "                sample = jr.multivariate_normal(key, mean, cov)\n",
        "                carry = new_key, h, sample\n",
        "                output = sample\n",
        "                return carry, output\n",
        "\n",
        "            init = (key, params[\"latent_dummy\"], params[\"output_dummy\"])\n",
        "            _, sample = scan(_sample_step, init, inputs)\n",
        "            return sample\n",
        "\n",
        "        if (len(self.inputs.shape) == 2):\n",
        "            if (len(sample_shape) == 0):\n",
        "                return _sample_single(seed, self.params, self.inputs)\n",
        "            else:\n",
        "                assert (len(sample_shape) == 1)\n",
        "                return vmap(_sample_single, in_axes=(0, None, None))(jr.split(seed, sample_shape[0]),\n",
        "                                            self.params, self.inputs)\n",
        "        else:\n",
        "            # This is a batched distribution object\n",
        "            assert(len(self.inputs.shape) == 3)\n",
        "            batch_size = self.inputs.shape[0]\n",
        "            if (len(sample_shape) == 0):\n",
        "                return vmap(_sample_single)(\n",
        "                            jr.split(seed, batch_size), \n",
        "                            self.params,\n",
        "                            self.inputs\n",
        "                        )\n",
        "            else:\n",
        "                assert (len(sample_shape) == 1)\n",
        "                return vmap(\n",
        "                        lambda s:vmap(_sample_single)(\n",
        "                            jr.split(s, batch_size), \n",
        "                            self.params,\n",
        "                            self.inputs\n",
        "                        )\n",
        "                    )(jr.split(seed, sample_shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "id": "qDsz18BcyDcf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title LDS object (might wanna refactor this)\n",
        "\n",
        "# Takes a linear Gaussian chain as its base\n",
        "class LDS(LinearGaussianChainPrior):\n",
        "    def __init__(self, latent_dims, seq_len, base=None, posterior=None):\n",
        "        super().__init__(latent_dims, seq_len)\n",
        "        self.posterior = posterior or LDSSVAEPosterior(latent_dims, seq_len)\n",
        "        self.base = base or LinearGaussianChainPrior(latent_dims, seq_len) # Slightly redundant...\n",
        "\n",
        "    # Takes unconstrained params\n",
        "    def sample(self, params, shape, key):\n",
        "        latents = self.base.sample(params, shape, key)\n",
        "        sample_shape = latents.shape[:-1]\n",
        "        key, _ = jr.split(key)\n",
        "        C, d, R = params[\"C\"], params[\"d\"], params[\"R\"]\n",
        "        obs_noise = tfd.MultivariateNormalFullCovariance(loc=d, covariance_matrix=R)\\\n",
        "            .sample(sample_shape=sample_shape, seed=key)\n",
        "        obs = np.einsum(\"ij,...tj->...ti\", C, latents) + obs_noise\n",
        "        return latents, obs\n",
        "\n",
        "    # Should work with any batch dimension\n",
        "    def log_prob(self, params, states, data):\n",
        "        latent_dist = self.base.distribution(self.base.get_constrained_params(params))\n",
        "        latent_ll = latent_dist.log_prob(states)\n",
        "        C, d, R = params[\"C\"], params[\"d\"], params[\"R\"]\n",
        "        # Gets around batch dimensions\n",
        "        noise = tfd.MultivariateNormalFullCovariance(loc=d, covariance_matrix=R)\n",
        "        obs_ll = noise.log_prob(data - np.einsum(\"ij,...tj->...ti\", C, states))\n",
        "        return latent_ll + obs_ll.sum(axis=-1)\n",
        "\n",
        "    # Assumes single data points\n",
        "    def e_step(self, params, data):\n",
        "        # Shorthand names for parameters\n",
        "        C, d, R = params[\"C\"], params[\"d\"], params[\"R\"]\n",
        "\n",
        "        J = np.dot(C.T, np.linalg.solve(R, C))\n",
        "        J = np.tile(J[None, :, :], (self.seq_len, 1, 1))\n",
        "        # linear potential\n",
        "        h = np.dot(data - d, np.linalg.solve(R, C))\n",
        "\n",
        "        Sigma = solve(J, np.eye(self.latent_dims)[None])\n",
        "        mu = vmap(solve)(J, h)\n",
        "\n",
        "        return self.posterior.infer(self.base.get_constrained_params(params), {\"J\": J, \"h\": h, \n",
        "                                                                    \"mu\": mu, \"Sigma\": Sigma})\n",
        "        \n",
        "    # Also assumes single data points\n",
        "    def marginal_log_likelihood(self, params, data):\n",
        "        posterior = self.posterior.distribution(self.e_step(params, data))\n",
        "        states = posterior.mean()\n",
        "        prior_ll = self.log_prob(params, states, data)\n",
        "        posterior_ll = posterior.log_prob(states)\n",
        "        # This is numerically unstable!\n",
        "        lps = prior_ll - posterior_ll\n",
        "        return lps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGISYI1CN6cv"
      },
      "source": [
        "## Making a mean parameter posterior object"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Parallel Kalman filtering and smoothing\n",
        "\n",
        "def _make_associative_sampling_elements(params, key, filtered_means, filtered_covariances):\n",
        "    \"\"\"Preprocess filtering output to construct input for smoothing assocative scan.\"\"\"\n",
        "\n",
        "    F = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "\n",
        "    def _last_sampling_element(key, m, P):\n",
        "        return np.zeros_like(P), MVN(m, P).sample(seed=key)\n",
        "\n",
        "    def _generic_sampling_element(params, key, m, P):\n",
        "\n",
        "        eps = 1e-3\n",
        "        P += np.eye(dims) * eps\n",
        "        Pp = F @ P @ F.T + Q\n",
        "\n",
        "        FP = F @ P\n",
        "        E  = psd_solve(Pp, FP).T\n",
        "        g  = m - E @ F @ m\n",
        "        L  = P - E @ Pp @ E.T\n",
        "\n",
        "        L = (L + L.T) * .5 + np.eye(dims) * eps # Add eps to the crucial covariance matrix\n",
        "\n",
        "        h = MVN(g, L).sample(seed=key)\n",
        "        return E, h\n",
        "\n",
        "    num_timesteps = len(filtered_means)\n",
        "    dims = filtered_means.shape[-1]\n",
        "    keys = jr.split(key, num_timesteps)\n",
        "    last_elems = _last_sampling_element(keys[-1], filtered_means[-1], \n",
        "                                        filtered_covariances[-1])\n",
        "    generic_elems = vmap(_generic_sampling_element, (None, 0, 0, 0))(\n",
        "        params, keys[:-1], filtered_means[:-1], filtered_covariances[:-1]\n",
        "        )\n",
        "    combined_elems = tuple(np.append(gen_elm, last_elm[None,:], axis=0)\n",
        "                           for gen_elm, last_elm in zip(generic_elems, last_elems))\n",
        "    return combined_elems\n",
        "\n",
        "def _make_associative_filtering_elements(params, potentials):\n",
        "    \"\"\"Preprocess observations to construct input for filtering assocative scan.\"\"\"\n",
        "\n",
        "    F = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "    Q1 = params[\"Q1\"]\n",
        "    P0 = Q1\n",
        "    P1 = Q1\n",
        "    m1 = params[\"m1\"]\n",
        "    dim = Q.shape[0]\n",
        "    H = np.eye(dim)\n",
        "\n",
        "    def _first_filtering_element(params, mu, Sigma):\n",
        "\n",
        "        y, R = mu, Sigma\n",
        "\n",
        "        S = H @ Q @ H.T + R\n",
        "        CF, low = scipy.linalg.cho_factor(S)\n",
        "\n",
        "        S1 = H @ P1 @ H.T + R\n",
        "        K1 = psd_solve(S1, H @ P1).T\n",
        "\n",
        "        A = np.zeros_like(F)\n",
        "        b = m1 + K1 @ (y - H @ m1)\n",
        "        C = P1 - K1 @ S1 @ K1.T\n",
        "        eta = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), y)\n",
        "        J = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), H @ F)\n",
        "\n",
        "        logZ = -MVN(loc=np.zeros_like(y), covariance_matrix=H @ P0 @ H.T + R).log_prob(y)\n",
        "\n",
        "        return A, b, C, J, eta, logZ\n",
        "\n",
        "\n",
        "    def _generic_filtering_element(params, mu, Sigma):\n",
        "\n",
        "        y, R = mu, Sigma\n",
        "\n",
        "        S = H @ Q @ H.T + R\n",
        "        CF, low = scipy.linalg.cho_factor(S)\n",
        "        K = scipy.linalg.cho_solve((CF, low), H @ Q).T\n",
        "        A = F - K @ H @ F\n",
        "        b = K @ y\n",
        "        C = Q - K @ H @ Q\n",
        "\n",
        "        eta = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), y)\n",
        "        J = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), H @ F)\n",
        "\n",
        "        logZ = -MVN(loc=np.zeros_like(y), covariance_matrix=S).log_prob(y)\n",
        "\n",
        "        return A, b, C, J, eta, logZ\n",
        "\n",
        "    mus, Sigmas = potentials[\"mu\"], potentials[\"Sigma\"]\n",
        "\n",
        "    first_elems = _first_filtering_element(params, mus[0], Sigmas[0])\n",
        "    generic_elems = vmap(_generic_filtering_element, (None, 0, 0))(params, mus[1:], Sigmas[1:])\n",
        "    combined_elems = tuple(np.concatenate((first_elm[None,...], gen_elm))\n",
        "                           for first_elm, gen_elm in zip(first_elems, generic_elems))\n",
        "    return combined_elems\n",
        "\n",
        "def lgssm_filter(params, emissions):\n",
        "    \"\"\"A parallel version of the lgssm filtering algorithm.\n",
        "    See S. Särkkä and Á. F. García-Fernández (2021) - https://arxiv.org/abs/1905.13002.\n",
        "    Note: This function does not yet handle `inputs` to the system.\n",
        "    \"\"\"\n",
        "\n",
        "    initial_elements = _make_associative_filtering_elements(params, emissions)\n",
        "\n",
        "    @vmap\n",
        "    def filtering_operator(elem1, elem2):\n",
        "        A1, b1, C1, J1, eta1, logZ1 = elem1\n",
        "        A2, b2, C2, J2, eta2, logZ2 = elem2\n",
        "        dim = A1.shape[0]\n",
        "        I = np.eye(dim)\n",
        "\n",
        "        I_C1J2 = I + C1 @ J2\n",
        "        temp = scipy.linalg.solve(I_C1J2.T, A2.T).T\n",
        "        A = temp @ A1\n",
        "        b = temp @ (b1 + C1 @ eta2) + b2\n",
        "        C = temp @ C1 @ A2.T + C2\n",
        "\n",
        "        I_J2C1 = I + J2 @ C1\n",
        "        temp = scipy.linalg.solve(I_J2C1.T, A1).T\n",
        "\n",
        "        eta = temp @ (eta2 - J2 @ b1) + eta1\n",
        "        J = temp @ J2 @ A1 + J1\n",
        "\n",
        "        # mu = scipy.linalg.solve(J2, eta2)\n",
        "        # t2 = - eta2 @ mu + (b1 - mu) @ scipy.linalg.solve(I_J2C1, (J2 @ b1 - eta2))\n",
        "\n",
        "        mu = np.linalg.solve(C1, b1)\n",
        "        t1 = (b1 @ mu - (eta2 + mu) @ np.linalg.solve(I_C1J2, C1 @ eta2 + b1))\n",
        "\n",
        "        logZ = (logZ1 + logZ2 + 0.5 * np.linalg.slogdet(I_C1J2)[1] + 0.5 * t1)\n",
        "\n",
        "        return A, b, C, J, eta, logZ\n",
        "\n",
        "    _, filtered_means, filtered_covs, _, _, logZ = lax.associative_scan(\n",
        "                                                filtering_operator, initial_elements\n",
        "                                                )\n",
        "\n",
        "    return {\n",
        "        \"marginal_logliks\": -logZ,\n",
        "        \"marginal_loglik\": -logZ[-1],\n",
        "        \"filtered_means\": filtered_means, \n",
        "        \"filtered_covariances\": filtered_covs\n",
        "    }\n",
        "\n",
        "def _make_associative_smoothing_elements(params, filtered_means, filtered_covariances):\n",
        "    \"\"\"Preprocess filtering output to construct input for smoothing assocative scan.\"\"\"\n",
        "\n",
        "    F = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "\n",
        "    def _last_smoothing_element(m, P):\n",
        "        return np.zeros_like(P), m, P\n",
        "\n",
        "    def _generic_smoothing_element(params, m, P):\n",
        "\n",
        "        Pp = F @ P @ F.T + Q\n",
        "\n",
        "        E  = psd_solve(Pp, F @ P).T\n",
        "        g  = m - E @ F @ m\n",
        "        L  = P - E @ Pp @ E.T\n",
        "        return E, g, L\n",
        "\n",
        "    last_elems = _last_smoothing_element(filtered_means[-1], filtered_covariances[-1])\n",
        "    generic_elems = vmap(_generic_smoothing_element, (None, 0, 0))(\n",
        "        params, filtered_means[:-1], filtered_covariances[:-1]\n",
        "        )\n",
        "    combined_elems = tuple(np.append(gen_elm, last_elm[None,:], axis=0)\n",
        "                           for gen_elm, last_elm in zip(generic_elems, last_elems))\n",
        "    return combined_elems\n",
        "\n",
        "\n",
        "def parallel_lgssm_smoother(params, emissions):\n",
        "    \"\"\"A parallel version of the lgssm smoothing algorithm.\n",
        "    See S. Särkkä and Á. F. García-Fernández (2021) - https://arxiv.org/abs/1905.13002.\n",
        "    Note: This function does not yet handle `inputs` to the system.\n",
        "    \"\"\"\n",
        "    filtered_posterior = lgssm_filter(params, emissions)\n",
        "    filtered_means = filtered_posterior[\"filtered_means\"]\n",
        "    filtered_covs = filtered_posterior[\"filtered_covariances\"]\n",
        "    initial_elements = _make_associative_smoothing_elements(params, filtered_means, filtered_covs)\n",
        "\n",
        "    @vmap\n",
        "    def smoothing_operator(elem1, elem2):\n",
        "        E1, g1, L1 = elem1\n",
        "        E2, g2, L2 = elem2\n",
        "\n",
        "        E = E2 @ E1\n",
        "        g = E2 @ g1 + g2\n",
        "        L = E2 @ L1 @ E2.T + L2\n",
        "\n",
        "        return E, g, L\n",
        "\n",
        "    _, smoothed_means, smoothed_covs, *_ = lax.associative_scan(\n",
        "                                                smoothing_operator, initial_elements, reverse=True\n",
        "                                                )\n",
        "    return {\n",
        "        \"marginal_loglik\": filtered_posterior[\"marginal_loglik\"],\n",
        "        \"filtered_means\": filtered_means,\n",
        "        \"filtered_covariances\": filtered_covs,\n",
        "        \"smoothed_means\": smoothed_means,\n",
        "        \"smoothed_covariances\": smoothed_covs\n",
        "    }\n",
        "\n",
        "def lgssm_log_normalizer(dynamics_params, mu_filtered, Sigma_filtered, potentials):\n",
        "    p = dynamics_params\n",
        "    Q, A, b = p[\"Q\"][None], p[\"A\"][None], p[\"b\"][None]\n",
        "    AT = (p[\"A\"].T)[None]\n",
        "\n",
        "    I = np.eye(Q.shape[-1])\n",
        "\n",
        "    Sigma_filtered, mu_filtered = Sigma_filtered[:-1], mu_filtered[:-1]\n",
        "    Sigma = Q + A @ Sigma_filtered @ AT\n",
        "    mu = (A[0] @ mu_filtered.T).T + b\n",
        "    # Append the first element\n",
        "    Sigma_pred = np.concatenate([p[\"Q1\"][None], Sigma])\n",
        "    mu_pred = np.concatenate([p[\"m1\"][None], mu])\n",
        "    mu_rec, Sigma_rec = potentials[\"mu\"], potentials[\"Sigma\"]\n",
        "\n",
        "    def log_Z_single(mu_pred, Sigma_pred, mu_rec, Sigma_rec):\n",
        "        return MVN(loc=mu_pred, covariance_matrix=Sigma_pred+Sigma_rec).log_prob(mu_rec)\n",
        "\n",
        "    log_Z = vmap(log_Z_single)(mu_pred, Sigma_pred, mu_rec, Sigma_rec)\n",
        "    return np.sum(log_Z)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "il0xUSa3nn0K"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "id": "_zQepN9hRCdG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Parallel linear Gaussian state space model object\n",
        "def lgssm_log_normalizer(dynamics_params, mu_filtered, Sigma_filtered, potentials):\n",
        "    p = dynamics_params\n",
        "    Q, A, b = p[\"Q\"][None], p[\"A\"][None], p[\"b\"][None]\n",
        "    AT = (p[\"A\"].T)[None]\n",
        "\n",
        "    I = np.eye(Q.shape[-1])\n",
        "\n",
        "    Sigma_filtered, mu_filtered = Sigma_filtered[:-1], mu_filtered[:-1]\n",
        "    Sigma = Q + A @ Sigma_filtered @ AT\n",
        "    mu = (A[0] @ mu_filtered.T).T + b\n",
        "    # Append the first element\n",
        "    Sigma_pred = np.concatenate([p[\"Q1\"][None], Sigma])\n",
        "    mu_pred = np.concatenate([p[\"m1\"][None], mu])\n",
        "    mu_rec, Sigma_rec = potentials[\"mu\"], potentials[\"Sigma\"]\n",
        "\n",
        "    def log_Z_single(mu_pred, Sigma_pred, mu_rec, Sigma_rec):\n",
        "        return MVN(loc=mu_pred, covariance_matrix=Sigma_pred+Sigma_rec).log_prob(mu_rec)\n",
        "\n",
        "    log_Z = vmap(log_Z_single)(mu_pred, Sigma_pred, mu_rec, Sigma_rec)\n",
        "    return np.sum(log_Z)\n",
        "\n",
        "class LinearGaussianSSM(tfd.Distribution):\n",
        "    def __init__(self,\n",
        "                 initial_mean,\n",
        "                 initial_covariance,\n",
        "                 dynamics_matrix,\n",
        "                 dynamics_bias,\n",
        "                 dynamics_noise_covariance,\n",
        "                 emissions_means,\n",
        "                 emissions_covariances,\n",
        "                 log_normalizer,\n",
        "                 filtered_means,\n",
        "                 filtered_covariances,\n",
        "                 smoothed_means,\n",
        "                 smoothed_covariances,\n",
        "                 smoothed_cross,\n",
        "                 validate_args=False,\n",
        "                 allow_nan_stats=True,\n",
        "                 name=\"LinearGaussianSSM\",\n",
        "             ) -> None:\n",
        "        # Dynamics\n",
        "        self._initial_mean = initial_mean\n",
        "        self._initial_covariance = initial_covariance\n",
        "        self._dynamics_matrix = dynamics_matrix\n",
        "        self._dynamics_bias = dynamics_bias\n",
        "        self._dynamics_noise_covariance = dynamics_noise_covariance\n",
        "        # Emissions\n",
        "        self._emissions_means = emissions_means\n",
        "        self._emissions_covariances = emissions_covariances\n",
        "        # Filtered\n",
        "        self._log_normalizer = log_normalizer\n",
        "        self._filtered_means = filtered_means\n",
        "        self._filtered_covariances = filtered_covariances\n",
        "        # Smoothed\n",
        "        self._smoothed_means = smoothed_means\n",
        "        self._smoothed_covariances = smoothed_covariances\n",
        "        self._smoothed_cross = smoothed_cross\n",
        "\n",
        "        # We would detect the dtype dynamically but that would break vmap\n",
        "        # see https://github.com/tensorflow/probability/issues/1271\n",
        "        dtype = np.float32\n",
        "        super(LinearGaussianSSM, self).__init__(\n",
        "            dtype=dtype,\n",
        "            validate_args=validate_args,\n",
        "            allow_nan_stats=allow_nan_stats,\n",
        "            reparameterization_type=reparameterization.NOT_REPARAMETERIZED,\n",
        "            parameters=dict(initial_mean=self._initial_mean,\n",
        "                            initial_covariance=self._initial_covariance,\n",
        "                            dynamics_matrix=self._dynamics_matrix,\n",
        "                            dynamics_bias=self._dynamics_bias,\n",
        "                            dynamics_noise_covariance=self._dynamics_noise_covariance,\n",
        "                            emissions_means=self._emissions_means,\n",
        "                            emissions_covariances=self._emissions_covariances,\n",
        "                            log_normalizer=self._log_normalizer,\n",
        "                            filtered_means=self._filtered_means,\n",
        "                            filtered_covariances=self._filtered_covariances,\n",
        "                            smoothed_means=self._smoothed_means,\n",
        "                            smoothed_covariances=self._smoothed_covariances,\n",
        "                            smoothed_cross=self._smoothed_cross),\n",
        "            name=name,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _parameter_properties(cls, dtype, num_classes=None):\n",
        "        # pylint: disable=g-long-lambda\n",
        "        return dict(initial_mean=tfp.internal.parameter_properties.ParameterProperties(event_ndims=1),\n",
        "                    initial_covariance=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    dynamics_matrix=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    dynamics_bias=tfp.internal.parameter_properties.ParameterProperties(event_ndims=1),\n",
        "                    dynamics_noise_covariance=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    emissions_means=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    emissions_covariances=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "                    log_normalizer=tfp.internal.parameter_properties.ParameterProperties(event_ndims=0),\n",
        "                    filtered_means=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    filtered_covariances=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "                    smoothed_means=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    smoothed_covariances=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "                    smoothed_cross=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3)\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def infer_from_dynamics_and_potential(cls, dynamics_params, emissions_potentials):\n",
        "        p = dynamics_params\n",
        "        mus, Sigmas = emissions_potentials[\"mu\"], emissions_potentials[\"Sigma\"]\n",
        "\n",
        "        dim = mus.shape[-1]\n",
        "        C = np.eye(dim)\n",
        "        d = np.zeros(dim)\n",
        "\n",
        "        params = make_lgssm_params(p[\"m1\"], p[\"Q1\"], p[\"A\"], p[\"Q\"], C, Sigmas, \n",
        "                                   dynamics_bias=p[\"b\"], emissions_bias=d)\n",
        "\n",
        "        smoothed = lgssm_smoother(params, mus)._asdict()\n",
        "        \n",
        "        # Compute ExxT\n",
        "        A, Q = dynamics_params[\"A\"], dynamics_params[\"Q\"]\n",
        "        filtered_cov = smoothed[\"filtered_covariances\"]\n",
        "        filtered_mean = smoothed[\"smoothed_means\"]\n",
        "        smoothed_cov = smoothed[\"smoothed_covariances\"]\n",
        "        smoothed_mean = smoothed[\"smoothed_means\"]\n",
        "        G = vmap(lambda C: psd_solve(Q + A @ C @ A.T, A @ C).T)(filtered_cov)\n",
        "\n",
        "        # Compute the smoothed expectation of z_t z_{t+1}^T\n",
        "        smoothed_cross = vmap(\n",
        "            lambda Gt, mean, next_mean, next_cov: Gt @ next_cov + np.outer(mean, next_mean))\\\n",
        "            (G[:-1], smoothed_mean[:-1], smoothed_mean[1:], smoothed_cov[1:])\n",
        "\n",
        "        log_Z = lgssm_log_normalizer(dynamics_params, \n",
        "                                     smoothed[\"filtered_means\"], \n",
        "                                     smoothed[\"filtered_covariances\"], \n",
        "                                     emissions_potentials)\n",
        "\n",
        "        return cls(dynamics_params[\"m1\"],\n",
        "                   dynamics_params[\"Q1\"],\n",
        "                   dynamics_params[\"A\"],\n",
        "                   dynamics_params[\"b\"],\n",
        "                   dynamics_params[\"Q\"],\n",
        "                   emissions_potentials[\"mu\"],\n",
        "                   emissions_potentials[\"Sigma\"],\n",
        "                   log_Z, # smoothed[\"marginal_loglik\"],\n",
        "                   smoothed[\"filtered_means\"],\n",
        "                   smoothed[\"filtered_covariances\"],\n",
        "                   smoothed[\"smoothed_means\"],\n",
        "                   smoothed[\"smoothed_covariances\"],\n",
        "                   smoothed_cross)\n",
        "        \n",
        "    # Properties to get private class variables\n",
        "    @property\n",
        "    def log_normalizer(self):\n",
        "        return self._log_normalizer\n",
        "\n",
        "    @property\n",
        "    def filtered_means(self):\n",
        "        return self._filtered_means\n",
        "\n",
        "    @property\n",
        "    def filtered_covariances(self):\n",
        "        return self._filtered_covariances\n",
        "\n",
        "    @property\n",
        "    def smoothed_means(self):\n",
        "        return self._smoothed_means\n",
        "\n",
        "    @property\n",
        "    def smoothed_covariances(self):\n",
        "        return self._smoothed_covariances\n",
        "\n",
        "    @property\n",
        "    def expected_states(self):\n",
        "        return self._smoothed_means\n",
        "\n",
        "    @property\n",
        "    def expected_states_squared(self):\n",
        "        Ex = self._smoothed_means\n",
        "        return self._smoothed_covariances + np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "\n",
        "    @property\n",
        "    def expected_states_next_states(self):\n",
        "        return self._smoothed_cross\n",
        "\n",
        "    def _mean(self):\n",
        "        return self.smoothed_means\n",
        "\n",
        "    def _covariance(self):\n",
        "        return self.smoothed_covariances\n",
        "    \n",
        "    # TODO: currently this function does not depend on the dynamics bias\n",
        "    def _log_prob(self, data, **kwargs):\n",
        "        A = self._dynamics_matrix #params[\"A\"]\n",
        "        Q = self._dynamics_noise_covariance #params[\"Q\"]\n",
        "        Q1 = self._initial_covariance #params[\"Q1\"]\n",
        "        m1 = self._initial_mean #params[\"m1\"]\n",
        "\n",
        "        num_batch_dims = len(data.shape) - 2\n",
        "\n",
        "        ll = np.sum(\n",
        "            MVN(loc=np.einsum(\"ij,...tj->...ti\", A, data[...,:-1,:]), \n",
        "                covariance_matrix=Q).log_prob(data[...,1:,:])\n",
        "            )\n",
        "        ll += MVN(loc=m1, covariance_matrix=Q1).log_prob(data[...,0,:])\n",
        "\n",
        "        # Add the observation potentials\n",
        "        # ll += - 0.5 * np.einsum(\"...ti,tij,...tj->...\", data, self._emissions_precisions, data) \\\n",
        "        #       + np.einsum(\"...ti,ti->...\", data, self._emissions_linear_potentials)\n",
        "        ll += np.sum(MVN(loc=self._emissions_means, \n",
        "                  covariance_matrix=self._emissions_covariances).log_prob(data), axis=-1)\n",
        "        # Add the log normalizer\n",
        "        ll -= self._log_normalizer\n",
        "\n",
        "        return ll\n",
        "\n",
        "    def _sample_n(self, n, seed=None):\n",
        "\n",
        "        F = self._dynamics_matrix\n",
        "        b = self._dynamics_bias\n",
        "        Q = self._dynamics_noise_covariance\n",
        "        \n",
        "        def sample_single(\n",
        "            key,\n",
        "            filtered_means,\n",
        "            filtered_covariances\n",
        "        ):\n",
        "\n",
        "            initial_elements = _make_associative_sampling_elements(\n",
        "                { \"A\": F, \"b\": b, \"Q\": Q }, key, filtered_means, filtered_covariances)\n",
        "\n",
        "            @vmap\n",
        "            def sampling_operator(elem1, elem2):\n",
        "                E1, h1 = elem1\n",
        "                E2, h2 = elem2\n",
        "\n",
        "                E = E2 @ E1\n",
        "                h = E2 @ h1 + h2\n",
        "                return E, h\n",
        "\n",
        "            _, sample = \\\n",
        "                lax.associative_scan(sampling_operator, initial_elements, reverse=True)\n",
        "                \n",
        "            return sample\n",
        "\n",
        "        # TODO: Handle arbitrary batch shapes\n",
        "        if self._filtered_covariances.ndim == 4:\n",
        "            # batch mode\n",
        "            samples = vmap(vmap(sample_single, in_axes=(None, 0, 0)), in_axes=(0, None, None))\\\n",
        "                (jr.split(seed, n), self._filtered_means, self._filtered_covariances)\n",
        "            # Transpose to be (num_samples, num_batches, num_timesteps, dim)\n",
        "            # samples = np.transpose(samples, (1, 0, 2, 3))\n",
        "        else:\n",
        "            # non-batch mode\n",
        "            samples = vmap(sample_single, in_axes=(0, None, None))\\\n",
        "                (jr.split(seed, n), self._filtered_means, self._filtered_covariances)\n",
        "        return samples\n",
        "\n",
        "    def _entropy(self):\n",
        "        \"\"\"\n",
        "        Compute the entropy\n",
        "\n",
        "            H[X] = -E[\\log p(x)]\n",
        "                 = -E[-1/2 x^T J x + x^T h - log Z(J, h)]\n",
        "                 = 1/2 <J, E[x x^T] - <h, E[x]> + log Z(J, h)\n",
        "        \"\"\"\n",
        "        Ex = self.expected_states\n",
        "        ExxT = self.expected_states_squared\n",
        "        ExnxT = self.expected_states_next_states\n",
        "        p = dynamics_to_tridiag(\n",
        "            {\n",
        "                \"m1\": self._initial_mean,\n",
        "                \"Q1\": self._initial_covariance,\n",
        "                \"A\": self._dynamics_matrix,\n",
        "                \"b\": self._dynamics_bias,\n",
        "                \"Q\": self._dynamics_noise_covariance,\n",
        "            }, Ex.shape[0], Ex.shape[1]\n",
        "        )\n",
        "        J_diag = p[\"J\"] + solve(self._emissions_covariances, np.eye(Ex.shape[-1])[None])\n",
        "        J_lower_diag = p[\"L\"]\n",
        "\n",
        "        Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "        Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "        entropy = 0.5 * np.sum(J_diag * Sigmatt)\n",
        "        entropy += np.sum(J_lower_diag * Sigmatnt)\n",
        "        return entropy - self.log_prob(Ex)\n",
        "\n",
        "class ParallelLinearGaussianSSM(LinearGaussianSSM):\n",
        "    def __init__(self, *args, **kwargs) -> None:\n",
        "        kwargs[\"name\"] = \"ParallelLinearGaussianSSM\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def infer_from_dynamics_and_potential(cls, dynamics_params, emissions_potentials):\n",
        "        # p = dynamics_params\n",
        "        # mus, Sigmas = emissions_potentials[\"mu\"], emissions_potentials[\"Sigma\"]\n",
        "\n",
        "        # dim = mus.shape[-1]\n",
        "        # C = np.eye(dim)\n",
        "        # d = np.zeros(dim)\n",
        "\n",
        "        smoothed = parallel_lgssm_smoother(dynamics_params, emissions_potentials)\n",
        "        \n",
        "        # Compute ExxT\n",
        "        A, Q = dynamics_params[\"A\"], dynamics_params[\"Q\"]\n",
        "        filtered_cov = smoothed[\"filtered_covariances\"]\n",
        "        filtered_mean = smoothed[\"smoothed_means\"]\n",
        "        smoothed_cov = smoothed[\"smoothed_covariances\"]\n",
        "        smoothed_mean = smoothed[\"smoothed_means\"]\n",
        "        G = vmap(lambda C: psd_solve(Q + A @ C @ A.T, A @ C).T)(filtered_cov)\n",
        "\n",
        "        # Compute the smoothed expectation of z_t z_{t+1}^T\n",
        "        smoothed_cross = vmap(\n",
        "            lambda Gt, mean, next_mean, next_cov: Gt @ next_cov + np.outer(mean, next_mean))\\\n",
        "            (G[:-1], smoothed_mean[:-1], smoothed_mean[1:], smoothed_cov[1:])\n",
        "\n",
        "        log_Z = lgssm_log_normalizer(dynamics_params, \n",
        "                                     smoothed[\"filtered_means\"], \n",
        "                                     smoothed[\"filtered_covariances\"], \n",
        "                                     emissions_potentials)\n",
        "\n",
        "        return cls(dynamics_params[\"m1\"],\n",
        "                   dynamics_params[\"Q1\"],\n",
        "                   dynamics_params[\"A\"],\n",
        "                   dynamics_params[\"b\"],\n",
        "                   dynamics_params[\"Q\"],\n",
        "                   emissions_potentials[\"mu\"],\n",
        "                   emissions_potentials[\"Sigma\"],\n",
        "                   log_Z, # smoothed[\"marginal_loglik\"],\n",
        "                   smoothed[\"filtered_means\"],\n",
        "                   smoothed[\"filtered_covariances\"],\n",
        "                   smoothed[\"smoothed_means\"],\n",
        "                   smoothed[\"smoothed_covariances\"],\n",
        "                   smoothed_cross)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "HOdprEnnTpSN"
      },
      "outputs": [],
      "source": [
        "# @title Parallel versions of the same priors and posteriors\n",
        "\n",
        "# Super simple because all the machinary is already taken care of\n",
        "class LDSSVAEPosterior(SVAEPrior):\n",
        "    def __init__(self, latent_dims, seq_len, use_parallel=False):\n",
        "        self.latent_dims = latent_dims\n",
        "        # The only annoying thing is that we have to specify the sequence length\n",
        "        # ahead of time\n",
        "        self.seq_len = seq_len\n",
        "        self.dist = ParallelLinearGaussianSSM if use_parallel else LinearGaussianSSM\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return (self.seq_len, self.latent_dims)\n",
        "\n",
        "    # Must be the full set of constrained parameters!\n",
        "    def distribution(self, p):\n",
        "        m1, Q1, A, b, Q, mus, Sigmas = p[\"m1\"], p[\"Q1\"], p[\"A\"], p[\"b\"], p[\"Q\"], p[\"mu\"], p[\"Sigma\"]\n",
        "        log_Z, mu_filtered, Sigma_filtered = p[\"log_Z\"], p[\"mu_filtered\"], p[\"Sigma_filtered\"]\n",
        "        mu_smoothed, Sigma_smoothed, ExnxT = p[\"mu_smoothed\"], p[\"Sigma_smoothed\"], p[\"ExnxT\"]\n",
        "        return self.dist(m1, Q1, A, b, Q, mus, Sigmas, \n",
        "                             log_Z, mu_filtered, Sigma_filtered, \n",
        "                             mu_smoothed, Sigma_smoothed, ExnxT)\n",
        "\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        p = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"Q1\": np.eye(D),\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": np.eye(D),\n",
        "            \"Sigma\": np.tile(np.eye(D)[None], (T, 1, 1)),\n",
        "            \"mu\": np.zeros((T, D))\n",
        "        }\n",
        "\n",
        "        dist = self.dist.infer_from_dynamics_and_potential(p, \n",
        "                                    {\"mu\": p[\"mu\"], \"Sigma\": p[\"Sigma\"]})\n",
        "        \n",
        "        p.update({\n",
        "            \"log_Z\": dist.log_normalizer,\n",
        "            \"mu_filtered\": dist.filtered_means,\n",
        "            \"Sigma_filtered\": dist.filtered_covariances,\n",
        "            \"mu_smoothed\": dist.smoothed_means,\n",
        "            \"Sigma_smoothed\": dist.smoothed_covariances,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        return params\n",
        "\n",
        "    def infer(self, prior_params, potential_params):\n",
        "        p = {\n",
        "            \"m1\": prior_params[\"m1\"],\n",
        "            \"Q1\": prior_params[\"Q1\"],\n",
        "            \"A\": prior_params[\"A\"],\n",
        "            \"b\": prior_params[\"b\"],\n",
        "            \"Q\": prior_params[\"Q\"],\n",
        "            \"Sigma\": potential_params[\"Sigma\"],\n",
        "            \"mu\": potential_params[\"mu\"]\n",
        "        }\n",
        "\n",
        "        dist = self.dist.infer_from_dynamics_and_potential(prior_params, \n",
        "                                    {\"mu\": p[\"mu\"], \"Sigma\": p[\"Sigma\"]})\n",
        "        p.update({\n",
        "            \"log_Z\": dist.log_normalizer,\n",
        "            \"mu_filtered\": dist.filtered_means,\n",
        "            \"Sigma_filtered\": dist.filtered_covariances,\n",
        "            \"mu_smoothed\": dist.smoothed_means,\n",
        "            \"Sigma_smoothed\": dist.smoothed_covariances,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtvHU_dlOklC"
      },
      "source": [
        "## Define neural network architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "cellView": "form",
        "id": "40wvAwfoOn_E"
      },
      "outputs": [],
      "source": [
        "# @title Neural network utils\n",
        "\n",
        "PRNGKey = Any\n",
        "Shape = Iterable[int]\n",
        "Dtype = Any  # this could be a real type?\n",
        "Array = Any\n",
        "\n",
        "# Note: the last layer output does not have a relu activation!\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Define a simple fully connected MLP with ReLU activations.\n",
        "    \"\"\"\n",
        "    features: Sequence[int]\n",
        "    kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.he_normal()\n",
        "    bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.zeros\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for feat in self.features[:-1]:\n",
        "            x = nn.relu(nn.Dense(feat, \n",
        "                kernel_init=self.kernel_init,\n",
        "                bias_init=self.bias_init,)(x))\n",
        "        x = nn.Dense(self.features[-1], \n",
        "            kernel_init=self.kernel_init, \n",
        "            bias_init=self.bias_init)(x)\n",
        "        return x\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    \"\"\"\n",
        "    A layer which passes the input through unchanged.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "class Static(nn.Module):\n",
        "    \"\"\"\n",
        "    A layer which just returns some static parameters.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "    kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.lecun_normal()\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        kernel = self.param('kernel',\n",
        "                            self.kernel_init,\n",
        "                            (self.features, ))\n",
        "        return kernel\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"A simple CNN model.\"\"\"\n",
        "    input_rank : int = None   \n",
        "    output_dim : int = None\n",
        "    layer_params : Sequence[dict] = None\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for params in self.layer_params:\n",
        "            x = nn.relu(Conv(**params)(x))\n",
        "        # No activations at the output\n",
        "        x = nn.Dense(features=self.output_dim)(x.flatten())\n",
        "        return x\n",
        "\n",
        "class DCNN(nn.Module):\n",
        "    \"\"\"A simple DCNN model.\"\"\"   \n",
        "\n",
        "    input_shape: Sequence[int] = None\n",
        "    layer_params: Sequence[dict] = None\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        input_features = onp.prod(onp.array(self.input_shape))\n",
        "        x = nn.Dense(features=input_features)(x)\n",
        "        x = x.reshape(self.input_shape)\n",
        "        # Note that the last layer doesn't have an activation\n",
        "        for params in self.layer_params:\n",
        "            x = ConvTranspose(**params)(nn.relu(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "id": "Ja-LJ88rybCV"
      },
      "outputs": [],
      "source": [
        "# @title Potential networks (outputs potentials on single observations)\n",
        "class PotentialNetwork(nn.Module):\n",
        "    def __call__(self, inputs):\n",
        "        Sigma, mu = self._generate_distribution_parameters(inputs)\n",
        "        # J, h = solve(Sigma, np.eye(mu.shape[-1])[None]), solve(Sigma, mu)\n",
        "        # if (len(J.shape) == 3):\n",
        "        #     seq_len, latent_dims, _ = J.shape\n",
        "        #     # lower diagonal blocks of precision matrix\n",
        "        #     L = np.zeros((seq_len-1, latent_dims, latent_dims))\n",
        "        # elif (len(J.shape) == 4):\n",
        "        #     batch_size, seq_len, latent_dims, _ = J.shape\n",
        "        #     # lower diagonal blocks of precision matrix\n",
        "        #     L = np.zeros((batch_size, seq_len-1, latent_dims, latent_dims))\n",
        "        # else:\n",
        "        #     L = np.zeros(tuple())\n",
        "        return {#\"J\": J, \"L\": L, \"h\": h, \n",
        "                \"Sigma\": Sigma, \"mu\": mu}\n",
        "\n",
        "    def _generate_distribution_parameters(self, inputs):\n",
        "        if (len(inputs.shape) == self.input_rank + 2):\n",
        "            # We have both a batch dimension and a time dimension\n",
        "            # and we have to vmap over both...!\n",
        "            return vmap(vmap(self._call_single, 0), 0)(inputs)\n",
        "        elif (len(inputs.shape) == self.input_rank + 1):\n",
        "            return vmap(self._call_single)(inputs)\n",
        "        elif (len(inputs.shape) == self.input_rank):\n",
        "            return self._call_single(inputs)\n",
        "        else:\n",
        "            # error\n",
        "            return None\n",
        "\n",
        "    def _call_single(self, inputs):\n",
        "        pass\n",
        "\n",
        "# A new, more general implementation of the Gaussian recognition network\n",
        "# Uses mean parameterization which works better empirically\n",
        "class GaussianRecognition(PotentialNetwork):\n",
        "\n",
        "    use_diag : int = None\n",
        "    input_rank : int = None\n",
        "    latent_dims : int = None\n",
        "    trunk_fn : nn.Module = None\n",
        "    head_mean_fn : nn.Module = None\n",
        "    head_log_var_fn : nn.Module = None\n",
        "    eps : float = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_params(cls, input_rank=1, input_dim=None, output_dim=None, \n",
        "                    trunk_type=\"Identity\", trunk_params=None, \n",
        "                    head_mean_type=\"MLP\", head_mean_params=None,\n",
        "                    head_var_type=\"MLP\", head_var_params=None, diagonal_covariance=False,\n",
        "                    cov_init=1, eps=1e-3): \n",
        "\n",
        "        if trunk_type == \"Identity\":\n",
        "            trunk_params = { \"features\": input_dim }\n",
        "        if head_mean_type == \"MLP\":\n",
        "            head_mean_params[\"features\"] += [output_dim]\n",
        "        if head_var_type == \"MLP\":\n",
        "            if (diagonal_covariance):\n",
        "                head_var_params[\"features\"] += [output_dim]\n",
        "            else:\n",
        "                head_var_params[\"features\"] += [output_dim * (output_dim + 1) // 2]\n",
        "            head_var_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_var_params[\"bias_init\"] = nn.initializers.constant(cov_init)\n",
        "\n",
        "        trunk_fn = globals()[trunk_type](**trunk_params)\n",
        "        head_mean_fn = globals()[head_mean_type](**head_mean_params)\n",
        "        head_log_var_fn = globals()[head_var_type](**head_var_params)\n",
        "\n",
        "        return cls(diagonal_covariance, input_rank, output_dim, trunk_fn, \n",
        "                   head_mean_fn, head_log_var_fn, eps)\n",
        "\n",
        "    def _call_single(self, inputs):\n",
        "        # Apply the trunk.\n",
        "        trunk_output = self.trunk_fn(inputs)\n",
        "        # Get the mean.\n",
        "        mu = self.head_mean_fn(trunk_output)\n",
        "        # Get the covariance parameters and build a full matrix from it.\n",
        "        var_output_flat = self.head_log_var_fn(trunk_output)\n",
        "        if self.use_diag:\n",
        "            Sigma = np.diag(softplus(var_output_flat) + self.eps)\n",
        "        else:\n",
        "            Sigma = lie_params_to_constrained(var_output_flat, self.latent_dims, self.eps)\n",
        "        # h = np.linalg.solve(Sigma, mu)\n",
        "        # J = np.linalg.inv(Sigma)\n",
        "        # lower diagonal blocks of precision matrix\n",
        "        return (Sigma, mu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "id": "gmeCrsbCy96H"
      },
      "outputs": [],
      "source": [
        "# @title Posterior networks (outputs full posterior for entire sequence)\n",
        "# Outputs Gaussian distributions for the entire sequence at once\n",
        "class PosteriorNetwork(PotentialNetwork):\n",
        "    def __call__(self, inputs):\n",
        "        As, bs, Qs = self._generate_distribution_parameters(inputs)\n",
        "        return {\"As\": As, \"bs\": bs, \"Qs\": Qs}\n",
        "\n",
        "    def _generate_distribution_parameters(self, inputs):\n",
        "        is_batched = (len(inputs.shape) == self.input_rank+2)\n",
        "        if is_batched:\n",
        "            return vmap(self._call_single, in_axes=0)(inputs)\n",
        "        else:\n",
        "            assert(len(inputs.shape) == self.input_rank+1)\n",
        "            return self._call_single(inputs)\n",
        "\n",
        "class GaussianBiRNN(PosteriorNetwork):\n",
        "    \n",
        "    input_rank : int = None\n",
        "    rnn_dim : int = None\n",
        "    output_dim : int = None\n",
        "    forward_RNN : nn.Module = None\n",
        "    backward_RNN : nn.Module = None\n",
        "    input_fn : nn.Module = None\n",
        "    trunk_fn: nn.Module = None\n",
        "    head_mean_fn : nn.Module = None\n",
        "    head_log_var_fn : nn.Module = None\n",
        "    head_dyn_fn : nn.Module = None\n",
        "    eps : float = None\n",
        "        \n",
        "    @classmethod\n",
        "    def from_params(cls, input_rank=1, cell_type=nn.GRUCell,\n",
        "                    input_dim=None, rnn_dim=None, output_dim=None, \n",
        "                    input_type=\"MLP\", input_params=None,\n",
        "                    trunk_type=\"Identity\", trunk_params=None, \n",
        "                    head_mean_type=\"MLP\", head_mean_params=None,\n",
        "                    head_var_type=\"MLP\", head_var_params=None,\n",
        "                    head_dyn_type=\"MLP\", head_dyn_params=None,\n",
        "                    cov_init=1, eps=1e-4): \n",
        "\n",
        "        forward_RNN = nn.scan(cell_type, variable_broadcast=\"params\", \n",
        "                                             split_rngs={\"params\": False})()\n",
        "        backward_RNN = nn.scan(cell_type, variable_broadcast=\"params\", \n",
        "                                               split_rngs={\"params\": False}, reverse=True)()\n",
        "        if trunk_type == \"Identity\":\n",
        "            trunk_params = { \"features\": rnn_dim }\n",
        "        if input_type == \"MLP\":\n",
        "            input_params[\"features\"] += [rnn_dim]\n",
        "        if head_mean_type == \"MLP\":\n",
        "            head_mean_params[\"features\"] += [output_dim]\n",
        "        if head_var_type == \"MLP\":\n",
        "            head_var_params[\"features\"] += [output_dim * (output_dim + 1) // 2]\n",
        "            head_var_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_var_params[\"bias_init\"] = nn.initializers.constant(cov_init)\n",
        "        if head_dyn_type == \"MLP\":\n",
        "            head_dyn_params[\"features\"] += [output_dim ** 2,]\n",
        "            head_dyn_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_dyn_params[\"bias_init\"] = nn.initializers.zeros\n",
        "\n",
        "        trunk_fn = globals()[trunk_type](**trunk_params)\n",
        "        input_fn = globals()[input_type](**input_params)\n",
        "        head_mean_fn = globals()[head_mean_type](**head_mean_params)\n",
        "        head_log_var_fn = globals()[head_var_type](**head_var_params)\n",
        "        head_dyn_fn = globals()[head_dyn_type](**head_dyn_params)\n",
        "\n",
        "        return cls(input_rank, rnn_dim, output_dim, \n",
        "                   forward_RNN, backward_RNN, \n",
        "                   input_fn, trunk_fn, \n",
        "                   head_mean_fn, head_log_var_fn, head_dyn_fn, eps)\n",
        "\n",
        "    # Applied the BiRNN to a single sequence of inputs\n",
        "    def _call_single(self, inputs):\n",
        "        output_dim = self.output_dim\n",
        "        \n",
        "        inputs = vmap(self.input_fn)(inputs)\n",
        "        init_carry_forward = np.zeros((self.rnn_dim,))\n",
        "        _, out_forward = self.forward_RNN(init_carry_forward, inputs)\n",
        "        init_carry_backward = np.zeros((self.rnn_dim,))\n",
        "        _, out_backward = self.backward_RNN(init_carry_backward, inputs)\n",
        "        # Concatenate the forward and backward outputs\n",
        "        out_combined = np.concatenate([out_forward, out_backward], axis=-1)\n",
        "        \n",
        "        # Get the mean.\n",
        "        # vmap over the time dimension\n",
        "        b = vmap(self.head_mean_fn)(out_combined)\n",
        "\n",
        "        # Get the variance output and reshape it.\n",
        "        # vmap over the time dimension\n",
        "        var_output_flat = vmap(self.head_log_var_fn)(out_combined)\n",
        "        Q = vmap(lie_params_to_constrained, in_axes=(0, None, None))\\\n",
        "            (var_output_flat, output_dim, self.eps)\n",
        "        dynamics_flat = vmap(self.head_dyn_fn)(out_combined)\n",
        "        A = dynamics_flat.reshape((-1, output_dim, output_dim))\n",
        "\n",
        "        return (A, b, Q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "cellView": "form",
        "id": "caU8H-hMEB_f"
      },
      "outputs": [],
      "source": [
        "# @title Special architectures for PlaNet\n",
        "class PlaNetRecognitionWrapper:\n",
        "    def __init__(self, rec_net):\n",
        "        self.rec_net = rec_net\n",
        "\n",
        "    def init(self, key, *inputs):\n",
        "        return self.rec_net.init(key, *inputs)\n",
        "    \n",
        "    def apply(self, params, x):\n",
        "        return {\n",
        "            \"network_input\": self.rec_net.apply(params[\"rec_params\"], x)[\"h\"],\n",
        "            \"network_params\": params[\"post_params\"],\n",
        "        }\n",
        "\n",
        "class StochasticRNNCell(nn.Module):\n",
        "\n",
        "    output_dim : int = None\n",
        "    rnn_cell : nn.Module = None\n",
        "    trunk_fn: nn.Module = None\n",
        "    head_mean_fn : nn.Module = None\n",
        "    head_log_var_fn : nn.Module = None\n",
        "    eps : float = None\n",
        "        \n",
        "    @classmethod\n",
        "    def from_params(cls, cell_type=nn.GRUCell,\n",
        "                    rnn_dim=None, output_dim=None, \n",
        "                    trunk_type=\"Identity\", trunk_params=None, \n",
        "                    head_mean_type=\"MLP\", head_mean_params=None,\n",
        "                    head_var_type=\"MLP\", head_var_params=None,\n",
        "                    cov_init=1, eps=1e-4, **kwargs): \n",
        "\n",
        "        rnn_cell = cell_type()\n",
        "\n",
        "        if trunk_type == \"Identity\":\n",
        "            trunk_params = { \"features\": rnn_dim }\n",
        "        if head_mean_type == \"MLP\":\n",
        "            head_mean_params[\"features\"] += [output_dim]\n",
        "        if head_var_type == \"MLP\":\n",
        "            head_var_params[\"features\"] += [output_dim * (output_dim + 1) // 2]\n",
        "            head_var_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_var_params[\"bias_init\"] = nn.initializers.constant(cov_init)\n",
        "\n",
        "        trunk_fn = globals()[trunk_type](**trunk_params)\n",
        "        head_mean_fn = globals()[head_mean_type](**head_mean_params)\n",
        "        head_log_var_fn = globals()[head_var_type](**head_var_params)\n",
        "\n",
        "        return cls(output_dim, rnn_cell, trunk_fn, head_mean_fn, head_log_var_fn, eps)\n",
        "\n",
        "    # h: latent state that's carried to the next\n",
        "    # x: last sample\n",
        "    # u: input at this timestep\n",
        "    def __call__(self, h, x, u):\n",
        "        h, out = self.rnn_cell(h, np.concatenate([x, u]))\n",
        "        out = self.trunk_fn(out)\n",
        "        mean, cov_flat = self.head_mean_fn(out), self.head_log_var_fn(out)\n",
        "        cov = lie_params_to_constrained(cov_flat, self.output_dim, self.eps)\n",
        "        return h, (cov, mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "cellView": "form",
        "id": "_DU0-NEWx_Il"
      },
      "outputs": [],
      "source": [
        "# @title Emission network (outputs distribution instead of parameters)\n",
        "\n",
        "# This is largely for convenience\n",
        "class GaussianEmission(GaussianRecognition):\n",
        "    def __call__(self, inputs):\n",
        "        J, h = self._generate_distribution_parameters(inputs)\n",
        "        # TODO: inverting J is pretty bad numerically, perhaps save Cholesky instead?\n",
        "        if (len(J.shape) == 3):\n",
        "            Sigma = vmap(inv)(J)\n",
        "            mu = np.einsum(\"tij,tj->ti\", Sigma, h)\n",
        "        elif (len(J.shape) == 2):\n",
        "            Sigma = inv(J)\n",
        "            mu = np.linalg.solve(J, h)\n",
        "        else:\n",
        "            # Error\n",
        "            return None\n",
        "        return tfd.MultivariateNormalFullCovariance(\n",
        "            loc=mu, covariance_matrix=Sigma)\n",
        "        \n",
        "class GaussianDCNNEmission(PotentialNetwork):\n",
        "\n",
        "    input_rank : int = None\n",
        "    network : nn.Module = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_params(cls, **params):\n",
        "        network = DCNN(**params)\n",
        "        return cls(1, network)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        out = self._generate_distribution_parameters(inputs)\n",
        "        mu = out[\"mu\"]\n",
        "        # Adding a constant to prevent the model from getting too crazy\n",
        "        sigma = out[\"sigma\"] + 1e-4\n",
        "        return tfd.Normal(loc=mu, scale=sigma)\n",
        "\n",
        "    def _call_single(self, x):\n",
        "        out_raw = self.network(x)\n",
        "        mu_raw, sigma_raw = np.split(out_raw, 2, axis=-1)\n",
        "        mu = sigmoid(mu_raw)\n",
        "        sigma = softplus(sigma_raw)\n",
        "        # sigma = np.ones_like(mu) * 0.1\n",
        "        return { \"mu\": mu, \"sigma\": sigma }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xevru2BSSSZ"
      },
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "id": "28XHvF41SVWK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Visualization/animation helpers\n",
        "\n",
        "# Returns a random projection matrix from ND to 2D\n",
        "def random_projection(seed, N):\n",
        "    key1, key2 = jr.split(seed, 2)\n",
        "    v1 = jr.normal(key1, (N,))\n",
        "    v2 = jr.normal(key2, (N,))\n",
        "\n",
        "    v1 /= np.linalg.norm(v1)\n",
        "    v2 -= v1 * np.dot(v1, v2)\n",
        "    v2 /= np.linalg.norm(v2)\n",
        "\n",
        "    return np.stack([v1, v2])\n",
        "\n",
        "def get_gaussian_draw_params(mu, Sigma, proj_seed=None):\n",
        "\n",
        "    Sigma = (Sigma + Sigma.T) * .5\n",
        "\n",
        "    if (mu.shape[0] > 2):\n",
        "        P = random_projection(proj_seed, mu.shape[0])\n",
        "        mu = P @ mu\n",
        "    angles = np.hstack([np.arange(0, 2*np.pi, 0.01), 0])\n",
        "    circle = np.vstack([np.sin(angles), np.cos(angles)])\n",
        "    min_eig = np.min(eigh(Sigma)[0])\n",
        "    eps = 1e-6\n",
        "    if (min_eig <= eps): Sigma += np.eye(Sigma.shape[0]) * eps\n",
        "    L = np.linalg.cholesky(Sigma)\n",
        "    u, svs, vt = svd(P @ L)\n",
        "    ellipse = np.dot(u * svs, circle) * 2\n",
        "    return (mu[0], mu[1]), (ellipse[0, :] + mu[0], ellipse[1, :] + mu[1])\n",
        "\n",
        "def plot_gaussian_2D(mu, Sigma, proj_seed=None, ax=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Helper function to plot 2D Gaussian contours\n",
        "    \"\"\"\n",
        "    (px, py), (exs, eys) = get_gaussian_draw_params(mu, Sigma, proj_seed)\n",
        "\n",
        "    ax = plt.gca() if ax is None else ax\n",
        "    point = ax.plot(px, py, marker='D', **kwargs)\n",
        "    line, = ax.plot(exs, eys, **kwargs)\n",
        "    return (point, line)\n",
        "\n",
        "def get_artists(ax, mus, Sigmas, proj_seed, num_pts, **draw_params):\n",
        "    point_artists = []\n",
        "    line_artists = []\n",
        "\n",
        "    for j in range(num_pts):\n",
        "        mean_params, cov_params = get_gaussian_draw_params(mus[j], \n",
        "                                                           Sigmas[j], \n",
        "                                                           proj_seed)\n",
        "        point = ax.plot(mean_params[0], mean_params[1], marker='D', \n",
        "                        color=colors[j], **draw_params)[0]\n",
        "        line = ax.plot(cov_params[0], cov_params[1], \n",
        "                       color=colors[j], **draw_params)[0]\n",
        "        point_artists.append(point)\n",
        "        line_artists.append(line)\n",
        "    return point_artists, line_artists\n",
        "\n",
        "def update_draw_params(point_artists, line_artists, mus, Sigmas, proj_seed, num_pts):\n",
        "    for i in range(num_pts):\n",
        "        mean_params, cov_params = get_gaussian_draw_params(mus[i], Sigmas[i], proj_seed)\n",
        "        point_artists[i].set_data(mean_params[0], mean_params[1])\n",
        "        line_artists[i].set_data(cov_params[0], cov_params[1])\n",
        "\n",
        "# Some animation helpers\n",
        "def animate_gaussians(inf_mus, inf_Sigmas, \n",
        "                      tgt_mus, tgt_Sigmas, \n",
        "                      true_mus, true_Sigmas,\n",
        "                      num_pts,\n",
        "                      proj_seed=None, x_lim=None, y_lim=None, **kwargs):\n",
        "    proj_seed = jr.PRNGKey(0) if proj_seed is None else proj_seed\n",
        "    print(\"Animating Gaussian blobs...!\")\n",
        "    # First set up the figure, the axis, and the plot element we want to animate\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlim(-10, 10)\n",
        "    ax.set_ylim(-10, 10)\n",
        "    plt.close()\n",
        "    \n",
        "\n",
        "    tgt_points, tgt_lines = get_artists(ax, tgt_mus[0], tgt_Sigmas[0], proj_seed, num_pts,\n",
        "                                        alpha=0.3, linestyle=\"dotted\")\n",
        "    inf_points, inf_lines = get_artists(ax, inf_mus[0], inf_Sigmas[0], proj_seed, num_pts)\n",
        "    true_points, true_lines = get_artists(ax, true_mus, true_Sigmas, \n",
        "                                          proj_seed, num_pts, alpha=0.2)\n",
        "\n",
        "    artists = tgt_points + tgt_lines + inf_points + inf_lines + true_points + true_lines\n",
        "\n",
        "    T = len(inf_mus)\n",
        "\n",
        "    # animation function. This is called sequentially  \n",
        "    def animate(i):\n",
        "        update_draw_params(tgt_points, tgt_lines, tgt_mus[i], tgt_Sigmas[i], proj_seed, num_pts)\n",
        "        update_draw_params(inf_points, inf_lines, inf_mus[i], inf_Sigmas[i], proj_seed, num_pts)\n",
        "        clear_output(wait=True)\n",
        "        print(\"Processing frame #{}/{}\".format(i+1, T))\n",
        "        return artists\n",
        "    \n",
        "    if x_lim is not None:\n",
        "        ax.set_xlim(x_lim)\n",
        "    if y_lim is not None:\n",
        "        ax.set_ylim(y_lim)\n",
        "\n",
        "    anim = animation.FuncAnimation(fig, animate, \n",
        "                                frames=T, interval=50, blit=True)\n",
        "    print(\"Frames created! Displaying animation in output cell...\")\n",
        "    # Note: below is the part which makes it work on Colab\n",
        "    rc('animation', html='jshtml')\n",
        "    return anim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "EUG5JgVpTkC3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Helper for computing posterior marginals\n",
        "def get_emission_matrices(dec_params):\n",
        "    eps = 1e-4\n",
        "    dec_mean_params = dec_params[\"params\"][\"head_mean_fn\"][\"Dense_0\"]\n",
        "    dec_cov_params = dec_params[\"params\"][\"head_log_var_fn\"][\"Dense_0\"]\n",
        "    C_, d_ = dec_mean_params[\"kernel\"].T, dec_mean_params[\"bias\"]\n",
        "    R_ = np.diag(softplus(dec_cov_params[\"bias\"]) + eps)\n",
        "    return { \"C\": C_, \"d\": d_, \"R\": R_ }\n",
        "\n",
        "def get_marginals_and_targets(seed, data, num_points, model, \n",
        "                              past_params, true_model_params):\n",
        "    N, T = data.shape[:2]\n",
        "    rand_sample = jr.permutation(seed, onp.arange(N * T))[:num_points]\n",
        "    trials = rand_sample // T\n",
        "    times = rand_sample % T\n",
        "\n",
        "    # Compute a linear transformation in the latent space that will attempt to \n",
        "    # align the learned posterior to the true posterior\n",
        "    C, d = true_model_params[\"C\"], true_model_params[\"d\"]\n",
        "    CTC = C.T @ C\n",
        "    C_pinv = np.linalg.solve(CTC, C.T)\n",
        "\n",
        "    def align_latents(mus, Sigmas, p):\n",
        "        emissions_matrices = get_emission_matrices(p)\n",
        "        C_, d_ = emissions_matrices[\"C\"], emissions_matrices[\"d\"]\n",
        "        P = C_pinv @ C_\n",
        "        mus = np.einsum(\"ij,nj->ni\", P, mus) + (C_pinv @ (d_ - d))[None,:]\n",
        "        Sigmas = np.einsum(\"ij,njk,kl->nil\", P, Sigmas, P.T)\n",
        "        return mus, Sigmas\n",
        "\n",
        "    def posterior_mean_and_cov(post_params, t):\n",
        "        dist = model.posterior.distribution(post_params)\n",
        "        return dist.mean()[t], dist.covariance()[t]\n",
        "\n",
        "    def gaussian_posterior_mean_and_cov(post_params, t):\n",
        "        dist = true_lds.posterior.distribution(post_params)\n",
        "        return dist.mean()[t], dist.covariance()[t]\n",
        "\n",
        "    index_into_leaves = lambda l: l[trials]\n",
        "    \n",
        "    inf_mus = []\n",
        "    inf_Sigmas = []\n",
        "    tgt_mus = []\n",
        "    tgt_Sigmas = []\n",
        "    \n",
        "    true_lds = LDS(model.prior.latent_dims, T)\n",
        "    true_post_params = vmap(true_lds.e_step, in_axes=(None, 0))\\\n",
        "        (true_model_params, data[trials])\n",
        "    true_mus, true_Sigmas = vmap(gaussian_posterior_mean_and_cov)(true_post_params, times)\n",
        "\n",
        "    # TODO: this is temporary! Only for testing parallel KF!\n",
        "    base = LieParameterizedLinearGaussianChainPrior(model.prior.latent_dims, T)\n",
        "    model_lds = LDS(model.prior.latent_dims, T, base=base)\n",
        "\n",
        "    for i in range(len(past_params)):\n",
        "        model_params = past_params[i]\n",
        "        post_params = jax.tree_util.tree_map(index_into_leaves, \n",
        "                                             model_params[\"post_params\"])\n",
        "        dec_params = model_params[\"dec_params\"]\n",
        "        prior_params = deepcopy(model_params[\"prior_params\"])\n",
        "        # Compute posterior marginals\n",
        "        mus, Sigmas = vmap(posterior_mean_and_cov)(post_params, times)\n",
        "        mus, Sigmas = align_latents(mus, Sigmas, dec_params)\n",
        "        inf_mus.append(mus)\n",
        "        inf_Sigmas.append(Sigmas)\n",
        "\n",
        "        # Infer true posterior under current model params\n",
        "        prior_params.update(get_emission_matrices(dec_params))\n",
        "        tgt_post_params = vmap(model_lds.e_step, in_axes=(None, 0))(prior_params, data[trials])\n",
        "        mus, Sigmas = vmap(gaussian_posterior_mean_and_cov)(tgt_post_params, times)\n",
        "        mus, Sigmas = align_latents(mus, Sigmas, dec_params)\n",
        "\n",
        "        tgt_mus.append(mus)\n",
        "        tgt_Sigmas.append(Sigmas)\n",
        "    return inf_mus, inf_Sigmas, tgt_mus, tgt_Sigmas, true_mus, true_Sigmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "id": "aTYdrgHMdJct"
      },
      "outputs": [],
      "source": [
        "# Trying to figure out the proper way of plotting the projection\n",
        "# Of high dimensional Gaussians\n",
        "# key = jr.split(key)[1]\n",
        "# dim = 5\n",
        "# Q = jr.uniform(key, shape=(dim, dim))\n",
        "# A = Q @ Q.T\n",
        "# L = cholesky(A)\n",
        "# P = random_projection(key_0, dim)\n",
        "# plot_gaussian_2D(np.zeros(dim), A, key_0)\n",
        "# points = jr.normal(key_0, (400, dim))\n",
        "# points /= np.sum(points ** 2, axis=1, keepdims=True) ** .5\n",
        "# points = P @ L @ points.T\n",
        "# plt.scatter(points[0, :], points[1, :], s=1)\n",
        "# u, svs, vt = svd(P @ L)\n",
        "# plt.scatter(u[0][0] * svs[0], u[1][0] * svs[0])\n",
        "# plt.scatter(u[0][1] * svs[1], u[1][1] * svs[1])\n",
        "# # plt.scatter(u[0][0] * np.sqrt(svs[0]), u[1][0] * np.sqrt(svs[0]))\n",
        "# # plt.scatter(u[0][1] * svs[1], u[1][1] * svs[1])\n",
        "# # P = random_projection(key_0, 3)\n",
        "# angles = np.hstack([np.arange(0, 2*np.pi, 0.01), 0])\n",
        "# circle = np.vstack([np.sin(angles), np.cos(angles)])\n",
        "# ellipse = np.dot(u * svs, circle) * 2\n",
        "# plt.plot(ellipse[0,:], ellipse[1,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ImmaouPD-G"
      },
      "source": [
        "## Define training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "id": "cPqoyXb6PgpV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Trainer object \n",
        "from time import time\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    model: a pytree node\n",
        "    loss (key, params, model, data, **train_params) -> (loss, aux)\n",
        "        Returns a loss (a single float) and an auxillary output (e.g. posterior)\n",
        "    init (key, model, data, **train_params) -> (params, opts)\n",
        "        Returns the initial parameters and optimizers to go with those parameters\n",
        "    update (params, grads, opts, model, aux, **train_params) -> (params, opts)\n",
        "        Returns updated parameters, optimizers\n",
        "    \"\"\"\n",
        "    def __init__(self, model, \n",
        "                 train_params=None, \n",
        "                 init=None, \n",
        "                 loss=None, \n",
        "                 val_loss=None,\n",
        "                 update=None,\n",
        "                 initial_params=None):\n",
        "        # Trainer state\n",
        "        self.params = initial_params\n",
        "        self.model = model\n",
        "        self.past_params = []\n",
        "        self.time_spent = []\n",
        "\n",
        "        if train_params is None:\n",
        "            train_params = dict()\n",
        "\n",
        "        self.train_params = train_params\n",
        "\n",
        "        if init is not None:\n",
        "            self.init = init\n",
        "        if loss is not None:\n",
        "            self.loss = loss\n",
        "\n",
        "        self.val_loss = val_loss or self.loss\n",
        "        if update is not None: \n",
        "            self.update = update\n",
        "\n",
        "    # @partial(jit, static_argnums=(0,))\n",
        "    def train_step(self, key, params, data, opt_states):\n",
        "        model = self.model\n",
        "        results = \\\n",
        "            jax.value_and_grad(\n",
        "                lambda params: partial(self.loss, **self.train_params)(key, model, data, params), has_aux=True)(params)\n",
        "        (loss, aux), grads = results\n",
        "        params, opts = self.update(params, grads, self.opts, opt_states, model, aux, **self.train_params)\n",
        "        return params, opts, (loss, aux), grads\n",
        "\n",
        "    # @partial(jit, static_argnums=(0,))\n",
        "    def val_step(self, key, params, data):\n",
        "        return self.val_loss(key, self.model, data, params, **self.train_params)\n",
        "\n",
        "    # def test_step(self, key, params, model, data):\n",
        "    #     loss_out = self.loss(key, params, model, data)\n",
        "    #     return loss_out\n",
        "\n",
        "    \"\"\"\n",
        "    Callback: a function that takes training iterations and relevant parameter\n",
        "        And logs to WandB\n",
        "    \"\"\"\n",
        "    def train(self, data_dict, max_iters, \n",
        "              callback=None, val_callback=None, \n",
        "              summary=None, key=None,\n",
        "              early_stop_start=5000, \n",
        "              max_lose_streak=1000):\n",
        "\n",
        "        if key is None:\n",
        "            key = jr.PRNGKey(0)\n",
        "\n",
        "        model = self.model\n",
        "        train_data = data_dict[\"train_data\"]\n",
        "        batch_size = self.train_params.get(\"batch_size\") or train_data.shape[0]\n",
        "        num_batches = train_data.shape[0] // batch_size\n",
        "\n",
        "        init_key, key = jr.split(key, 2)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.params, self.opts, self.opt_states = self.init(init_key, model, \n",
        "                                                       train_data[:batch_size], \n",
        "                                                       self.params,\n",
        "                                                       **self.train_params)\n",
        "        self.train_losses = []\n",
        "        self.test_losses = []\n",
        "        self.val_losses = []\n",
        "        self.past_params = []\n",
        "\n",
        "        pbar = trange(max_iters)\n",
        "        pbar.set_description(\"[jit compling...]\")\n",
        "        \n",
        "        mask_start = self.train_params.get(\"mask_start\")\n",
        "        if (mask_start):\n",
        "            mask_size = self.train_params[\"mask_size\"]\n",
        "            self.train_params[\"mask_size\"] = 0\n",
        "\n",
        "        train_step = jit(self.train_step)\n",
        "        val_step = jit(self.val_step)\n",
        "\n",
        "        best_loss = None\n",
        "        best_itr = 0\n",
        "        val_loss = None\n",
        "\n",
        "        for itr in pbar:\n",
        "            train_key, val_key, key = jr.split(key, 3)\n",
        "\n",
        "            batch_id = itr % num_batches\n",
        "            batch_start = batch_id * batch_size\n",
        "\n",
        "            t = time()\n",
        "            # Training step\n",
        "            # ----------------------------------------\n",
        "            step_results = train_step(train_key, self.params, \n",
        "                           train_data[batch_start:batch_start+batch_size], self.opt_states)\n",
        "            self.params, self.opt_states, loss_out, grads = \\\n",
        "                jax.tree_map(lambda x: x.block_until_ready(), step_results)\n",
        "            # ----------------------------------------\n",
        "            dt = time() - t\n",
        "            self.time_spent.append(dt)\n",
        "\n",
        "            loss, aux = loss_out\n",
        "            self.train_losses.append(loss)\n",
        "            pbar.set_description(\"LP: {:.3f}\".format(loss))\n",
        "\n",
        "            if batch_id == num_batches - 1:\n",
        "                # We're at the end of an epoch\n",
        "                # We could randomly shuffle the data\n",
        "                # train_data = jr.permutation(key, train_data)\n",
        "                if (self.train_params.get(\"use_validation\")):\n",
        "                    val_loss_out = val_step(val_key, self.params, data_dict[\"val_data\"])\n",
        "                    if (val_callback): val_callback(self, val_loss_out, data_dict)\n",
        "                    val_loss, _ = val_loss_out\n",
        "                    \n",
        "            if not self.train_params.get(\"use_validation\") or val_loss is None:\n",
        "                curr_loss = loss\n",
        "            else:\n",
        "                curr_loss = val_loss\n",
        "\n",
        "            if itr >= early_stop_start:\n",
        "                if best_loss is None or curr_loss < best_loss:\n",
        "                    best_itr = itr\n",
        "                    best_loss = curr_loss\n",
        "                if curr_loss > best_loss and itr - best_itr > max_lose_streak:\n",
        "                    print(\"Early stopping!\")\n",
        "                    break\n",
        "\n",
        "            if (callback): callback(self, loss_out, data_dict, grads)\n",
        "\n",
        "            # Record parameters\n",
        "            record_params = self.train_params.get(\"record_params\")\n",
        "            if record_params and record_params(itr):\n",
        "                curr_params = deepcopy(self.params)\n",
        "                curr_params[\"iteration\"] = itr\n",
        "                self.past_params.append(curr_params)\n",
        "\n",
        "            if (mask_start and itr == mask_start):\n",
        "                self.train_params[\"mask_size\"] = mask_size\n",
        "                train_step = jit(self.train_step)\n",
        "                val_step = jit(self.val_step)\n",
        "\n",
        "        if summary:\n",
        "            summary(self)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "id": "KmTBoMwgArBw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Logging to WandB\n",
        "\n",
        "def visualize_lds(trainer, data_dict, aux):\n",
        "    data = data_dict[\"train_data\"]\n",
        "    true_model_params = data_dict[\"lds_params\"]\n",
        "    model = trainer.model\n",
        "    params = [trainer.params]\n",
        "    num_pts = 10\n",
        "    # We want to visualize the posterior marginals\n",
        "    inf_mus, inf_Sigmas, tgt_mus, tgt_Sigmas, true_mus, true_Sigmas = \\\n",
        "        get_marginals_and_targets(key_0, data, num_pts, model, params, true_model_params)\n",
        "    # Create the axis\n",
        "    fig, ax = plt.subplots()\n",
        "    # Plot each of the groups\n",
        "    tgt_points, tgt_lines = get_artists(ax, tgt_mus[0], tgt_Sigmas[0], key_0, num_pts,\n",
        "                                        alpha=0.3, linestyle=\"dotted\", label=\"current target\")\n",
        "    inf_points, inf_lines = get_artists(ax, inf_mus[0], inf_Sigmas[0], key_0, num_pts, label=\"inferred\")\n",
        "    true_points, true_lines = get_artists(ax, true_mus, true_Sigmas, \n",
        "                                          key_0, num_pts, alpha=0.2, label=\"true target\")\n",
        "    # The legend is too large and blocks most of the plot\n",
        "    # plt.legend()\n",
        "    # Relimit the axes\n",
        "    ax.relim()\n",
        "    # update ax.viewLim using the new dataLim\n",
        "    ax.autoscale_view()\n",
        "    fig.canvas.draw()\n",
        "    img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
        "    post_img = wandb.Image(img, caption=\"Posterior marginals versus targets\")\n",
        "    plt.close()\n",
        "    return {\n",
        "        \"Posterior marginals\": post_img\n",
        "    }\n",
        "\n",
        "def visualize_pendulum(trainer, aux):\n",
        "    # This assumes single sequence has shape (100, 24, 24, 1)\n",
        "    recon = aux[\"reconstruction\"][0][0]\n",
        "    # Show the sequence as a block of images\n",
        "    stacked = recon.reshape(10, 24 * 10, 24)\n",
        "    imgrid = stacked.swapaxes(0, 1).reshape(24 * 10, 24 * 10)\n",
        "    recon_img = wandb.Image(onp.array(imgrid), caption=\"Sample Reconstruction\")\n",
        "\n",
        "    fig = plt.figure()\n",
        "    mask = aux[\"mask\"][0]\n",
        "    post_sample = aux[\"posterior_samples\"][0][0]\n",
        "    top, bot = np.max(post_sample) + 5, np.min(post_sample) - 5\n",
        "    left, right = 0, post_sample.shape[0]\n",
        "    plt.imshow(mask[None], cmap=\"gray\", alpha=.4, vmin=0, vmax=1,\n",
        "               extent=(left, right, top, bot))\n",
        "    plt.plot(post_sample)\n",
        "    fig.canvas.draw()\n",
        "    img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
        "    post_img = wandb.Image(img, caption=\"Posterior Sample\")\n",
        "    plt.close()\n",
        "    return {\n",
        "        \"Reconstruction\": recon_img, \n",
        "        \"Posterior Sample\": post_img\n",
        "    }\n",
        "\n",
        "def get_group_name(run_params):\n",
        "    p = run_params\n",
        "    run_type = \"\" if p[\"inference_method\"] in [\"EM\", \"GT\", \"SMC\"] else \"_\" + p[\"run_type\"]\n",
        "    if p[\"dataset\"] == \"pendulum\":\n",
        "        dataset_summary = \"pendulum\"\n",
        "    elif p[\"dataset\"] == \"lds\":\n",
        "        d = p[\"dataset_params\"]\n",
        "        dataset_summary = \"lds_dims_{}_{}_noises_{}_{}\".format(\n",
        "            d[\"latent_dims\"], d[\"emission_dims\"], \n",
        "            d[\"dynamics_cov\"], d[\"emission_cov\"])\n",
        "    else:\n",
        "        dataset_summary = \"???\"\n",
        "\n",
        "    model_summary = \"_{}d_latent_\".format(p[\"latent_dims\"]) + p[\"inference_method\"]\n",
        "\n",
        "    group_tag = p.get(\"group_tag\") or \"\"\n",
        "    if group_tag != \"\": group_tag += \"_\"\n",
        "\n",
        "    group_name = (group_tag +\n",
        "        dataset_summary\n",
        "        + model_summary\n",
        "        + run_type\n",
        "    )\n",
        "    return group_name\n",
        "\n",
        "def validation_log_to_wandb(trainer, loss_out, data_dict):\n",
        "    p = trainer.train_params\n",
        "    if not p.get(\"log_to_wandb\"): return\n",
        "    \n",
        "    project_name = p[\"project_name\"]\n",
        "    group_name = get_group_name(p)\n",
        "\n",
        "    obj, aux = loss_out\n",
        "    elbo = -obj\n",
        "    kl = np.mean(aux[\"kl\"])\n",
        "    ell = np.mean(aux[\"ell\"])\n",
        "\n",
        "    model = trainer.model\n",
        "    prior = model.prior\n",
        "    D = prior.latent_dims\n",
        "    prior_params = trainer.params[\"prior_params\"]\n",
        "\n",
        "    visualizations = {}\n",
        "    if p[\"dataset\"] == \"pendulum\":\n",
        "        visualizations = visualize_pendulum(trainer, aux)\n",
        "        pred_ll = np.mean(aux[\"prediction_ll\"])\n",
        "        visualizations = {\n",
        "            \"Validation reconstruction\": visualizations[\"Reconstruction\"], \n",
        "            \"Validation posterior sample\": visualizations[\"Posterior Sample\"],\n",
        "            \"Validation prediction log likelihood\": pred_ll\n",
        "        }\n",
        "        \n",
        "    to_log = {\"Validation ELBO\": elbo, \"Validation KL\": kl, \"Validation likelihood\": ell,}\n",
        "    to_log.update(visualizations)\n",
        "    wandb.log(to_log)\n",
        "\n",
        "def log_to_wandb(trainer, loss_out, data_dict, grads):\n",
        "    p = trainer.train_params\n",
        "    if not p.get(\"log_to_wandb\"): return\n",
        "    \n",
        "    project_name = p[\"project_name\"]\n",
        "    group_name = get_group_name(p)\n",
        "\n",
        "    itr = len(trainer.train_losses) - 1\n",
        "    if len(trainer.train_losses) == 1:\n",
        "        wandb.init(project=project_name, group=group_name, config=p)\n",
        "        pprint(p)\n",
        "\n",
        "    obj, aux = loss_out\n",
        "    elbo = -obj\n",
        "    kl = np.mean(aux[\"kl\"])\n",
        "    ell = np.mean(aux[\"ell\"])\n",
        "\n",
        "    model = trainer.model\n",
        "    prior = model.prior\n",
        "    D = prior.latent_dims\n",
        "    prior_params = trainer.params[\"prior_params\"]\n",
        "    if (p.get(\"use_natural_grad\")):\n",
        "        Q = prior_params[\"Q\"]\n",
        "        A = prior_params[\"A\"]\n",
        "    else:\n",
        "        Q = lie_params_to_constrained(prior_params[\"Q\"], D)\n",
        "        A = prior_params[\"A\"]\n",
        "\n",
        "    eigs = eigh(Q)[0]\n",
        "    Q_cond_num = np.max(eigs) / np.min(eigs)\n",
        "    svs = svd(A)[1]\n",
        "    max_sv, min_sv = np.max(svs), np.min(svs)\n",
        "    A_cond_num = max_sv / min_sv\n",
        "\n",
        "    # Also log the prior params gradients\n",
        "    # prior_grads = grads[\"prior_params\"][\"sgd_params\"]\n",
        "    # prior_grads_norm = np.linalg.norm(\n",
        "    #     jax.tree_util.tree_leaves(tree_map(np.linalg.norm, prior_grads)))\n",
        "\n",
        "    visualizations = {}\n",
        "    if (itr % p[\"plot_interval\"] == 0):\n",
        "        if p[\"dataset\"] == \"lds\" and p.get(\"visualize_training\"):\n",
        "            visualizations = visualize_lds(trainer, data_dict, aux)\n",
        "        elif p[\"dataset\"] == \"pendulum\" and p.get(\"visualize_training\"):\n",
        "            visualizations = visualize_pendulum(trainer, aux)\n",
        "\n",
        "        # fig = plt.figure()\n",
        "        # prior_sample = prior.sample(prior_params, shape=(1,), key=jr.PRNGKey(0))[0]\n",
        "        # plt.plot(prior_sample)\n",
        "        # fig.canvas.draw()\n",
        "        # img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
        "        # prior_img = wandb.Image(img, caption=\"Prior Sample\")\n",
        "        # plt.close()\n",
        "        # visualizations[\"Prior sample\"] = prior_img\n",
        "    # Also log the learning rates\n",
        "    lr = p[\"learning_rate\"] \n",
        "    lr = lr if isinstance(lr, float) else lr(itr)\n",
        "    prior_lr = p[\"prior_learning_rate\"] \n",
        "    prior_lr = prior_lr if isinstance(prior_lr, float) else prior_lr(itr)\n",
        "\n",
        "    to_log = { \"ELBO\": elbo, \"KL\": kl, \"Likelihood\": ell, # \"Prior graident norm\": prior_grads_norm,\n",
        "               \"Max singular value of A\": max_sv, \"Min singular value of A\": min_sv,\n",
        "               \"Condition number of A\": A_cond_num, \"Condition number of Q\": Q_cond_num,\n",
        "               \"Learning rate\": lr, \"Prior learning rate\": prior_lr }\n",
        "    to_log.update(visualizations)\n",
        "    wandb.log(to_log)\n",
        "\n",
        "def save_params_to_wandb(trainer):\n",
        "    file_name = \"parameters.pkl\"\n",
        "    with open(file_name, \"wb\") as f:\n",
        "        pkl.dump(trainer.past_params, f)\n",
        "        wandb.save(file_name, policy=\"now\")\n",
        "\n",
        "def on_error(data_dict, model_dict):\n",
        "    save_params_to_wandb(model_dict[\"trainer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "id": "ve7zlI0-P8tP",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Specifics of SVAE training\n",
        "def svae_init(key, model, data, initial_params=None, **train_params):\n",
        "    init_params = model.init(key)\n",
        "    if (initial_params): init_params.update(initial_params)\n",
        "    \n",
        "    if (train_params[\"inference_method\"] == \"planet\"):\n",
        "        init_params[\"rec_params\"] = {\n",
        "            \"rec_params\": init_params[\"rec_params\"],\n",
        "            \"post_params\": init_params[\"post_params\"]\n",
        "        }\n",
        "    # Expand the posterior parameters by batch size\n",
        "    init_params[\"post_params\"] = vmap(lambda _: init_params[\"post_params\"])(data)\n",
        "    init_params[\"post_samples\"] = np.zeros((data.shape[0], \n",
        "                                            train_params.get(\"obj_samples\") or 1) \n",
        "                                             + model.posterior.shape)\n",
        "\n",
        "    learning_rate = train_params[\"learning_rate\"]\n",
        "    rec_opt = opt.adam(learning_rate=learning_rate)\n",
        "    rec_opt_state = rec_opt.init(init_params[\"rec_params\"])\n",
        "    dec_opt = opt.adam(learning_rate=learning_rate)\n",
        "    dec_opt_state = dec_opt.init(init_params[\"dec_params\"])\n",
        "\n",
        "    if (train_params.get(\"use_natural_grad\")):\n",
        "        prior_lr = None\n",
        "        prior_opt = None\n",
        "        prior_opt_state = None\n",
        "    else:\n",
        "        # Add the option of using an gradient optimizer for prior parameters\n",
        "        prior_lr = train_params.get(\"prior_learning_rate\") or learning_rate\n",
        "        prior_opt = opt.adam(learning_rate=prior_lr)\n",
        "        prior_opt_state = prior_opt.init(init_params[\"prior_params\"])\n",
        "\n",
        "    return (init_params, \n",
        "            (rec_opt, dec_opt, prior_opt), \n",
        "            (rec_opt_state, dec_opt_state, prior_opt_state))\n",
        "    \n",
        "def svae_loss(key, model, data_batch, model_params, **train_params):\n",
        "    batch_size = data_batch.shape[0]\n",
        "    # Axes specification for vmap\n",
        "    # We're just going to ignore this for now\n",
        "    params_in_axes = None\n",
        "    # params_in_axes = dict.fromkeys(model_params.keys(), None)\n",
        "    # params_in_axes[\"post_samples\"] = 0\n",
        "    result = vmap(partial(model.compute_objective, **train_params), \n",
        "                  in_axes=(0, 0, params_in_axes))(jr.split(key, batch_size), data_batch, model_params)\n",
        "    objs = result[\"objective\"]\n",
        "    post_params = result[\"posterior_params\"]\n",
        "    post_samples = result[\"posterior_samples\"]\n",
        "    # Need to compute sufficient stats if we want the natural gradient update\n",
        "    if (train_params.get(\"use_natural_grad\")):\n",
        "        post_suff_stats = vmap(model.posterior.sufficient_statistics)(post_params)\n",
        "        expected_post_suff_stats = tree_map(\n",
        "            lambda l: np.mean(l,axis=0), post_suff_stats)\n",
        "        result[\"sufficient_statistics\"] = expected_post_suff_stats\n",
        "    return -np.mean(objs), result\n",
        "\n",
        "def predict_forward(x, A, b, T):\n",
        "    def _step(carry, t):\n",
        "        carry = A @ carry + b\n",
        "        return carry, carry\n",
        "    return scan(_step, x, np.arange(T))[1]\n",
        "\n",
        "# Note: this is for pendulum data only\n",
        "def svae_val_loss(key, model, data_batch, model_params, **train_params):  \n",
        "    N, T = data_batch.shape[:2]\n",
        "    # We only care about the first 100 timesteps\n",
        "    T = T // 2\n",
        "    D = model.prior.latent_dims\n",
        "\n",
        "    # obs_data, pred_data = data_batch[:,:T//2], data_batch[:,T//2:]\n",
        "    obs_data = data_batch[:,:T]\n",
        "    obj, out_dict = svae_loss(key, model, obs_data, model_params, **train_params)\n",
        "    # Compute the prediction accuracy\n",
        "    prior_params = model_params[\"prior_params\"] \n",
        "    # Instead of this, we want to evaluate the expected log likelihood of the future observations\n",
        "    # under the posterior given the current set of observations\n",
        "    # So E_{q(x'|y)}[p(y'|x')] where the primes represent the future\n",
        "    post_params = out_dict[\"posterior_params\"]\n",
        "\n",
        "    posterior = model.posterior.distribution(post_params)\n",
        "    # J = posterior.filtered_precisions\n",
        "    # h = posterior.filtered_linear_potentials\n",
        "    Sigma_filtered = posterior.filtered_covariances # inv(J)\n",
        "    mu_filtered = posterior.filtered_means # np.einsum(\"...ij,...j->...i\", Sigma_filtered, h)\n",
        "    horizon = train_params[\"prediction_horizon\"] or 5\n",
        "\n",
        "    def _prediction_lls(data_id, key):\n",
        "        num_windows = T-horizon-1\n",
        "        pred_lls = vmap(_sliding_window_prediction_lls, in_axes=(None, None, None, 0, 0))(\n",
        "            mu_filtered[data_id], Sigma_filtered[data_id], obs_data[data_id],\n",
        "            np.arange(num_windows), jr.split(key, num_windows))\n",
        "        return pred_lls.mean()\n",
        "\n",
        "    # TODO: change this...!\n",
        "    def _sliding_window_prediction_lls(mu, Sigma, data, t, key):\n",
        "        # Build the posterior object on the future latent states \n",
        "        # (\"the posterior predictive distribution\")\n",
        "        # Convert unconstrained params to constrained dynamics parameters\n",
        "        prior_params_constrained = model.prior.get_dynamics_params(prior_params)\n",
        "        dynamics_params = {\n",
        "            \"m1\": mu[t],\n",
        "            \"Q1\": Sigma[t],\n",
        "            \"A\": prior_params_constrained[\"A\"],\n",
        "            \"b\": prior_params_constrained[\"b\"],\n",
        "            \"Q\": prior_params_constrained[\"Q\"]\n",
        "        }\n",
        "        tridiag_params = dynamics_to_tridiag(dynamics_params, horizon+1, D) # Note the +1\n",
        "        J, L, h = tridiag_params[\"J\"], tridiag_params[\"L\"], tridiag_params[\"h\"]\n",
        "        pred_posterior = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "        # Sample from it and evaluate the log likelihood\n",
        "        x_pred = pred_posterior.sample(seed=key)[1:]\n",
        "        likelihood_dist = model.decoder.apply(model_params[\"dec_params\"], x_pred)\n",
        "        return likelihood_dist.log_prob(\n",
        "            lax.dynamic_slice(data, (t+1, 0, 0, 0), (horizon,) + data.shape[1:])).sum(axis=(1, 2, 3))\n",
        "\n",
        "    # pred_lls = vmap(_prediction_lls)(\n",
        "    #     out_dict[\"posterior_params\"], data_batch, jr.split(key, N))\n",
        "    pred_lls = vmap(_prediction_lls)(np.arange(N), jr.split(key, N))\n",
        "    out_dict[\"prediction_ll\"] = pred_lls\n",
        "    return obj, out_dict\n",
        "\n",
        "def svae_update(params, grads, opts, opt_states, model, aux, **train_params):\n",
        "    rec_opt, dec_opt, prior_opt = opts\n",
        "    rec_opt_state, dec_opt_state, prior_opt_state = opt_states\n",
        "    rec_grad, dec_grad = grads[\"rec_params\"], grads[\"dec_params\"]\n",
        "    updates, rec_opt_state = rec_opt.update(rec_grad, rec_opt_state)\n",
        "    params[\"rec_params\"] = opt.apply_updates(params[\"rec_params\"], updates)\n",
        "    params[\"post_params\"] = aux[\"posterior_params\"]\n",
        "    params[\"post_samples\"] = aux[\"posterior_samples\"]\n",
        "    if train_params[\"run_type\"] == \"model_learning\":\n",
        "        # Update decoder\n",
        "        updates, dec_opt_state = dec_opt.update(dec_grad, dec_opt_state)\n",
        "        params[\"dec_params\"] = opt.apply_updates(params[\"dec_params\"], updates)\n",
        "\n",
        "        old_Q = deepcopy(params[\"prior_params\"][\"Q\"])\n",
        "        old_b = deepcopy(params[\"prior_params\"][\"b\"])\n",
        "\n",
        "        # Update prior parameters\n",
        "        if (train_params.get(\"use_natural_grad\")):\n",
        "            # Here we interpolate the sufficient statistics instead of the parameters\n",
        "            suff_stats = aux[\"sufficient_statistics\"]\n",
        "            lr = params.get(\"prior_learning_rate\") or 1\n",
        "            avg_suff_stats = params[\"prior_params\"][\"avg_suff_stats\"]\n",
        "            # Interpolate the sufficient statistics\n",
        "            params[\"prior_params\"][\"avg_suff_stats\"] = tree_map(lambda x,y : (1 - lr) * x + lr * y, \n",
        "                avg_suff_stats, suff_stats)\n",
        "            params[\"prior_params\"] = model.prior.m_step(params[\"prior_params\"])\n",
        "        else:\n",
        "            updates, prior_opt_state = prior_opt.update(grads[\"prior_params\"], prior_opt_state)\n",
        "            params[\"prior_params\"] = opt.apply_updates(params[\"prior_params\"], updates)\n",
        "        \n",
        "        if (train_params.get(\"constrain_prior\")):\n",
        "            # Revert Q and b to their previous values\n",
        "            params[\"prior_params\"][\"Q\"] = old_Q\n",
        "            params[\"prior_params\"][\"b\"] = old_b\n",
        "\n",
        "        if (train_params.get(\"constrain_dynamics\")):\n",
        "            # Scale A so that its maximum singular value does not exceed 1\n",
        "            params[\"prior_params\"][\"A\"] = truncate_singular_values(params[\"prior_params\"][\"A\"])\n",
        "            # params[\"prior_params\"][\"A\"] = scale_singular_values(params[\"prior_params\"][\"A\"])\n",
        "\n",
        "    return params, (rec_opt_state, dec_opt_state, prior_opt_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {
        "id": "8AdW1WXA4n1U"
      },
      "outputs": [],
      "source": [
        "# @title Model initialization and trainer\n",
        "def init_model(run_params, data_dict):\n",
        "    p = deepcopy(run_params)\n",
        "    d = p[\"dataset_params\"]\n",
        "    latent_dims = p[\"latent_dims\"]\n",
        "    input_shape = data_dict[\"train_data\"].shape[1:]\n",
        "    num_timesteps = input_shape[0]\n",
        "    data = data_dict[\"train_data\"]\n",
        "    seed = p[\"seed\"]\n",
        "    seed_model, seed_elbo, seed_ems, seed_rec = jr.split(seed, 4)\n",
        "\n",
        "    run_type = p[\"run_type\"]\n",
        "    recnet_class = globals()[p[\"recnet_class\"]]\n",
        "    decnet_class = globals()[p[\"decnet_class\"]]\n",
        "\n",
        "    if p[\"inference_method\"] == \"dkf\":\n",
        "        posterior = DKFPosterior(latent_dims, num_timesteps)\n",
        "    elif p[\"inference_method\"] == \"cdkf\":\n",
        "        posterior = CDKFPosterior(latent_dims, num_timesteps)\n",
        "    elif p[\"inference_method\"] == \"planet\":\n",
        "        posterior = PlaNetPosterior(p[\"posterior_architecture\"],\n",
        "                                    latent_dims, num_timesteps)\n",
        "    elif p[\"inference_method\"] == \"svae\":\n",
        "        # The parallel Kalman stuff only applies to SVAE\n",
        "        # Since RNN based methods are inherently sequential\n",
        "        posterior = LDSSVAEPosterior(latent_dims, num_timesteps, \n",
        "                                     use_parallel=p.get(\"use_parallel_kf\"))\n",
        "        \n",
        "    rec_net = recnet_class.from_params(**p[\"recnet_architecture\"])\n",
        "    dec_net = decnet_class.from_params(**p[\"decnet_architecture\"])\n",
        "    if p[\"inference_method\"] == \"planet\":\n",
        "        # Wrap the recognition network\n",
        "        rec_net = PlaNetRecognitionWrapper(rec_net)\n",
        "\n",
        "    if (p.get(\"use_natural_grad\")):\n",
        "        prior = LinearGaussianChainPrior(latent_dims, num_timesteps)\n",
        "    else:\n",
        "        prior = LieParameterizedLinearGaussianChainPrior(latent_dims, num_timesteps)\n",
        "\n",
        "    model = DeepLDS(\n",
        "        recognition=rec_net,\n",
        "        decoder=dec_net,\n",
        "        prior=prior,\n",
        "        posterior=posterior,\n",
        "        input_dummy=np.zeros(input_shape),\n",
        "        latent_dummy=np.zeros((num_timesteps, latent_dims))\n",
        "    )\n",
        "    \n",
        "    # TODO: Let's get the full linear version working first before moving on\n",
        "    # assert(run_params[\"run_type\"] == \"full_linear\")\n",
        "    if (run_type == \"inference_only\"):\n",
        "        p = data_dict[\"lds_params\"]\n",
        "        prior_params = { \"A\": p[\"A\"], \"b\": p[\"b\"], \n",
        "                        \"Q\": p[\"Q\"], \"m1\": p[\"m1\"], \"Q1\": p[\"Q1\"],\n",
        "                        \"avg_suff_stats\": p[\"avg_suff_stats\"]}\n",
        "        dec_params = fd.FrozenDict(\n",
        "            {\n",
        "                \"params\": {\n",
        "                    \"head_log_var_fn\":{\n",
        "                        \"Dense_0\":{\n",
        "                            \"bias\": inv_softplus(np.diag(p[\"R\"])),\n",
        "                            \"kernel\": np.zeros_like(p[\"C\"]).T\n",
        "                        }\n",
        "                    },\n",
        "                    \"head_mean_fn\":{\n",
        "                        \"Dense_0\":{\n",
        "                            \"bias\": p[\"d\"],\n",
        "                            \"kernel\": p[\"C\"].T\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            })\n",
        "        initial_params = { \"prior_params\": prior_params, \"dec_params\": dec_params }\n",
        "    else:\n",
        "        initial_params = None\n",
        "\n",
        "    # emission_params = emission.init(seed_ems, np.ones((num_latent_dims,)))\n",
        "    # Define the trainer object here\n",
        "    trainer = Trainer(model, train_params=run_params, init=svae_init, \n",
        "                      loss=svae_loss, \n",
        "                      val_loss=svae_val_loss, \n",
        "                      update=svae_update, initial_params=initial_params)\n",
        "\n",
        "    return {\n",
        "        # We don't actually need to include model here\n",
        "        # 'cause it's included in the trainer object\n",
        "        \"model\": model,\n",
        "        # \"emission_params\": emission_params\n",
        "        \"trainer\": trainer\n",
        "    }\n",
        "\n",
        "def start_trainer(model_dict, data_dict, run_params):\n",
        "    trainer = model_dict[\"trainer\"]\n",
        "    summary = save_params_to_wandb if run_params.get(\"log_to_wandb\") else None\n",
        "    trainer.train(data_dict,\n",
        "                  max_iters=run_params[\"max_iters\"],\n",
        "                  key=run_params[\"seed\"],\n",
        "                  callback=log_to_wandb, val_callback=validation_log_to_wandb,\n",
        "                  summary=summary)\n",
        "    return (trainer.model, trainer.params, trainer.train_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NSj6XUpPZRG"
      },
      "source": [
        "## Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "ZeG1j7HgqWya",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Sample from LDS\n",
        "def sample_lds_dataset(run_params):    \n",
        "    d = run_params[\"dataset_params\"]\n",
        "    \n",
        "    global data_dict\n",
        "    if data_dict is not None \\\n",
        "        and \"dataset_params\" in data_dict \\\n",
        "        and str(data_dict[\"dataset_params\"]) == str(fd.freeze(d)):\n",
        "        print(\"Using existing data.\")\n",
        "        print(\"Data MLL: \", data_dict[\"marginal_log_likelihood\"])\n",
        "        \n",
        "        # return jax.device_put(data_dict, jax.devices(\"gpu\")[0])\n",
        "        return data_dict\n",
        "\n",
        "    data_dict = {}\n",
        "\n",
        "    seed = d[\"seed\"]\n",
        "    emission_dims = d[\"emission_dims\"]\n",
        "    latent_dims = d[\"latent_dims\"]\n",
        "    emission_cov = d[\"emission_cov\"]\n",
        "    dynamics_cov = d[\"dynamics_cov\"]\n",
        "    num_timesteps = d[\"num_timesteps\"]\n",
        "    num_trials = d[\"num_trials\"]\n",
        "    seed_m1, seed_C, seed_d, seed_A, seed_sample = jr.split(seed, 5)\n",
        "\n",
        "    R = emission_cov * np.eye(emission_dims)\n",
        "    Q = dynamics_cov * np.eye(latent_dims)\n",
        "    C = jr.normal(seed_C, shape=(emission_dims, latent_dims))\n",
        "    d = jr.normal(seed_d, shape=(emission_dims,))\n",
        "\n",
        "    # Here we let Q1 = Q\n",
        "    lds = LDS(latent_dims, num_timesteps)\n",
        "    \n",
        "    params = {\n",
        "            \"m1\": jr.normal(key=seed_m1, shape=(latent_dims,)),\n",
        "            \"Q1\": Q,\n",
        "            \"Q\": Q,\n",
        "            \"A\": random_rotation(seed_A, latent_dims, theta=np.pi/20),\n",
        "            \"b\": np.zeros(latent_dims),\n",
        "            \"R\": R,\n",
        "            \"C\": C,\n",
        "            \"d\": d,\n",
        "        }\n",
        "    constrained = lds.get_constrained_params(params)\n",
        "    params[\"avg_suff_stats\"] = { \"Ex\": constrained[\"Ex\"], \n",
        "                                \"ExxT\": constrained[\"ExxT\"], \n",
        "                                \"ExnxT\": constrained[\"ExnxT\"] }\n",
        "\n",
        "    states, data = lds.sample(params, \n",
        "                              shape=(num_trials,), \n",
        "                              key=seed_sample)\n",
        "    \n",
        "    mll = vmap(lds.marginal_log_likelihood, in_axes=(None, 0))(params, data)\n",
        "    mll = np.mean(mll, axis=0)\n",
        "    print(\"Data MLL: \", mll)\n",
        "    \n",
        "    seed_val, _ = jr.split(seed_sample)\n",
        "    val_states, val_data = lds.sample(params, \n",
        "                              shape=(num_trials,), \n",
        "                              key=seed_val)\n",
        "\n",
        "    # data_dict[\"generative_model\"] = lds\n",
        "    data_dict[\"marginal_log_likelihood\"] = mll\n",
        "    data_dict[\"train_data\"] = data\n",
        "    data_dict[\"train_states\"] = states\n",
        "    data_dict[\"val_data\"] = val_data\n",
        "    data_dict[\"val_states\"] = val_states\n",
        "    data_dict[\"dataset_params\"] = fd.freeze(run_params[\"dataset_params\"])\n",
        "    data_dict[\"lds_params\"] = params\n",
        "    return data_dict\n",
        "    # return jax.device_put(data_dict, jax.devices(\"gpu\")[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "cellView": "form",
        "id": "MKj_UmfS6JdM"
      },
      "outputs": [],
      "source": [
        "# @title Testing the correctness of LDS m-step\n",
        "# seed = jr.PRNGKey(0)\n",
        "# emission_dims = 5\n",
        "# latent_dims = 3\n",
        "# emission_cov = 10.\n",
        "# dynamics_cov = .1\n",
        "# num_timesteps = 100\n",
        "# seed, seed_C, seed_d, seed_sample = jr.split(seed, 4)\n",
        "# R = emission_cov * np.eye(emission_dims)\n",
        "# Q = dynamics_cov * np.eye(latent_dims)\n",
        "# C = jr.normal(seed_C, shape=(emission_dims, latent_dims))\n",
        "# d = jr.normal(seed_d, shape=(emission_dims,))\n",
        "# # Here we let Q1 = Q\n",
        "# lds = LDS(latent_dims, num_timesteps)\n",
        "# params = lds.init(seed)\n",
        "# params.update(\n",
        "#     {\n",
        "#         \"Q1\": Q,\n",
        "#         \"Q\": Q,\n",
        "#         \"R\": R,\n",
        "#         \"C\": C,\n",
        "#         \"d\": d,\n",
        "#     }\n",
        "# )\n",
        "# states, data = lds.sample(params, \n",
        "#                           shape=(50,), \n",
        "#                           key=seed_sample)\n",
        "# post_params = vmap(lds.e_step, in_axes=(None, 0))(params, data)\n",
        "# posterior = LinearGaussianChainPosterior(latent_dims, num_timesteps)\n",
        "# suff_stats = vmap(posterior.sufficient_statistics)(post_params)\n",
        "# expected_suff_stats = jax.tree_util.tree_map(\n",
        "#         lambda l: np.mean(l,axis=0), suff_stats)\n",
        "# inferred_params = lds.m_step(params, expected_suff_stats)\n",
        "# for key in [\"m1\", \"Q1\", \"A\", \"Q\", \"b\"]:\n",
        "#     print(inferred_params[key] - params[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "cellView": "form",
        "id": "Qzp5Fb0CGtqk"
      },
      "outputs": [],
      "source": [
        "# @title Code for the pendulum dataset (~128 mb)\n",
        "\n",
        "\n",
        "# Modeling Irregular Time Series with Continuous Recurrent Units (CRUs)\n",
        "# Copyright (c) 2022 Robert Bosch GmbH\n",
        "# This program is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Affero General Public License as published\n",
        "# by the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "#\n",
        "# This program is distributed in the hope that it will be useful,\n",
        "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "# GNU Affero General Public License for more details.\n",
        "#\n",
        "# You should have received a copy of the GNU Affero General Public License\n",
        "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
        "#\n",
        "# This source code is derived from Pytorch RKN Implementation (https://github.com/ALRhub/rkn_share)\n",
        "# Copyright (c) 2021 Philipp Becker (Autonomous Learning Robots Lab @ KIT)\n",
        "# licensed under MIT License\n",
        "# cf. 3rd-party-licenses.txt file in the root directory of this source tree.\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "def add_img_noise(imgs, first_n_clean, random, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75, t_uu=1.0):\n",
        "    \"\"\"\n",
        "    :param imgs: Images to add noise to\n",
        "    :param first_n_clean: Keep first_n_images clean to allow the filter to burn in\n",
        "    :param random: np.random.RandomState used for sampling\n",
        "    :param r: \"correlation (over time) factor\" the smaller the more the noise is correlated\n",
        "    :param t_ll: lower bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_lu: upper bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_ul: lower bound of the interval the upper bound for each sequence is sampled from\n",
        "    :param t_uu: upper bound of the interval the upper bound for each sequence is sampled from\n",
        "    :return: noisy images, factors used to create them\n",
        "    \"\"\"\n",
        "\n",
        "    assert t_ll <= t_lu <= t_ul <= t_uu, \"Invalid bounds for noise generation\"\n",
        "    if len(imgs.shape) < 5:\n",
        "        imgs = onp.expand_dims(imgs, -1)\n",
        "    batch_size, seq_len = imgs.shape[:2]\n",
        "    factors = onp.zeros([batch_size, seq_len])\n",
        "    factors[:, 0] = random.uniform(low=0.0, high=1.0, size=batch_size)\n",
        "    for i in range(seq_len - 1):\n",
        "        factors[:, i + 1] = onp.clip(factors[:, i] + random.uniform(\n",
        "            low=-r, high=r, size=batch_size), a_min=0.0, a_max=1.0)\n",
        "\n",
        "    t1 = random.uniform(low=t_ll, high=t_lu, size=(batch_size, 1))\n",
        "    t2 = random.uniform(low=t_ul, high=t_uu, size=(batch_size, 1))\n",
        "\n",
        "    factors = (factors - t1) / (t2 - t1)\n",
        "    factors = onp.clip(factors, a_min=0.0, a_max=1.0)\n",
        "    factors = onp.reshape(factors, list(factors.shape) + [1, 1, 1])\n",
        "    factors[:, :first_n_clean] = 1.0\n",
        "    noisy_imgs = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        if imgs.dtype == onp.uint8:\n",
        "            noise = random.uniform(low=0.0, high=255, size=imgs.shape[1:])\n",
        "            noisy_imgs.append(\n",
        "                (factors[i] * imgs[i] + (1 - factors[i]) * noise).astype(onp.uint8))\n",
        "        else:\n",
        "            noise = random.uniform(low=0.0, high=1.1, size=imgs.shape[1:])\n",
        "            noisy_imgs.append(factors[i] * imgs[i] + (1 - factors[i]) * noise)\n",
        "\n",
        "    return onp.squeeze(onp.concatenate([onp.expand_dims(n, 0) for n in noisy_imgs], 0)), factors\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "def add_img_noise4(imgs, first_n_clean, random, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75, t_uu=1.0):\n",
        "    \"\"\"\n",
        "    :param imgs: Images to add noise to\n",
        "    :param first_n_clean: Keep first_n_images clean to allow the filter to burn in\n",
        "    :param random: np.random.RandomState used for sampling\n",
        "    :param r: \"correlation (over time) factor\" the smaller the more the noise is correlated\n",
        "    :param t_ll: lower bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_lu: upper bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_ul: lower bound of the interval the upper bound for each sequence is sampled from\n",
        "    :param t_uu: upper bound of the interval the upper bound for each sequence is sampled from\n",
        "    :return: noisy images, factors used to create them\n",
        "    \"\"\"\n",
        "\n",
        "    half_x = int(imgs.shape[2] / 2)\n",
        "    half_y = int(imgs.shape[3] / 2)\n",
        "    assert t_ll <= t_lu <= t_ul <= t_uu, \"Invalid bounds for noise generation\"\n",
        "    if len(imgs.shape) < 5:\n",
        "        imgs = np.expand_dims(imgs, -1)\n",
        "    batch_size, seq_len = imgs.shape[:2]\n",
        "    factors = np.zeros([batch_size, seq_len, 4])\n",
        "    factors[:, 0] = random.uniform(low=0.0, high=1.0, size=(batch_size, 4))\n",
        "    for i in range(seq_len - 1):\n",
        "        factors[:, i + 1] = np.clip(factors[:, i] + random.uniform(\n",
        "            low=-r, high=r, size=(batch_size, 4)), a_min=0.0, a_max=1.0)\n",
        "\n",
        "    t1 = random.uniform(low=t_ll, high=t_lu, size=(batch_size, 1, 4))\n",
        "    t2 = random.uniform(low=t_ul, high=t_uu, size=(batch_size, 1, 4))\n",
        "\n",
        "    factors = (factors - t1) / (t2 - t1)\n",
        "    factors = np.clip(factors, a_min=0.0, a_max=1.0)\n",
        "    factors = np.reshape(factors, list(factors.shape) + [1, 1, 1])\n",
        "    factors[:, :first_n_clean] = 1.0\n",
        "    noisy_imgs = []\n",
        "    qs = []\n",
        "    for i in range(batch_size):\n",
        "        if imgs.dtype == np.uint8:\n",
        "            qs.append(detect_pendulums(imgs[i], half_x, half_y))\n",
        "            noise = random.uniform(low=0.0, high=255, size=[\n",
        "                                   4, seq_len, half_x, half_y, imgs.shape[-1]]).astype(np.uint8)\n",
        "            curr = np.zeros(imgs.shape[1:], dtype=np.uint8)\n",
        "            curr[:, :half_x, :half_y] = (factors[i, :, 0] * imgs[i, :, :half_x, :half_y] + (\n",
        "                1 - factors[i, :, 0]) * noise[0]).astype(np.uint8)\n",
        "            curr[:, :half_x, half_y:] = (factors[i, :, 1] * imgs[i, :, :half_x, half_y:] + (\n",
        "                1 - factors[i, :, 1]) * noise[1]).astype(np.uint8)\n",
        "            curr[:, half_x:, :half_y] = (factors[i, :, 2] * imgs[i, :, half_x:, :half_y] + (\n",
        "                1 - factors[i, :, 2]) * noise[2]).astype(np.uint8)\n",
        "            curr[:, half_x:, half_y:] = (factors[i, :, 3] * imgs[i, :, half_x:, half_y:] + (\n",
        "                1 - factors[i, :, 3]) * noise[3]).astype(np.uint8)\n",
        "        else:\n",
        "            noise = random.uniform(low=0.0, high=1.0, size=[\n",
        "                                   4, seq_len, half_x, half_y, imgs.shape[-1]])\n",
        "            curr = np.zeros(imgs.shape[1:])\n",
        "            curr[:, :half_x, :half_y] = factors[i, :, 0] * imgs[i, :,\n",
        "                                                                :half_x, :half_y] + (1 - factors[i, :, 0]) * noise[0]\n",
        "            curr[:, :half_x, half_y:] = factors[i, :, 1] * imgs[i, :,\n",
        "                                                                :half_x, half_y:] + (1 - factors[i, :, 1]) * noise[1]\n",
        "            curr[:, half_x:, :half_y] = factors[i, :, 2] * imgs[i, :,\n",
        "                                                                half_x:, :half_y] + (1 - factors[i, :, 2]) * noise[2]\n",
        "            curr[:, half_x:, half_y:] = factors[i, :, 3] * imgs[i, :,\n",
        "                                                                half_x:, half_y:] + (1 - factors[i, :, 3]) * noise[3]\n",
        "        noisy_imgs.append(curr)\n",
        "\n",
        "    factors_ext = np.concatenate([np.squeeze(factors), np.zeros(\n",
        "        [factors.shape[0], factors.shape[1], 1])], -1)\n",
        "    q = np.concatenate([np.expand_dims(q, 0) for q in qs], 0)\n",
        "    f = np.zeros(q.shape)\n",
        "    for i in range(f.shape[0]):\n",
        "        for j in range(f.shape[1]):\n",
        "            for k in range(3):\n",
        "                f[i, j, k] = factors_ext[i, j, q[i, j, k]]\n",
        "\n",
        "    return np.squeeze(np.concatenate([np.expand_dims(n, 0) for n in noisy_imgs], 0)), f\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "def detect_pendulums(imgs, half_x, half_y):\n",
        "    qs = [imgs[:, :half_x, :half_y], imgs[:, :half_x, half_y:],\n",
        "          imgs[:, half_x:, :half_y], imgs[:, half_x:, half_y:]]\n",
        "\n",
        "    r_cts = np.array(\n",
        "        [np.count_nonzero(q[:, :, :, 0] > 5, axis=(-1, -2)) for q in qs]).T\n",
        "    g_cts = np.array(\n",
        "        [np.count_nonzero(q[:, :, :, 1] > 5, axis=(-1, -2)) for q in qs]).T\n",
        "    b_cts = np.array(\n",
        "        [np.count_nonzero(q[:, :, :, 2] > 5, axis=(-1, -2)) for q in qs]).T\n",
        "\n",
        "    cts = np.concatenate([np.expand_dims(c, 1)\n",
        "                         for c in [r_cts, g_cts, b_cts]], 1)\n",
        "\n",
        "    q_max = np.max(cts, -1)\n",
        "    q = np.argmax(cts, -1)\n",
        "    q[q_max < 10] = 4\n",
        "    return q\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "class Pendulum:\n",
        "\n",
        "    MAX_VELO_KEY = 'max_velo'\n",
        "    MAX_TORQUE_KEY = 'max_torque'\n",
        "    MASS_KEY = 'mass'\n",
        "    LENGTH_KEY = 'length'\n",
        "    GRAVITY_KEY = 'g'\n",
        "    FRICTION_KEY = 'friction'\n",
        "    DT_KEY = 'dt'\n",
        "    SIM_DT_KEY = 'sim_dt'\n",
        "    TRANSITION_NOISE_TRAIN_KEY = 'transition_noise_train'\n",
        "    TRANSITION_NOISE_TEST_KEY = 'transition_noise_test'\n",
        "\n",
        "    OBSERVATION_MODE_LINE = \"line\"\n",
        "    OBSERVATION_MODE_BALL = \"ball\"\n",
        "\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def __init__(self,\n",
        "                 img_size,\n",
        "                 observation_mode,\n",
        "                 generate_actions=False,\n",
        "                 transition_noise_std=0.0,\n",
        "                 observation_noise_std=0.0,\n",
        "                 pendulum_params=None,\n",
        "                 seed=0):\n",
        "\n",
        "        assert observation_mode == Pendulum.OBSERVATION_MODE_BALL or observation_mode == Pendulum.OBSERVATION_MODE_LINE\n",
        "        # Global Parameters\n",
        "        self.state_dim = 2\n",
        "        self.action_dim = 1\n",
        "        self.img_size = img_size\n",
        "        self.observation_dim = img_size ** 2\n",
        "        self.observation_mode = observation_mode\n",
        "\n",
        "        self.random = onp.random.RandomState(seed)\n",
        "\n",
        "        # image parameters\n",
        "        self.img_size_internal = 128\n",
        "        self.x0 = self.y0 = 64\n",
        "        self.plt_length = 55 if self.observation_mode == Pendulum.OBSERVATION_MODE_LINE else 50\n",
        "        self.plt_width = 8\n",
        "\n",
        "        self.generate_actions = generate_actions\n",
        "\n",
        "        # simulation parameters\n",
        "        if pendulum_params is None:\n",
        "            pendulum_params = self.pendulum_default_params()\n",
        "        self.max_velo = pendulum_params[Pendulum.MAX_VELO_KEY]\n",
        "        self.max_torque = pendulum_params[Pendulum.MAX_TORQUE_KEY]\n",
        "        self.dt = pendulum_params[Pendulum.DT_KEY]\n",
        "        self.mass = pendulum_params[Pendulum.MASS_KEY]\n",
        "        self.length = pendulum_params[Pendulum.LENGTH_KEY]\n",
        "        self.inertia = self.mass * self.length**2 / 3\n",
        "        self.g = pendulum_params[Pendulum.GRAVITY_KEY]\n",
        "        self.friction = pendulum_params[Pendulum.FRICTION_KEY]\n",
        "        self.sim_dt = pendulum_params[Pendulum.SIM_DT_KEY]\n",
        "\n",
        "        self.observation_noise_std = observation_noise_std\n",
        "        self.transition_noise_std = transition_noise_std\n",
        "\n",
        "        self.tranisition_covar_mat = onp.diag(\n",
        "            np.array([1e-8, self.transition_noise_std**2, 1e-8, 1e-8]))\n",
        "        self.observation_covar_mat = onp.diag(\n",
        "            [self.observation_noise_std**2, self.observation_noise_std**2])\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def sample_data_set(self, num_episodes, episode_length, full_targets):\n",
        "        states = onp.zeros((num_episodes, episode_length, self.state_dim))\n",
        "        actions = self._sample_action(\n",
        "            (num_episodes, episode_length, self.action_dim))\n",
        "        states[:, 0, :] = self._sample_init_state(num_episodes)\n",
        "        t = onp.zeros((num_episodes, episode_length))\n",
        "\n",
        "        for i in range(1, episode_length):\n",
        "            states[:, i, :], dt = self._get_next_states(\n",
        "                states[:, i - 1, :], actions[:, i - 1, :])\n",
        "            t[:, i:] += dt\n",
        "        states[..., 0] -= onp.pi\n",
        "\n",
        "        if self.observation_noise_std > 0.0:\n",
        "            observation_noise = self.random.normal(loc=0.0,\n",
        "                                                   scale=self.observation_noise_std,\n",
        "                                                   size=states.shape)\n",
        "        else:\n",
        "            observation_noise = onp.zeros(states.shape)\n",
        "\n",
        "        targets = self.pendulum_kinematic(states)\n",
        "        if self.observation_mode == Pendulum.OBSERVATION_MODE_LINE:\n",
        "            noisy_states = states + observation_noise\n",
        "            noisy_targets = self.pendulum_kinematic(noisy_states)\n",
        "        elif self.observation_mode == Pendulum.OBSERVATION_MODE_BALL:\n",
        "            noisy_targets = targets + observation_noise\n",
        "        imgs = self._generate_images(noisy_targets[..., :2])\n",
        "\n",
        "        return imgs, targets[..., :(4 if full_targets else 2)], states, noisy_targets[..., :(4 if full_targets else 2)], t/self.dt\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    @staticmethod\n",
        "    def pendulum_default_params():\n",
        "        return {\n",
        "            Pendulum.MAX_VELO_KEY: 8,\n",
        "            Pendulum.MAX_TORQUE_KEY: 10,\n",
        "            Pendulum.MASS_KEY: 1,\n",
        "            Pendulum.LENGTH_KEY: 1,\n",
        "            Pendulum.GRAVITY_KEY: 9.81,\n",
        "            Pendulum.FRICTION_KEY: 0,\n",
        "            Pendulum.DT_KEY: 0.05,\n",
        "            Pendulum.SIM_DT_KEY: 1e-4\n",
        "        }\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _sample_action(self, shape):\n",
        "        if self.generate_actions:\n",
        "            return self.random.uniform(-self.max_torque, self.max_torque, shape)\n",
        "        else:\n",
        "            return np.zeros(shape=shape)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _transition_function(self, states, actions):\n",
        "        dt = self.dt\n",
        "        n_steps = dt / self.sim_dt\n",
        "\n",
        "        if n_steps != np.round(n_steps):\n",
        "            #print(n_steps, 'Warning from Pendulum: dt does not match up')\n",
        "            n_steps = np.round(n_steps)\n",
        "\n",
        "        c = self.g * self.length * self.mass / self.inertia\n",
        "        for i in range(0, int(n_steps)):\n",
        "            velNew = states[..., 1:2] + self.sim_dt * (c * np.sin(states[..., 0:1])\n",
        "                                                       + actions / self.inertia\n",
        "                                                       - states[..., 1:2] * self.friction)\n",
        "            states = onp.concatenate(\n",
        "                (states[..., 0:1] + self.sim_dt * velNew, velNew), axis=1)\n",
        "        return states, dt\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _get_next_states(self, states, actions):\n",
        "        actions = np.maximum(-self.max_torque,\n",
        "                             np.minimum(actions, self.max_torque))\n",
        "\n",
        "        states, dt = self._transition_function(states, actions)\n",
        "        if self.transition_noise_std > 0.0:\n",
        "            states[:, 1] += self.random.normal(loc=0.0,\n",
        "                                               scale=self.transition_noise_std,\n",
        "                                               size=[len(states)])\n",
        "\n",
        "        states[:, 0] = ((states[:, 0]) % (2 * np.pi))\n",
        "        return states, dt\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def get_ukf_smothing(self, obs):\n",
        "        batch_size, seq_length = obs.shape[:2]\n",
        "        succ = np.zeros(batch_size, dtype=np.bool)\n",
        "        means = np.zeros([batch_size, seq_length, 4])\n",
        "        covars = np.zeros([batch_size, seq_length, 4, 4])\n",
        "        fail_ct = 0\n",
        "        for i in range(batch_size):\n",
        "            if i % 10 == 0:\n",
        "                print(i)\n",
        "            try:\n",
        "                means[i], covars[i] = self.ukf.filter(obs[i])\n",
        "                succ[i] = True\n",
        "            except:\n",
        "                fail_ct += 1\n",
        "        print(fail_ct / batch_size, \"failed\")\n",
        "\n",
        "        return means[succ], covars[succ], succ\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _sample_init_state(self, nr_epochs):\n",
        "        return onp.concatenate((self.random.uniform(0, 2 * np.pi, (nr_epochs, 1)), np.zeros((nr_epochs, 1))), 1)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def add_observation_noise(self, imgs, first_n_clean, r=0.2, t_ll=0.1, t_lu=0.4, t_ul=0.6, t_uu=0.9):\n",
        "        return add_img_noise(imgs, first_n_clean, self.random, r, t_ll, t_lu, t_ul, t_uu)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _get_task_space_pos(self, joint_states):\n",
        "        task_space_pos = onp.zeros(list(joint_states.shape[:-1]) + [2])\n",
        "        task_space_pos[..., 0] = np.sin(joint_states[..., 0]) * self.length\n",
        "        task_space_pos[..., 1] = np.cos(joint_states[..., 0]) * self.length\n",
        "        return task_space_pos\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _generate_images(self, ts_pos):\n",
        "        imgs = onp.zeros(shape=list(ts_pos.shape)[\n",
        "                        :-1] + [self.img_size, self.img_size], dtype=np.uint8)\n",
        "        for seq_idx in range(ts_pos.shape[0]):\n",
        "            for idx in range(ts_pos.shape[1]):\n",
        "                imgs[seq_idx, idx] = self._generate_single_image(\n",
        "                    ts_pos[seq_idx, idx])\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _generate_single_image(self, pos):\n",
        "        x1 = pos[0] * (self.plt_length / self.length) + self.x0\n",
        "        y1 = pos[1] * (self.plt_length / self.length) + self.y0\n",
        "        img = Image.new('F', (self.img_size_internal,\n",
        "                        self.img_size_internal), 0.0)\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        if self.observation_mode == Pendulum.OBSERVATION_MODE_LINE:\n",
        "            draw.line([(self.x0, self.y0), (x1, y1)],\n",
        "                      fill=1.0, width=self.plt_width)\n",
        "        elif self.observation_mode == Pendulum.OBSERVATION_MODE_BALL:\n",
        "            x_l = x1 - self.plt_width\n",
        "            x_u = x1 + self.plt_width\n",
        "            y_l = y1 - self.plt_width\n",
        "            y_u = y1 + self.plt_width\n",
        "            draw.ellipse((x_l, y_l, x_u, y_u), fill=1.0)\n",
        "\n",
        "        img = img.resize((self.img_size, self.img_size),\n",
        "                         resample=Image.ANTIALIAS)\n",
        "        img_as_array = onp.asarray(img)\n",
        "        img_as_array = onp.clip(img_as_array, 0, 1)\n",
        "        return 255.0 * img_as_array\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _kf_transition_function(self, state, noise):\n",
        "        nSteps = self.dt / self.sim_dt\n",
        "\n",
        "        if nSteps != np.round(nSteps):\n",
        "            print('Warning from Pendulum: dt does not match up')\n",
        "            nSteps = np.round(nSteps)\n",
        "\n",
        "        c = self.g * self.length * self.mass / self.inertia\n",
        "        for i in range(0, int(nSteps)):\n",
        "            velNew = state[1] + self.sim_dt * \\\n",
        "                (c * np.sin(state[0]) - state[1] * self.friction)\n",
        "            state = onp.array([state[0] + self.sim_dt * velNew, velNew])\n",
        "        state[0] = state[0] % (2 * np.pi)\n",
        "        state[1] = state[1] + noise[1]\n",
        "        return state\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def pendulum_kinematic_single(self, js):\n",
        "        theta, theat_dot = js\n",
        "        x = np.sin(theta)\n",
        "        y = np.cos(theta)\n",
        "        x_dot = theat_dot * y\n",
        "        y_dot = theat_dot * -x\n",
        "        return onp.array([x, y, x_dot, y_dot]) * self.length\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def pendulum_kinematic(self, js_batch):\n",
        "        theta = js_batch[..., :1]\n",
        "        theta_dot = js_batch[..., 1:]\n",
        "        x = np.sin(theta)\n",
        "        y = np.cos(theta)\n",
        "        x_dot = theta_dot * y\n",
        "        y_dot = theta_dot * -x\n",
        "        return onp.concatenate([x, y, x_dot, y_dot], axis=-1)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def inverse_pendulum_kinematics(self, ts_batch):\n",
        "        x = ts_batch[..., :1]\n",
        "        y = ts_batch[..., 1:2]\n",
        "        x_dot = ts_batch[..., 2:3]\n",
        "        y_dot = ts_batch[..., 3:]\n",
        "        val = x / y\n",
        "        theta = np.arctan2(x, y)\n",
        "        theta_dot_outer = 1 / (1 + val**2)\n",
        "        theta_dot_inner = (x_dot * y - y_dot * x) / y**2\n",
        "        return onp.concatenate([theta, theta_dot_outer * theta_dot_inner], axis=-1)\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and modified\n",
        "def generate_pendulums(file_path, task, \n",
        "                       num_train_trials=200, num_test_trials=100,\n",
        "                       impute_rate=0.5, seq_len=100, file_tag=\"\"):\n",
        "    \n",
        "    if task == 'interpolation':\n",
        "        pend_params = Pendulum.pendulum_default_params()\n",
        "        pend_params[Pendulum.FRICTION_KEY] = 0.1\n",
        "        n = seq_len\n",
        "\n",
        "        pendulum = Pendulum(24, observation_mode=Pendulum.OBSERVATION_MODE_LINE,\n",
        "                            transition_noise_std=0.1, observation_noise_std=1e-5,\n",
        "                            seed=42, pendulum_params=pend_params)\n",
        "        rng = pendulum.random\n",
        "\n",
        "        train_obs, _, _, _, train_ts = pendulum.sample_data_set(\n",
        "            num_train_trials, n, full_targets=False)\n",
        "        train_obs = onp.expand_dims(train_obs, -1)\n",
        "        train_targets = train_obs.copy()\n",
        "        train_obs_valid = rng.rand(\n",
        "            train_obs.shape[0], train_obs.shape[1], 1) > impute_rate\n",
        "        train_obs_valid[:, :5] = True\n",
        "        train_obs[onp.logical_not(onp.squeeze(train_obs_valid))] = 0\n",
        "\n",
        "        test_obs, _, _, _, test_ts = pendulum.sample_data_set(\n",
        "            num_test_trials, n, full_targets=False)\n",
        "        test_obs = onp.expand_dims(test_obs, -1)\n",
        "        test_targets = test_obs.copy()\n",
        "        test_obs_valid = rng.rand(\n",
        "            test_obs.shape[0], test_obs.shape[1], 1) > impute_rate\n",
        "        test_obs_valid[:, :5] = True\n",
        "        test_obs[onp.logical_not(onp.squeeze(test_obs_valid))] = 0\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            os.makedirs(file_path)\n",
        "        onp.savez_compressed(os.path.join(file_path, f\"pend_interpolation_ir{impute_rate}\"),\n",
        "                            train_obs=train_obs, train_targets=train_targets, train_obs_valid=train_obs_valid, train_ts=train_ts,\n",
        "                            test_obs=test_obs, test_targets=test_targets, test_obs_valid=test_obs_valid, test_ts=test_ts)\n",
        "    \n",
        "    elif task == 'regression':\n",
        "        pend_params = Pendulum.pendulum_default_params()\n",
        "        pend_params[Pendulum.FRICTION_KEY] = 0.1\n",
        "        pend_params[Pendulum.DT_KEY] = 0.01\n",
        "        n = seq_len\n",
        "\n",
        "        pendulum = Pendulum(24, observation_mode=Pendulum.OBSERVATION_MODE_LINE,\n",
        "                            transition_noise_std=0.1, observation_noise_std=1e-5,\n",
        "                            seed=42, pendulum_params=pend_params)\n",
        "\n",
        "        train_obs, train_targets, _, _, train_ts = pendulum.sample_data_set(\n",
        "            num_train_trials, n, full_targets=False)\n",
        "        # train_obs, _ = pendulum.add_observation_noise(train_obs, first_n_clean=5, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75,\n",
        "        #                                               t_uu=1.0)\n",
        "        train_obs = onp.expand_dims(train_obs, -1)\n",
        "\n",
        "        test_obs, test_targets, _, _, test_ts = pendulum.sample_data_set(\n",
        "            num_test_trials, n, full_targets=False)\n",
        "        # test_obs, _ = pendulum.add_observation_noise(test_obs, first_n_clean=5, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75,\n",
        "        #                                              t_uu=1.0)\n",
        "        test_obs = onp.expand_dims(test_obs, -1)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            os.makedirs(file_path)\n",
        "        onp.savez_compressed(os.path.join(file_path, f\"pend_regression\"+file_tag+\".npz\"),\n",
        "                            train_obs=train_obs, train_targets=train_targets, train_ts=train_ts,\n",
        "                            test_obs=test_obs, test_targets=test_targets, test_ts=test_ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Roo84N4NN9-"
      },
      "source": [
        "The full dataset is (2000, 100, 24, 24, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "id": "RaD7Wl1f9mWx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Create the pendulum dataset (uncomment this block!)\n",
        "# Takes about 2 minutes\n",
        "# generate_pendulums(\"pendulum\", \"regression\", seq_len=200)\n",
        "# generate_pendulums(\"pendulum\", \"regression\", \n",
        "#                    num_train_trials=0, num_test_trials=100, \n",
        "#                    seq_len=400, file_tag=\"_longer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "cellView": "form",
        "id": "5Op7ol3UKP0g"
      },
      "outputs": [],
      "source": [
        "# @title Load the pendulum dataset\n",
        "def load_pendulum(run_params, log=False):    \n",
        "    d = run_params[\"dataset_params\"]\n",
        "    train_trials = d[\"train_trials\"]\n",
        "    val_trials = d[\"val_trials\"]\n",
        "    noise_scale = d[\"emission_cov\"] ** 0.5\n",
        "    key_train, key_val, key_pred = jr.split(d[\"seed\"], 3)\n",
        "\n",
        "    data = np.load(\"pendulum/pend_regression.npz\")\n",
        "\n",
        "    def _process_data(data, key):\n",
        "        processed = data[:, ::2] / 255.0\n",
        "        processed += jr.normal(key=key, shape=processed.shape) * noise_scale\n",
        "        return np.clip(processed, 0, 1)\n",
        "\n",
        "    # Take subset, subsample every 2 frames, normalize to [0, 1]\n",
        "    train_data = _process_data(data[\"train_obs\"][:train_trials], key_train)\n",
        "    # val_data = _process_data(data[\"test_obs\"][:val_trials], key_val)\n",
        "    val_data = _process_data(\n",
        "        np.load(\"pendulum/pend_regression_longer.npz\")[\"test_obs\"][:val_trials], key_pred)\n",
        "\n",
        "    print(\"Full dataset:\", data[\"train_obs\"].shape)\n",
        "    print(\"Subset:\", train_data.shape)\n",
        "    return {\n",
        "        \"train_data\": train_data,\n",
        "        \"val_data\": val_data,\n",
        "        # \"val_data_pred\": val_data_pred\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU2AZWhAPjux"
      },
      "source": [
        "# Run experiments!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "_MograqHxqqs"
      },
      "outputs": [],
      "source": [
        "if (\"data_dict\" not in globals()): data_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {
        "cellView": "form",
        "id": "DXDxYDcDrCBG"
      },
      "outputs": [],
      "source": [
        "# @title Define network architecture parameters\n",
        "\n",
        "linear_recnet_architecture = {\n",
        "    \"diagonal_covariance\": False,\n",
        "    \"input_rank\": 1,\n",
        "    \"head_mean_params\": { \"features\": [] },\n",
        "    \"head_var_params\": { \"features\": [] },\n",
        "    \"eps\": 1e-3,\n",
        "    \"cov_init\": 2,\n",
        "}\n",
        "\n",
        "BiRNN_recnet_architecture = {\n",
        "    \"input_rank\": 1,\n",
        "    \"input_type\": \"MLP\",\n",
        "    \"input_params\":{ \"features\": [20,] },\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"head_dyn_params\": { \"features\": [20,] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "planet_posterior_architecture = {\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "linear_decnet_architecture = {\n",
        "    \"diagonal_covariance\": True,\n",
        "    \"input_rank\": 1,\n",
        "    \"head_mean_params\": { \"features\": [] },\n",
        "    \"head_var_params\": { \"features\": [] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "CNN_layers = [\n",
        "            {\"features\": 32, \"kernel_size\": (3, 3), \"strides\": (1, 1) },\n",
        "            {\"features\": 64, \"kernel_size\": (3, 3), \"strides\": (2, 2) },\n",
        "            {\"features\": 32, \"kernel_size\": (3, 3), \"strides\": (2, 2) }\n",
        "]\n",
        "\n",
        "CNN_recnet_architecture = {\n",
        "    \"input_rank\": 3,\n",
        "    \"trunk_type\": \"CNN\",\n",
        "    \"trunk_params\": {\n",
        "        \"layer_params\": CNN_layers\n",
        "    },\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "CNN_BiRNN_recnet_architecture = {\n",
        "    \"input_rank\": 3,\n",
        "    \"input_type\": \"CNN\",\n",
        "    \"input_params\":{\n",
        "        \"layer_params\": CNN_layers\n",
        "    },\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"head_dyn_params\": { \"features\": [20,] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "DCNN_decnet_architecture = {\n",
        "    \"input_shape\": (6, 6, 32),\n",
        "    \"layer_params\": [\n",
        "        { \"features\": 64, \"kernel_size\": (3, 3), \"strides\": (2, 2) },\n",
        "        { \"features\": 32, \"kernel_size\": (3, 3), \"strides\": (2, 2) },\n",
        "        { \"features\": 2, \"kernel_size\": (3, 3) }\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {
        "cellView": "form",
        "id": "92Lh9HVoqqZO"
      },
      "outputs": [],
      "source": [
        "# @title Run parameter expanders\n",
        "def get_lr(params, max_iters):\n",
        "    base_lr = params[\"base_lr\"]\n",
        "    prior_base_lr = params[\"prior_base_lr\"]\n",
        "    lr = base_lr\n",
        "    prior_lr = prior_base_lr\n",
        "    pprint(params)\n",
        "    if params[\"lr_decay\"]:\n",
        "        print(\"Using learning rate decay!\")\n",
        "        lr = opt.exponential_decay(init_value=base_lr, \n",
        "                                     transition_steps=max_iters,\n",
        "                                     decay_rate=0.99, \n",
        "                                   transition_begin=.8*max_iters, staircase=False)\n",
        "        # This is kind of a different scheme but whatever...\n",
        "        if params[\"prior_lr_warmup\"]:\n",
        "            prior_lr = opt.cosine_onecycle_schedule(max_iters, prior_base_lr, 0.5)\n",
        "    else:\n",
        "        lr = base_lr\n",
        "        if params[\"prior_lr_warmup\"]: \n",
        "            prior_lr = opt.linear_schedule(0, prior_base_lr, .2 * max_iters, 0)\n",
        "    return lr, prior_lr\n",
        "\n",
        "def expand_lds_parameters(params):\n",
        "    num_timesteps = params.get(\"num_timesteps\") or 200\n",
        "    train_trials = { \"small\": 10, \"medium\": 100, \"large\": 1000 }\n",
        "    batch_sizes = {\"small\": 10, \"medium\": 10, \"large\": 20 }\n",
        "    emission_noises = { \"small\": 10., \"medium\": 1., \"large\": .1 }\n",
        "    dynamics_noises = { \"small\": 0.01, \"medium\": .1, \"large\": .1 }\n",
        "    max_iters = 8000\n",
        "\n",
        "    # Modify all the architectures according to the parameters given\n",
        "    D, H, N = params[\"latent_dims\"], params[\"rnn_dims\"], params[\"emission_dims\"]\n",
        "    inf_params = {}\n",
        "    if (params[\"inference_method\"] == \"svae\"):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(linear_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "    elif (params[\"inference_method\"] in [\"dkf\", \"cdkf\"]):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianBiRNN\"\n",
        "        architecture = deepcopy(BiRNN_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "        architecture[\"rnn_dim\"] = H\n",
        "    elif (params[\"inference_method\"] == \"planet\"):\n",
        "        # Here we're considering the filtering setting as default\n",
        "        # Most likely we're not even going to use this so it should be fine\n",
        "        # If we want smoothing then we can probably just replace these with the\n",
        "        # BiRNN version\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(linear_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = H\n",
        "        post_arch = deepcopy(planet_posterior_architecture)\n",
        "        post_arch[\"input_dim\"] = H\n",
        "        post_arch[\"rnn_dim\"] = H\n",
        "        post_arch[\"output_dim\"] = D\n",
        "        inf_params[\"posterior_architecture\"] = post_arch\n",
        "        inf_params[\"sample_kl\"] = True # PlaNet doesn't have built-in suff-stats\n",
        "    else:\n",
        "        print(\"Inference method not found: \" + params[\"inference_method\"])\n",
        "        assert(False)\n",
        "    decnet_architecture = deepcopy(linear_decnet_architecture)\n",
        "    decnet_architecture[\"output_dim\"] = N\n",
        "    inf_params[\"decnet_class\"] = \"GaussianEmission\"\n",
        "    inf_params[\"decnet_architecture\"] = decnet_architecture\n",
        "    inf_params[\"recnet_architecture\"] = architecture\n",
        "\n",
        "    lr, prior_lr = get_lr(params, max_iters)\n",
        "\n",
        "    extended_params = {\n",
        "        \"project_name\": \"SVAE-LDS-Final\",\n",
        "        \"log_to_wandb\": True,\n",
        "        \"dataset\": \"lds\",\n",
        "        \"dataset_params\": {\n",
        "            \"seed\": key_0,\n",
        "            \"num_trials\": train_trials[params[\"dataset_size\"]],\n",
        "            \"num_timesteps\": num_timesteps,\n",
        "            \"emission_cov\": emission_noises[params[\"snr\"]],\n",
        "            \"dynamics_cov\": dynamics_noises[params[\"snr\"]],\n",
        "            \"latent_dims\": D,\n",
        "            \"emission_dims\": N,\n",
        "        },\n",
        "        # Implementation choice\n",
        "        \"use_parallel_kf\": False,\n",
        "        # Training specifics\n",
        "        \"max_iters\": max_iters,\n",
        "        \"elbo_samples\": 1,\n",
        "        \"sample_kl\": False,\n",
        "        \"batch_size\": batch_sizes[params[\"dataset_size\"]],\n",
        "        \"record_params\": lambda i: i % 100 == 0,\n",
        "        \"plot_interval\": 100,\n",
        "        \"learning_rate\": lr, \n",
        "        \"prior_learning_rate\": prior_lr\n",
        "    }\n",
        "    extended_params.update(inf_params)\n",
        "    # This allows us to override ANY of the above...!\n",
        "    extended_params.update(params)\n",
        "    return extended_params\n",
        "\n",
        "def expand_pendulum_parameters(params):\n",
        "    train_trials = { \"small\": 20, \"medium\": 100, \"large\": 2000 }\n",
        "    batch_sizes = {\"small\": 10, \"medium\": 10, \"large\": 40 }\n",
        "    # Not a very good validation split (mostly because we're doing one full batch for val)\n",
        "    val_trials = { \"small\": 4, \"medium\": 20, \"large\": 200 }\n",
        "    noise_scales = { \"small\": 1., \"medium\": .1, \"large\": .01 }\n",
        "    max_iters = 20000\n",
        "\n",
        "    # Modify all the architectures according to the parameters given\n",
        "    D, H = params[\"latent_dims\"], params[\"rnn_dims\"]\n",
        "    inf_params = {}\n",
        "    if (params[\"inference_method\"] == \"svae\"):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(CNN_recnet_architecture)\n",
        "        architecture[\"trunk_params\"][\"output_dim\"] = D\n",
        "        architecture[\"output_dim\"] = D\n",
        "    elif (params[\"inference_method\"] in [\"dkf\", \"cdkf\"]):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianBiRNN\"\n",
        "        architecture = deepcopy(CNN_BiRNN_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "        architecture[\"rnn_dim\"] = H\n",
        "        architecture[\"input_params\"][\"output_dim\"] = H\n",
        "    elif (params[\"inference_method\"] == \"planet\"):\n",
        "        # Here we're considering the filtering setting as default\n",
        "        # Most likely we're not even going to use this so it should be fine\n",
        "        # If we want smoothing then we can probably just replace these with the\n",
        "        # BiRNN version\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(CNN_recnet_architecture)\n",
        "        architecture[\"trunk_params\"][\"output_dim\"] = H\n",
        "        architecture[\"output_dim\"] = H\n",
        "        post_arch = deepcopy(planet_posterior_architecture)\n",
        "        post_arch[\"input_dim\"] = H\n",
        "        post_arch[\"rnn_dim\"] = H\n",
        "        post_arch[\"output_dim\"] = D\n",
        "        inf_params[\"posterior_architecture\"] = post_arch\n",
        "        inf_params[\"sample_kl\"] = True # PlaNet doesn't have built-in suff-stats\n",
        "    else:\n",
        "        print(\"Inference method not found: \" + params[\"inference_method\"])\n",
        "        assert(False)\n",
        "    inf_params[\"recnet_architecture\"] = architecture\n",
        "    \n",
        "    lr, prior_lr = get_lr(params, max_iters)\n",
        "\n",
        "    extended_params = {\n",
        "        \"project_name\": \"SVAE-Pendulum-Final\",\n",
        "        \"log_to_wandb\": True,\n",
        "        \"dataset\": \"pendulum\",\n",
        "        # Must be model learning\n",
        "        \"run_type\": \"model_learning\",\n",
        "        \"decnet_class\": \"GaussianDCNNEmission\",\n",
        "        \"decnet_architecture\": DCNN_decnet_architecture,\n",
        "        \"dataset_params\": {\n",
        "            \"seed\": key_0,\n",
        "            \"train_trials\": train_trials[params[\"dataset_size\"]],\n",
        "            \"val_trials\": val_trials[params[\"dataset_size\"]],\n",
        "            \"emission_cov\": noise_scales[params[\"snr\"]]\n",
        "        },\n",
        "        # Implementation choice\n",
        "        \"use_parallel_kf\": False,\n",
        "        # Training specifics\n",
        "        \"max_iters\": max_iters,\n",
        "        \"elbo_samples\": 1,\n",
        "        \"sample_kl\": False,\n",
        "        \"batch_size\": batch_sizes[params[\"dataset_size\"]],\n",
        "        \"record_params\": lambda i: i % 100 == 0,\n",
        "        \"plot_interval\": 200,\n",
        "        \"mask_type\": \"potential\" if params[\"inference_method\"] == \"svae\" else \"data\",\n",
        "        \"learning_rate\": lr, \n",
        "        \"prior_learning_rate\": prior_lr,\n",
        "        \"use_validation\": True,\n",
        "        \"constrain_dynamics\": True,\n",
        "        \"prediction_horizon\": 5,\n",
        "    }\n",
        "    extended_params.update(inf_params)\n",
        "    # This allows us to override ANY of the above...!\n",
        "    extended_params.update(params)\n",
        "    return extended_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the LDS experiments"
      ],
      "metadata": {
        "id": "CpwzMT9YQSMT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "id": "Q5saocWweLzt"
      },
      "outputs": [],
      "source": [
        "# @title LDS run parameters\n",
        "run_params = {\n",
        "    # Most important: inference method\n",
        "    \"inference_method\": \"cdkf\",\n",
        "    \"use_parallel_kf\": False,\n",
        "    # Relevant dimensions\n",
        "    \"latent_dims\": 3,\n",
        "    \"rnn_dims\": 10,\n",
        "    \"seed\": jr.PRNGKey(0),\n",
        "    \"dataset_size\": \"small\", # \"small\", \"medium\", \"large\"\n",
        "    \"snr\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "    \"use_natural_grad\": False,\n",
        "    \"constrain_prior\": True,\n",
        "    # ---------------------------------------------\n",
        "    \"constrain_dynamics\": True, # Truncates the singular values of A\n",
        "    # ---------------------------------------------\n",
        "    # We set it to true for since it's extra work to compute the sufficient stats \n",
        "    # from smoothed potentials in the current parallel KF\n",
        "    \"sample_kl\": False,\n",
        "    \"base_lr\": 1e-3,\n",
        "    \"prior_base_lr\": 1e-3, # Set to 0 for debugging\n",
        "    \"prior_lr_warmup\": True,\n",
        "    \"lr_decay\": False,\n",
        "    \"group_tag\": \"\",\n",
        "    # The only LDS-specific entries\n",
        "    \"emission_dims\": 5,\n",
        "    \"num_timesteps\": 200,\n",
        "    \"run_type\": \"model_learning\", # \"inference_only\"\n",
        "    \"log_to_wandb\": False,\n",
        "    \"visualize_training\": False,\n",
        "    \"max_iters\": 2000\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# run_variations = {\n",
        "#     \"num_timesteps\": [50, 100, 200, 400, 800, 1000]\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "id": "cqhimlvNt39G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c2e3c893-2ece-410c-abe3-f9da3a38b096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of runs: 1\n",
            "Base paramerters:\n",
            "{'base_lr': 0.001,\n",
            " 'constrain_dynamics': True,\n",
            " 'constrain_prior': True,\n",
            " 'dataset_size': 'small',\n",
            " 'emission_dims': 5,\n",
            " 'group_tag': '',\n",
            " 'inference_method': 'cdkf',\n",
            " 'latent_dims': 3,\n",
            " 'log_to_wandb': False,\n",
            " 'lr_decay': False,\n",
            " 'max_iters': 2000,\n",
            " 'num_timesteps': 200,\n",
            " 'prior_base_lr': 0.001,\n",
            " 'prior_lr_warmup': True,\n",
            " 'rnn_dims': 10,\n",
            " 'run_type': 'model_learning',\n",
            " 'sample_kl': False,\n",
            " 'seed': DeviceArray([0, 0], dtype=uint32),\n",
            " 'snr': 'medium',\n",
            " 'use_natural_grad': False,\n",
            " 'use_parallel_kf': False,\n",
            " 'visualize_training': False}\n",
            "##########################################\n",
            "Starting run #0\n",
            "##########################################\n",
            "{'base_lr': 0.001,\n",
            " 'constrain_dynamics': True,\n",
            " 'constrain_prior': True,\n",
            " 'dataset_size': 'small',\n",
            " 'emission_dims': 5,\n",
            " 'group_tag': '',\n",
            " 'inference_method': 'cdkf',\n",
            " 'latent_dims': 3,\n",
            " 'log_to_wandb': False,\n",
            " 'lr_decay': False,\n",
            " 'max_iters': 2000,\n",
            " 'num_timesteps': 200,\n",
            " 'prior_base_lr': 0.001,\n",
            " 'prior_lr_warmup': True,\n",
            " 'rnn_dims': 10,\n",
            " 'run_type': 'model_learning',\n",
            " 'sample_kl': False,\n",
            " 'seed': DeviceArray([0, 0], dtype=uint32),\n",
            " 'snr': 'medium',\n",
            " 'use_natural_grad': False,\n",
            " 'use_parallel_kf': False,\n",
            " 'visualize_training': False}\n",
            "Loading dataset!\n",
            "Using existing data.\n",
            "Data MLL:  -1608.1849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LP: 2247.480:  59%|█████▉    | 1183/2000 [02:06<01:27,  9.36it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-294-c823ce57018e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jax_enable_x64\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m results = experiment_scheduler(run_params, \n\u001b[0m\u001b[1;32m      5\u001b[0m                     \u001b[0;31m#  run_variations=run_variations,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                      \u001b[0mdataset_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_lds_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-229-ddc6a11be03c>\u001b[0m in \u001b[0;36mexperiment_scheduler\u001b[0;34m(run_params, dataset_getter, model_getter, train_func, logger_func, err_logger_func, run_variations, params_expander, on_error, continue_on_error, use_wandb)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mmodel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_on_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0m_single_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-229-ddc6a11be03c>\u001b[0m in \u001b[0;36m_single_run\u001b[0;34m(data_out, model_out)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mmodel_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mall_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mall_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogger_func\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-282-d4b98fc6ca02>\u001b[0m in \u001b[0;36mstart_trainer\u001b[0;34m(model_dict, data_dict, run_params)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trainer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_params_to_wandb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_to_wandb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     trainer.train(data_dict,\n\u001b[0m\u001b[1;32m     96\u001b[0m                   \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_iters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                   \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-279-bda982f4c243>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_dict, max_iters, callback, val_callback, summary, key, early_stop_start, max_lose_streak)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m# Training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# ----------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             step_results = train_step(train_key, self.params, \n\u001b[0m\u001b[1;32m    114\u001b[0m                            train_data[batch_start:batch_start+batch_size], self.opt_states)\n\u001b[1;32m    115\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltering_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m       out_flat = call_bind_continuation(\n\u001b[0;32m--> 626\u001b[0;31m           top_trace.process_call(primitive, fun_, tracers, params))\n\u001b[0m\u001b[1;32m    627\u001b[0m     \u001b[0mout_pytree_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_pytree_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_call\u001b[0;34m(self, primitive, f, tracers, params)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m   \u001b[0mprocess_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36m_xla_call_impl\u001b[0;34m(fun, device, backend, name, donated_invars, inline, keep_unused, *args)\u001b[0m\n\u001b[1;32m    252\u001b[0m   \u001b[0;31m# fallback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mFloatingPointError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_debug_nans\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_debug_infs\u001b[0m  \u001b[0;31m# compiled_fun can only raise in this case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36m_execute_compiled\u001b[0;34m(name, compiled, input_handler, output_buffer_counts, result_handler, has_unordered_effects, ordered_effects, kept_var_idx, has_host_callbacks, *args)\u001b[0m\n\u001b[1;32m    894\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     \u001b[0mout_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m   \u001b[0mcheck_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_buffer_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mordered_effects\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhas_unordered_effects\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhas_host_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mcheck_special\u001b[0;34m(name, bufs)\u001b[0m\n\u001b[1;32m    842\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mneeds_check_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbufs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m       \u001b[0m_check_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36m_check_special\u001b[0;34m(name, dtype, buf)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minexact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_debug_nans\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mFloatingPointError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"invalid value (nan) encountered in {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_debug_infs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "jax.config.update(\"jax_enable_x64\", False)\n",
        "\n",
        "results = experiment_scheduler(run_params, \n",
        "                    #  run_variations=run_variations,\n",
        "                     dataset_getter=sample_lds_dataset, \n",
        "                     model_getter=init_model, \n",
        "                     train_func=start_trainer,\n",
        "                     params_expander=expand_lds_parameters,\n",
        "                    #  on_error=on_error,\n",
        "                     continue_on_error=False,\n",
        "                     )\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = results[1][0][\"model\"]\n",
        "params = results[1][0][\"trainer\"].params\n",
        "\n",
        "latent_dim = 20\n",
        "seq_len = 200"
      ],
      "metadata": {
        "id": "P97PoE4yJsMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_64 = lambda x: x\n",
        "data = to_64(data_dict[\"train_data\"][4])\n",
        "prior_params = to_64(params[\"prior_params\"])\n",
        "potentials = to_64(model.recognition.apply(params[\"rec_params\"], data))\n",
        "# Sigmas = solve(potentials[\"J\"], np.eye(latent_dim)[None])\n",
        "# mus = vmap(solve)(potentials[\"J\"], potentials[\"h\"])\n",
        "# potentials[\"mu\"] = mus\n",
        "# potentials[\"Sigma\"] = Sigmas\n",
        "# prior_para = ParallelLieParameterizedLinearGaussianChain(latent_dim, seq_len)\n",
        "# posterior_para = ParallelLDSSVAEPosterior_Mean(latent_dim, seq_len)\n",
        "# model.posterior = posterior_para\n",
        "prior_params = model.prior.get_constrained_params(prior_params)\n",
        "post_params = model.posterior.infer(prior_params, potentials)\n",
        "# model.posterior."
      ],
      "metadata": {
        "id": "7wMWDS9-JwQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posterior = model.posterior.distribution(post_params)\n",
        "prior = model.prior.distribution(prior_params)"
      ],
      "metadata": {
        "id": "0x8M0emZKX8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ex = posterior.expected_states\n",
        "ExxT = posterior.expected_states_squared\n",
        "ExnxT = posterior.expected_states_next_states\n",
        "Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "J, L = prior_params[\"J\"], prior_params[\"L\"]\n",
        "trm1 = -prior.log_prob(Ex)\n",
        "trm2 = 0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "trm3 = np.einsum(\"tij,tij->\", L, Sigmatnt)\n",
        "# return cross_entropy - posterior.entropy()"
      ],
      "metadata": {
        "id": "zysUi9BsK_Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trm1, trm2, trm3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JG-cFLALg4S",
        "outputId": "48510a12-2b81-42a5-93b4-aba9291cd481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray(850.06604, dtype=float32),\n",
              " DeviceArray(52.273884, dtype=float32),\n",
              " DeviceArray(-78.78047, dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 670
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(self):\n",
        "    \"\"\"\n",
        "    Compute the entropy\n",
        "\n",
        "        H[X] = -E[\\log p(x)]\n",
        "             = -E[-1/2 x^T J x + x^T h - log Z(J, h)]\n",
        "             = 1/2 <J, E[x x^T] - <h, E[x]> + log Z(J, h)\n",
        "    \"\"\"\n",
        "    Ex = self.expected_states\n",
        "    ExxT = self.expected_states_squared\n",
        "    ExnxT = self.expected_states_next_states\n",
        "    \n",
        "    dim = Ex.shape[-1]        \n",
        "    Q_inv = solve(self._noise_covariance, np.eye(dim)[None])\n",
        "    A = self._dynamics_matrix\n",
        "\n",
        "    J_lower_diag = np.einsum(\"til,tlj->tji\", -Q_inv[1:], A[1:])\n",
        "    # print(J_lower_diag)\n",
        "    ATQinvA = np.einsum(\"tji,tjl,tlk->tik\", A[1:], Q_inv[1:], A[1:])\n",
        "    J_diag = Q_inv.at[:-1].add(ATQinvA)\n",
        "\n",
        "    Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "    Sigmatnt = ExnxT - np.einsum(\"ti,tj->tij\", Ex[:-1], Ex[1:])\n",
        "\n",
        "    trm1 = 0.5 * np.sum(J_diag * Sigmatt)\n",
        "    trm2 = np.sum(J_lower_diag * Sigmatnt)\n",
        "\n",
        "    print(trm1, trm2)\n",
        "\n",
        "    return trm1 + trm2 - self.log_prob(Ex)\n",
        "\n",
        "entropy(posterior)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UieVxejkL4j-",
        "outputId": "7b3ad1a9-cdec-49be-a601-dece8ad1634b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "738.1157 -94.88603\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(62.11084, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 684
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "posterior._expected_states_next_states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PazfSq7YMvgP",
        "outputId": "b6bc6ac3-ca34-4fbe-eaf7-3c6d2b88a18e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[[  3.6144025,  -2.2926602,  -5.124145 ],\n",
              "              [ -4.827894 ,   3.0629516,   6.8584323],\n",
              "              [ -4.6065965,   2.9145641,   6.546231 ]],\n",
              "\n",
              "             [[  6.8913517,  -6.523874 ,  -8.057386 ],\n",
              "              [ -4.3741827,   4.143811 ,   5.1178975],\n",
              "              [ -9.795105 ,   9.26654  ,  11.464159 ]],\n",
              "\n",
              "             [[  4.9360065,  -8.790205 ,  -5.40839  ],\n",
              "              [ -4.6835036,   8.331469 ,   5.1386423],\n",
              "              [ -5.778558 ,  10.283738 ,   6.341441 ]],\n",
              "\n",
              "             ...,\n",
              "\n",
              "             [[  6.455812 ,  -7.839123 ,  -7.434242 ],\n",
              "              [ -5.052163 ,   6.132967 ,   5.823932 ],\n",
              "              [ -7.5384393,   9.147334 ,   8.691641 ]],\n",
              "\n",
              "             [[  6.3005977,  -8.444034 ,  -7.2068706],\n",
              "              [ -7.6608415,  10.26073  ,   8.773736 ],\n",
              "              [ -7.2662907,   9.731301 ,   8.322654 ]],\n",
              "\n",
              "             [[  6.66835  , -10.014889 ,  -8.0200815],\n",
              "              [ -8.946388 ,  13.426497 ,  10.771441 ],\n",
              "              [ -7.639729 ,  11.466377 ,   9.199517 ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 680
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post_dist.entropy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euzUU3L8K7Xw",
        "outputId": "aecca0f0-a9e1-42f4-d517-f2eeb5a54953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(72150.805, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 672
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(post_dist.expected_states)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "hm2CPo55K1lE",
        "outputId": "76461448-764c-4343-cf61-478150d3dedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd956214d60>,\n",
              " <matplotlib.lines.Line2D at 0x7fd955ee2b80>,\n",
              " <matplotlib.lines.Line2D at 0x7fd953888820>]"
            ]
          },
          "metadata": {},
          "execution_count": 661
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mean = np.array([np.mean(np.array(exp[\"trainer\"].time_spent[1:])) for exp in results[1]])\n",
        "# std = np.array([np.std(np.array(exp[\"trainer\"].time_spent[1:])) for exp in results[1]])\n",
        "# plt.plot([50, 100, 200, 400, 800, 1000], mean+std)\n",
        "# plt.plot([50, 100, 200, 400, 800, 1000], mean-std)\n",
        "# assert False"
      ],
      "metadata": {
        "id": "3UoVwAwpQYdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLOSSwKQ9vl3"
      },
      "source": [
        "## Dealing with float32 imprecision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4jST1ldPGfO"
      },
      "outputs": [],
      "source": [
        "# # Code for unpacking and examining results\n",
        "# model = results[1][0][\"model\"]\n",
        "# params = results[1][0][\"trainer\"].params\n",
        "\n",
        "# latent_dim = 20\n",
        "# seq_len = 200\n",
        "\n",
        "# use_x64 = True\n",
        "# # Switch to x64\n",
        "# if (use_x64):\n",
        "#     jax.config.update(\"jax_enable_x64\", True)\n",
        "#     to_64 = lambda x: jax.tree_map(lambda y: np.array(y, dtype=np.float64), x)\n",
        "# else:\n",
        "#     jax.config.update(\"jax_enable_x64\", False)\n",
        "#     to_64 = lambda x: x\n",
        "\n",
        "# data = to_64(data_dict[\"train_data\"][4])\n",
        "# prior_params = to_64(params[\"prior_params\"])\n",
        "# potentials = to_64(model.recognition.apply(params[\"rec_params\"], data))\n",
        "# Sigmas = solve(potentials[\"J\"], np.eye(latent_dim)[None])\n",
        "# mus = vmap(solve)(potentials[\"J\"], potentials[\"h\"])\n",
        "# potentials[\"mu\"] = mus\n",
        "# potentials[\"Sigma\"] = Sigmas\n",
        "\n",
        "# prior_para = ParallelLieParameterizedLinearGaussianChain(latent_dim, seq_len)\n",
        "# posterior_para = ParallelLDSSVAEPosterior_Mean(latent_dim, seq_len)\n",
        "# model.posterior = posterior_para\n",
        "# prior_params_para = prior_para.get_constrained_params(prior_params)\n",
        "# post_params_para = posterior_para.infer(prior_params_para, potentials)\n",
        "\n",
        "# if (use_x64):\n",
        "#     kl_x64 = model.kl_posterior_prior(post_params_para, prior_para.get_constrained_params(prior_params))\n",
        "#     print(\"KL with x64:\", kl_x64)\n",
        "# else:\n",
        "#     kl_x32 = model.kl_posterior_prior(post_params_para, prior_para.get_constrained_params(prior_params))\n",
        "#     print(\"KL with x32:\", kl_x32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li5AH0eRa3in"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_para = posterior_para.distribution(post_params_para)\n",
        "# posterior_dist_para.log_prob(posterior_dist_para.expected_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI-YukjSbqj6"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_para.entropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_caqLPKFcnio"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_old.entropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLvURJK7bDg9"
      },
      "outputs": [],
      "source": [
        "# err = np.mean(posterior_dist_para.expected_states_squared \n",
        "#             - posterior_dist_old.expected_states_squared, axis=(1,2))\n",
        "# plt.plot(err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QOiE80IZ9vY"
      },
      "outputs": [],
      "source": [
        "# posterior_old = ParallelLDSSVAEPosterior(latent_dim, seq_len)\n",
        "# post_params_old = posterior_old.infer(prior_params_para, potentials)\n",
        "# posterior_dist_old = posterior_old.distribution(post_params_old)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mfPPecNavsk"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_old.log_prob(posterior_dist_old.expected_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHtj6ZwJaGKh"
      },
      "outputs": [],
      "source": [
        "# plt.plot(post_params_old[\"mu_filtered\"] - post_params_para[\"mu_filtered\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZP9aMCP3k6l"
      },
      "outputs": [],
      "source": [
        "# Sigmas = solve(potentials[\"J\"], np.eye(latent_dim)[None])\n",
        "# mus = vmap(solve)(potentials[\"J\"], potentials[\"h\"])\n",
        "# p = prior_params_para\n",
        "# params = { \"m1\": p[\"m1\"], \"Q1\": p[\"Q1\"], \n",
        "#           \"A\": p[\"A\"], \"b\": p[\"b\"], \"Q\": p[\"Q\"], \n",
        "#           \"mus\": mus, \"Sigmas\": Sigmas }\n",
        "# np.save(\"faulty_params.npy\", params, allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II5Q019TDHkb"
      },
      "outputs": [],
      "source": [
        "# def log_normalizer(p):\n",
        "#     Q, A, b = p[\"Q\"][None], p[\"A\"][None], p[\"b\"][None]\n",
        "#     AT = (p[\"A\"].T)[None]\n",
        "\n",
        "#     I = np.eye(Q.shape[-1])\n",
        "\n",
        "#     Sigma_filtered, mu_filtered = p[\"Sigma_filtered\"][:-1], p[\"mu_filtered\"][:-1]\n",
        "#     Sigma = Q + A @ Sigma_filtered @ AT\n",
        "#     mu = (A[0] @ mu_filtered.T).T + b\n",
        "#     # Append the first element\n",
        "#     Sigma_pred = np.concatenate([p[\"Q1\"][None], Sigma])\n",
        "#     mu_pred = np.concatenate([p[\"m1\"][None], mu])\n",
        "#     mu_rec, Sigma_rec = p[\"mu\"], p[\"Sigma\"]\n",
        "\n",
        "#     def log_Z_single(mu_pred, Sigma_pred, mu_rec, Sigma_rec):\n",
        "#         return MVN(loc=mu_pred, covariance_matrix=Sigma_pred+Sigma_rec).log_prob(mu_rec)\n",
        "\n",
        "#     log_Z = vmap(log_Z_single)(mu_pred, Sigma_pred, mu_rec, Sigma_rec)\n",
        "#     return np.sum(log_Z)\n",
        "\n",
        "# log_normalizer(post_params_para) - post_params_para[\"log_Z\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbHRN57av7Hk"
      },
      "outputs": [],
      "source": [
        "# def log_prob(self, p, data):\n",
        "\n",
        "#     A = self._dynamics_matrix #params[\"A\"]\n",
        "#     Q = self._dynamics_noise_covariance #params[\"Q\"]\n",
        "#     Q1 = self._initial_covariance #params[\"Q1\"]\n",
        "#     m1 = self._initial_mean #params[\"m1\"]\n",
        "\n",
        "#     num_batch_dims = len(data.shape) - 2\n",
        "\n",
        "#     t1 = np.sum(\n",
        "#         MVN(loc=np.einsum(\"ij,...tj->...ti\", A, data[...,:-1,:]), \n",
        "#             covariance_matrix=Q).log_prob(data[...,1:,:])\n",
        "#         )\n",
        "    \n",
        "#     t2 = MVN(loc=m1, covariance_matrix=Q1).log_prob(data[...,0,:])\n",
        "\n",
        "#     # Add the observation potentials\n",
        "#     # t3_ = - 0.5 * np.einsum(\"...ti,tij,...tj->...t\", data, self._emissions_precisions, data)\n",
        "#     # t3 = np.sum(t3_)\n",
        "#     # t4_ = + np.einsum(\"...ti,ti->...t\", data, self._emissions_linear_potentials)\n",
        "#     # t4 = np.sum(t4_)\n",
        "#     t3 = 0\n",
        "#     t4 = np.sum(MVN(loc=self._emissions_means, \n",
        "#                   covariance_matrix=self._emissions_covariances).log_prob(data))\n",
        "#     # print(t4)\n",
        "#     # Add the log normalizer\n",
        "#     # t5 = -self._log_normalizer\n",
        "#     # t5_ = np.ones(seq_len) * t5 / seq_len\n",
        "#     t5 = -log_normalizer(p)\n",
        "\n",
        "#     s1 = t3 + t4 + t5\n",
        "#     # s1_ = np.sum(t3_ + t4_ + t5_)\n",
        "\n",
        "#     # return s1, (t1, t2, t3, t4, t5, s1, s1_)\n",
        "#     return t1 + t2 + s1, (t1, t2, t3, t4, t5, s1)\n",
        "\n",
        "# if (use_x64):\n",
        "#     dist = posterior_para.distribution(post_params_para)\n",
        "#     lp, x64_trace_ll = log_prob(dist, post_params_para, dist.expected_states)\n",
        "#     print(\"Log prob computed stepwise with x64:\", lp)\n",
        "# else:\n",
        "#     dist = posterior_para.distribution(post_params_para)\n",
        "#     lp, x32_trace_ll = log_prob(dist, post_params_para, dist.expected_states)\n",
        "#     print(\"Log prob computed stepwise with x32:\", lp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liSyGmnRlE9l"
      },
      "outputs": [],
      "source": [
        "# def kl_posterior_prior(posterior, prior, posterior_params, prior_params):\n",
        "#     posterior = posterior.distribution(posterior_params)\n",
        "#     prior = prior.distribution(prior_params)\n",
        "#     Ex = posterior.expected_states\n",
        "#     ExxT = posterior.expected_states_squared\n",
        "#     ExnxT = posterior.expected_states_next_states\n",
        "#     Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "#     Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "#     # J, L = prior_params[\"J\"], prior_params[\"L\"]\n",
        "#     p = dynamics_to_tridiag(prior_params, Ex.shape[0], Ex.shape[1])\n",
        "#     J, L = p[\"J\"], p[\"L\"]\n",
        "\n",
        "#     p = dynamics_to_tridiag(posterior_params, Ex.shape[0], Ex.shape[1])\n",
        "#     J_post, L_post = p[\"J\"] + potentials[\"J\"], p[\"L\"]\n",
        "\n",
        "#     t1 = -prior.log_prob(Ex) \n",
        "#     # t1 = 0#-log_prob(prior, prior_params, Ex)[0]\n",
        "#     t2 = 0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "#     t3 = np.einsum(\"tij,tij->\", L, Sigmatnt)\n",
        "#     # t4 = -posterior.log_prob(Ex) \n",
        "#     t4 = -log_prob(posterior, posterior_params, Ex)[0]\n",
        "#     t5 = 0.5 * np.einsum(\"tij,tij->\", J_post, Sigmatt)\n",
        "#     t6 = np.einsum(\"tij,tij->\", L_post, Sigmatnt)\n",
        "\n",
        "#     # print(t1, t2, t3, t4, t5, t6)\n",
        "\n",
        "#     cross_entropy = t1\n",
        "#     cross_entropy += t2 #0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "#     cross_entropy += t3\n",
        "#     cross_entropy -= t4+t5+t6\n",
        "\n",
        "#     return cross_entropy, (t1, t2, t3, t4, t5, t6, t1 - t4, t2 - t5, t3 - t6)\n",
        "\n",
        "# if (use_x64):\n",
        "#     x64_kl, x64_trace = kl_posterior_prior(posterior_para, prior_para, post_params_para, prior_params_para)\n",
        "#     print(\"KL computed stepwise with x64:\", x64_kl)\n",
        "# else:\n",
        "#     x32_kl, x32_trace = kl_posterior_prior(posterior_para, prior_para, post_params_para, prior_params_para)\n",
        "#     print(\"KL computed stepwise with x32:\", x32_kl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kjE9yc1eNIA"
      },
      "outputs": [],
      "source": [
        "# print(\"KL computed stepwise with x32:\", x32_kl)\n",
        "# print(\"KL computed stepwise with x64:\", x64_kl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2LaiyyRuBRD"
      },
      "outputs": [],
      "source": [
        "# pprint([x64_trace[i] - x32_trace[i] for i in range(len(x64_trace))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ELK6UUjTnNr"
      },
      "source": [
        "## Pendulum experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JjtwI49pGk7U"
      },
      "outputs": [],
      "source": [
        "# @title Pendulum run params\n",
        "run_params = {\n",
        "    # Most important: inference method\n",
        "    \"inference_method\": \"svae\",#\"svae\",\n",
        "    \"use_parallel_kf\": True,\n",
        "    # Relevant dimensions\n",
        "    \"latent_dims\": 5,\n",
        "    \"rnn_dims\": 10,\n",
        "    \"seed\": jr.PRNGKey(0),\n",
        "    \"dataset_size\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "    \"snr\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "    \"use_natural_grad\": False,\n",
        "    \"constrain_prior\": True,\n",
        "    \"base_lr\": 1e-2,\n",
        "    \"prior_base_lr\": 1e-2,\n",
        "    \"prior_lr_warmup\": True,\n",
        "    \"lr_decay\": False,\n",
        "    \"group_tag\": \"test_parallel_scan\",\n",
        "    # The only pendulum-specific entry, will be overridden by params expander\n",
        "    \"mask_size\": 40,\n",
        "    # \"plot_interval\": 1,\n",
        "    \"mask_start\": 0,#1000,\n",
        "    \"sample_kl\": True,\n",
        "    \"log_to_wandb\": True\n",
        "}\n",
        "\n",
        "# methods = {\n",
        "#     # \"inference_method\": [\"svae\", \"cdkf\", \"planet\", \"dkf\"],\n",
        "#     # \"mask_start\": [0, 2000, 2000, 2000]\n",
        "#     \"inference_method\": [\"planet\", \"svae\"],\n",
        "#     \"mask_start\": [2000, 0],\n",
        "#     \"use_natural_grad\": [False, False],\n",
        "#     \"constrain_prior\": [True, True]\n",
        "# }\n",
        "\n",
        "seeds = {\n",
        "    \"seed\": [jr.PRNGKey(i) for i in range(3)]\n",
        "}\n",
        "\n",
        "run_variations = seeds#dict_product(seeds, methods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7cd8Vsd5q9Dl"
      },
      "outputs": [],
      "source": [
        "# @title Run the pendulum experiments\n",
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "jax.config.update(\"jax_enable_x64\", False)\n",
        "\n",
        "results = experiment_scheduler(run_params, \n",
        "                    #  run_variations=run_variations,\n",
        "                     dataset_getter=load_pendulum, \n",
        "                     model_getter=init_model, \n",
        "                     train_func=start_trainer,\n",
        "                     params_expander=expand_pendulum_parameters,\n",
        "                     on_error=on_error,\n",
        "                     continue_on_error=True)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwKD63PsK9so"
      },
      "outputs": [],
      "source": [
        "# Params used previously\n",
        "run_params = {\n",
        "    # Most important: inference method\n",
        "    \"inference_method\": \"svae\",\n",
        "    \"use_parallel_kf\": True,\n",
        "    # Relevant dimensions\n",
        "    \"latent_dims\": 3,\n",
        "    \"rnn_dims\": 10,\n",
        "    \"seed\": jr.PRNGKey(0),\n",
        "    \"dataset_size\": \"small\", # \"small\", \"medium\", \"large\"\n",
        "    \"snr\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "    \"use_natural_grad\": False,\n",
        "    \"constrain_prior\": True,\n",
        "    # We set it to true for since it's extra work to compute the sufficient stats \n",
        "    # from smoothed potentials in the current parallel KF\n",
        "    \"sample_kl\": True,\n",
        "    \"base_lr\": 1e-2,\n",
        "    \"prior_base_lr\": 0, #1e-3, # Debugging: set prior lr to 0\n",
        "    \"prior_lr_warmup\": True,\n",
        "    \"lr_decay\": False,\n",
        "    \"group_tag\": \"\",\n",
        "    # The only LDS-specific entries\n",
        "    \"emission_dims\": 5,\n",
        "    \"run_type\": \"model_learning\", # \"inference_only\"\n",
        "    \"log_to_wandb\": True,\n",
        "    # \"max_iters\": 200,\n",
        "}\n",
        "\n",
        "results = experiment_scheduler(run_params, \n",
        "                    #  run_variations=run_variations,\n",
        "                     dataset_getter=sample_lds_dataset, \n",
        "                     model_getter=init_model, \n",
        "                     train_func=start_trainer,\n",
        "                     params_expander=expand_lds_parameters,\n",
        "                     on_error=on_error,\n",
        "                     continue_on_error=True\n",
        "                     )\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXi0h-N5qMmv"
      },
      "outputs": [],
      "source": [
        "prior_params = copy.deepcopy(results[1][0][\"trainer\"].params[\"prior_params\"])\n",
        "# post_params = copy.deepcopy(results[1][0][\"trainer\"].params[\"post_params\"])\n",
        "rec_params = copy.deepcopy(results[1][0][\"trainer\"].params[\"rec_params\"])\n",
        "model = results[1][0][\"model\"]\n",
        "posterior = model.posterior\n",
        "alt_posterior = LDSSVAEPosterior(3, seq_len=200)\n",
        "prior = ParallelLieParameterizedLinearGaussianChain(3, 200)# model.prior\n",
        "alt_prior = LieParameterizedLinearGaussianChain(3, 200)\n",
        "constrained_params = model.prior.get_constrained_params(prior_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6U_up9AvC58"
      },
      "outputs": [],
      "source": [
        "# m1 = samples[0][0]\n",
        "# prior_params[\"Q\"] = np.array([-10, -10, -10, 0, 0, 0])\n",
        "# prior_params[\"m1\"] = m1\n",
        "# prior_params[\"Q1\"] = prior_params[\"Q\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4of2dkBrQWr"
      },
      "outputs": [],
      "source": [
        "samples = prior.sample(prior_params, (1,), key_0)\n",
        "plt.plot(samples[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzrObIG3rgyf"
      },
      "outputs": [],
      "source": [
        "plt.plot(alt_prior.sample(prior_params, (1,), key_0)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9EyjEmbo2yt"
      },
      "outputs": [],
      "source": [
        "print(prior.distribution(prior.get_constrained_params(prior_params)).log_prob(samples[0]))\n",
        "print(alt_prior.distribution(alt_prior.get_constrained_params(prior_params)).log_prob(samples[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjgZ8TveqctX"
      },
      "outputs": [],
      "source": [
        "data = data_dict[\"train_data\"][0]\n",
        "potential = vmap(model.recognition.apply, in_axes=(None, 0))(rec_params, data)\n",
        "post_params = posterior.infer(constrained_params, potential)\n",
        "# alt_post_params = alt_posterior.infer(alt_prior.get_constrained_params(prior_params), potential)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVXofatGsHlv"
      },
      "outputs": [],
      "source": [
        "post_dist = posterior.distribution(post_params)\n",
        "post_sample = posterior.sample(post_params, shape=(1,), key=key_0)\n",
        "alt_post_dist = alt_posterior.distribution(alt_post_params)\n",
        "alt_post_sample = alt_post_dist.sample(seed=key_0)\n",
        "print(alt_post_dist.log_prob(post_sample))\n",
        "print(post_dist.log_prob(post_sample))\n",
        "print(prior.distribution(prior.get_constrained_params(prior_params)).log_prob(post_sample))\n",
        "print(alt_prior.distribution(alt_prior.get_constrained_params(prior_params)).log_prob(post_sample))\n",
        "plt.plot(alt_post_sample - post_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvcblvND14Lh"
      },
      "outputs": [],
      "source": [
        "plt.plot(post_sample[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8dGeHGMyQBE"
      },
      "outputs": [],
      "source": [
        "model.kl_posterior_prior(post_params, constrained_params, samples=post_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umLIjpMc1oN5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASs5866Y0gaw"
      },
      "outputs": [],
      "source": [
        "model.elbo(key_0, data, results[1][0][\"trainer\"].params, sample_kl=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzngmpgUw4cb"
      },
      "source": [
        "# Pendulum Diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XbEGFOLU2i4"
      },
      "source": [
        "- [x] Visualize the samples from the predictive posterior\n",
        "- [x] Implement a shorter horizon prediction \n",
        "- [x] See how the svae performs as prediction horizon gets longer\n",
        "- [x] What about the other frameworks? -- best SVAE beats the cDKF...!\n",
        "- [x] Test the linear decoding for the entire dataset\n",
        "- [x] Package the diagnostics code into helper functions\n",
        "- [x] Also include the linear decoding accuracies (MSE) analysis\n",
        "\n",
        "TODOS:\n",
        "- [ ] Investigate why the sliding window prediction is still not giving us what we what\n",
        "- [ ] Pick one best run for all methods and plot the uncertainty estimates learned by them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mX32LKg6xXj"
      },
      "outputs": [],
      "source": [
        "# @title Load the run from WandB\n",
        "data_dict = {}\n",
        "\n",
        "def load_run(project_path, run_name):\n",
        "    api = wandb.Api()\n",
        "    \n",
        "    try:\n",
        "        os.remove(\"parameters.pkl\")\n",
        "    except:\n",
        "        pass\n",
        "    with open(wandb.restore(\"parameters.pkl\", project_path+run_name).name, \"rb\") as f:\n",
        "        d = pkl.load(f)\n",
        "    params = d[-1]\n",
        "\n",
        "    # Get the configs for that specific run\n",
        "    run = api.run(project_path+run_name)\n",
        "    run_params = deepcopy(run.config)\n",
        "    # Get the dataset and the model object\n",
        "\n",
        "    # run_params[\"seed\"] = np.array(run_params[\"seed\"], dtype=np.uint32)\n",
        "    # For some unknown reason we're getting an int instead of a PRNGKey object \n",
        "    run_params[\"seed\"] = jr.PRNGKey(run_params[\"seed\"])\n",
        "    # For some old runs the architecture logged is incorrect\n",
        "    # del(run_params[\"recnet_architecture\"][\"head_mean_params\"][\"features\"][-1])\n",
        "    # del(run_params[\"recnet_architecture\"][\"head_var_params\"][\"features\"][-1])\n",
        "    run_params[\"dataset_params\"][\"seed\"] = np.array(\n",
        "        run_params[\"dataset_params\"][\"seed\"], dtype=np.uint32)\n",
        "    \n",
        "    global data_dict\n",
        "\n",
        "    data_dict = load_pendulum(run_params)\n",
        "    model_dict = init_model(run_params, data_dict)\n",
        "    model = model_dict[\"model\"]\n",
        "    return run_params, params, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA1iJuvv4o39"
      },
      "outputs": [],
      "source": [
        "def predict_multiple(run_params, model_params, model, data, key, num_samples=6):\n",
        "    out = model.elbo(key, data, model_params, **run_params)\n",
        "    post_params = out[\"posterior_params\"]\n",
        "    posterior = model.posterior.distribution(post_params)\n",
        "    T, D = model.prior.seq_len // 2, model.prior.latent_dims\n",
        "    # Get the final mean and covariance\n",
        "    mu, Sigma = posterior.mean()[T-1], posterior.covariance()[T-1]\n",
        "    # Build the posterior object on the future latent states \n",
        "    # (\"the posterior predictive distribution\")\n",
        "    # Convert unconstrained params to constrained dynamics parameters\n",
        "    prior_params_constrained = model.prior.get_dynamics_params(model_params[\"prior_params\"])\n",
        "    dynamics_params = {\n",
        "        \"m1\": mu,\n",
        "        \"Q1\": Sigma,\n",
        "        \"A\": prior_params_constrained[\"A\"],\n",
        "        \"b\": prior_params_constrained[\"b\"],\n",
        "        \"Q\": prior_params_constrained[\"Q\"]\n",
        "    }\n",
        "\n",
        "    tridiag_params = dynamics_to_tridiag(dynamics_params, T+1, D) # Note the +1\n",
        "    J, L, h = tridiag_params[\"J\"], tridiag_params[\"L\"], tridiag_params[\"h\"]\n",
        "    pred_posterior = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "    # Sample from it and evaluate the log likelihood\n",
        "    x_preds = pred_posterior.sample(seed=key, sample_shape=(num_samples,))[:,1:]\n",
        "\n",
        "    def pred_ll(x_pred):\n",
        "        likelihood_dist = model.decoder.apply(model_params[\"dec_params\"], x_pred)\n",
        "        return likelihood_dist.log_prob(data[T:])\n",
        "\n",
        "    pred_lls = vmap(pred_ll)(x_preds)\n",
        "    # This assumes the pendulum dataset\n",
        "    pred_lls = pred_lls.sum(axis=(2, 3, 4))\n",
        "    pred_lls = pred_lls.mean(axis=0)\n",
        "    return posterior.mean(), x_preds, pred_lls\n",
        "\n",
        "def evaluate_run(project_path, run_name, key=None):\n",
        "\n",
        "    key = key_0 if key is None else key\n",
        "\n",
        "    run_params, model_params, model = load_run(project_path, run_name)\n",
        "    run_params = deepcopy(run_params)\n",
        "    run_params[\"mask_size\"] = 0\n",
        "\n",
        "    train_data = data_dict[\"train_data\"][:20,:100]\n",
        "    Ex, x_preds, pred_lls = vmap(predict_multiple, in_axes=(None, None, None, 0, None, None))\\\n",
        "        (run_params, model_params, model, train_data, key, 10)\n",
        "\n",
        "    # Also visualize a sample prediction\n",
        "    # out_dist = model.decoder.apply(model_params[\"dec_params\"], np.concatenate([Ex[0, :50], x_preds[0, 0]]))\n",
        "    # y_decoded = out_dist.mean()\n",
        "    # plt.figure()\n",
        "    # plot_img_grid(y_decoded)\n",
        "\n",
        "    targets = np.load(\"pendulum/pend_regression.npz\")[\"train_targets\"][:20]\n",
        "    states = targets[:,::2]\n",
        "    train_thetas = np.arctan2(states[:,:,0], states[:,:,1])\n",
        "    train_omegas = train_thetas[:,1:]-train_thetas[:,:-1]\n",
        "    thetas = train_thetas.flatten()\n",
        "    omegas = train_omegas.flatten()\n",
        "    D = model.prior.latent_dims\n",
        "    xs_theta = Ex.reshape((-1, D))\n",
        "    xs_omega = Ex[:,1:].reshape((-1, D))\n",
        "    W_theta, _, _, _ = np.linalg.lstsq(xs_theta, thetas)\n",
        "    W_omega, _, _, _ = np.linalg.lstsq(xs_omega, omegas)\n",
        "\n",
        "    test_targets = np.load(\"pendulum/pend_regression_longer.npz\")[\"test_targets\"][:20, ::2][:, :100]\n",
        "    test_data = data_dict[\"val_data\"][:, :100]#np.load(\"pendulum/pend_regression.npz\")[\"test_obs\"][:, ::2]\n",
        "    thetas = np.arctan2(test_targets[:,:,0], test_targets[:,:,1])\n",
        "    omegas = thetas[:,1:]-thetas[:,:-1]\n",
        "\n",
        "    def encode(data):\n",
        "        out = model.elbo(jr.PRNGKey(0), data, model_params, **run_params)\n",
        "        post_params = out[\"posterior_params\"]\n",
        "        post_dist = model.posterior.distribution(post_params)\n",
        "        return post_dist.mean()\n",
        "\n",
        "    Ex_test = vmap(encode)(test_data)\n",
        "    pred_thetas = np.einsum(\"i,...i->...\", W_theta, Ex_test)\n",
        "    theta_mse = np.mean((pred_thetas - thetas) ** 2)\n",
        "    pred_omegas = np.einsum(\"i,...i->...\", W_omega, Ex_test[:,1:])\n",
        "    omega_mse = np.mean((pred_omegas - omegas) ** 2)\n",
        "    # test_id = 0 #if \"test_id\" not in globals() else test_id + 1\n",
        "    # plt.plot(Ex_test[test_id] @ W_theta, label=\"decoded\")\n",
        "    # plt.plot(thetas[test_id], label=\"true\")\n",
        "    # plt.title(\"Linear decoding of pendulum angle (test sequence #{})\".format(test_id))\n",
        "    # plt.legend()\n",
        "    # plt.figure()\n",
        "    # plt.plot(Ex_test[test_id] @ W_omega, label=\"decoded\")\n",
        "    # plt.plot(omegas[test_id], label=\"true\")\n",
        "    # plt.title(\"Linear decoding of pendulum velocity (test sequence #{})\".format(test_id))\n",
        "    # plt.legend()\n",
        "\n",
        "    return {\n",
        "        # \"prior_sample\":,\n",
        "        \"long_horizon_pred_lls\": pred_lls,\n",
        "        \"predictions\": x_preds,\n",
        "        # \"sliding_window_pred_lls\":,\n",
        "        \"w_theta\": W_theta,\n",
        "        \"w_omega\": W_omega,\n",
        "        \"theta_mse\": theta_mse,\n",
        "        \"omega_mse\": omega_mse,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxCueh0pIQK_"
      },
      "outputs": [],
      "source": [
        "# result = evaluate_run(project_path, \"v6sbb9xh\", key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns5x_wrtKc1W"
      },
      "outputs": [],
      "source": [
        "# result = evaluate_run(project_path, \"cdt4gir1\", key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8foTbjOSMiq2"
      },
      "outputs": [],
      "source": [
        "# _ = evaluate_run(project_path, \"dgudjrut\", key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsHe74D5FFVK"
      },
      "outputs": [],
      "source": [
        "# del all_results\n",
        "if (\"all_results\" not in globals()): all_results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uVctTR3JcBN"
      },
      "outputs": [],
      "source": [
        "api = wandb.Api()\n",
        "latent_dims = 5\n",
        "runs = api.runs(\"matthew9671/SVAE-Pendulum-ICML-3\", filters={\"State\": \"finished\", \n",
        "                                                             \"config.inference_method\": \"svae\",\n",
        "                                                             \"config.latent_dims\": latent_dims})\n",
        "svae_runs = []\n",
        "for run in runs:\n",
        "    svae_runs.append(run.id)\n",
        "runs = api.runs(\"matthew9671/SVAE-Pendulum-ICML-3\", filters={\"State\": \"finished\", \n",
        "                                                             \"config.inference_method\": \"cdkf\",\n",
        "                                                             \"config.latent_dims\": latent_dims})\n",
        "cdkf_runs = []\n",
        "for run in runs:\n",
        "    cdkf_runs.append(run.id)\n",
        "runs = api.runs(\"matthew9671/SVAE-Pendulum-ICML-3\", filters={\"State\": \"finished\", \n",
        "                                                             \"config.inference_method\": \"planet\",\n",
        "                                                             \"config.latent_dims\": latent_dims})\n",
        "planet_runs = []\n",
        "for run in runs:\n",
        "    planet_runs.append(run.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKPe7m3kD70V"
      },
      "outputs": [],
      "source": [
        "ax = plt.subplot()\n",
        "project_path = \"matthew9671/SVAE-Pendulum-ICML-3/\"\n",
        "# svae_runs = [\"v6sbb9xh\", \"xpf9s9ie\", \"1jxw27wp\", \"yo3fprzr\"]\n",
        "# cdkf_runs = [\"ik6i6igs\"] #\"cdt4gir1\", \"mpc58ktj\", \"dgudjrut\", \"4nctqeby\", \"cc8s8xxs\"]\n",
        "key = key_0\n",
        "for run_name in svae_runs + cdkf_runs:# + planet_runs:\n",
        "    print(\"Loading run \" + run_name)\n",
        "    key = jr.split(key)[1]\n",
        "    if (run_name not in all_results):\n",
        "        all_results[run_name] = evaluate_run(project_path, run_name, key)\n",
        "    result = all_results[run_name]\n",
        "    mean_pred_lls = result[\"long_horizon_pred_lls\"].mean(axis=0)\n",
        "    if (mean_pred_lls.mean() < -100 and run_name in svae_runs):\n",
        "        print(\"Run \" + run_name + \" gives really bad likelihoods!\")\n",
        "    if (run_name in svae_runs):\n",
        "        ax.plot(mean_pred_lls, color=\"blue\", alpha=.3)\n",
        "    elif (run_name in cdkf_runs):\n",
        "        ax.plot(mean_pred_lls, color=\"red\", alpha=.3)\n",
        "    elif (run_name in planet_runs):\n",
        "        # pass\n",
        "        ax.plot(mean_pred_lls, color=\"green\", alpha=.3)\n",
        "ax.plot(0, label=\"svae\", color=\"blue\")\n",
        "ax.plot(0, label=\"cdkf\", color=\"red\")\n",
        "ax.plot(0, label=\"planet\", color=\"green\")\n",
        "ax.set_title(\"Prediction log likelihood vs. horizon\")\n",
        "# ax.set_ylim(-100, 150)\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjH4oyyKkE-D"
      },
      "outputs": [],
      "source": [
        "svae_thetas = []\n",
        "svae_omegas = []\n",
        "for run_name in svae_runs:\n",
        "    svae_thetas.append(all_results[run_name][\"theta_mse\"])\n",
        "    svae_omegas.append(all_results[run_name][\"omega_mse\"])\n",
        "cdkf_thetas = []\n",
        "cdkf_omegas = []\n",
        "for run_name in cdkf_runs:\n",
        "    cdkf_thetas.append(all_results[run_name][\"theta_mse\"])\n",
        "    cdkf_omegas.append(all_results[run_name][\"omega_mse\"])\n",
        "planet_thetas = []\n",
        "planet_omegas = []\n",
        "for run_name in planet_runs:\n",
        "    planet_thetas.append(all_results[run_name][\"theta_mse\"])\n",
        "    planet_omegas.append(all_results[run_name][\"omega_mse\"])\n",
        "\n",
        "svae_thetas = np.array(svae_thetas)\n",
        "cdkf_thetas = np.array(cdkf_thetas)\n",
        "planet_thetas = np.array(planet_thetas)\n",
        "svae_omegas = np.array(svae_omegas) * 100\n",
        "cdkf_omegas = np.array(cdkf_omegas) * 100\n",
        "planet_omegas = np.array(planet_omegas) * 100\n",
        "bar_width = .2\n",
        "\n",
        "plt.scatter(np.zeros_like(svae_thetas)-bar_width/2, svae_thetas, s=2, color=\"blue\")\n",
        "plt.bar(-bar_width/2, svae_thetas.mean(), width=bar_width, color=\"blue\", alpha=.3, label=\"svae\")\n",
        "plt.scatter(np.zeros_like(cdkf_thetas)+bar_width/2, cdkf_thetas, s=2, color=\"red\")\n",
        "plt.bar(bar_width/2, cdkf_thetas.mean(), width=bar_width, color=\"red\", alpha=.3, label=\"cdkf\")\n",
        "plt.scatter(np.zeros_like(planet_thetas)+bar_width*3/2, planet_thetas, s=2, color=\"green\")\n",
        "plt.bar(bar_width*3/2, planet_thetas.mean(), width=bar_width, color=\"green\", alpha=.3, label=\"planet\")\n",
        "\n",
        "plt.scatter(np.zeros_like(svae_omegas)+1-bar_width/2, svae_omegas, s=2, color=\"blue\")\n",
        "plt.bar(1-bar_width/2, svae_omegas.mean(), width=bar_width, color=\"blue\", alpha=.3)\n",
        "plt.scatter(np.zeros_like(cdkf_omegas)+1+bar_width/2, cdkf_omegas, s=2, color=\"red\")\n",
        "plt.bar(1+bar_width/2, cdkf_omegas.mean(), width=bar_width, color=\"red\", alpha=.3)\n",
        "plt.scatter(np.zeros_like(planet_omegas)+1+bar_width*3/2, planet_omegas, s=2, color=\"green\")\n",
        "plt.bar(1+bar_width*3/2, planet_omegas.mean(), width=bar_width, color=\"green\", alpha=.3)\n",
        "plt.title(\"MSE for linear decoding of true pendulum state\")\n",
        "plt.xticks([0, 1], [\"theta\", \"omega\"])\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iychu_ZEV6Z7"
      },
      "outputs": [],
      "source": [
        "# @title Turns out the correlation between runs comes from the fluctuating averge pixel intensity of the image\n",
        "i = 1 # data id\n",
        "result = all_results[\"v6sbb9xh\"]\n",
        "plt.plot(result[\"long_horizon_pred_lls\"][i])\n",
        "# plt.plot(result[\"predictions\"][i][0])\n",
        "result = all_results[\"xpf9s9ie\"]\n",
        "plt.plot(result[\"long_horizon_pred_lls\"][i])\n",
        "# plt.plot(result[\"predictions\"][i][0])\n",
        "obs_mean = data_dict[\"train_data\"][i,50:100].sum(axis=(1, 2, 3))\n",
        "plt.plot((obs_mean - obs_mean.mean()) * -5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7tG1ohb4wl-"
      },
      "outputs": [],
      "source": [
        "# @title SVAE run names\n",
        "# run_name = \"391tsihg\" # 5d\n",
        "# run_name = \"vs2dkdje\" # 3d\n",
        "# run_name = \"zp5manco\" # 2d\n",
        "# 5d sinusoidal\n",
        "# run_name = \"0q0c0hbw\" \n",
        "# run_name = \"0vogdb8f\"\n",
        "# run_name = \"5b8cefgf\" # ICML-2 hopeful-sweep\n",
        "# run_name = \"7ntuood6\" # ICML-2 dainty-sweep (best prediction I've seen so far)\n",
        "# run_name = \"xpf9s9ie\" # ICML-3 good-sweep-21\n",
        "run_name = \"v6sbb9xh\" # ICML-3 rose-sweep-20 (good prediction)\n",
        "# \"1jxw27wp\" # ICML-3 solar-sweep\n",
        "# \"yo3fprzr\" # ICML-3 dry-sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJi79Ias4IP_"
      },
      "outputs": [],
      "source": [
        "# @title CDKF run names\n",
        "# run_name = \"cy1j7jyg\" # ICML-2 fancy-sweep\n",
        "run_name = \"dgudjrut\" # ICML-3 quiet-sweep\n",
        "run_name = \"4nctqeby\" # ICML-3 honest-sweep\n",
        "# \"cdt4gir1\" tough-sweep\n",
        "# \"mpc58ktj\" young-sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okamp0foh-Yg"
      },
      "outputs": [],
      "source": [
        "# If sampling from the prior becomes problematic, run this to truncate the singular values of A\n",
        "# prior_params[\"A\"] = truncate_singular_values(prior_params[\"A\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlSFRedYCyUR"
      },
      "outputs": [],
      "source": [
        "# @title Sample from the prior and visualize its decoding\n",
        "key = key_0\n",
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "prior_sample = model.prior.sample(prior_params, shape=(1,), key=key)[0]\n",
        "plt.plot(prior_sample)\n",
        "plt.figure()\n",
        "# plot_pcs(prior_sample, 2)\n",
        "out_dist = model.decoder.apply(dec_params, prior_sample)\n",
        "y_decoded = out_dist.mean()\n",
        "plt.figure()\n",
        "plot_img_grid(y_decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPgdXO-fDWue"
      },
      "outputs": [],
      "source": [
        "key = jr.split(key)[0]\n",
        "data_id = 3#jr.choice(key, 100)\n",
        "data = data_dict[\"train_data\"][data_id]\n",
        "# states = targets[data_id]\n",
        "# angles = np.arctan2(states[:,0], states[:,1])\n",
        "plot_img_grid(data)\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sZcQduzMC5m-"
      },
      "outputs": [],
      "source": [
        "# @title Visualize the mean predicted trajectory from the model\n",
        "# This might be the wrong thing to do, because the prior dynamics might not be accurate for\n",
        "# specific observation sequences\n",
        "temp_params = deepcopy(run_params)\n",
        "temp_params[\"mask_size\"] = 0\n",
        "out = model.elbo(jr.PRNGKey(0), data, params, **temp_params)\n",
        "post_params = out[\"posterior_params\"]\n",
        "post_dist = model.posterior.distribution(post_params)\n",
        "Ex = post_dist.mean()\n",
        "A = prior_params[\"A\"]\n",
        "b = prior_params[\"b\"]\n",
        "T = 100\n",
        "Ex_pred = predict_forward(Ex[T//2-1], A, b, T//2)\n",
        "hs = plt.plot(Ex)\n",
        "hs_ = plt.plot(np.arange(T//2-1, T), np.concatenate([Ex[T//2-1][None], Ex_pred]), linestyle=\":\")\n",
        "plt.title(\"Posterior and predictions\")\n",
        "for i in range(len(hs)):\n",
        "    hs_[i].set_color(hs[i].get_color())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDmP8-yldd8U"
      },
      "outputs": [],
      "source": [
        "out_dist = model.decoder.apply(dec_params, np.concatenate([Ex[:T//2], Ex_pred]))\n",
        "y_decoded = out_dist.mean()\n",
        "plt.figure()\n",
        "plot_img_grid(y_decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0aed57nAXHoP"
      },
      "outputs": [],
      "source": [
        "# @title Visualize multiple possible future paths predicted by the model...!\n",
        "\n",
        "\n",
        "jax.config.update(\"jax_debug_nans\", False)\n",
        "train_data = data_dict[\"train_data\"][:20,:100]\n",
        "x_preds, svae_pred_lls = vmap(predict_multiple, in_axes=(0, None, None))\\\n",
        "    (train_data, key_0, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhKaOqzTZHRW"
      },
      "outputs": [],
      "source": [
        "# D = model.prior.latent_dims\n",
        "# offset = 100\n",
        "\n",
        "# plt.figure(figsize=(20, 10))\n",
        "# for i in range(D):\n",
        "#     plt.plot(Ex[:,i] + i * offset, color=colors[i])\n",
        "#     plt.plot(np.arange(50, 100), x_preds[data_id,:,:,i].T + i * offset, color=colors[i], linestyle=\":\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEaCclVd0AZv"
      },
      "source": [
        "## Look at how well the physical state can be decoded from the latent representations linearly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnfLNMgc-eRl"
      },
      "outputs": [],
      "source": [
        "temp_params = deepcopy(run_params)\n",
        "temp_params[\"mask_size\"] = 0\n",
        "\n",
        "targets = np.load(\"pendulum/pend_regression.npz\")[\"train_targets\"][:100]\n",
        "\n",
        "def encode(data):\n",
        "    out = model.elbo(jr.PRNGKey(0), data, params, **temp_params)\n",
        "    post_params = out[\"posterior_params\"]\n",
        "    post_dist = model.posterior.distribution(post_params)\n",
        "    return post_dist.mean()\n",
        "\n",
        "all_latents_train = vmap(encode)(data_dict[\"train_data\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qsufn5KZYUN1"
      },
      "outputs": [],
      "source": [
        "states = targets[:,::2]\n",
        "train_thetas = np.arctan2(states[:,:,0], states[:,:,1])\n",
        "train_omegas = train_thetas[:,1:]-train_thetas[:,:-1]\n",
        "thetas = train_thetas.flatten()\n",
        "omegas = train_omegas.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ofB0o2F_qOS"
      },
      "outputs": [],
      "source": [
        "D = 5\n",
        "xs_theta = all_latents_train.reshape((-1, D))\n",
        "xs_omega = all_latents_train[:,1:].reshape((-1, D))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es8t-Udfasa8"
      },
      "outputs": [],
      "source": [
        "W_theta, _, _, _ = np.linalg.lstsq(xs_theta, thetas)\n",
        "W_omega, _, _, _ = np.linalg.lstsq(xs_omega, omegas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPZckeRTDEbx"
      },
      "outputs": [],
      "source": [
        "test_id = 0 if \"test_id\" not in globals() else test_id + 1\n",
        "plt.plot(all_latents_train[test_id] @ W_theta, label=\"decoded\")\n",
        "plt.plot(train_thetas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum angle (train sequence #{})\".format(test_id))\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(all_latents_train[test_id] @ W_omega, label=\"decoded\")\n",
        "plt.plot(train_omegas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum velocity (train sequence #{})\".format(test_id))\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQAY3_BZA4Px"
      },
      "outputs": [],
      "source": [
        "test_targets = np.load(\"pendulum/pend_regression_longer.npz\")[\"test_targets\"][:20, ::2][:, :100]\n",
        "test_data = data_dict[\"val_data\"][:, :100]#np.load(\"pendulum/pend_regression.npz\")[\"test_obs\"][:, ::2]\n",
        "thetas = np.arctan2(test_targets[:,:,0], test_targets[:,:,1])\n",
        "omegas = thetas[:,1:]-thetas[:,:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmcd1UabBcZg"
      },
      "outputs": [],
      "source": [
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "all_latents_test = vmap(encode)(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QOzPyLqZ78y"
      },
      "outputs": [],
      "source": [
        "test_id = 0 if \"test_id\" not in globals() else test_id + 1\n",
        "plt.plot(all_latents_test[test_id] @ W_theta, label=\"decoded\")\n",
        "plt.plot(thetas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum angle (test sequence #{})\".format(test_id))\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(all_latents_test[test_id] @ W_omega, label=\"decoded\")\n",
        "plt.plot(omegas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum velocity (test sequence #{})\".format(test_id))\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxjbHKWcPSmd"
      },
      "source": [
        "## Evaluate the sliding window prediction log likelihoods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvNpnf-wPXsk"
      },
      "outputs": [],
      "source": [
        "def prediction_lls(post_params):\n",
        "    posterior = model.posterior.distribution(post_params)\n",
        "    # Get the final mean and covariance\n",
        "    mu, Sigma = posterior.mean()[-1], posterior.covariance()[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAuuK-FGRUW2"
      },
      "outputs": [],
      "source": [
        "obj, out_dict = svae_loss(key, model, data_dict[\"train_data\"][:10], params, **temp_params)\n",
        "post_params = out_dict[\"posterior_params\"]\n",
        "posterior = model.posterior.distribution(post_params)\n",
        "J = posterior.filtered_precisions\n",
        "h = posterior.filtered_linear_potentials\n",
        "Sigma_filtered = inv(J)\n",
        "mu_filtered = np.einsum(\"...ij,...j->...i\", Sigma_filtered, h)\n",
        "data_batch = data_dict[\"train_data\"]\n",
        "horizon = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ1y0BMfSNWf"
      },
      "outputs": [],
      "source": [
        "def pred_ll(data_id, key):\n",
        "    num_windows = T-horizon-1\n",
        "    pred_lls = vmap(sliding_window_prediction_lls, in_axes=(None, None, None, 0, 0))(\n",
        "        mu_filtered[data_id], Sigma_filtered[data_id], data_batch[data_id],\n",
        "        np.arange(num_windows), jr.split(key, num_windows))\n",
        "    return pred_lls.mean(axis=0)\n",
        "\n",
        "def sliding_window_prediction_lls(mu, Sigma, data, t, key):\n",
        "    # Build the posterior object on the future latent states \n",
        "    # (\"the posterior predictive distribution\")\n",
        "    # Convert unconstrained params to constrained dynamics parameters\n",
        "    prior_params_constrained = model.prior.get_dynamics_params(prior_params)\n",
        "    dynamics_params = {\n",
        "        \"m1\": mu[t],\n",
        "        \"Q1\": Sigma[t],\n",
        "        \"A\": prior_params_constrained[\"A\"],\n",
        "        \"b\": prior_params_constrained[\"b\"],\n",
        "        \"Q\": prior_params_constrained[\"Q\"]\n",
        "    }\n",
        "    tridiag_params = dynamics_to_tridiag(dynamics_params, horizon+1, D) # Note the +1\n",
        "    J, L, h = tridiag_params[\"J\"], tridiag_params[\"L\"], tridiag_params[\"h\"]\n",
        "    pred_posterior = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "    # Sample from it and evaluate the log likelihood\n",
        "    x_pred = pred_posterior.sample(seed=key)[1:]\n",
        "    likelihood_dist = model.decoder.apply(dec_params, x_pred)\n",
        "    return likelihood_dist.log_prob(\n",
        "        lax.dynamic_slice(data, (t+1, 0, 0, 0), (horizon,) + data.shape[1:])).sum(axis=(1, 2, 3))\n",
        "\n",
        "num_windows = T-horizon-1\n",
        "pred_lls = vmap(pred_ll)(np.arange(10), jr.split(key_0, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoDd5GdpUAqi"
      },
      "outputs": [],
      "source": [
        "plt.plot(pred_lls.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRM7zBPx4pdE"
      },
      "source": [
        "## Evaluation code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD3BXGWQ6ko_"
      },
      "source": [
        "# What is going on with the dynamics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtChfl-eG5qm"
      },
      "outputs": [],
      "source": [
        "theta = 2 * np.pi / 100\n",
        "lds_params = {\n",
        "    \"m1\": np.zeros(2),\n",
        "    \"Q1\": np.eye(2),\n",
        "    \"A\": np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]]),\n",
        "    \"Q\": np.eye(2) / 100,\n",
        "    \"b\": np.zeros(2)\n",
        "}\n",
        "prior = LinearGaussianChain(2, 100)\n",
        "prior_dist = prior.distribution(prior.get_constrained_params(lds_params))\n",
        "plt.plot(prior_dist.sample(seed=key_0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEvsYfef6kdW"
      },
      "outputs": [],
      "source": [
        "temp_params = deepcopy(run_params)\n",
        "temp_params[\"mask_size\"] = 0\n",
        "_, aux = svae_loss(key_0, model, data_dict[\"train_data\"][:10], params, **temp_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRmFHEyb67Un"
      },
      "outputs": [],
      "source": [
        "pp = deepcopy(params[\"prior_params\"])\n",
        "suff_stats = aux[\"sufficient_statistics\"]\n",
        "pp[\"avg_suff_stats\"] = suff_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YMXhGbU7RmK"
      },
      "outputs": [],
      "source": [
        "fit_prior_params = model.prior.m_step(pp)\n",
        "# fit_prior_params[\"A\"] = scale_singular_values(fit_prior_params[\"A\"])\n",
        "# fit_prior_params[\"A\"] = truncate_singular_values(fit_prior_params[\"A\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qU7e1QiDMvy"
      },
      "outputs": [],
      "source": [
        "key = key_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnFVPlKBBhz0"
      },
      "outputs": [],
      "source": [
        "m1 = Q = fit_prior_params[\"m1\"]\n",
        "Q1 = fit_prior_params[\"Q1\"]\n",
        "Q = fit_prior_params[\"Q\"]\n",
        "A = fit_prior_params[\"A\"]\n",
        "b = fit_prior_params[\"b\"]\n",
        "\n",
        "\n",
        "x = jr.multivariate_normal(key=key, mean=m1, cov=Q1)\n",
        "xs = []\n",
        "for i in range(200):\n",
        "    xs.append(x)\n",
        "    key, _ = jr.split(key)\n",
        "    noise = jr.multivariate_normal(key=key, mean=np.zeros_like(x), cov=Q)\n",
        "    x = A @ x + b + noise\n",
        "\n",
        "plt.plot(np.array(xs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Beq52DGTBr0U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHEwqDfA8Awl"
      },
      "outputs": [],
      "source": [
        "prior = LinearGaussianChain(model.prior.latent_dims, model.prior.seq_len)\n",
        "prior_dist = prior.distribution(prior.get_constrained_params(fit_prior_params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT-OnbBX9SC7"
      },
      "outputs": [],
      "source": [
        "Q = fit_prior_params[\"Q\"]\n",
        "A = fit_prior_params[\"A\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uACoUB3T89M_"
      },
      "outputs": [],
      "source": [
        "sample = prior_dist.sample(seed=key_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISTJrxMD9Mq6"
      },
      "outputs": [],
      "source": [
        "plt.plot(sample)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}