{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthew9671/MattLab/blob/main/Revisiting_SVAE_Refactor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFyEe0Xi_84F"
      },
      "source": [
        "- [x] Implement and test Kalman filtering and smoothing with parallel scan\n",
        "- [ ] Make parallel scan KF work with non-zero biases\n",
        "- [x] Write analysis code for pendulum\n",
        "  - [x] Evaluate predictive accuracy\n",
        "  - [x] Do linear regression from latents to angle and velocity "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6u_EWGy0phmX"
      },
      "outputs": [],
      "source": [
        "# This reloads files not modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BJs02uuMM79",
        "outputId": "c343f11a-a2df-48d2-f542-4c31c8a03424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# @title Download stuff \n",
        "import os\n",
        "# Download and install the relevant libraries\n",
        "!pip install -q git+https://github.com/google/flax\n",
        "!pip install -q wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UbqX5CKc11Uc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde29ab1-bc28-4249-b904-7a8362c0b128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing dynamax\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dynamax\n",
            "  Cloning https://github.com/probml/dynamax.git to /tmp/pip-install-986r77bk/dynamax_10219299e75d49e9ac4ba8a19dc1dc43\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/probml/dynamax.git /tmp/pip-install-986r77bk/dynamax_10219299e75d49e9ac4ba8a19dc1dc43\n",
            "  Resolved https://github.com/probml/dynamax.git to commit 3d190b5b89990afe8d7bb43c08c1353f61555eb7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from dynamax) (4.4.0)\n",
            "Collecting jaxtyping\n",
            "  Downloading jaxtyping-0.2.11-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.8/dist-packages (from dynamax) (0.17.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.8/dist-packages (from dynamax) (0.1.4)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.8/dist-packages (from dynamax) (0.3.25+cuda11.cudnn805)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from dynamax) (1.0.2)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.8/dist-packages (from dynamax) (0.3.25)\n",
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.8/dist-packages (from dynamax) (1.0.3)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.15->dynamax) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.15->dynamax) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.15->dynamax) (1.21.6)\n",
            "Collecting typeguard>=2.13.3\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax->dynamax) (1.3.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from optax->dynamax) (0.1.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->dynamax) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->dynamax) (1.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->dynamax) (1.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->dynamax) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->dynamax) (0.4.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->dynamax) (0.1.8)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->dynamax) (2.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax->dynamax) (0.12.0)\n",
            "Building wheels for collected packages: dynamax\n",
            "  Building wheel for dynamax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dynamax: filename=dynamax-0.1.0+134.g3d190b5-py3-none-any.whl size=147460 sha256=a994e24a32327989411e081371dae26335a7a527fe5541dd329418dce56d0bc3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i7wchjpg/wheels/52/e1/7c/4664778646c92bb31957a1dac9e17ef231020b69e0b2e4d113\n",
            "Successfully built dynamax\n",
            "Installing collected packages: typeguard, jaxtyping, dynamax\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 2.7.1\n",
            "    Uninstalling typeguard-2.7.1:\n",
            "      Successfully uninstalled typeguard-2.7.1\n",
            "Successfully installed dynamax-0.1.0+134.g3d190b5 jaxtyping-0.2.11 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import dynamax\n",
        "except ModuleNotFoundError:\n",
        "    print('installing dynamax')\n",
        "    !pip install git+https://github.com/probml/dynamax.git#egg=dynamax\n",
        "    import dynamax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njW9wRLEPG7T"
      },
      "source": [
        "# Set everything up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "cellView": "form",
        "id": "b1ikkl1ULTEB"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "# Misc\n",
        "import os\n",
        "from importlib import reload\n",
        "import numpy as onp\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "from tqdm import trange\n",
        "import copy, traceback\n",
        "from pprint import pprint\n",
        "from copy import deepcopy\n",
        "import pickle as pkl\n",
        "\n",
        "# for logging\n",
        "import wandb\n",
        "# Debug\n",
        "import pdb\n",
        "# Jax\n",
        "import jax\n",
        "from jax import vmap, lax, jit, value_and_grad\n",
        "import jax.numpy as np\n",
        "import jax.scipy as scipy\n",
        "import jax.random as jr\n",
        "key_0 = jr.PRNGKey(0) # Convenience\n",
        "from jax.lax import scan, stop_gradient\n",
        "from jax.tree_util import tree_map\n",
        "# optax\n",
        "import optax as opt\n",
        "# Flax\n",
        "import flax.linen as nn\n",
        "from flax.linen import Conv, ConvTranspose\n",
        "from flax.core import frozen_dict as fd\n",
        "\n",
        "# Tensorflow probability\n",
        "import tensorflow_probability.substrates.jax as tfp\n",
        "import tensorflow_probability.substrates.jax.distributions as tfd\n",
        "from tensorflow_probability.python.internal import reparameterization\n",
        "MVN = tfd.MultivariateNormalFullCovariance\n",
        "\n",
        "# Dynamax (central to our implementation)\n",
        "from dynamax.linear_gaussian_ssm.inference import make_lgssm_params, lgssm_smoother\n",
        "# from dynamax.linear_gaussian_ssm.parallel_inference import lgssm_smoother as parallel_lgssm_smoother\n",
        "from dynamax.utils.utils import psd_solve\n",
        "\n",
        "# Common math functions\n",
        "from flax.linen import softplus, sigmoid\n",
        "from jax.scipy.special import logsumexp\n",
        "from jax.scipy.linalg import solve_triangular\n",
        "from jax.numpy.linalg import eigh, cholesky, svd, inv, solve\n",
        "\n",
        "# For typing in neural network utils\n",
        "from typing import (NamedTuple, Any, Callable, Sequence, Iterable, List, Optional, Tuple,\n",
        "                    Set, Type, Union, TypeVar, Generic, Dict)\n",
        "\n",
        "# For making the pendulum dataset\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "\n",
        "# For making nice visualizations\n",
        "from sklearn.decomposition import PCA\n",
        "from IPython.display import clear_output, HTML\n",
        "from matplotlib import animation, rc\n",
        "import seaborn as sns\n",
        "color_names = [\"windows blue\",\n",
        "                \"red\",\n",
        "                \"amber\",\n",
        "                \"faded green\",\n",
        "                \"dusty purple\",\n",
        "                \"orange\",\n",
        "                \"clay\",\n",
        "                \"pink\",\n",
        "                \"greyish\",\n",
        "                \"mint\",\n",
        "                \"light cyan\",\n",
        "                \"steel blue\",\n",
        "                \"forest green\",\n",
        "                \"pastel purple\",\n",
        "                \"salmon\",\n",
        "                \"dark brown\",\n",
        "               \"violet\",\n",
        "               \"mauve\",\n",
        "               \"ocean\",\n",
        "               \"ugly yellow\"]\n",
        "colors = sns.xkcd_palette(color_names)\n",
        "\n",
        "# Get rid of the check types warning\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "class CheckTypesFilter(logging.Filter):\n",
        "    def filter(self, record):\n",
        "        return \"check_types\" not in record.getMessage()\n",
        "logger.addFilter(CheckTypesFilter())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "cellView": "form",
        "id": "2MjuUxHxR5O0"
      },
      "outputs": [],
      "source": [
        "# @title Misc helpers\n",
        "def get_value(x):\n",
        "    try:\n",
        "        return x.val.val.primal\n",
        "    except:\n",
        "        try:\n",
        "            return x.val.val\n",
        "        except:\n",
        "            try:\n",
        "                return x.val\n",
        "            except:\n",
        "                return x  # Oh well.\n",
        "\n",
        "def plot_img_grid(recon):\n",
        "    plt.figure(figsize=(8,8))\n",
        "    # Show the sequence as a block of images\n",
        "    stacked = recon.reshape(10, 24 * 10, 24)\n",
        "    imgrid = stacked.swapaxes(0, 1).reshape(24 * 10, 24 * 10)\n",
        "    plt.imshow(imgrid, vmin=0, vmax=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "cellView": "form",
        "id": "FGleKPUALeEd"
      },
      "outputs": [],
      "source": [
        "# @title Math helpers\n",
        "def softplus(x):\n",
        "    return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0)\n",
        "\n",
        "def inv_softplus(x, eps=1e-4):\n",
        "    return np.log(np.exp(x - eps) - 1)\n",
        "\n",
        "def vectorize_pytree(*args):\n",
        "    \"\"\"\n",
        "    Flatten an arbitrary PyTree into a vector.\n",
        "    :param args:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    flat_tree, _ = jax.tree_util.tree_flatten(args)\n",
        "    flat_vs = [x.flatten() for x in flat_tree]\n",
        "    return np.concatenate(flat_vs, axis=0)\n",
        "\n",
        "# converts an (n(n+1)/2,) vector of Lie parameters\n",
        "# to an (n, n) matrix\n",
        "def lie_params_to_constrained(out_flat, dim, eps=1e-4):\n",
        "    D, A = out_flat[:dim], out_flat[dim:]\n",
        "    # ATTENTION: we changed this!\n",
        "    # D = np.maximum(softplus(D), eps)\n",
        "    D = softplus(D) + eps\n",
        "    # Build a skew-symmetric matrix\n",
        "    S = np.zeros((dim, dim))\n",
        "    i1, i2 = np.tril_indices(dim - 1)\n",
        "    S = S.at[i1+1, i2].set(A)\n",
        "    S = S.T\n",
        "    S = S.at[i1+1, i2].set(-A)\n",
        "\n",
        "    O = scipy.linalg.expm(S)\n",
        "    J = O.T @ np.diag(D) @ O\n",
        "    return J\n",
        "\n",
        "# converts an (n, n) matrix \n",
        "# to an (n, n) matrix with singular values in (0, 1)\n",
        "def get_constrained_dynamics(A):\n",
        "    dim = A.shape[0]\n",
        "    diag = np.diag(A)\n",
        "    diag = sigmoid(diag)\n",
        "    # Build a skew-symmetric matrix\n",
        "    S = np.zeros((dim, dim))\n",
        "    i1, i2 = np.tril_indices(dim - 1)\n",
        "    S = S.at[i1+1, i2].set(A[i1+1, i2])\n",
        "    S = S.T\n",
        "    S = S.at[i1+1, i2].set(-A[i1+1, i2])\n",
        "    U = scipy.linalg.expm(S)\n",
        "\n",
        "    S = np.zeros((dim, dim))\n",
        "    i1, i2 = np.tril_indices(dim - 1)\n",
        "    S = S.at[i1+1, i2].set(A.T[i1+1, i2])\n",
        "    S = S.T\n",
        "    S = S.at[i1+1, i2].set(-A.T[i1+1, i2])\n",
        "    V = scipy.linalg.expm(S)\n",
        "\n",
        "    A = U @ np.diag(diag) @ V\n",
        "    return A, U, V\n",
        "\n",
        "def scale_singular_values(A):\n",
        "    _, s, _ = svd(A)\n",
        "    return A / (np.maximum(1, np.max(s)))\n",
        "\n",
        "def truncate_singular_values(A):\n",
        "    eps = 1e-3\n",
        "    u, s, vt = svd(A)\n",
        "    return u @ np.diag(np.clip(s, eps, 1)) @ vt\n",
        "\n",
        "# Assume that h has a batch shape here\n",
        "def sample_info_gaussian(seed, J, h):\n",
        "    # Avoid inversion.\n",
        "    # see https://github.com/mattjj/pybasicbayes/blob/master/pybasicbayes/util/stats.py#L117-L122\n",
        "    L = np.linalg.cholesky(J)\n",
        "    x = jr.normal(key=seed, shape=h.shape)\n",
        "    return solve_triangular(L,x.T,lower=True,trans='T').T \\\n",
        "        + np.linalg.solve(J,h.T).T\n",
        "\n",
        "def sample_info_gaussian_old(seed, J, h):\n",
        "    cov = np.linalg.inv(J)\n",
        "    loc = np.einsum(\"...ij,...j->...i\", cov, h)\n",
        "    return tfp.distributions.MultivariateNormalFullCovariance(\n",
        "        loc=loc, covariance_matrix=cov).sample(sample_shape=(), seed=seed)\n",
        "\n",
        "def random_rotation(seed, n, theta=None):\n",
        "    key1, key2 = jr.split(seed)\n",
        "\n",
        "    if theta is None:\n",
        "        # Sample a random, slow rotation\n",
        "        theta = 0.5 * np.pi * jr.uniform(key1)\n",
        "\n",
        "    if n == 1:\n",
        "        return jr.uniform(key1) * np.eye(1)\n",
        "\n",
        "    rot = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
        "    out = np.eye(n)\n",
        "    out = out.at[:2, :2].set(rot)\n",
        "    q = np.linalg.qr(jr.uniform(key2, shape=(n, n)))[0]\n",
        "    return q.dot(out).dot(q.T)\n",
        "\n",
        "# Computes ATQ-1A in a way that's guaranteed to be symmetric\n",
        "def inv_quad_form(Q, A):\n",
        "    sqrt_Q = np.linalg.cholesky(Q)\n",
        "    trm = solve_triangular(sqrt_Q, A, lower=True, check_finite=False)\n",
        "    return trm.T @ trm\n",
        "\n",
        "def inv_symmetric(Q):\n",
        "    sqrt_Q = np.linalg.cholesky(Q)\n",
        "    sqrt_Q_inv = np.linalg.inv(sqrt_Q)\n",
        "    return sqrt_Q_inv.T @ sqrt_Q_inv\n",
        "\n",
        "# Converts from (A, b, Q) to (J, L, h)\n",
        "def dynamics_to_tridiag(dynamics_params, T, D):\n",
        "    Q1, m1, A, Q, b = dynamics_params[\"Q1\"], \\\n",
        "        dynamics_params[\"m1\"], dynamics_params[\"A\"], \\\n",
        "        dynamics_params[\"Q\"], dynamics_params[\"b\"]\n",
        "    # diagonal blocks of precision matrix\n",
        "    J = np.zeros((T, D, D))\n",
        "    J = J.at[0].add(inv_symmetric(Q1))\n",
        "\n",
        "    J = J.at[:-1].add(inv_quad_form(Q, A))\n",
        "    J = J.at[1:].add(inv_symmetric(Q))\n",
        "    # lower diagonal blocks of precision matrix\n",
        "    L = -np.linalg.solve(Q, A)\n",
        "    L = np.tile(L[None, :, :], (T - 1, 1, 1))\n",
        "    # linear potential\n",
        "    h = np.zeros((T, D)) \n",
        "    h = h.at[0].add(np.linalg.solve(Q1, m1))\n",
        "    h = h.at[:-1].add(-np.dot(A.T, np.linalg.solve(Q, b)))\n",
        "    h = h.at[1:].add(np.linalg.solve(Q, b))\n",
        "    return { \"J\": J, \"L\": L, \"h\": h }\n",
        "\n",
        "# Helper function: solve a linear regression given expected sufficient statistics\n",
        "def fit_linear_regression(Ex, Ey, ExxT, EyxT, EyyT, En):\n",
        "    big_ExxT = np.row_stack([np.column_stack([ExxT, Ex]),\n",
        "                            np.concatenate( [Ex.T, np.array([En])])])\n",
        "    big_EyxT = np.column_stack([EyxT, Ey])\n",
        "    Cd = np.linalg.solve(big_ExxT, big_EyxT.T).T\n",
        "    C, d = Cd[:, :-1], Cd[:, -1]\n",
        "    R = (EyyT - 2 * Cd @ big_EyxT.T + Cd @ big_ExxT @ Cd.T) / En\n",
        "\n",
        "    # Manually symmetrize R\n",
        "    R = (R + R.T) / 2\n",
        "    return C, d, R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "NeeYgySA0hPT"
      },
      "outputs": [],
      "source": [
        "# @title Experiment scheduler\n",
        "LINE_SEP = \"#\" * 42\n",
        "\n",
        "def dict_len(d):\n",
        "    if (type(d) == list):\n",
        "        return len(d)\n",
        "    else:\n",
        "        return dict_len(d[list(d.keys())[0]])\n",
        "\n",
        "def dict_map(d, func):\n",
        "    if type(d) == list:\n",
        "        return func(d)\n",
        "    elif type(d) == dict:\n",
        "        r = copy.deepcopy(d)\n",
        "        for key in d.keys():\n",
        "            r[key] = dict_map(r[key], func)\n",
        "            # Ignore all the Nones\n",
        "            if r[key] is None:\n",
        "                r.pop(key)\n",
        "        if len(r.keys()) == 0:\n",
        "            # There's no content\n",
        "            return None\n",
        "        else:\n",
        "            return r\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def dict_product(d1, d2):\n",
        "    l1, l2 = dict_len(d1), dict_len(d2)\n",
        "    def expand_list(d):\n",
        "        result = []\n",
        "        for item in d:\n",
        "            result.append(item)\n",
        "            result.extend([None] * (l2-1))\n",
        "        return result\n",
        "    def multiply_list(d):\n",
        "        return d * l1\n",
        "    result = dict_map(d1, expand_list)\n",
        "    additions = dict_map(d2, multiply_list)\n",
        "    return dict_update(result, additions)\n",
        "\n",
        "def dict_get(d, id):\n",
        "    return dict_map(d, lambda l: l[id])\n",
        "\n",
        "def dict_update(d, u):\n",
        "    if d is None:\n",
        "        d = dict()\n",
        "    for key in u.keys():\n",
        "        if type(u[key]) == dict:\n",
        "            d.update({\n",
        "                key: dict_update(d.get(key), u[key])\n",
        "            })\n",
        "        else:\n",
        "            d.update({key: u[key]})\n",
        "    return d\n",
        "\n",
        "# A standardized function that structures and schedules experiments\n",
        "# Can chain multiple variations of experiment parameters together\n",
        "def experiment_scheduler(run_params, dataset_getter, model_getter, train_func, \n",
        "                         logger_func=None, err_logger_func=None, \n",
        "                         run_variations=None, params_expander=None,\n",
        "                         on_error=None, continue_on_error=True, use_wandb=True):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        run_params: dict{\"dataset_params\"} \n",
        "            A large dictionary containing all relevant parameters to the run \n",
        "        dataset_getter: run_params -> dict{\"train_data\", [\"generative_model\"]}\n",
        "            A function that loads/samples a dataset\n",
        "        model_getter: run_params, data_dict -> model\n",
        "            A function that creates a model given parameters. Note that the model\n",
        "            could depend on the specifics of the dataset/generative model as well\n",
        "        train_func: model, data, run_params -> results\n",
        "            A function that contains the training loop. \n",
        "            TODO: later we might wanna open up this pipeline and customize further!\n",
        "        (optional) logger_func: results, run_params -> ()\n",
        "            A function that logs the current run.\n",
        "        (optional) err_logger_func: message, run_params -> ()\n",
        "            A function that is called when the run fails.\n",
        "        (optional) run_variations: dict{}\n",
        "            A nested dictionary where the leaves are lists of different parameters.\n",
        "            None means no change from parameters of the last run.\n",
        "        (optional) params_expander: dict{} -> dict{}\n",
        "            Turns high level parameters into specific low level parameters.\n",
        "    returns:\n",
        "        all_results: List<result>\n",
        "            A list containing results from all runs. Failed runs are indicated\n",
        "            with a None value.\n",
        "    \"\"\"\n",
        "    params_expander = params_expander or (lambda d: d)\n",
        "\n",
        "    num_runs = dict_len(run_variations) if run_variations else 1\n",
        "    params = copy.deepcopy(run_params)\n",
        "    print(\"Total number of runs: {}\".format(num_runs))\n",
        "    print(\"Base paramerters:\")\n",
        "    pprint(params)\n",
        "\n",
        "    global data_dict\n",
        "    all_results = []\n",
        "    all_models = []\n",
        "\n",
        "    def _single_run(data_out, model_out):\n",
        "        print(\"Loading dataset!\")\n",
        "        data_dict = dataset_getter(curr_params)\n",
        "        data_out.append(data_dict)\n",
        "        # Make a new model\n",
        "        model_dict = model_getter(curr_params, data_dict)\n",
        "        model_out.append(model_dict)\n",
        "        all_models.append(model_dict)\n",
        "        results = train_func(model_dict, data_dict, curr_params)\n",
        "        all_results.append(results)\n",
        "        if logger_func:\n",
        "            logger_func(results, curr_params, data_dict)\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        print(LINE_SEP)\n",
        "        print(\"Starting run #{}\".format(run))\n",
        "        print(LINE_SEP)\n",
        "        curr_variation = dict_get(run_variations, run)\n",
        "        if curr_variation is None:\n",
        "            if (run != 0):\n",
        "                print(\"Variation #{} is a duplicate, skipping run.\".format(run))\n",
        "                continue\n",
        "            curr_params = params_expander(params)\n",
        "        else:\n",
        "            print(\"Current parameter variation:\")\n",
        "            pprint(curr_variation)\n",
        "            curr_params = dict_update(params, curr_variation)\n",
        "            curr_params = params_expander(curr_params)\n",
        "            print(\"Current full parameters:\")\n",
        "            pprint(curr_params)\n",
        "            if curr_variation.get(\"dataset_params\"):\n",
        "                reload_data = True\n",
        "        # Hack to get the values even when they err out\n",
        "        data_out = []\n",
        "        model_out = []\n",
        "        if not continue_on_error:\n",
        "            _single_run(data_out, model_out)\n",
        "        else:\n",
        "            try:\n",
        "                _single_run(data_out, model_out)\n",
        "                if use_wandb: wandb.finish()\n",
        "            except:\n",
        "                all_results.append(None)\n",
        "                if (on_error): \n",
        "                    try:\n",
        "                        on_error(data_out[0], model_out[0])\n",
        "                    except:\n",
        "                        pass # Oh well...\n",
        "                print(\"Run errored out due to some the following reason:\")\n",
        "                traceback.print_exc()\n",
        "                if use_wandb: wandb.finish(exit_code=1)\n",
        "    return all_results, all_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OPHhcbZObuu"
      },
      "source": [
        "## Define the base SVAE object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "9Pj8Xn9jNBz1"
      },
      "outputs": [],
      "source": [
        "class SVAE:\n",
        "    def __init__(self,\n",
        "                 recognition=None, decoder=None, prior=None, posterior=None,\n",
        "                 input_dummy=None, latent_dummy=None):\n",
        "        \"\"\"\n",
        "        rec_net, dec_net, prior are all objects that take in parameters\n",
        "        rec_net.apply(params, data) returns Gaussian potentials (parameters)\n",
        "        dec_net.apply(params, latents) returns probability distributions\n",
        "        prior : SVAEPrior\n",
        "        \"\"\"\n",
        "        self.recognition = recognition\n",
        "        self.decoder = decoder\n",
        "        self.prior = prior\n",
        "        self.posterior = posterior\n",
        "        self.input_dummy = input_dummy\n",
        "        self.latent_dummy = latent_dummy\n",
        "\n",
        "    def init(self, key=None):\n",
        "        if key is None:\n",
        "            key = jr.PRNGKey(0)\n",
        "\n",
        "        rec_key, dec_key, prior_key, post_key = jr.split(key, 4)\n",
        "\n",
        "        return {\n",
        "            \"rec_params\": self.recognition.init(rec_key, self.input_dummy),\n",
        "            \"dec_params\": self.decoder.init(dec_key, self.latent_dummy),\n",
        "            \"prior_params\": self.prior.init(prior_key),\n",
        "            \"post_params\": self.posterior.init(post_key)\n",
        "        }\n",
        "\n",
        "    def kl_posterior_prior(self, posterior_params, prior_params, \n",
        "                           samples=None):\n",
        "        posterior = self.posterior.distribution(posterior_params)\n",
        "        prior = self.prior.distribution(prior_params)\n",
        "        if samples is None:\n",
        "            return posterior.kl_divergence(prior)\n",
        "        else:\n",
        "            return np.mean(posterior.log_prob(samples) - prior.log_prob(samples))\n",
        "\n",
        "    def elbo(self, key, data, model_params, sample_kl=False, **params):\n",
        "        rec_params = model_params[\"rec_params\"]\n",
        "        dec_params = model_params[\"dec_params\"]\n",
        "        prior_params = self.prior.get_constrained_params(model_params[\"prior_params\"])\n",
        "\n",
        "        # Mask out a large window of states\n",
        "        mask_size = params.get(\"mask_size\")\n",
        "        T = data.shape[0]\n",
        "        D = self.prior.latent_dims\n",
        "\n",
        "        mask = onp.ones((T,))\n",
        "        key, dropout_key = jr.split(key)\n",
        "        if mask_size:\n",
        "            # Potential dropout...!\n",
        "            # Use a trick to generate the mask without indexing with a tracer\n",
        "            start_id = jr.choice(dropout_key, T - mask_size + 1)\n",
        "            mask = np.array(np.arange(T) >= start_id) \\\n",
        "                 * np.array(np.arange(T) < start_id + mask_size)\n",
        "            mask = 1 - mask\n",
        "            if params.get(\"mask_type\") == \"potential\":\n",
        "                # This only works with svaes\n",
        "                potential = self.recognition.apply(rec_params, data)\n",
        "                # Uninformative potential\n",
        "                infinity = 1e5\n",
        "                uninf_potential = {\"mu\": np.zeros((T, D)), \n",
        "                                   \"Sigma\": np.tile(np.eye(D) * infinity, (T, 1, 1))}\n",
        "                # Replace masked parts with uninformative potentials\n",
        "                potential = tree_map(\n",
        "                    lambda t1, t2: np.einsum(\"i,i...->i...\", mask[:t1.shape[0]], t1) \n",
        "                                 + np.einsum(\"i,i...->i...\", 1-mask[:t2.shape[0]], t2), \n",
        "                    potential, \n",
        "                    uninf_potential)\n",
        "            else:\n",
        "                potential = self.recognition.apply(rec_params, \n",
        "                                                   np.einsum(\"t...,t->t...\", data, mask))\n",
        "        else:\n",
        "            # Don't do any masking\n",
        "            potential = self.recognition.apply(rec_params, data)\n",
        "\n",
        "        # Update: it makes more sense that inference is done in the posterior object\n",
        "        posterior_params = self.posterior.infer(prior_params, potential)\n",
        "        \n",
        "        # Take samples under the posterior\n",
        "        num_samples = params.get(\"obj_samples\") or 1\n",
        "        samples = self.posterior.sample(posterior_params, (num_samples,), key)\n",
        "        # and compute average ll\n",
        "\n",
        "        def likelihood_outputs(latent):\n",
        "            likelihood_dist = self.decoder.apply(dec_params, latent)\n",
        "            return likelihood_dist.mean(), likelihood_dist.log_prob(data)\n",
        "\n",
        "        mean, ells = vmap(likelihood_outputs)(samples)\n",
        "        # Take average over samples then sum the rest\n",
        "        ell = np.sum(np.mean(ells, axis=0))\n",
        "        # Compute kl from posterior to prior\n",
        "        if sample_kl:\n",
        "            kl = self.kl_posterior_prior(posterior_params, prior_params, \n",
        "                                         samples=samples)\n",
        "        else:\n",
        "            kl = self.kl_posterior_prior(posterior_params, prior_params)\n",
        "\n",
        "        elbo = ell - kl\n",
        "\n",
        "        return {\n",
        "            \"elbo\": elbo,\n",
        "            \"ell\": ell,\n",
        "            \"kl\": kl,\n",
        "            \"posterior_params\": posterior_params,\n",
        "            \"posterior_samples\": samples,\n",
        "            \"reconstruction\": mean,\n",
        "            \"mask\": mask\n",
        "        }\n",
        "\n",
        "    def compute_objective(self, key, data, model_params, **params):\n",
        "        results = self.elbo(key, data, model_params, **params)\n",
        "        results[\"objective\"] = results[\"elbo\"]\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "cellView": "form",
        "id": "7ckaLRUL1QVb"
      },
      "outputs": [],
      "source": [
        "# @title The DeepLDS object (implements custom kl function)\n",
        "class DeepLDS(SVAE):\n",
        "    def kl_posterior_prior(self, posterior_params, prior_params, \n",
        "                           samples=None):\n",
        "        posterior = self.posterior.distribution(posterior_params)\n",
        "        prior = self.prior.distribution(prior_params)\n",
        "        if samples is None:\n",
        "            Ex = posterior.expected_states\n",
        "            ExxT = posterior.expected_states_squared\n",
        "            ExnxT = posterior.expected_states_next_states\n",
        "            Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "            Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "            J, L = prior_params[\"J\"], prior_params[\"L\"]\n",
        "\n",
        "            cross_entropy = -prior.log_prob(Ex)\n",
        "            cross_entropy += 0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "            cross_entropy += np.einsum(\"tij,tij->\", L, Sigmatnt)\n",
        "            return cross_entropy - posterior.entropy()\n",
        "            \n",
        "        else:\n",
        "            return np.mean(posterior.log_prob(samples) - prior.log_prob(samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "Nre5PK2MPt2N"
      },
      "outputs": [],
      "source": [
        "# @title SVAE Prior object\n",
        "class SVAEPrior:\n",
        "    def init(self, key):\n",
        "        \"\"\"\n",
        "        Returns the initial prior parameters.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def distribution(self, prior_params):\n",
        "        \"\"\"\n",
        "        Returns a tfp distribution object\n",
        "        Takes constrained params\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def m_step(self, prior_params, posterior, post_params):\n",
        "        \"\"\"\n",
        "        Returns updated prior parameters.\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def sample(self, params, shape, key):\n",
        "        return self.distribution(\n",
        "            self.get_constrained_params(params)).sample(sample_shape=shape, seed=key)\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        return deepcopy(params)\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOPpeMzjj42H"
      },
      "source": [
        "## Information form (deprecated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "cellView": "form",
        "id": "NHkdKpcxOi1S"
      },
      "outputs": [],
      "source": [
        "# @title MVN tridiag object (taken from ssm)\n",
        "from tensorflow_probability.python.internal import reparameterization\n",
        "\n",
        "def block_tridiag_mvn_log_normalizer(J_diag, J_lower_diag, h):\n",
        "    \"\"\" TODO\n",
        "    \"\"\"\n",
        "    # extract dimensions\n",
        "    num_timesteps, dim = J_diag.shape[:2]\n",
        "\n",
        "    # Pad the L's with one extra set of zeros for the last predict step\n",
        "    J_lower_diag_pad = np.concatenate((J_lower_diag, np.zeros((1, dim, dim))), axis=0)\n",
        "\n",
        "    def marginalize(carry, t):\n",
        "        Jp, hp, lp = carry\n",
        "\n",
        "        # Condition\n",
        "        Jc = J_diag[t] + Jp\n",
        "        hc = h[t] + hp\n",
        "\n",
        "        # Predict -- Cholesky approach seems unstable!\n",
        "        # sqrt_Jc = np.linalg.cholesky(Jc)\n",
        "        # trm1 = solve_triangular(sqrt_Jc, hc, lower=True)\n",
        "        # trm2 = solve_triangular(sqrt_Jc, J_lower_diag_pad[t].T, lower=True)\n",
        "        # log_Z = 0.5 * dim * np.log(2 * np.pi)\n",
        "        # log_Z += -np.sum(np.log(np.diag(sqrt_Jc)))  # sum these terms only to get approx log|J|\n",
        "        # log_Z += 0.5 * np.dot(trm1.T, trm1)\n",
        "        # Jp = -np.dot(trm2.T, trm2)\n",
        "        # hp = -np.dot(trm2.T, trm1)\n",
        "\n",
        "        # Alternative predict step:\n",
        "        log_Z = 0.5 * dim * np.log(2 * np.pi)\n",
        "        log_Z += -0.5 * np.linalg.slogdet(Jc)[1]\n",
        "        log_Z += 0.5 * np.dot(hc, np.linalg.solve(Jc, hc))\n",
        "        Jp = -np.dot(J_lower_diag_pad[t], np.linalg.solve(Jc, J_lower_diag_pad[t].T))\n",
        "        # Jp = (Jp + Jp.T) * .5   # Manual symmetrization\n",
        "        hp = -np.dot(J_lower_diag_pad[t], np.linalg.solve(Jc, hc))\n",
        "\n",
        "        new_carry = Jp, hp, lp + log_Z\n",
        "        return new_carry, (Jc, hc)\n",
        "\n",
        "    # Initialize\n",
        "    Jp0 = np.zeros((dim, dim))\n",
        "    hp0 = np.zeros((dim,))\n",
        "    (_, _, log_Z), (filtered_Js, filtered_hs) = lax.scan(marginalize, (Jp0, hp0, 0), np.arange(num_timesteps))\n",
        "    return log_Z, (filtered_Js, filtered_hs)\n",
        "\n",
        "class MultivariateNormalBlockTridiag(tfd.Distribution):\n",
        "    \"\"\"\n",
        "    The Gaussian linear dynamical system's posterior distribution over latent states\n",
        "    is a multivariate normal distribution whose _precision_ matrix is\n",
        "    block tridiagonal.\n",
        "\n",
        "        x | y ~ N(\\mu, \\Sigma)\n",
        "\n",
        "    where\n",
        "\n",
        "        \\Sigma^{-1} = J = [[J_{0,0},   J_{0,1},   0,       0,      0],\n",
        "                           [J_{1,0},   J_{1,1},   J_{1,2}, 0,      0],\n",
        "                           [0,         J_{2,1},   J_{2,2}, \\ddots, 0],\n",
        "                           [0,         0,         \\ddots,  \\ddots,  ],\n",
        "\n",
        "    is block tridiagonal, and J_{t, t+1} = J_{t+1, t}^T.\n",
        "\n",
        "    The pdf is\n",
        "\n",
        "        p(x) = exp \\{-1/2 x^T J x + x^T h - \\log Z(J, h) \\}\n",
        "             = exp \\{- 1/2 \\sum_{t=1}^T x_t^T J_{t,t} x_t\n",
        "                     - \\sum_{t=1}^{T-1} x_{t+1}^T J_{t+1,t} x_t\n",
        "                     + \\sum_{t=1}^T x_t^T h_t\n",
        "                     -\\log Z(J, h)\\}\n",
        "\n",
        "    where J = \\Sigma^{-1} and h = \\Sigma^{-1} \\mu = J \\mu.\n",
        "\n",
        "    Using exponential family tricks we know that\n",
        "\n",
        "        E[x_t] = \\grad_{h_t} \\log Z(J, h)\n",
        "        E[x_t x_t^T] = -2 \\grad_{J_{t,t}} \\log Z(J, h)\n",
        "        E[x_{t+1} x_t^T] = -\\grad_{J_{t+1,t}} \\log Z(J, h)\n",
        "\n",
        "    These are the expectations we need for EM.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 precision_diag_blocks,\n",
        "                 precision_lower_diag_blocks,\n",
        "                 linear_potential,\n",
        "                 log_normalizer,\n",
        "                 filtered_precisions,\n",
        "                 filtered_linear_potentials,\n",
        "                 expected_states,\n",
        "                 expected_states_squared,\n",
        "                 expected_states_next_states,\n",
        "                 validate_args=False,\n",
        "                 allow_nan_stats=True,\n",
        "                 name=\"MultivariateNormalBlockTridiag\",\n",
        "             ) -> None:\n",
        "\n",
        "        self._precision_diag_blocks = precision_diag_blocks\n",
        "        self._precision_lower_diag_blocks = precision_lower_diag_blocks\n",
        "        self._linear_potential = linear_potential\n",
        "        self._log_normalizer = log_normalizer\n",
        "        self._filtered_precisions = filtered_precisions\n",
        "        self._filtered_linear_potentials = filtered_linear_potentials\n",
        "        self._expected_states = expected_states\n",
        "        self._expected_states_squared = expected_states_squared\n",
        "        self._expected_states_next_states = expected_states_next_states\n",
        "\n",
        "        # We would detect the dtype dynamically but that would break vmap\n",
        "        # see https://github.com/tensorflow/probability/issues/1271\n",
        "        dtype = np.float32\n",
        "        super(MultivariateNormalBlockTridiag, self).__init__(\n",
        "            dtype=dtype,\n",
        "            validate_args=validate_args,\n",
        "            allow_nan_stats=allow_nan_stats,\n",
        "            reparameterization_type=reparameterization.NOT_REPARAMETERIZED,\n",
        "            parameters=dict(precision_diag_blocks=self._precision_diag_blocks,\n",
        "                            precision_lower_diag_blocks=self._precision_lower_diag_blocks,\n",
        "                            linear_potential=self._linear_potential,\n",
        "                            log_normalizer=self._log_normalizer,\n",
        "                            filtered_precisions=self._filtered_precisions,\n",
        "                            filtered_linear_potentials=self._filtered_linear_potentials,\n",
        "                            expected_states=self._expected_states,\n",
        "                            expected_states_squared=self._expected_states_squared,\n",
        "                            expected_states_next_states=self._expected_states_next_states),\n",
        "            name=name,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _parameter_properties(cls, dtype, num_classes=None):\n",
        "        # pylint: disable=g-long-lambda\n",
        "        return dict(\n",
        "            precision_diag_blocks=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            precision_lower_diag_blocks=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            linear_potential=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "            log_normalizer=tfp.internal.parameter_properties.ParameterProperties(event_ndims=0),\n",
        "            filtered_precisions=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            filtered_linear_potentials=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "            expected_states=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "            expected_states_squared=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            expected_states_next_states=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def infer(cls,\n",
        "              precision_diag_blocks,\n",
        "              precision_lower_diag_blocks,\n",
        "              linear_potential):\n",
        "        assert precision_diag_blocks.ndim == 3\n",
        "        num_timesteps, dim = precision_diag_blocks.shape[:2]\n",
        "        assert precision_diag_blocks.shape[2] == dim\n",
        "        assert precision_lower_diag_blocks.shape == (num_timesteps - 1, dim, dim)\n",
        "        assert linear_potential.shape == (num_timesteps, dim)\n",
        "\n",
        "        # Run message passing code to get the log normalizer, the filtering potentials,\n",
        "        # and the expected values of x. Technically, the natural parameters are -1/2 J\n",
        "        # so we need to do a little correction of the gradients to get the expectations.\n",
        "        f = value_and_grad(block_tridiag_mvn_log_normalizer, argnums=(0, 1, 2), has_aux=True)\n",
        "        (log_normalizer, (filtered_precisions, filtered_linear_potentials)), grads = \\\n",
        "            f(precision_diag_blocks, precision_lower_diag_blocks, linear_potential)\n",
        "\n",
        "        # Manually symmetrize ExxT due to numerical issues...!!!\n",
        "        # Correct for the -1/2 J -> J implementation\n",
        "        expected_states_squared = - grads[0] - np.swapaxes(grads[0], -2, -1)\n",
        "        expected_states_next_states = -grads[1]\n",
        "        expected_states = grads[2]\n",
        "\n",
        "        return cls(precision_diag_blocks,\n",
        "                   precision_lower_diag_blocks,\n",
        "                   linear_potential,\n",
        "                   log_normalizer,\n",
        "                   filtered_precisions,\n",
        "                   filtered_linear_potentials,\n",
        "                   expected_states,\n",
        "                   expected_states_squared,\n",
        "                   expected_states_next_states)\n",
        "\n",
        "    @classmethod\n",
        "    def infer_from_precision_and_mean(cls,\n",
        "                                      precision_diag_blocks,\n",
        "                                      precision_lower_diag_blocks,\n",
        "                                      mean):\n",
        "        assert precision_diag_blocks.ndim == 3\n",
        "        num_timesteps, dim = precision_diag_blocks.shape[:2]\n",
        "        assert precision_diag_blocks.shape[2] == dim\n",
        "        assert precision_lower_diag_blocks.shape == (num_timesteps - 1, dim, dim)\n",
        "        assert mean.shape == (num_timesteps, dim)\n",
        "\n",
        "        # Convert the mean to the linear potential\n",
        "        linear_potential = np.einsum('tij,tj->ti', precision_diag_blocks, mean)\n",
        "        linear_potential = linear_potential.at[:-1].add(\n",
        "            np.einsum('tji,tj->ti', precision_lower_diag_blocks, mean[1:]))\n",
        "        linear_potential = linear_potential.at[1:].add(\n",
        "            np.einsum('tij,tj->ti', precision_lower_diag_blocks, mean[:-1]))\n",
        "\n",
        "        # Call the constructor above\n",
        "        return cls.infer(precision_diag_blocks,\n",
        "                         precision_lower_diag_blocks,\n",
        "                         linear_potential)\n",
        "\n",
        "    # Properties to get private class variables\n",
        "    @property\n",
        "    def precision_diag_blocks(self):\n",
        "        return self._precision_diag_blocks\n",
        "\n",
        "    @property\n",
        "    def precision_lower_diag_blocks(self):\n",
        "        return self._precision_lower_diag_blocks\n",
        "\n",
        "    @property\n",
        "    def linear_potential(self):\n",
        "        return self._linear_potential\n",
        "\n",
        "    @property\n",
        "    def log_normalizer(self):\n",
        "        return self._log_normalizer\n",
        "\n",
        "    @property\n",
        "    def filtered_precisions(self):\n",
        "        return self._filtered_precisions\n",
        "\n",
        "    @property\n",
        "    def filtered_linear_potentials(self):\n",
        "        return self._filtered_linear_potentials\n",
        "\n",
        "    @property\n",
        "    def filtered_covariances(self):\n",
        "        return inv(self._filtered_precisions)\n",
        "\n",
        "    @property\n",
        "    def filtered_means(self):\n",
        "        # TODO: this is bad numerically\n",
        "        return np.einsum(\"...ij,...j->...i\", self.filtered_covariances, \n",
        "                         self.filtered_linear_potentials)\n",
        "\n",
        "    @property\n",
        "    def expected_states(self):\n",
        "        return self._expected_states\n",
        "\n",
        "    @property\n",
        "    def expected_states_squared(self):\n",
        "        return self._expected_states_squared\n",
        "\n",
        "    @property\n",
        "    def expected_states_next_states(self):\n",
        "        return self._expected_states_next_states\n",
        "\n",
        "    def _log_prob(self, data, **kwargs):\n",
        "        lp = -0.5 * np.einsum('...ti,...tij,...tj->...', data, self._precision_diag_blocks, data)\n",
        "        lp += -np.einsum('...ti,...tij,...tj->...', data[...,1:,:], self._precision_lower_diag_blocks, data[...,:-1,:])\n",
        "        lp += np.einsum('...ti,...ti->...', data, self._linear_potential)\n",
        "        lp -= self.log_normalizer\n",
        "        return lp\n",
        "\n",
        "    def _mean(self):\n",
        "        return self.expected_states\n",
        "\n",
        "    def _covariance(self):\n",
        "        \"\"\"\n",
        "        NOTE: This computes the _marginal_ covariance Cov[x_t] for each t\n",
        "        \"\"\"\n",
        "        Ex = self._expected_states\n",
        "        ExxT = self._expected_states_squared\n",
        "        return ExxT - np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "\n",
        "    def _sample_n(self, n, seed=None):\n",
        "        filtered_Js = self._filtered_precisions\n",
        "        filtered_hs = self._filtered_linear_potentials\n",
        "        J_lower_diag = self._precision_lower_diag_blocks\n",
        "\n",
        "        def sample_single(seed, filtered_Js, filtered_hs, J_lower_diag):\n",
        "\n",
        "            def _sample_info_gaussian(seed, J, h, sample_shape=()):\n",
        "                # TODO: avoid inversion.\n",
        "                # see https://github.com/mattjj/pybasicbayes/blob/master/pybasicbayes/util/stats.py#L117-L122\n",
        "                # L = np.linalg.cholesky(J)\n",
        "                # x = np.random.randn(h.shape[0])\n",
        "                # return scipy.linalg.solve_triangular(L,x,lower=True,trans='T') \\\n",
        "                #     + dpotrs(L,h,lower=True)[0]\n",
        "                cov = np.linalg.inv(J)\n",
        "                loc = np.einsum(\"...ij,...j->...i\", cov, h)\n",
        "                return tfp.distributions.MultivariateNormalFullCovariance(\n",
        "                    loc=loc, covariance_matrix=cov).sample(sample_shape=sample_shape, seed=seed)\n",
        "\n",
        "            def _step(carry, inpt):\n",
        "                x_next, seed = carry\n",
        "                Jf, hf, L = inpt\n",
        "\n",
        "                # Condition on the next observation\n",
        "                Jc = Jf\n",
        "                hc = hf - np.einsum('ni,ij->nj', x_next, L)\n",
        "\n",
        "                # Split the seed\n",
        "                seed, this_seed = jr.split(seed)\n",
        "                x = _sample_info_gaussian(this_seed, Jc, hc)\n",
        "                return (x, seed), x\n",
        "\n",
        "            # Initialize with sample of last timestep and sample in reverse\n",
        "            seed_T, seed = jr.split(seed)\n",
        "            x_T = _sample_info_gaussian(seed_T, filtered_Js[-1], filtered_hs[-1], sample_shape=(n,))\n",
        "            inputs = (filtered_Js[:-1][::-1], filtered_hs[:-1][::-1], J_lower_diag[::-1])\n",
        "            _, x_rev = lax.scan(_step, (x_T, seed), inputs)\n",
        "\n",
        "            # Reverse and concatenate the last time-step's sample\n",
        "            x = np.concatenate((x_rev[::-1], x_T[None, ...]), axis=0)\n",
        "\n",
        "            # Transpose to be (num_samples, num_timesteps, dim)\n",
        "            return np.transpose(x, (1, 0, 2))\n",
        "\n",
        "        # TODO: Handle arbitrary batch shapes\n",
        "        if filtered_Js.ndim == 4:\n",
        "            # batch mode\n",
        "            samples = vmap(sample_single)(seed, filtered_Js, filtered_hs, J_lower_diag)\n",
        "            # Transpose to be (num_samples, num_batches, num_timesteps, dim)\n",
        "            samples = np.transpose(samples, (1, 0, 2, 3))\n",
        "        else:\n",
        "            # non-batch mode\n",
        "            samples = sample_single(seed, filtered_Js, filtered_hs, J_lower_diag)\n",
        "        return samples\n",
        "\n",
        "    def _entropy(self):\n",
        "        \"\"\"\n",
        "        Compute the entropy\n",
        "\n",
        "            H[X] = -E[\\log p(x)]\n",
        "                 = -E[-1/2 x^T J x + x^T h - log Z(J, h)]\n",
        "                 = 1/2 <J, E[x x^T] - <h, E[x]> + log Z(J, h)\n",
        "        \"\"\"\n",
        "        Ex = self._expected_states\n",
        "        ExxT = self._expected_states_squared\n",
        "        ExnxT = self._expected_states_next_states\n",
        "        J_diag = self._precision_diag_blocks\n",
        "        J_lower_diag = self._precision_lower_diag_blocks\n",
        "        h = self._linear_potential\n",
        "\n",
        "        entropy = 0.5 * np.sum(J_diag * ExxT)\n",
        "        entropy += np.sum(J_lower_diag * ExnxT)\n",
        "        entropy -= np.sum(h * Ex)\n",
        "        entropy += self.log_normalizer\n",
        "        return entropy\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        children = (self._precision_diag_blocks,\n",
        "                    self._precision_lower_diag_blocks,\n",
        "                    self._linear_potential,\n",
        "                    self._log_normalizer,\n",
        "                    self._filtered_precisions,\n",
        "                    self._filtered_linear_potentials,\n",
        "                    self._expected_states,\n",
        "                    self._expected_states_squared,\n",
        "                    self._expected_states_next_states)\n",
        "        aux_data = None\n",
        "        return children, aux_data\n",
        "\n",
        "    @classmethod\n",
        "    def tree_unflatten(cls, aux_data, children):\n",
        "        return cls(*children)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bss7PlIgQRDp"
      },
      "source": [
        "## Important distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "cellView": "form",
        "id": "KZ0Avt4Ad8G-"
      },
      "outputs": [],
      "source": [
        "# @title Linear Gaussian chain distribution object\n",
        "# As a prior distribution, we only need to be able to 1) Evaluate log prob 2) sample\n",
        "# As a posterior distribution, we also need to figure out the sufficient stats (Ex, ExxT, ExnxT)\n",
        "class LinearGaussianChain:\n",
        "    def __init__(self, dynamics_matrix, dynamics_bias, noise_covariance,\n",
        "                 expected_states, expected_states_squared, expected_states_next_states):\n",
        "        \"\"\"\n",
        "        params: dictionary containing the following keys:\n",
        "            A:  (seq_len, dim, dim)\n",
        "            Q:  (seq_len, dim, dim)\n",
        "            b:  (seq_len, dim)\n",
        "        \"\"\"\n",
        "        self._dynamics_matrix = dynamics_matrix\n",
        "        self._dynamics_bias = dynamics_bias\n",
        "        self._noise_covariance = noise_covariance\n",
        "        self._expected_states = expected_states\n",
        "        self._expected_states_squared = expected_states_squared\n",
        "        self._expected_states_next_states = expected_states_next_states\n",
        "\n",
        "    @classmethod\n",
        "    def from_stationary_dynamics(cls, m1, Q1, A, b, Q, T):\n",
        "        dynamics_matrix = np.tile(A[None], (T, 1, 1))\n",
        "        dynamics_bias = np.concatenate([m1[None], \n",
        "                                        np.tile(b[None], (T-1, 1))])\n",
        "        noise_covariance = np.concatenate([Q1[None], \n",
        "                                           np.tile(Q[None], (T-1, 1, 1))])\n",
        "        return cls.from_nonstationary_dynamics(dynamics_matrix, dynamics_bias, noise_covariance)\n",
        "    \n",
        "    @classmethod\n",
        "    def from_nonstationary_dynamics(cls, dynamics_matrix, dynamics_bias, noise_covariance):\n",
        "        # Compute the means and covariances via parallel scan\n",
        "        init_elems = (dynamics_matrix, dynamics_bias, noise_covariance)\n",
        "\n",
        "        @vmap\n",
        "        def assoc_op(elem1, elem2):\n",
        "            A1, b1, Q1 = elem1\n",
        "            A2, b2, Q2 = elem2\n",
        "            return A2 @ A1, A2 @ b1 + b2, A2 @ Q1 @ A2.T + Q2\n",
        "\n",
        "        _, Ex, covariances = lax.associative_scan(assoc_op, init_elems)\n",
        "        expected_states = Ex\n",
        "        expected_states_squared = covariances + np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "        expected_states_next_states = np.einsum(\"...ij,...jk->...ik\", \n",
        "            covariances[:-1], dynamics_matrix[1:]) + np.einsum(\"...i,...j->...ji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "        return cls(dynamics_matrix, dynamics_bias, noise_covariance,\n",
        "                   expected_states, expected_states_squared, expected_states_next_states)\n",
        "\n",
        "    @property\n",
        "    def mean(self):\n",
        "        return self._expected_states\n",
        "\n",
        "    @property\n",
        "    def covariance(self):\n",
        "        Ex = self._expected_states\n",
        "        ExxT = self._expected_states_squared\n",
        "        return ExxT - np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "        \n",
        "    @property\n",
        "    def expected_states(self):\n",
        "        return self._expected_states\n",
        "\n",
        "    @property\n",
        "    def expected_states_squared(self):\n",
        "        return self._expected_states_squared\n",
        "\n",
        "    @property\n",
        "    def expected_states_next_states(self):\n",
        "        return self._expected_states_next_states\n",
        "\n",
        "    # Works with batched distributions and arguments...!\n",
        "    def log_prob(self, xs):\n",
        "\n",
        "        @partial(np.vectorize, signature=\"(t,d,d),(t,d),(t,d,d),(t,d)->()\")\n",
        "        def log_prob_single(A, b, Q, x):\n",
        "            ll = MVN(loc=b[0], covariance_matrix=Q[0]).log_prob(x[0])\n",
        "            ll += MVN(loc=np.einsum(\"tij,tj->ti\", A[1:], x[:-1]) + b[1:], \n",
        "                      covariance_matrix=Q[1:]).log_prob(x[1:]).sum()\n",
        "            return ll\n",
        "\n",
        "        return log_prob_single(self._dynamics_matrix,\n",
        "                               self._dynamics_bias, \n",
        "                               self._noise_covariance, xs)\n",
        "        \n",
        "    # Only supports 0d and 1d sample shapes\n",
        "    # Does not support sampling with batched object\n",
        "    def sample(self, seed, sample_shape=()):\n",
        "\n",
        "        @partial(np.vectorize, signature=\"(n),(t,d,d),(t,d),(t,d,d)->(t,d)\")\n",
        "        def sample_single(key, A, b, Q):\n",
        "\n",
        "            biases = MVN(loc=b, covariance_matrix=Q).sample(seed=key)\n",
        "            init_elems = (A, biases)\n",
        "\n",
        "            @vmap\n",
        "            def assoc_op(elem1, elem2):\n",
        "                A1, b1 = elem1\n",
        "                A2, b2 = elem2\n",
        "                return A2 @ A1, A2 @ b1 + b2\n",
        "\n",
        "            _, sample = lax.associative_scan(assoc_op, init_elems)\n",
        "            return sample\n",
        "        \n",
        "        if (len(sample_shape) == 0):\n",
        "            return sample_single(seed, self._dynamics_matrix,\n",
        "                                 self._dynamics_bias, \n",
        "                                 self._noise_covariance)\n",
        "        elif (len(sample_shape) == 1):\n",
        "            return sample_single(jr.split(seed, sample_shape[0]),\n",
        "                                 self._dynamics_matrix[None],\n",
        "                                 self._dynamics_bias[None],\n",
        "                                 self._noise_covariance[None]) \n",
        "        else:\n",
        "            raise Exception(\"More than one sample dimensions are not supported!\")\n",
        "\n",
        "    def entropy(self):\n",
        "        \"\"\"\n",
        "        Compute the entropy\n",
        "\n",
        "            H[X] = -E[\\log p(x)]\n",
        "                 = -E[-1/2 x^T J x + x^T h - log Z(J, h)]\n",
        "                 = 1/2 <J, E[x x^T] - <h, E[x]> + log Z(J, h)\n",
        "        \"\"\"\n",
        "        Ex = self.expected_states\n",
        "        ExxT = self.expected_states_squared\n",
        "        ExnxT = self.expected_states_next_states\n",
        "        \n",
        "        dim = Ex.shape[-1]        \n",
        "        Q_inv = solve(self._noise_covariance, np.eye(dim)[None])\n",
        "        A = self._dynamics_matrix\n",
        "\n",
        "        J_lower_diag = np.einsum(\"til,tlj->tij\", -Q_inv[1:], A[1:])\n",
        "        ATQinvA = np.einsum(\"tji,tjl,tlk->tik\", A[1:], Q_inv[1:], A[1:])\n",
        "        J_diag = Q_inv.at[:-1].add(ATQinvA)\n",
        "\n",
        "        Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "        Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "        trm1 = 0.5 * np.sum(J_diag * Sigmatt)\n",
        "        trm2 = np.sum(J_lower_diag * Sigmatnt)\n",
        "\n",
        "        return trm1 + trm2 - self.log_prob(Ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "cellView": "form",
        "id": "lBVe4-YTOQLp"
      },
      "outputs": [],
      "source": [
        "# @title Testing the correctness of the linear Gaussian chain\n",
        "# jax.config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "# D = 3\n",
        "# T = 200\n",
        "\n",
        "# mu1 = np.zeros(D)\n",
        "# Q1 = np.eye(D)\n",
        "# # TODO: A could be a sensitive parameter\n",
        "# # A = jnp.linspace(0.001, 1.0, D)\n",
        "# A = random_rotation(key_0, D, np.pi / 20)\n",
        "# b = jr.normal(key_0, shape=(D,))#np.zeros(D)\n",
        "# Q = np.eye(D)\n",
        "\n",
        "# C = np.eye(D)\n",
        "# d = np.zeros(D)\n",
        "\n",
        "# dist = LinearGaussianChain.from_stationary_dynamics(mu1, Q1, A, b, Q, T)\n",
        "# dist.entropy()\n",
        "# # Sigmas = np.tile(0.01 * np.eye(D), (T, 1, 1))\n",
        "\n",
        "# p = dynamics_to_tridiag({\"m1\": mu1, \"Q1\": Q1, \"A\": A, \"b\": b, \"Q\": Q}, T, D)\n",
        "# dist_ = MultivariateNormalBlockTridiag.infer(p[\"J\"], p[\"L\"], p[\"h\"])\n",
        "\n",
        "# ExnxT_ = dist_.expected_states_next_states\n",
        "# ExnxT = dist.expected_states_next_states\n",
        "# print(ExnxT - ExnxT_)\n",
        "# print(dist_.entropy() - dist.entropy())\n",
        "# print(dist.expected_states_squared - dist_.expected_states_squared)\n",
        "# posterior = LinearGaussianChain.from_stationary_dynamics(mu1, Q1, A, b, Q, T)\n",
        "# key_1 = jr.split(key_0)[0]\n",
        "# A1 = random_rotation(key_1, D, np.pi / 20)\n",
        "# b1 = jr.normal(key_1, shape=(D,))#np.zeros(D)\n",
        "# prior_params = dynamics_to_tridiag({\"m1\": mu1, \"Q1\": Q1, \"A\": A1, \"b\": b1, \"Q\": Q}, T, D)\n",
        "# prior = LinearGaussianChain.from_stationary_dynamics(mu1, Q1, A1, b1, Q, T)\n",
        "# Ex = posterior.expected_states\n",
        "# ExxT = posterior.expected_states_squared\n",
        "# ExnxT = posterior.expected_states_next_states\n",
        "# Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "# Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "# J, L = prior_params[\"J\"], prior_params[\"L\"]\n",
        "# cross_entropy = -prior.log_prob(Ex)\n",
        "# cross_entropy += 0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "# cross_entropy += np.einsum(\"tij,tij->\", L, Sigmatnt)\n",
        "# print(\"Closed form KL:\", cross_entropy - posterior.entropy())\n",
        "# samples = posterior.sample(key_1, (100,))\n",
        "# print(\"Sampled KL:\", np.mean(posterior.log_prob(samples) - prior.log_prob(samples)))\n",
        "# # return np.mean(posterior.log_prob(samples) - prior.log_prob(samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "6D7qIBFpRyH8"
      },
      "outputs": [],
      "source": [
        "# @title Linear Gaussian chain prior\n",
        "# This is a linear Gaussian chain\n",
        "class LinearGaussianChainPrior(SVAEPrior):\n",
        "\n",
        "    def __init__(self, latent_dims, seq_len):\n",
        "        self.latent_dims = latent_dims\n",
        "        # The only annoying thing is that we have to specify the sequence length\n",
        "        # ahead of time\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return (self.seq_len, self.latent_dims)\n",
        "\n",
        "    # Must be the full set of constrained parameters!\n",
        "    def distribution(self, params):\n",
        "        As, bs, Qs = params[\"As\"], params[\"bs\"], params[\"Qs\"]\n",
        "        Ex, ExxT, ExnxT = params[\"Ex\"], params[\"ExxT\"], params[\"ExnxT\"]\n",
        "        return LinearGaussianChain(As, bs, Qs, Ex, ExxT, ExnxT)\n",
        "\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        params = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"Q1\": np.eye(D),\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": np.eye(D)\n",
        "        }\n",
        "        constrained = self.get_constrained_params(params)\n",
        "        return params\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        return params\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        tridiag = dynamics_to_tridiag(params, self.seq_len, self.latent_dims)\n",
        "        p.update(tridiag)\n",
        "        dist = LinearGaussianChain.from_stationary_dynamics(p[\"m1\"], p[\"Q1\"], \n",
        "                                         p[\"A\"], p[\"b\"], p[\"Q\"], self.seq_len)\n",
        "        p.update({\n",
        "            \"As\": dist._dynamics_matrix,\n",
        "            \"bs\": dist._dynamics_bias,\n",
        "            \"Qs\": dist._noise_covariance,\n",
        "            \"Ex\": dist.expected_states,\n",
        "            \"ExxT\": dist.expected_states_squared,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    # This is pretty much deprecated since we're using sgd\n",
        "    def m_step(self, prior_params):\n",
        "        suff_stats = prior_params[\"avg_suff_stats\"]\n",
        "        ExxT = suff_stats[\"ExxT\"]\n",
        "        ExnxT = suff_stats[\"ExnxT\"]\n",
        "        Ex = suff_stats[\"Ex\"]\n",
        "        seq_len = Ex.shape[0]\n",
        "        # Update the initials\n",
        "        m1 = Ex[0]\n",
        "        Q1 = ExxT[0] - np.outer(m1, m1)\n",
        "        D = self.latent_dims\n",
        "        A, b, Q = fit_linear_regression(Ex[:-1].sum(axis=0), \n",
        "                                        Ex[1:].sum(axis=0), \n",
        "                                        ExxT[:-1].sum(axis=0), \n",
        "                                        ExnxT.sum(axis=0), \n",
        "                                        ExxT[1:].sum(axis=0), \n",
        "                                        seq_len - 1)\n",
        "        out = { \"m1\": m1, \"Q1\": Q1, \"A\": A, \"b\": b, \"Q\": Q }\n",
        "        out[\"avg_suff_stats\"] = deepcopy(suff_stats)\n",
        "        return out\n",
        "\n",
        "# This is a bit clumsy but it's the best we can do without using some sophisticated way\n",
        "# Of marking the constrained/optimized parameters vs. unconstrained parameters\n",
        "class LieParameterizedLinearGaussianChainPrior(LinearGaussianChainPrior):\n",
        "    def init(self, key):\n",
        "        D = self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        # Equivalent to the unit matrix\n",
        "        Q_flat = np.concatenate([np.ones(D) * inv_softplus(1), np.zeros((D*(D-1)//2))])\n",
        "        params = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"Q1\": Q_flat,\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": Q_flat\n",
        "        }\n",
        "        return params\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        return {\n",
        "            \"m1\": params[\"m1\"],\n",
        "            \"Q1\": lie_params_to_constrained(params[\"Q1\"], self.latent_dims),\n",
        "            \"A\": params[\"A\"],\n",
        "            \"b\": params[\"b\"],\n",
        "            \"Q\": lie_params_to_constrained(params[\"Q\"], self.latent_dims)   \n",
        "        }\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        D = self.latent_dims\n",
        "        p = self.get_dynamics_params(params)\n",
        "        return super().get_constrained_params(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "6c7K9dJWzyG8"
      },
      "outputs": [],
      "source": [
        "# @title Linear Gaussian chain posteriors\n",
        "\n",
        "# The infer function for the DKF version just uses the posterior params \n",
        "class CDKFPosterior(LinearGaussianChainPrior):\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        params = {\n",
        "            \"As\": np.zeros((T, D, D)), \n",
        "            \"bs\": np.zeros((T, D)),\n",
        "            \"Qs\": np.tile(np.eye(D)[None], (T, 1, 1))\n",
        "        }\n",
        "        return self.get_constrained_params(params)\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        dist = LinearGaussianChain.from_nonstationary_dynamics(p[\"As\"], p[\"bs\"], p[\"Qs\"])\n",
        "        p.update({\n",
        "            \"Ex\": dist.expected_states,\n",
        "            \"ExxT\": dist.expected_states_squared,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    def sufficient_statistics(self, params):\n",
        "        return {\n",
        "            \"Ex\": params[\"Ex\"],\n",
        "            \"ExxT\": params[\"ExxT\"],\n",
        "            \"ExnxT\": params[\"ExnxT\"]\n",
        "        }\n",
        "\n",
        "    def infer(self, prior_params, posterior_params):\n",
        "        return self.get_constrained_params(posterior_params)\n",
        "\n",
        "class DKFPosterior(CDKFPosterior):\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        # The DKF produces a factored posterior\n",
        "        # So the dynamics matrix is zeroed out\n",
        "        p[\"As\"] *= 0\n",
        "        dist = LinearGaussianChain.from_nonstationary_dynamics(p[\"As\"], p[\"bs\"], p[\"Qs\"])\n",
        "        p.update({\n",
        "            \"Ex\": dist.expected_states,\n",
        "            \"ExxT\": dist.expected_states_squared,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "cellView": "form",
        "id": "Rbe3PsAIHliY"
      },
      "outputs": [],
      "source": [
        "# @title PlaNet type posterior\n",
        "# The infer function for the DKF version just uses the posterior params \n",
        "# TODO: Put the dummies in the params dictionary as well\n",
        "class PlaNetPosterior(DKFPosterior):\n",
        "    def __init__(self, network_params, latent_dims, seq_len):\n",
        "        super().__init__(latent_dims, seq_len)\n",
        "        self.network = StochasticRNNCell.from_params(**network_params)\n",
        "        self.input_dim = network_params[\"input_dim\"]      # u\n",
        "        self.latent_dim = network_params[\"rnn_dim\"]       # h\n",
        "        self.output_dim = network_params[\"output_dim\"]    # x\n",
        "\n",
        "    def init(self, key):\n",
        "        input_dummy = np.zeros((self.input_dim,))\n",
        "        latent_dummy = np.zeros((self.latent_dim,))\n",
        "        output_dummy = np.zeros((self.output_dim,))\n",
        "        rnn_params = self.network.init(key, input_dummy, latent_dummy, output_dummy)\n",
        "        return {\n",
        "            \"rnn_params\": rnn_params,\n",
        "            \"input_dummy\": input_dummy,\n",
        "            \"latent_dummy\": latent_dummy,\n",
        "            \"output_dummy\": output_dummy,\n",
        "        }\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        # All of the information is stored in the second argument already\n",
        "        return params\n",
        "\n",
        "    def distribution(self, params):\n",
        "        return DeepAutoregressiveDynamics(self.network, params)\n",
        "        \n",
        "    # These are just dummies\n",
        "    def sufficient_statistics(self, params):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        return {\n",
        "            \"Ex\": np.zeros((T, D)),\n",
        "            \"ExxT\": np.zeros((T, D, D)),\n",
        "            \"ExnxT\": np.zeros((T-1, D, D))\n",
        "        }\n",
        "\n",
        "# We only need to be able to 1) Evaluate log prob 2) sample\n",
        "# The tricky thing here is evaluating the \n",
        "class DeepAutoregressiveDynamics:\n",
        "\n",
        "    def __init__(self, network, params):\n",
        "        self.cell = network\n",
        "        self.params = params[\"network_params\"]\n",
        "        self.inputs = params[\"network_input\"]\n",
        "        # self.input_dummy = params[\"network_params\"][\"input_dummy\"]\n",
        "        # self.latent_dummy = params[\"network_params\"][\"latent_dummy\"]\n",
        "        # self.output_dummy = params[\"network_params\"][\"output_dummy\"]\n",
        "        self._mean = None\n",
        "        self._covariance = None\n",
        "\n",
        "    def mean(self):\n",
        "        if (self._mean is None):\n",
        "            self.compute_mean_and_cov()\n",
        "        return self._mean\n",
        "\n",
        "    def covariance(self):\n",
        "        if (self._covariance is None):\n",
        "            self.compute_mean_and_cov()\n",
        "        return self._covariance\n",
        "\n",
        "    @property\n",
        "    def filtered_covariances(self):\n",
        "        return self.covariance()\n",
        "\n",
        "    @property\n",
        "    def filtered_means(self):\n",
        "        return self.mean()\n",
        "        \n",
        "    def compute_mean_and_cov(self):\n",
        "        num_samples = 25\n",
        "        samples = self.sample((num_samples,), key_0)\n",
        "        Ex = np.mean(samples, axis=0)\n",
        "        self._mean = Ex\n",
        "        ExxT = np.einsum(\"s...ti,s...tj->s...tij\", samples, samples).mean(axis=0)\n",
        "        self._covariance = ExxT - np.einsum(\"...ti,...tj->...tij\", Ex, Ex)\n",
        "\n",
        "    # TODO: make this work properly with a batched distribution object\n",
        "    def log_prob(self, xs):\n",
        "        params = self.params\n",
        "        def log_prob_single(x_):\n",
        "            def _log_prob_step(carry, i):\n",
        "                h, prev_x = carry\n",
        "                x, u = x_[i], self.inputs[i]\n",
        "                h, (cov, mean) = self.cell.apply(params[\"rnn_params\"], h, prev_x, u)\n",
        "                pred_dist = tfd.MultivariateNormalFullCovariance(loc=mean, \n",
        "                                                            covariance_matrix=cov)\n",
        "                log_prob = pred_dist.log_prob(x)\n",
        "                carry = h, x\n",
        "                return carry, log_prob\n",
        "            # Assuming these are zero arrays already\n",
        "            init = (params[\"latent_dummy\"], params[\"output_dummy\"])\n",
        "            _, log_probs = scan(_log_prob_step, init, np.arange(x_.shape[0]))\n",
        "            return np.sum(log_probs, axis=0)\n",
        "        return vmap(log_prob_single)(xs)\n",
        "\n",
        "    # TODO: make this work with a batched distribution object\n",
        "    # Only supports rank 0 and 1 sample shapes\n",
        "    # Output: ([num_samples,] [batch_size,] seq_len, event_dim)\n",
        "    def sample(self, sample_shape, seed):\n",
        "        def _sample_single(key, params, inputs):\n",
        "            def _sample_step(carry, u):\n",
        "                key, h, x = carry\n",
        "                key, new_key = jr.split(key)\n",
        "                h, (cov, mean) = self.cell.apply(params[\"rnn_params\"], h, x, u)\n",
        "                sample = jr.multivariate_normal(key, mean, cov)\n",
        "                carry = new_key, h, sample\n",
        "                output = sample\n",
        "                return carry, output\n",
        "\n",
        "            init = (key, params[\"latent_dummy\"], params[\"output_dummy\"])\n",
        "            _, sample = scan(_sample_step, init, inputs)\n",
        "            return sample\n",
        "\n",
        "        if (len(self.inputs.shape) == 2):\n",
        "            if (len(sample_shape) == 0):\n",
        "                return _sample_single(seed, self.params, self.inputs)\n",
        "            else:\n",
        "                assert (len(sample_shape) == 1)\n",
        "                return vmap(_sample_single, in_axes=(0, None, None))(jr.split(seed, sample_shape[0]),\n",
        "                                            self.params, self.inputs)\n",
        "        else:\n",
        "            # This is a batched distribution object\n",
        "            assert(len(self.inputs.shape) == 3)\n",
        "            batch_size = self.inputs.shape[0]\n",
        "            if (len(sample_shape) == 0):\n",
        "                return vmap(_sample_single)(\n",
        "                            jr.split(seed, batch_size), \n",
        "                            self.params,\n",
        "                            self.inputs\n",
        "                        )\n",
        "            else:\n",
        "                assert (len(sample_shape) == 1)\n",
        "                return vmap(\n",
        "                        lambda s:vmap(_sample_single)(\n",
        "                            jr.split(s, batch_size), \n",
        "                            self.params,\n",
        "                            self.inputs\n",
        "                        )\n",
        "                    )(jr.split(seed, sample_shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "cellView": "form",
        "id": "qDsz18BcyDcf"
      },
      "outputs": [],
      "source": [
        "# @title LDS object (might wanna refactor this)\n",
        "\n",
        "# Takes a linear Gaussian chain as its base\n",
        "class LDS(LinearGaussianChainPrior):\n",
        "    def __init__(self, latent_dims, seq_len, base=None, posterior=None):\n",
        "        super().__init__(latent_dims, seq_len)\n",
        "        self.posterior = posterior or LDSSVAEPosterior(latent_dims, seq_len)\n",
        "        self.base = base or LinearGaussianChainPrior(latent_dims, seq_len) # Slightly redundant...\n",
        "\n",
        "    # Takes unconstrained params\n",
        "    def sample(self, params, shape, key):\n",
        "        latents = self.base.sample(params, shape, key)\n",
        "        sample_shape = latents.shape[:-1]\n",
        "        key, _ = jr.split(key)\n",
        "        C, d, R = params[\"C\"], params[\"d\"], params[\"R\"]\n",
        "        obs_noise = tfd.MultivariateNormalFullCovariance(loc=d, covariance_matrix=R)\\\n",
        "            .sample(sample_shape=sample_shape, seed=key)\n",
        "        obs = np.einsum(\"ij,...tj->...ti\", C, latents) + obs_noise\n",
        "        return latents, obs\n",
        "\n",
        "    # Should work with any batch dimension\n",
        "    def log_prob(self, params, states, data):\n",
        "        latent_dist = self.base.distribution(self.base.get_constrained_params(params))\n",
        "        latent_ll = latent_dist.log_prob(states)\n",
        "        C, d, R = params[\"C\"], params[\"d\"], params[\"R\"]\n",
        "        # Gets around batch dimensions\n",
        "        noise = tfd.MultivariateNormalFullCovariance(loc=d, covariance_matrix=R)\n",
        "        obs_ll = noise.log_prob(data - np.einsum(\"ij,...tj->...ti\", C, states))\n",
        "        return latent_ll + obs_ll.sum(axis=-1)\n",
        "\n",
        "    # Assumes single data points\n",
        "    def e_step(self, params, data):\n",
        "        # Shorthand names for parameters\n",
        "        C, d, R = params[\"C\"], params[\"d\"], params[\"R\"]\n",
        "\n",
        "        J = np.dot(C.T, np.linalg.solve(R, C))\n",
        "        J = np.tile(J[None, :, :], (self.seq_len, 1, 1))\n",
        "        # linear potential\n",
        "        h = np.dot(data - d, np.linalg.solve(R, C))\n",
        "\n",
        "        Sigma = solve(J, np.eye(self.latent_dims)[None])\n",
        "        mu = vmap(solve)(J, h)\n",
        "\n",
        "        return self.posterior.infer(self.base.get_constrained_params(params), {\"J\": J, \"h\": h, \n",
        "                                                                    \"mu\": mu, \"Sigma\": Sigma})\n",
        "        \n",
        "    # Also assumes single data points\n",
        "    def marginal_log_likelihood(self, params, data):\n",
        "        posterior = self.posterior.distribution(self.e_step(params, data))\n",
        "        states = posterior.mean()\n",
        "        prior_ll = self.log_prob(params, states, data)\n",
        "        posterior_ll = posterior.log_prob(states)\n",
        "        # This is numerically unstable!\n",
        "        lps = prior_ll - posterior_ll\n",
        "        return lps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGISYI1CN6cv"
      },
      "source": [
        "## Making a mean parameter posterior object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "cellView": "form",
        "id": "il0xUSa3nn0K"
      },
      "outputs": [],
      "source": [
        "# @title Parallel Kalman filtering and smoothing\n",
        "\n",
        "def _make_associative_sampling_elements(params, key, filtered_means, filtered_covariances):\n",
        "    \"\"\"Preprocess filtering output to construct input for smoothing assocative scan.\"\"\"\n",
        "\n",
        "    F = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "\n",
        "    def _last_sampling_element(key, m, P):\n",
        "        return np.zeros_like(P), MVN(m, P).sample(seed=key)\n",
        "\n",
        "    def _generic_sampling_element(params, key, m, P):\n",
        "\n",
        "        eps = 1e-3\n",
        "        P += np.eye(dims) * eps\n",
        "        Pp = F @ P @ F.T + Q\n",
        "\n",
        "        FP = F @ P\n",
        "        E  = psd_solve(Pp, FP).T\n",
        "        g  = m - E @ F @ m\n",
        "        L  = P - E @ Pp @ E.T\n",
        "\n",
        "        L = (L + L.T) * .5 + np.eye(dims) * eps # Add eps to the crucial covariance matrix\n",
        "\n",
        "        h = MVN(g, L).sample(seed=key)\n",
        "        return E, h\n",
        "\n",
        "    num_timesteps = len(filtered_means)\n",
        "    dims = filtered_means.shape[-1]\n",
        "    keys = jr.split(key, num_timesteps)\n",
        "    last_elems = _last_sampling_element(keys[-1], filtered_means[-1], \n",
        "                                        filtered_covariances[-1])\n",
        "    generic_elems = vmap(_generic_sampling_element, (None, 0, 0, 0))(\n",
        "        params, keys[:-1], filtered_means[:-1], filtered_covariances[:-1]\n",
        "        )\n",
        "    combined_elems = tuple(np.append(gen_elm, last_elm[None,:], axis=0)\n",
        "                           for gen_elm, last_elm in zip(generic_elems, last_elems))\n",
        "    return combined_elems\n",
        "\n",
        "def _make_associative_filtering_elements(params, potentials):\n",
        "    \"\"\"Preprocess observations to construct input for filtering assocative scan.\"\"\"\n",
        "\n",
        "    F = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "    Q1 = params[\"Q1\"]\n",
        "    P0 = Q1\n",
        "    P1 = Q1\n",
        "    m1 = params[\"m1\"]\n",
        "    dim = Q.shape[0]\n",
        "    H = np.eye(dim)\n",
        "\n",
        "    def _first_filtering_element(params, mu, Sigma):\n",
        "\n",
        "        y, R = mu, Sigma\n",
        "\n",
        "        S = H @ Q @ H.T + R\n",
        "        CF, low = scipy.linalg.cho_factor(S)\n",
        "\n",
        "        S1 = H @ P1 @ H.T + R\n",
        "        K1 = psd_solve(S1, H @ P1).T\n",
        "\n",
        "        A = np.zeros_like(F)\n",
        "        b = m1 + K1 @ (y - H @ m1)\n",
        "        C = P1 - K1 @ S1 @ K1.T\n",
        "        eta = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), y)\n",
        "        J = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), H @ F)\n",
        "\n",
        "        logZ = -MVN(loc=np.zeros_like(y), covariance_matrix=H @ P0 @ H.T + R).log_prob(y)\n",
        "\n",
        "        return A, b, C, J, eta, logZ\n",
        "\n",
        "\n",
        "    def _generic_filtering_element(params, mu, Sigma):\n",
        "\n",
        "        y, R = mu, Sigma\n",
        "\n",
        "        S = H @ Q @ H.T + R\n",
        "        CF, low = scipy.linalg.cho_factor(S)\n",
        "        K = scipy.linalg.cho_solve((CF, low), H @ Q).T\n",
        "        A = F - K @ H @ F\n",
        "        b = K @ y\n",
        "        C = Q - K @ H @ Q\n",
        "\n",
        "        eta = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), y)\n",
        "        J = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), H @ F)\n",
        "\n",
        "        logZ = -MVN(loc=np.zeros_like(y), covariance_matrix=S).log_prob(y)\n",
        "\n",
        "        return A, b, C, J, eta, logZ\n",
        "\n",
        "    mus, Sigmas = potentials[\"mu\"], potentials[\"Sigma\"]\n",
        "\n",
        "    first_elems = _first_filtering_element(params, mus[0], Sigmas[0])\n",
        "    generic_elems = vmap(_generic_filtering_element, (None, 0, 0))(params, mus[1:], Sigmas[1:])\n",
        "    combined_elems = tuple(np.concatenate((first_elm[None,...], gen_elm))\n",
        "                           for first_elm, gen_elm in zip(first_elems, generic_elems))\n",
        "    return combined_elems\n",
        "\n",
        "def lgssm_filter(params, emissions):\n",
        "    \"\"\"A parallel version of the lgssm filtering algorithm.\n",
        "    See S. Särkkä and Á. F. García-Fernández (2021) - https://arxiv.org/abs/1905.13002.\n",
        "    Note: This function does not yet handle `inputs` to the system.\n",
        "    \"\"\"\n",
        "\n",
        "    initial_elements = _make_associative_filtering_elements(params, emissions)\n",
        "\n",
        "    @vmap\n",
        "    def filtering_operator(elem1, elem2):\n",
        "        A1, b1, C1, J1, eta1, logZ1 = elem1\n",
        "        A2, b2, C2, J2, eta2, logZ2 = elem2\n",
        "        dim = A1.shape[0]\n",
        "        I = np.eye(dim)\n",
        "\n",
        "        I_C1J2 = I + C1 @ J2\n",
        "        temp = scipy.linalg.solve(I_C1J2.T, A2.T).T\n",
        "        A = temp @ A1\n",
        "        b = temp @ (b1 + C1 @ eta2) + b2\n",
        "        C = temp @ C1 @ A2.T + C2\n",
        "\n",
        "        I_J2C1 = I + J2 @ C1\n",
        "        temp = scipy.linalg.solve(I_J2C1.T, A1).T\n",
        "\n",
        "        eta = temp @ (eta2 - J2 @ b1) + eta1\n",
        "        J = temp @ J2 @ A1 + J1\n",
        "\n",
        "        # mu = scipy.linalg.solve(J2, eta2)\n",
        "        # t2 = - eta2 @ mu + (b1 - mu) @ scipy.linalg.solve(I_J2C1, (J2 @ b1 - eta2))\n",
        "\n",
        "        mu = np.linalg.solve(C1, b1)\n",
        "        t1 = (b1 @ mu - (eta2 + mu) @ np.linalg.solve(I_C1J2, C1 @ eta2 + b1))\n",
        "\n",
        "        logZ = (logZ1 + logZ2 + 0.5 * np.linalg.slogdet(I_C1J2)[1] + 0.5 * t1)\n",
        "\n",
        "        return A, b, C, J, eta, logZ\n",
        "\n",
        "    _, filtered_means, filtered_covs, _, _, logZ = lax.associative_scan(\n",
        "                                                filtering_operator, initial_elements\n",
        "                                                )\n",
        "\n",
        "    return {\n",
        "        \"marginal_logliks\": -logZ,\n",
        "        \"marginal_loglik\": -logZ[-1],\n",
        "        \"filtered_means\": filtered_means, \n",
        "        \"filtered_covariances\": filtered_covs\n",
        "    }\n",
        "\n",
        "def _make_associative_smoothing_elements(params, filtered_means, filtered_covariances):\n",
        "    \"\"\"Preprocess filtering output to construct input for smoothing assocative scan.\"\"\"\n",
        "\n",
        "    F = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "\n",
        "    def _last_smoothing_element(m, P):\n",
        "        return np.zeros_like(P), m, P\n",
        "\n",
        "    def _generic_smoothing_element(params, m, P):\n",
        "\n",
        "        Pp = F @ P @ F.T + Q\n",
        "\n",
        "        E  = psd_solve(Pp, F @ P).T\n",
        "        g  = m - E @ F @ m\n",
        "        L  = P - E @ Pp @ E.T\n",
        "        return E, g, L\n",
        "\n",
        "    last_elems = _last_smoothing_element(filtered_means[-1], filtered_covariances[-1])\n",
        "    generic_elems = vmap(_generic_smoothing_element, (None, 0, 0))(\n",
        "        params, filtered_means[:-1], filtered_covariances[:-1]\n",
        "        )\n",
        "    combined_elems = tuple(np.append(gen_elm, last_elm[None,:], axis=0)\n",
        "                           for gen_elm, last_elm in zip(generic_elems, last_elems))\n",
        "    return combined_elems\n",
        "\n",
        "\n",
        "def parallel_lgssm_smoother(params, emissions):\n",
        "    \"\"\"A parallel version of the lgssm smoothing algorithm.\n",
        "    See S. Särkkä and Á. F. García-Fernández (2021) - https://arxiv.org/abs/1905.13002.\n",
        "    Note: This function does not yet handle `inputs` to the system.\n",
        "    \"\"\"\n",
        "    filtered_posterior = lgssm_filter(params, emissions)\n",
        "    filtered_means = filtered_posterior[\"filtered_means\"]\n",
        "    filtered_covs = filtered_posterior[\"filtered_covariances\"]\n",
        "    initial_elements = _make_associative_smoothing_elements(params, filtered_means, filtered_covs)\n",
        "\n",
        "    @vmap\n",
        "    def smoothing_operator(elem1, elem2):\n",
        "        E1, g1, L1 = elem1\n",
        "        E2, g2, L2 = elem2\n",
        "\n",
        "        E = E2 @ E1\n",
        "        g = E2 @ g1 + g2\n",
        "        L = E2 @ L1 @ E2.T + L2\n",
        "\n",
        "        return E, g, L\n",
        "\n",
        "    _, smoothed_means, smoothed_covs, *_ = lax.associative_scan(\n",
        "                                                smoothing_operator, initial_elements, reverse=True\n",
        "                                                )\n",
        "    return {\n",
        "        \"marginal_loglik\": filtered_posterior[\"marginal_loglik\"],\n",
        "        \"filtered_means\": filtered_means,\n",
        "        \"filtered_covariances\": filtered_covs,\n",
        "        \"smoothed_means\": smoothed_means,\n",
        "        \"smoothed_covariances\": smoothed_covs\n",
        "    }\n",
        "\n",
        "def lgssm_log_normalizer(dynamics_params, mu_filtered, Sigma_filtered, potentials):\n",
        "    p = dynamics_params\n",
        "    Q, A, b = p[\"Q\"][None], p[\"A\"][None], p[\"b\"][None]\n",
        "    AT = (p[\"A\"].T)[None]\n",
        "\n",
        "    I = np.eye(Q.shape[-1])\n",
        "\n",
        "    Sigma_filtered, mu_filtered = Sigma_filtered[:-1], mu_filtered[:-1]\n",
        "    Sigma = Q + A @ Sigma_filtered @ AT\n",
        "    mu = (A[0] @ mu_filtered.T).T + b\n",
        "    # Append the first element\n",
        "    Sigma_pred = np.concatenate([p[\"Q1\"][None], Sigma])\n",
        "    mu_pred = np.concatenate([p[\"m1\"][None], mu])\n",
        "    mu_rec, Sigma_rec = potentials[\"mu\"], potentials[\"Sigma\"]\n",
        "\n",
        "    def log_Z_single(mu_pred, Sigma_pred, mu_rec, Sigma_rec):\n",
        "        return MVN(loc=mu_pred, covariance_matrix=Sigma_pred+Sigma_rec).log_prob(mu_rec)\n",
        "\n",
        "    log_Z = vmap(log_Z_single)(mu_pred, Sigma_pred, mu_rec, Sigma_rec)\n",
        "    return np.sum(log_Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "cellView": "form",
        "id": "_zQepN9hRCdG"
      },
      "outputs": [],
      "source": [
        "# @title Parallel linear Gaussian state space model object\n",
        "def lgssm_log_normalizer(dynamics_params, mu_filtered, Sigma_filtered, potentials):\n",
        "    p = dynamics_params\n",
        "    Q, A, b = p[\"Q\"][None], p[\"A\"][None], p[\"b\"][None]\n",
        "    AT = (p[\"A\"].T)[None]\n",
        "\n",
        "    I = np.eye(Q.shape[-1])\n",
        "\n",
        "    Sigma_filtered, mu_filtered = Sigma_filtered[:-1], mu_filtered[:-1]\n",
        "    Sigma = Q + A @ Sigma_filtered @ AT\n",
        "    mu = (A[0] @ mu_filtered.T).T + b\n",
        "    # Append the first element\n",
        "    Sigma_pred = np.concatenate([p[\"Q1\"][None], Sigma])\n",
        "    mu_pred = np.concatenate([p[\"m1\"][None], mu])\n",
        "    mu_rec, Sigma_rec = potentials[\"mu\"], potentials[\"Sigma\"]\n",
        "\n",
        "    def log_Z_single(mu_pred, Sigma_pred, mu_rec, Sigma_rec):\n",
        "        return MVN(loc=mu_pred, covariance_matrix=Sigma_pred+Sigma_rec).log_prob(mu_rec)\n",
        "\n",
        "    log_Z = vmap(log_Z_single)(mu_pred, Sigma_pred, mu_rec, Sigma_rec)\n",
        "    return np.sum(log_Z)\n",
        "\n",
        "class LinearGaussianSSM(tfd.Distribution):\n",
        "    def __init__(self,\n",
        "                 initial_mean,\n",
        "                 initial_covariance,\n",
        "                 dynamics_matrix,\n",
        "                 dynamics_bias,\n",
        "                 dynamics_noise_covariance,\n",
        "                 emissions_means,\n",
        "                 emissions_covariances,\n",
        "                 log_normalizer,\n",
        "                 filtered_means,\n",
        "                 filtered_covariances,\n",
        "                 smoothed_means,\n",
        "                 smoothed_covariances,\n",
        "                 smoothed_cross,\n",
        "                 validate_args=False,\n",
        "                 allow_nan_stats=True,\n",
        "                 name=\"LinearGaussianSSM\",\n",
        "             ) -> None:\n",
        "        # Dynamics\n",
        "        self._initial_mean = initial_mean\n",
        "        self._initial_covariance = initial_covariance\n",
        "        self._dynamics_matrix = dynamics_matrix\n",
        "        self._dynamics_bias = dynamics_bias\n",
        "        self._dynamics_noise_covariance = dynamics_noise_covariance\n",
        "        # Emissions\n",
        "        self._emissions_means = emissions_means\n",
        "        self._emissions_covariances = emissions_covariances\n",
        "        # Filtered\n",
        "        self._log_normalizer = log_normalizer\n",
        "        self._filtered_means = filtered_means\n",
        "        self._filtered_covariances = filtered_covariances\n",
        "        # Smoothed\n",
        "        self._smoothed_means = smoothed_means\n",
        "        self._smoothed_covariances = smoothed_covariances\n",
        "        self._smoothed_cross = smoothed_cross\n",
        "\n",
        "        # We would detect the dtype dynamically but that would break vmap\n",
        "        # see https://github.com/tensorflow/probability/issues/1271\n",
        "        dtype = np.float32\n",
        "        super(LinearGaussianSSM, self).__init__(\n",
        "            dtype=dtype,\n",
        "            validate_args=validate_args,\n",
        "            allow_nan_stats=allow_nan_stats,\n",
        "            reparameterization_type=reparameterization.NOT_REPARAMETERIZED,\n",
        "            parameters=dict(initial_mean=self._initial_mean,\n",
        "                            initial_covariance=self._initial_covariance,\n",
        "                            dynamics_matrix=self._dynamics_matrix,\n",
        "                            dynamics_bias=self._dynamics_bias,\n",
        "                            dynamics_noise_covariance=self._dynamics_noise_covariance,\n",
        "                            emissions_means=self._emissions_means,\n",
        "                            emissions_covariances=self._emissions_covariances,\n",
        "                            log_normalizer=self._log_normalizer,\n",
        "                            filtered_means=self._filtered_means,\n",
        "                            filtered_covariances=self._filtered_covariances,\n",
        "                            smoothed_means=self._smoothed_means,\n",
        "                            smoothed_covariances=self._smoothed_covariances,\n",
        "                            smoothed_cross=self._smoothed_cross),\n",
        "            name=name,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _parameter_properties(cls, dtype, num_classes=None):\n",
        "        # pylint: disable=g-long-lambda\n",
        "        return dict(initial_mean=tfp.internal.parameter_properties.ParameterProperties(event_ndims=1),\n",
        "                    initial_covariance=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    dynamics_matrix=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    dynamics_bias=tfp.internal.parameter_properties.ParameterProperties(event_ndims=1),\n",
        "                    dynamics_noise_covariance=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    emissions_means=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    emissions_covariances=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "                    log_normalizer=tfp.internal.parameter_properties.ParameterProperties(event_ndims=0),\n",
        "                    filtered_means=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    filtered_covariances=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "                    smoothed_means=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    smoothed_covariances=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "                    smoothed_cross=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3)\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def infer_from_dynamics_and_potential(cls, dynamics_params, emissions_potentials):\n",
        "        p = dynamics_params\n",
        "        mus, Sigmas = emissions_potentials[\"mu\"], emissions_potentials[\"Sigma\"]\n",
        "\n",
        "        dim = mus.shape[-1]\n",
        "        C = np.eye(dim)\n",
        "        d = np.zeros(dim)\n",
        "\n",
        "        params = make_lgssm_params(p[\"m1\"], p[\"Q1\"], p[\"A\"], p[\"Q\"], C, Sigmas, \n",
        "                                   dynamics_bias=p[\"b\"], emissions_bias=d)\n",
        "\n",
        "        smoothed = lgssm_smoother(params, mus)._asdict()\n",
        "        \n",
        "        # Compute ExxT\n",
        "        A, Q = dynamics_params[\"A\"], dynamics_params[\"Q\"]\n",
        "        filtered_cov = smoothed[\"filtered_covariances\"]\n",
        "        filtered_mean = smoothed[\"smoothed_means\"]\n",
        "        smoothed_cov = smoothed[\"smoothed_covariances\"]\n",
        "        smoothed_mean = smoothed[\"smoothed_means\"]\n",
        "        G = vmap(lambda C: psd_solve(Q + A @ C @ A.T, A @ C).T)(filtered_cov)\n",
        "\n",
        "        # Compute the smoothed expectation of z_t z_{t+1}^T\n",
        "        smoothed_cross = vmap(\n",
        "            lambda Gt, mean, next_mean, next_cov: Gt @ next_cov + np.outer(mean, next_mean))\\\n",
        "            (G[:-1], smoothed_mean[:-1], smoothed_mean[1:], smoothed_cov[1:])\n",
        "\n",
        "        log_Z = lgssm_log_normalizer(dynamics_params, \n",
        "                                     smoothed[\"filtered_means\"], \n",
        "                                     smoothed[\"filtered_covariances\"], \n",
        "                                     emissions_potentials)\n",
        "\n",
        "        return cls(dynamics_params[\"m1\"],\n",
        "                   dynamics_params[\"Q1\"],\n",
        "                   dynamics_params[\"A\"],\n",
        "                   dynamics_params[\"b\"],\n",
        "                   dynamics_params[\"Q\"],\n",
        "                   emissions_potentials[\"mu\"],\n",
        "                   emissions_potentials[\"Sigma\"],\n",
        "                   log_Z, # smoothed[\"marginal_loglik\"],\n",
        "                   smoothed[\"filtered_means\"],\n",
        "                   smoothed[\"filtered_covariances\"],\n",
        "                   smoothed[\"smoothed_means\"],\n",
        "                   smoothed[\"smoothed_covariances\"],\n",
        "                   smoothed_cross)\n",
        "        \n",
        "    # Properties to get private class variables\n",
        "    @property\n",
        "    def log_normalizer(self):\n",
        "        return self._log_normalizer\n",
        "\n",
        "    @property\n",
        "    def filtered_means(self):\n",
        "        return self._filtered_means\n",
        "\n",
        "    @property\n",
        "    def filtered_covariances(self):\n",
        "        return self._filtered_covariances\n",
        "\n",
        "    @property\n",
        "    def smoothed_means(self):\n",
        "        return self._smoothed_means\n",
        "\n",
        "    @property\n",
        "    def smoothed_covariances(self):\n",
        "        return self._smoothed_covariances\n",
        "\n",
        "    @property\n",
        "    def expected_states(self):\n",
        "        return self._smoothed_means\n",
        "\n",
        "    @property\n",
        "    def expected_states_squared(self):\n",
        "        Ex = self._smoothed_means\n",
        "        return self._smoothed_covariances + np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "\n",
        "    @property\n",
        "    def expected_states_next_states(self):\n",
        "        return self._smoothed_cross\n",
        "\n",
        "    def _mean(self):\n",
        "        return self.smoothed_means\n",
        "\n",
        "    def _covariance(self):\n",
        "        return self.smoothed_covariances\n",
        "    \n",
        "    # TODO: currently this function does not depend on the dynamics bias\n",
        "    def _log_prob(self, data, **kwargs):\n",
        "        A = self._dynamics_matrix #params[\"A\"]\n",
        "        Q = self._dynamics_noise_covariance #params[\"Q\"]\n",
        "        Q1 = self._initial_covariance #params[\"Q1\"]\n",
        "        m1 = self._initial_mean #params[\"m1\"]\n",
        "\n",
        "        num_batch_dims = len(data.shape) - 2\n",
        "\n",
        "        ll = np.sum(\n",
        "            MVN(loc=np.einsum(\"ij,...tj->...ti\", A, data[...,:-1,:]), \n",
        "                covariance_matrix=Q).log_prob(data[...,1:,:])\n",
        "            )\n",
        "        ll += MVN(loc=m1, covariance_matrix=Q1).log_prob(data[...,0,:])\n",
        "\n",
        "        # Add the observation potentials\n",
        "        # ll += - 0.5 * np.einsum(\"...ti,tij,...tj->...\", data, self._emissions_precisions, data) \\\n",
        "        #       + np.einsum(\"...ti,ti->...\", data, self._emissions_linear_potentials)\n",
        "        ll += np.sum(MVN(loc=self._emissions_means, \n",
        "                  covariance_matrix=self._emissions_covariances).log_prob(data), axis=-1)\n",
        "        # Add the log normalizer\n",
        "        ll -= self._log_normalizer\n",
        "\n",
        "        return ll\n",
        "\n",
        "    def _sample_n(self, n, seed=None):\n",
        "\n",
        "        F = self._dynamics_matrix\n",
        "        b = self._dynamics_bias\n",
        "        Q = self._dynamics_noise_covariance\n",
        "        \n",
        "        def sample_single(\n",
        "            key,\n",
        "            filtered_means,\n",
        "            filtered_covariances\n",
        "        ):\n",
        "\n",
        "            initial_elements = _make_associative_sampling_elements(\n",
        "                { \"A\": F, \"b\": b, \"Q\": Q }, key, filtered_means, filtered_covariances)\n",
        "\n",
        "            @vmap\n",
        "            def sampling_operator(elem1, elem2):\n",
        "                E1, h1 = elem1\n",
        "                E2, h2 = elem2\n",
        "\n",
        "                E = E2 @ E1\n",
        "                h = E2 @ h1 + h2\n",
        "                return E, h\n",
        "\n",
        "            _, sample = \\\n",
        "                lax.associative_scan(sampling_operator, initial_elements, reverse=True)\n",
        "                \n",
        "            return sample\n",
        "\n",
        "        # TODO: Handle arbitrary batch shapes\n",
        "        if self._filtered_covariances.ndim == 4:\n",
        "            # batch mode\n",
        "            samples = vmap(vmap(sample_single, in_axes=(None, 0, 0)), in_axes=(0, None, None))\\\n",
        "                (jr.split(seed, n), self._filtered_means, self._filtered_covariances)\n",
        "            # Transpose to be (num_samples, num_batches, num_timesteps, dim)\n",
        "            # samples = np.transpose(samples, (1, 0, 2, 3))\n",
        "        else:\n",
        "            # non-batch mode\n",
        "            samples = vmap(sample_single, in_axes=(0, None, None))\\\n",
        "                (jr.split(seed, n), self._filtered_means, self._filtered_covariances)\n",
        "        return samples\n",
        "\n",
        "    def _entropy(self):\n",
        "        \"\"\"\n",
        "        Compute the entropy\n",
        "\n",
        "            H[X] = -E[\\log p(x)]\n",
        "                 = -E[-1/2 x^T J x + x^T h - log Z(J, h)]\n",
        "                 = 1/2 <J, E[x x^T] - <h, E[x]> + log Z(J, h)\n",
        "        \"\"\"\n",
        "        Ex = self.expected_states\n",
        "        ExxT = self.expected_states_squared\n",
        "        ExnxT = self.expected_states_next_states\n",
        "        p = dynamics_to_tridiag(\n",
        "            {\n",
        "                \"m1\": self._initial_mean,\n",
        "                \"Q1\": self._initial_covariance,\n",
        "                \"A\": self._dynamics_matrix,\n",
        "                \"b\": self._dynamics_bias,\n",
        "                \"Q\": self._dynamics_noise_covariance,\n",
        "            }, Ex.shape[0], Ex.shape[1]\n",
        "        )\n",
        "        J_diag = p[\"J\"] + solve(self._emissions_covariances, np.eye(Ex.shape[-1])[None])\n",
        "        J_lower_diag = p[\"L\"]\n",
        "\n",
        "        Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "        Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "        entropy = 0.5 * np.sum(J_diag * Sigmatt)\n",
        "        entropy += np.sum(J_lower_diag * Sigmatnt)\n",
        "        return entropy - self.log_prob(Ex)\n",
        "\n",
        "class ParallelLinearGaussianSSM(LinearGaussianSSM):\n",
        "    def __init__(self, *args, **kwargs) -> None:\n",
        "        kwargs[\"name\"] = \"ParallelLinearGaussianSSM\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def infer_from_dynamics_and_potential(cls, dynamics_params, emissions_potentials):\n",
        "        # p = dynamics_params\n",
        "        # mus, Sigmas = emissions_potentials[\"mu\"], emissions_potentials[\"Sigma\"]\n",
        "\n",
        "        # dim = mus.shape[-1]\n",
        "        # C = np.eye(dim)\n",
        "        # d = np.zeros(dim)\n",
        "\n",
        "        smoothed = parallel_lgssm_smoother(dynamics_params, emissions_potentials)\n",
        "        \n",
        "        # Compute ExxT\n",
        "        A, Q = dynamics_params[\"A\"], dynamics_params[\"Q\"]\n",
        "        filtered_cov = smoothed[\"filtered_covariances\"]\n",
        "        filtered_mean = smoothed[\"smoothed_means\"]\n",
        "        smoothed_cov = smoothed[\"smoothed_covariances\"]\n",
        "        smoothed_mean = smoothed[\"smoothed_means\"]\n",
        "        G = vmap(lambda C: psd_solve(Q + A @ C @ A.T, A @ C).T)(filtered_cov)\n",
        "\n",
        "        # Compute the smoothed expectation of z_t z_{t+1}^T\n",
        "        smoothed_cross = vmap(\n",
        "            lambda Gt, mean, next_mean, next_cov: Gt @ next_cov + np.outer(mean, next_mean))\\\n",
        "            (G[:-1], smoothed_mean[:-1], smoothed_mean[1:], smoothed_cov[1:])\n",
        "\n",
        "        log_Z = lgssm_log_normalizer(dynamics_params, \n",
        "                                     smoothed[\"filtered_means\"], \n",
        "                                     smoothed[\"filtered_covariances\"], \n",
        "                                     emissions_potentials)\n",
        "\n",
        "        return cls(dynamics_params[\"m1\"],\n",
        "                   dynamics_params[\"Q1\"],\n",
        "                   dynamics_params[\"A\"],\n",
        "                   dynamics_params[\"b\"],\n",
        "                   dynamics_params[\"Q\"],\n",
        "                   emissions_potentials[\"mu\"],\n",
        "                   emissions_potentials[\"Sigma\"],\n",
        "                   log_Z, # smoothed[\"marginal_loglik\"],\n",
        "                   smoothed[\"filtered_means\"],\n",
        "                   smoothed[\"filtered_covariances\"],\n",
        "                   smoothed[\"smoothed_means\"],\n",
        "                   smoothed[\"smoothed_covariances\"],\n",
        "                   smoothed_cross)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "HOdprEnnTpSN"
      },
      "outputs": [],
      "source": [
        "# @title Parallel versions of the same priors and posteriors\n",
        "\n",
        "# Super simple because all the machinary is already taken care of\n",
        "class LDSSVAEPosterior(SVAEPrior):\n",
        "    def __init__(self, latent_dims, seq_len, use_parallel=False):\n",
        "        self.latent_dims = latent_dims\n",
        "        # The only annoying thing is that we have to specify the sequence length\n",
        "        # ahead of time\n",
        "        self.seq_len = seq_len\n",
        "        self.dist = ParallelLinearGaussianSSM if use_parallel else LinearGaussianSSM\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return (self.seq_len, self.latent_dims)\n",
        "\n",
        "    # Must be the full set of constrained parameters!\n",
        "    def distribution(self, p):\n",
        "        m1, Q1, A, b, Q, mus, Sigmas = p[\"m1\"], p[\"Q1\"], p[\"A\"], p[\"b\"], p[\"Q\"], p[\"mu\"], p[\"Sigma\"]\n",
        "        log_Z, mu_filtered, Sigma_filtered = p[\"log_Z\"], p[\"mu_filtered\"], p[\"Sigma_filtered\"]\n",
        "        mu_smoothed, Sigma_smoothed, ExnxT = p[\"mu_smoothed\"], p[\"Sigma_smoothed\"], p[\"ExnxT\"]\n",
        "        return self.dist(m1, Q1, A, b, Q, mus, Sigmas, \n",
        "                             log_Z, mu_filtered, Sigma_filtered, \n",
        "                             mu_smoothed, Sigma_smoothed, ExnxT)\n",
        "\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        p = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"Q1\": np.eye(D),\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": np.eye(D),\n",
        "            \"Sigma\": np.tile(np.eye(D)[None], (T, 1, 1)),\n",
        "            \"mu\": np.zeros((T, D))\n",
        "        }\n",
        "\n",
        "        dist = self.dist.infer_from_dynamics_and_potential(p, \n",
        "                                    {\"mu\": p[\"mu\"], \"Sigma\": p[\"Sigma\"]})\n",
        "        \n",
        "        p.update({\n",
        "            \"log_Z\": dist.log_normalizer,\n",
        "            \"mu_filtered\": dist.filtered_means,\n",
        "            \"Sigma_filtered\": dist.filtered_covariances,\n",
        "            \"mu_smoothed\": dist.smoothed_means,\n",
        "            \"Sigma_smoothed\": dist.smoothed_covariances,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        return params\n",
        "\n",
        "    def infer(self, prior_params, potential_params):\n",
        "        p = {\n",
        "            \"m1\": prior_params[\"m1\"],\n",
        "            \"Q1\": prior_params[\"Q1\"],\n",
        "            \"A\": prior_params[\"A\"],\n",
        "            \"b\": prior_params[\"b\"],\n",
        "            \"Q\": prior_params[\"Q\"],\n",
        "            \"Sigma\": potential_params[\"Sigma\"],\n",
        "            \"mu\": potential_params[\"mu\"]\n",
        "        }\n",
        "\n",
        "        dist = self.dist.infer_from_dynamics_and_potential(prior_params, \n",
        "                                    {\"mu\": p[\"mu\"], \"Sigma\": p[\"Sigma\"]})\n",
        "        p.update({\n",
        "            \"log_Z\": dist.log_normalizer,\n",
        "            \"mu_filtered\": dist.filtered_means,\n",
        "            \"Sigma_filtered\": dist.filtered_covariances,\n",
        "            \"mu_smoothed\": dist.smoothed_means,\n",
        "            \"Sigma_smoothed\": dist.smoothed_covariances,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtvHU_dlOklC"
      },
      "source": [
        "## Define neural network architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "40wvAwfoOn_E"
      },
      "outputs": [],
      "source": [
        "# @title Neural network utils\n",
        "\n",
        "PRNGKey = Any\n",
        "Shape = Iterable[int]\n",
        "Dtype = Any  # this could be a real type?\n",
        "Array = Any\n",
        "\n",
        "# Note: the last layer output does not have a relu activation!\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Define a simple fully connected MLP with ReLU activations.\n",
        "    \"\"\"\n",
        "    features: Sequence[int]\n",
        "    kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.he_normal()\n",
        "    bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.zeros\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for feat in self.features[:-1]:\n",
        "            x = nn.relu(nn.Dense(feat, \n",
        "                kernel_init=self.kernel_init,\n",
        "                bias_init=self.bias_init,)(x))\n",
        "        x = nn.Dense(self.features[-1], \n",
        "            kernel_init=self.kernel_init, \n",
        "            bias_init=self.bias_init)(x)\n",
        "        return x\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    \"\"\"\n",
        "    A layer which passes the input through unchanged.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "class Static(nn.Module):\n",
        "    \"\"\"\n",
        "    A layer which just returns some static parameters.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "    kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.lecun_normal()\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        kernel = self.param('kernel',\n",
        "                            self.kernel_init,\n",
        "                            (self.features, ))\n",
        "        return kernel\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"A simple CNN model.\"\"\"\n",
        "    input_rank : int = None   \n",
        "    output_dim : int = None\n",
        "    layer_params : Sequence[dict] = None\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for params in self.layer_params:\n",
        "            x = nn.relu(Conv(**params)(x))\n",
        "        # No activations at the output\n",
        "        x = nn.Dense(features=self.output_dim)(x.flatten())\n",
        "        return x\n",
        "\n",
        "class DCNN(nn.Module):\n",
        "    \"\"\"A simple DCNN model.\"\"\"   \n",
        "\n",
        "    input_shape: Sequence[int] = None\n",
        "    layer_params: Sequence[dict] = None\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        input_features = onp.prod(onp.array(self.input_shape))\n",
        "        x = nn.Dense(features=input_features)(x)\n",
        "        x = x.reshape(self.input_shape)\n",
        "        # Note that the last layer doesn't have an activation\n",
        "        for params in self.layer_params:\n",
        "            x = ConvTranspose(**params)(nn.relu(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "Ja-LJ88rybCV"
      },
      "outputs": [],
      "source": [
        "# @title Potential networks (outputs potentials on single observations)\n",
        "class PotentialNetwork(nn.Module):\n",
        "    def __call__(self, inputs):\n",
        "        Sigma, mu = self._generate_distribution_parameters(inputs)\n",
        "        # J, h = solve(Sigma, np.eye(mu.shape[-1])[None]), solve(Sigma, mu)\n",
        "        # if (len(J.shape) == 3):\n",
        "        #     seq_len, latent_dims, _ = J.shape\n",
        "        #     # lower diagonal blocks of precision matrix\n",
        "        #     L = np.zeros((seq_len-1, latent_dims, latent_dims))\n",
        "        # elif (len(J.shape) == 4):\n",
        "        #     batch_size, seq_len, latent_dims, _ = J.shape\n",
        "        #     # lower diagonal blocks of precision matrix\n",
        "        #     L = np.zeros((batch_size, seq_len-1, latent_dims, latent_dims))\n",
        "        # else:\n",
        "        #     L = np.zeros(tuple())\n",
        "        return {#\"J\": J, \"L\": L, \"h\": h, \n",
        "                \"Sigma\": Sigma, \"mu\": mu}\n",
        "\n",
        "    def _generate_distribution_parameters(self, inputs):\n",
        "        if (len(inputs.shape) == self.input_rank + 2):\n",
        "            # We have both a batch dimension and a time dimension\n",
        "            # and we have to vmap over both...!\n",
        "            return vmap(vmap(self._call_single, 0), 0)(inputs)\n",
        "        elif (len(inputs.shape) == self.input_rank + 1):\n",
        "            return vmap(self._call_single)(inputs)\n",
        "        elif (len(inputs.shape) == self.input_rank):\n",
        "            return self._call_single(inputs)\n",
        "        else:\n",
        "            # error\n",
        "            return None\n",
        "\n",
        "    def _call_single(self, inputs):\n",
        "        pass\n",
        "\n",
        "# A new, more general implementation of the Gaussian recognition network\n",
        "# Uses mean parameterization which works better empirically\n",
        "class GaussianRecognition(PotentialNetwork):\n",
        "\n",
        "    use_diag : int = None\n",
        "    input_rank : int = None\n",
        "    latent_dims : int = None\n",
        "    trunk_fn : nn.Module = None\n",
        "    head_mean_fn : nn.Module = None\n",
        "    head_log_var_fn : nn.Module = None\n",
        "    eps : float = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_params(cls, input_rank=1, input_dim=None, output_dim=None, \n",
        "                    trunk_type=\"Identity\", trunk_params=None, \n",
        "                    head_mean_type=\"MLP\", head_mean_params=None,\n",
        "                    head_var_type=\"MLP\", head_var_params=None, diagonal_covariance=False,\n",
        "                    cov_init=1, eps=1e-3): \n",
        "\n",
        "        if trunk_type == \"Identity\":\n",
        "            trunk_params = { \"features\": input_dim }\n",
        "        if head_mean_type == \"MLP\":\n",
        "            head_mean_params[\"features\"] += [output_dim]\n",
        "        if head_var_type == \"MLP\":\n",
        "            if (diagonal_covariance):\n",
        "                head_var_params[\"features\"] += [output_dim]\n",
        "            else:\n",
        "                head_var_params[\"features\"] += [output_dim * (output_dim + 1) // 2]\n",
        "            head_var_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_var_params[\"bias_init\"] = nn.initializers.constant(cov_init)\n",
        "\n",
        "        trunk_fn = globals()[trunk_type](**trunk_params)\n",
        "        head_mean_fn = globals()[head_mean_type](**head_mean_params)\n",
        "        head_log_var_fn = globals()[head_var_type](**head_var_params)\n",
        "\n",
        "        return cls(diagonal_covariance, input_rank, output_dim, trunk_fn, \n",
        "                   head_mean_fn, head_log_var_fn, eps)\n",
        "\n",
        "    def _call_single(self, inputs):\n",
        "        # Apply the trunk.\n",
        "        trunk_output = self.trunk_fn(inputs)\n",
        "        # Get the mean.\n",
        "        mu = self.head_mean_fn(trunk_output)\n",
        "        # Get the covariance parameters and build a full matrix from it.\n",
        "        var_output_flat = self.head_log_var_fn(trunk_output)\n",
        "        if self.use_diag:\n",
        "            Sigma = np.diag(softplus(var_output_flat) + self.eps)\n",
        "        else:\n",
        "            Sigma = lie_params_to_constrained(var_output_flat, self.latent_dims, self.eps)\n",
        "        # h = np.linalg.solve(Sigma, mu)\n",
        "        # J = np.linalg.inv(Sigma)\n",
        "        # lower diagonal blocks of precision matrix\n",
        "        return (Sigma, mu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "gmeCrsbCy96H"
      },
      "outputs": [],
      "source": [
        "# @title Posterior networks (outputs full posterior for entire sequence)\n",
        "# Outputs Gaussian distributions for the entire sequence at once\n",
        "class PosteriorNetwork(PotentialNetwork):\n",
        "    def __call__(self, inputs):\n",
        "        As, bs, Qs = self._generate_distribution_parameters(inputs)\n",
        "        return {\"As\": As, \"bs\": bs, \"Qs\": Qs}\n",
        "\n",
        "    def _generate_distribution_parameters(self, inputs):\n",
        "        is_batched = (len(inputs.shape) == self.input_rank+2)\n",
        "        if is_batched:\n",
        "            return vmap(self._call_single, in_axes=0)(inputs)\n",
        "        else:\n",
        "            assert(len(inputs.shape) == self.input_rank+1)\n",
        "            return self._call_single(inputs)\n",
        "\n",
        "class GaussianBiRNN(PosteriorNetwork):\n",
        "    \n",
        "    input_rank : int = None\n",
        "    rnn_dim : int = None\n",
        "    output_dim : int = None\n",
        "    forward_RNN : nn.Module = None\n",
        "    backward_RNN : nn.Module = None\n",
        "    input_fn : nn.Module = None\n",
        "    trunk_fn: nn.Module = None\n",
        "    head_mean_fn : nn.Module = None\n",
        "    head_log_var_fn : nn.Module = None\n",
        "    head_dyn_fn : nn.Module = None\n",
        "    eps : float = None\n",
        "        \n",
        "    @classmethod\n",
        "    def from_params(cls, input_rank=1, cell_type=nn.GRUCell,\n",
        "                    input_dim=None, rnn_dim=None, output_dim=None, \n",
        "                    input_type=\"MLP\", input_params=None,\n",
        "                    trunk_type=\"Identity\", trunk_params=None, \n",
        "                    head_mean_type=\"MLP\", head_mean_params=None,\n",
        "                    head_var_type=\"MLP\", head_var_params=None,\n",
        "                    head_dyn_type=\"MLP\", head_dyn_params=None,\n",
        "                    cov_init=1, eps=1e-4): \n",
        "\n",
        "        forward_RNN = nn.scan(cell_type, variable_broadcast=\"params\", \n",
        "                                             split_rngs={\"params\": False})()\n",
        "        backward_RNN = nn.scan(cell_type, variable_broadcast=\"params\", \n",
        "                                               split_rngs={\"params\": False}, reverse=True)()\n",
        "        if trunk_type == \"Identity\":\n",
        "            trunk_params = { \"features\": rnn_dim }\n",
        "        if input_type == \"MLP\":\n",
        "            input_params[\"features\"] += [rnn_dim]\n",
        "        if head_mean_type == \"MLP\":\n",
        "            head_mean_params[\"features\"] += [output_dim]\n",
        "        if head_var_type == \"MLP\":\n",
        "            head_var_params[\"features\"] += [output_dim * (output_dim + 1) // 2]\n",
        "            head_var_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_var_params[\"bias_init\"] = nn.initializers.constant(cov_init)\n",
        "        if head_dyn_type == \"MLP\":\n",
        "            head_dyn_params[\"features\"] += [output_dim ** 2,]\n",
        "            head_dyn_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_dyn_params[\"bias_init\"] = nn.initializers.zeros\n",
        "\n",
        "        trunk_fn = globals()[trunk_type](**trunk_params)\n",
        "        input_fn = globals()[input_type](**input_params)\n",
        "        head_mean_fn = globals()[head_mean_type](**head_mean_params)\n",
        "        head_log_var_fn = globals()[head_var_type](**head_var_params)\n",
        "        head_dyn_fn = globals()[head_dyn_type](**head_dyn_params)\n",
        "\n",
        "        return cls(input_rank, rnn_dim, output_dim, \n",
        "                   forward_RNN, backward_RNN, \n",
        "                   input_fn, trunk_fn, \n",
        "                   head_mean_fn, head_log_var_fn, head_dyn_fn, eps)\n",
        "\n",
        "    # Applied the BiRNN to a single sequence of inputs\n",
        "    def _call_single(self, inputs):\n",
        "        output_dim = self.output_dim\n",
        "        \n",
        "        inputs = vmap(self.input_fn)(inputs)\n",
        "        init_carry_forward = np.zeros((self.rnn_dim,))\n",
        "        _, out_forward = self.forward_RNN(init_carry_forward, inputs)\n",
        "        init_carry_backward = np.zeros((self.rnn_dim,))\n",
        "        _, out_backward = self.backward_RNN(init_carry_backward, inputs)\n",
        "        # Concatenate the forward and backward outputs\n",
        "        out_combined = np.concatenate([out_forward, out_backward], axis=-1)\n",
        "        \n",
        "        # Get the mean.\n",
        "        # vmap over the time dimension\n",
        "        b = vmap(self.head_mean_fn)(out_combined)\n",
        "\n",
        "        # Get the variance output and reshape it.\n",
        "        # vmap over the time dimension\n",
        "        var_output_flat = vmap(self.head_log_var_fn)(out_combined)\n",
        "        Q = vmap(lie_params_to_constrained, in_axes=(0, None, None))\\\n",
        "            (var_output_flat, output_dim, self.eps)\n",
        "        dynamics_flat = vmap(self.head_dyn_fn)(out_combined)\n",
        "        A = dynamics_flat.reshape((-1, output_dim, output_dim))\n",
        "\n",
        "        return (A, b, Q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "cellView": "form",
        "id": "caU8H-hMEB_f"
      },
      "outputs": [],
      "source": [
        "# @title Special architectures for PlaNet\n",
        "class PlaNetRecognitionWrapper:\n",
        "    def __init__(self, rec_net):\n",
        "        self.rec_net = rec_net\n",
        "\n",
        "    def init(self, key, *inputs):\n",
        "        return self.rec_net.init(key, *inputs)\n",
        "    \n",
        "    def apply(self, params, x):\n",
        "        return {\n",
        "            \"network_input\": self.rec_net.apply(params[\"rec_params\"], x)[\"h\"],\n",
        "            \"network_params\": params[\"post_params\"],\n",
        "        }\n",
        "\n",
        "class StochasticRNNCell(nn.Module):\n",
        "\n",
        "    output_dim : int = None\n",
        "    rnn_cell : nn.Module = None\n",
        "    trunk_fn: nn.Module = None\n",
        "    head_mean_fn : nn.Module = None\n",
        "    head_log_var_fn : nn.Module = None\n",
        "    eps : float = None\n",
        "        \n",
        "    @classmethod\n",
        "    def from_params(cls, cell_type=nn.GRUCell,\n",
        "                    rnn_dim=None, output_dim=None, \n",
        "                    trunk_type=\"Identity\", trunk_params=None, \n",
        "                    head_mean_type=\"MLP\", head_mean_params=None,\n",
        "                    head_var_type=\"MLP\", head_var_params=None,\n",
        "                    cov_init=1, eps=1e-4, **kwargs): \n",
        "\n",
        "        rnn_cell = cell_type()\n",
        "\n",
        "        if trunk_type == \"Identity\":\n",
        "            trunk_params = { \"features\": rnn_dim }\n",
        "        if head_mean_type == \"MLP\":\n",
        "            head_mean_params[\"features\"] += [output_dim]\n",
        "        if head_var_type == \"MLP\":\n",
        "            head_var_params[\"features\"] += [output_dim * (output_dim + 1) // 2]\n",
        "            head_var_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_var_params[\"bias_init\"] = nn.initializers.constant(cov_init)\n",
        "\n",
        "        trunk_fn = globals()[trunk_type](**trunk_params)\n",
        "        head_mean_fn = globals()[head_mean_type](**head_mean_params)\n",
        "        head_log_var_fn = globals()[head_var_type](**head_var_params)\n",
        "\n",
        "        return cls(output_dim, rnn_cell, trunk_fn, head_mean_fn, head_log_var_fn, eps)\n",
        "\n",
        "    # h: latent state that's carried to the next\n",
        "    # x: last sample\n",
        "    # u: input at this timestep\n",
        "    def __call__(self, h, x, u):\n",
        "        h, out = self.rnn_cell(h, np.concatenate([x, u]))\n",
        "        out = self.trunk_fn(out)\n",
        "        mean, cov_flat = self.head_mean_fn(out), self.head_log_var_fn(out)\n",
        "        cov = lie_params_to_constrained(cov_flat, self.output_dim, self.eps)\n",
        "        return h, (cov, mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "_DU0-NEWx_Il"
      },
      "outputs": [],
      "source": [
        "# @title Emission network (outputs distribution instead of parameters)\n",
        "\n",
        "# This is largely for convenience\n",
        "class GaussianEmission(GaussianRecognition):\n",
        "    def __call__(self, inputs):\n",
        "        J, h = self._generate_distribution_parameters(inputs)\n",
        "        # TODO: inverting J is pretty bad numerically, perhaps save Cholesky instead?\n",
        "        if (len(J.shape) == 3):\n",
        "            Sigma = vmap(inv)(J)\n",
        "            mu = np.einsum(\"tij,tj->ti\", Sigma, h)\n",
        "        elif (len(J.shape) == 2):\n",
        "            Sigma = inv(J)\n",
        "            mu = np.linalg.solve(J, h)\n",
        "        else:\n",
        "            # Error\n",
        "            return None\n",
        "        return tfd.MultivariateNormalFullCovariance(\n",
        "            loc=mu, covariance_matrix=Sigma)\n",
        "        \n",
        "class GaussianDCNNEmission(PotentialNetwork):\n",
        "\n",
        "    input_rank : int = None\n",
        "    network : nn.Module = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_params(cls, **params):\n",
        "        network = DCNN(**params)\n",
        "        return cls(1, network)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        out = self._generate_distribution_parameters(inputs)\n",
        "        mu = out[\"mu\"]\n",
        "        # Adding a constant to prevent the model from getting too crazy\n",
        "        sigma = out[\"sigma\"] + 1e-4\n",
        "        return tfd.Normal(loc=mu, scale=sigma)\n",
        "\n",
        "    def _call_single(self, x):\n",
        "        out_raw = self.network(x)\n",
        "        mu_raw, sigma_raw = np.split(out_raw, 2, axis=-1)\n",
        "        # Get rid of the Sigmoid\n",
        "        # mu = sigmoid(mu_raw)\n",
        "        mu = mu_raw\n",
        "        sigma = softplus(sigma_raw)\n",
        "        # sigma = np.ones_like(mu) * 0.1\n",
        "        return { \"mu\": mu, \"sigma\": sigma }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xevru2BSSSZ"
      },
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "28XHvF41SVWK"
      },
      "outputs": [],
      "source": [
        "# @title Visualization/animation helpers\n",
        "\n",
        "# Returns a random projection matrix from ND to 2D\n",
        "def random_projection(seed, N):\n",
        "    key1, key2 = jr.split(seed, 2)\n",
        "    v1 = jr.normal(key1, (N,))\n",
        "    v2 = jr.normal(key2, (N,))\n",
        "\n",
        "    v1 /= np.linalg.norm(v1)\n",
        "    v2 -= v1 * np.dot(v1, v2)\n",
        "    v2 /= np.linalg.norm(v2)\n",
        "\n",
        "    return np.stack([v1, v2])\n",
        "\n",
        "def get_gaussian_draw_params(mu, Sigma, proj_seed=None):\n",
        "\n",
        "    Sigma = (Sigma + Sigma.T) * .5\n",
        "\n",
        "    if (mu.shape[0] > 2):\n",
        "        P = random_projection(proj_seed, mu.shape[0])\n",
        "        mu = P @ mu\n",
        "    angles = np.hstack([np.arange(0, 2*np.pi, 0.01), 0])\n",
        "    circle = np.vstack([np.sin(angles), np.cos(angles)])\n",
        "    min_eig = np.min(eigh(Sigma)[0])\n",
        "    eps = 1e-6\n",
        "    if (min_eig <= eps): Sigma += np.eye(Sigma.shape[0]) * eps\n",
        "    L = np.linalg.cholesky(Sigma)\n",
        "    u, svs, vt = svd(P @ L)\n",
        "    ellipse = np.dot(u * svs, circle) * 2\n",
        "    return (mu[0], mu[1]), (ellipse[0, :] + mu[0], ellipse[1, :] + mu[1])\n",
        "\n",
        "def plot_gaussian_2D(mu, Sigma, proj_seed=None, ax=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Helper function to plot 2D Gaussian contours\n",
        "    \"\"\"\n",
        "    (px, py), (exs, eys) = get_gaussian_draw_params(mu, Sigma, proj_seed)\n",
        "\n",
        "    ax = plt.gca() if ax is None else ax\n",
        "    point = ax.plot(px, py, marker='D', **kwargs)\n",
        "    line, = ax.plot(exs, eys, **kwargs)\n",
        "    return (point, line)\n",
        "\n",
        "def get_artists(ax, mus, Sigmas, proj_seed, num_pts, **draw_params):\n",
        "    point_artists = []\n",
        "    line_artists = []\n",
        "\n",
        "    for j in range(num_pts):\n",
        "        mean_params, cov_params = get_gaussian_draw_params(mus[j], \n",
        "                                                           Sigmas[j], \n",
        "                                                           proj_seed)\n",
        "        point = ax.plot(mean_params[0], mean_params[1], marker='D', \n",
        "                        color=colors[j], **draw_params)[0]\n",
        "        line = ax.plot(cov_params[0], cov_params[1], \n",
        "                       color=colors[j], **draw_params)[0]\n",
        "        point_artists.append(point)\n",
        "        line_artists.append(line)\n",
        "    return point_artists, line_artists\n",
        "\n",
        "def update_draw_params(point_artists, line_artists, mus, Sigmas, proj_seed, num_pts):\n",
        "    for i in range(num_pts):\n",
        "        mean_params, cov_params = get_gaussian_draw_params(mus[i], Sigmas[i], proj_seed)\n",
        "        point_artists[i].set_data(mean_params[0], mean_params[1])\n",
        "        line_artists[i].set_data(cov_params[0], cov_params[1])\n",
        "\n",
        "# Some animation helpers\n",
        "def animate_gaussians(inf_mus, inf_Sigmas, \n",
        "                      tgt_mus, tgt_Sigmas, \n",
        "                      true_mus, true_Sigmas,\n",
        "                      num_pts,\n",
        "                      proj_seed=None, x_lim=None, y_lim=None, **kwargs):\n",
        "    proj_seed = jr.PRNGKey(0) if proj_seed is None else proj_seed\n",
        "    print(\"Animating Gaussian blobs...!\")\n",
        "    # First set up the figure, the axis, and the plot element we want to animate\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlim(-10, 10)\n",
        "    ax.set_ylim(-10, 10)\n",
        "    plt.close()\n",
        "    \n",
        "\n",
        "    tgt_points, tgt_lines = get_artists(ax, tgt_mus[0], tgt_Sigmas[0], proj_seed, num_pts,\n",
        "                                        alpha=0.3, linestyle=\"dotted\")\n",
        "    inf_points, inf_lines = get_artists(ax, inf_mus[0], inf_Sigmas[0], proj_seed, num_pts)\n",
        "    true_points, true_lines = get_artists(ax, true_mus, true_Sigmas, \n",
        "                                          proj_seed, num_pts, alpha=0.2)\n",
        "\n",
        "    artists = tgt_points + tgt_lines + inf_points + inf_lines + true_points + true_lines\n",
        "\n",
        "    T = len(inf_mus)\n",
        "\n",
        "    # animation function. This is called sequentially  \n",
        "    def animate(i):\n",
        "        update_draw_params(tgt_points, tgt_lines, tgt_mus[i], tgt_Sigmas[i], proj_seed, num_pts)\n",
        "        update_draw_params(inf_points, inf_lines, inf_mus[i], inf_Sigmas[i], proj_seed, num_pts)\n",
        "        clear_output(wait=True)\n",
        "        print(\"Processing frame #{}/{}\".format(i+1, T))\n",
        "        return artists\n",
        "    \n",
        "    if x_lim is not None:\n",
        "        ax.set_xlim(x_lim)\n",
        "    if y_lim is not None:\n",
        "        ax.set_ylim(y_lim)\n",
        "\n",
        "    anim = animation.FuncAnimation(fig, animate, \n",
        "                                frames=T, interval=50, blit=True)\n",
        "    print(\"Frames created! Displaying animation in output cell...\")\n",
        "    # Note: below is the part which makes it work on Colab\n",
        "    rc('animation', html='jshtml')\n",
        "    return anim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "cellView": "form",
        "id": "EUG5JgVpTkC3"
      },
      "outputs": [],
      "source": [
        "# @title Helper for computing posterior marginals\n",
        "def get_emission_matrices(dec_params):\n",
        "    eps = 1e-4\n",
        "    dec_mean_params = dec_params[\"params\"][\"head_mean_fn\"][\"Dense_0\"]\n",
        "    dec_cov_params = dec_params[\"params\"][\"head_log_var_fn\"][\"Dense_0\"]\n",
        "    C_, d_ = dec_mean_params[\"kernel\"].T, dec_mean_params[\"bias\"]\n",
        "    R_ = np.diag(softplus(dec_cov_params[\"bias\"]) + eps)\n",
        "    return { \"C\": C_, \"d\": d_, \"R\": R_ }\n",
        "\n",
        "def get_marginals_and_targets(seed, data, num_points, model, \n",
        "                              past_params, true_model_params):\n",
        "    N, T = data.shape[:2]\n",
        "    rand_sample = jr.permutation(seed, onp.arange(N * T))[:num_points]\n",
        "    trials = rand_sample // T\n",
        "    times = rand_sample % T\n",
        "\n",
        "    # Compute a linear transformation in the latent space that will attempt to \n",
        "    # align the learned posterior to the true posterior\n",
        "    C, d = true_model_params[\"C\"], true_model_params[\"d\"]\n",
        "    CTC = C.T @ C\n",
        "    C_pinv = np.linalg.solve(CTC, C.T)\n",
        "\n",
        "    def align_latents(mus, Sigmas, p):\n",
        "        emissions_matrices = get_emission_matrices(p)\n",
        "        C_, d_ = emissions_matrices[\"C\"], emissions_matrices[\"d\"]\n",
        "        P = C_pinv @ C_\n",
        "        mus = np.einsum(\"ij,nj->ni\", P, mus) + (C_pinv @ (d_ - d))[None,:]\n",
        "        Sigmas = np.einsum(\"ij,njk,kl->nil\", P, Sigmas, P.T)\n",
        "        return mus, Sigmas\n",
        "\n",
        "    def posterior_mean_and_cov(post_params, t):\n",
        "        dist = model.posterior.distribution(post_params)\n",
        "        return dist.mean()[t], dist.covariance()[t]\n",
        "\n",
        "    def gaussian_posterior_mean_and_cov(post_params, t):\n",
        "        dist = true_lds.posterior.distribution(post_params)\n",
        "        return dist.mean()[t], dist.covariance()[t]\n",
        "\n",
        "    index_into_leaves = lambda l: l[trials]\n",
        "    \n",
        "    inf_mus = []\n",
        "    inf_Sigmas = []\n",
        "    tgt_mus = []\n",
        "    tgt_Sigmas = []\n",
        "    \n",
        "    true_lds = LDS(model.prior.latent_dims, T)\n",
        "    true_post_params = vmap(true_lds.e_step, in_axes=(None, 0))\\\n",
        "        (true_model_params, data[trials])\n",
        "    true_mus, true_Sigmas = vmap(gaussian_posterior_mean_and_cov)(true_post_params, times)\n",
        "\n",
        "    # TODO: this is temporary! Only for testing parallel KF!\n",
        "    base = LieParameterizedLinearGaussianChainPrior(model.prior.latent_dims, T)\n",
        "    model_lds = LDS(model.prior.latent_dims, T, base=base)\n",
        "\n",
        "    for i in range(len(past_params)):\n",
        "        model_params = past_params[i]\n",
        "        post_params = jax.tree_util.tree_map(index_into_leaves, \n",
        "                                             model_params[\"post_params\"])\n",
        "        dec_params = model_params[\"dec_params\"]\n",
        "        prior_params = deepcopy(model_params[\"prior_params\"])\n",
        "        # Compute posterior marginals\n",
        "        mus, Sigmas = vmap(posterior_mean_and_cov)(post_params, times)\n",
        "        mus, Sigmas = align_latents(mus, Sigmas, dec_params)\n",
        "        inf_mus.append(mus)\n",
        "        inf_Sigmas.append(Sigmas)\n",
        "\n",
        "        # Infer true posterior under current model params\n",
        "        prior_params.update(get_emission_matrices(dec_params))\n",
        "        tgt_post_params = vmap(model_lds.e_step, in_axes=(None, 0))(prior_params, data[trials])\n",
        "        mus, Sigmas = vmap(gaussian_posterior_mean_and_cov)(tgt_post_params, times)\n",
        "        mus, Sigmas = align_latents(mus, Sigmas, dec_params)\n",
        "\n",
        "        tgt_mus.append(mus)\n",
        "        tgt_Sigmas.append(Sigmas)\n",
        "    return inf_mus, inf_Sigmas, tgt_mus, tgt_Sigmas, true_mus, true_Sigmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "aTYdrgHMdJct"
      },
      "outputs": [],
      "source": [
        "# Trying to figure out the proper way of plotting the projection\n",
        "# Of high dimensional Gaussians\n",
        "# key = jr.split(key)[1]\n",
        "# dim = 5\n",
        "# Q = jr.uniform(key, shape=(dim, dim))\n",
        "# A = Q @ Q.T\n",
        "# L = cholesky(A)\n",
        "# P = random_projection(key_0, dim)\n",
        "# plot_gaussian_2D(np.zeros(dim), A, key_0)\n",
        "# points = jr.normal(key_0, (400, dim))\n",
        "# points /= np.sum(points ** 2, axis=1, keepdims=True) ** .5\n",
        "# points = P @ L @ points.T\n",
        "# plt.scatter(points[0, :], points[1, :], s=1)\n",
        "# u, svs, vt = svd(P @ L)\n",
        "# plt.scatter(u[0][0] * svs[0], u[1][0] * svs[0])\n",
        "# plt.scatter(u[0][1] * svs[1], u[1][1] * svs[1])\n",
        "# # plt.scatter(u[0][0] * np.sqrt(svs[0]), u[1][0] * np.sqrt(svs[0]))\n",
        "# # plt.scatter(u[0][1] * svs[1], u[1][1] * svs[1])\n",
        "# # P = random_projection(key_0, 3)\n",
        "# angles = np.hstack([np.arange(0, 2*np.pi, 0.01), 0])\n",
        "# circle = np.vstack([np.sin(angles), np.cos(angles)])\n",
        "# ellipse = np.dot(u * svs, circle) * 2\n",
        "# plt.plot(ellipse[0,:], ellipse[1,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ImmaouPD-G"
      },
      "source": [
        "## Define training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "cPqoyXb6PgpV"
      },
      "outputs": [],
      "source": [
        "# @title Trainer object \n",
        "from time import time\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    model: a pytree node\n",
        "    loss (key, params, model, data, **train_params) -> (loss, aux)\n",
        "        Returns a loss (a single float) and an auxillary output (e.g. posterior)\n",
        "    init (key, model, data, **train_params) -> (params, opts)\n",
        "        Returns the initial parameters and optimizers to go with those parameters\n",
        "    update (params, grads, opts, model, aux, **train_params) -> (params, opts)\n",
        "        Returns updated parameters, optimizers\n",
        "    \"\"\"\n",
        "    def __init__(self, model, \n",
        "                 train_params=None, \n",
        "                 init=None, \n",
        "                 loss=None, \n",
        "                 val_loss=None,\n",
        "                 update=None,\n",
        "                 initial_params=None):\n",
        "        # Trainer state\n",
        "        self.params = initial_params\n",
        "        self.model = model\n",
        "        self.past_params = []\n",
        "        self.time_spent = []\n",
        "\n",
        "        if train_params is None:\n",
        "            train_params = dict()\n",
        "\n",
        "        self.train_params = train_params\n",
        "\n",
        "        if init is not None:\n",
        "            self.init = init\n",
        "        if loss is not None:\n",
        "            self.loss = loss\n",
        "\n",
        "        self.val_loss = val_loss or self.loss\n",
        "        if update is not None: \n",
        "            self.update = update\n",
        "\n",
        "    # @partial(jit, static_argnums=(0,))\n",
        "    def train_step(self, key, params, data, opt_states):\n",
        "        model = self.model\n",
        "        results = \\\n",
        "            jax.value_and_grad(\n",
        "                lambda params: partial(self.loss, **self.train_params)(key, model, data, params), has_aux=True)(params)\n",
        "        (loss, aux), grads = results\n",
        "        params, opts = self.update(params, grads, self.opts, opt_states, model, aux, **self.train_params)\n",
        "        return params, opts, (loss, aux), grads\n",
        "\n",
        "    # @partial(jit, static_argnums=(0,))\n",
        "    def val_step(self, key, params, data):\n",
        "        return self.val_loss(key, self.model, data, params, **self.train_params)\n",
        "\n",
        "    # def test_step(self, key, params, model, data):\n",
        "    #     loss_out = self.loss(key, params, model, data)\n",
        "    #     return loss_out\n",
        "\n",
        "    \"\"\"\n",
        "    Callback: a function that takes training iterations and relevant parameter\n",
        "        And logs to WandB\n",
        "    \"\"\"\n",
        "    def train(self, data_dict, max_iters, \n",
        "              callback=None, val_callback=None, \n",
        "              summary=None, key=None,\n",
        "              early_stop_start=5000, \n",
        "              max_lose_streak=1000):\n",
        "\n",
        "        if key is None:\n",
        "            key = jr.PRNGKey(0)\n",
        "\n",
        "        model = self.model\n",
        "        train_data = data_dict[\"train_data\"]\n",
        "        batch_size = self.train_params.get(\"batch_size\") or train_data.shape[0]\n",
        "        num_batches = train_data.shape[0] // batch_size\n",
        "\n",
        "        init_key, key = jr.split(key, 2)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.params, self.opts, self.opt_states = self.init(init_key, model, \n",
        "                                                       train_data[:batch_size], \n",
        "                                                       self.params,\n",
        "                                                       **self.train_params)\n",
        "        self.train_losses = []\n",
        "        self.test_losses = []\n",
        "        self.val_losses = []\n",
        "        self.past_params = []\n",
        "\n",
        "        pbar = trange(max_iters)\n",
        "        pbar.set_description(\"[jit compling...]\")\n",
        "        \n",
        "        mask_start = self.train_params.get(\"mask_start\")\n",
        "        if (mask_start):\n",
        "            mask_size = self.train_params[\"mask_size\"]\n",
        "            self.train_params[\"mask_size\"] = 0\n",
        "\n",
        "        train_step = jit(self.train_step)\n",
        "        val_step = jit(self.val_step)\n",
        "\n",
        "        best_loss = None\n",
        "        best_itr = 0\n",
        "        val_loss = None\n",
        "\n",
        "        for itr in pbar:\n",
        "            train_key, val_key, key = jr.split(key, 3)\n",
        "\n",
        "            batch_id = itr % num_batches\n",
        "            batch_start = batch_id * batch_size\n",
        "\n",
        "            t = time()\n",
        "            # Training step\n",
        "            # ----------------------------------------\n",
        "            step_results = train_step(train_key, self.params, \n",
        "                           train_data[batch_start:batch_start+batch_size], self.opt_states)\n",
        "            self.params, self.opt_states, loss_out, grads = \\\n",
        "                jax.tree_map(lambda x: x.block_until_ready(), step_results)\n",
        "            # ----------------------------------------\n",
        "            dt = time() - t\n",
        "            self.time_spent.append(dt)\n",
        "\n",
        "            loss, aux = loss_out\n",
        "            self.train_losses.append(loss)\n",
        "            pbar.set_description(\"LP: {:.3f}\".format(loss))\n",
        "\n",
        "            if batch_id == num_batches - 1:\n",
        "                # We're at the end of an epoch\n",
        "                # We could randomly shuffle the data\n",
        "                # train_data = jr.permutation(key, train_data)\n",
        "                if (self.train_params.get(\"use_validation\")):\n",
        "                    val_loss_out = val_step(val_key, self.params, data_dict[\"val_data\"])\n",
        "                    if (val_callback): val_callback(self, val_loss_out, data_dict)\n",
        "                    val_loss, _ = val_loss_out\n",
        "                    \n",
        "            if not self.train_params.get(\"use_validation\") or val_loss is None:\n",
        "                curr_loss = loss\n",
        "            else:\n",
        "                curr_loss = val_loss\n",
        "\n",
        "            if itr >= early_stop_start:\n",
        "                if best_loss is None or curr_loss < best_loss:\n",
        "                    best_itr = itr\n",
        "                    best_loss = curr_loss\n",
        "                if curr_loss > best_loss and itr - best_itr > max_lose_streak:\n",
        "                    print(\"Early stopping!\")\n",
        "                    break\n",
        "\n",
        "            if (callback): callback(self, loss_out, data_dict, grads)\n",
        "\n",
        "            # Record parameters\n",
        "            record_params = self.train_params.get(\"record_params\")\n",
        "            if record_params and record_params(itr):\n",
        "                curr_params = deepcopy(self.params)\n",
        "                curr_params[\"iteration\"] = itr\n",
        "                self.past_params.append(curr_params)\n",
        "\n",
        "            if (mask_start and itr == mask_start):\n",
        "                self.train_params[\"mask_size\"] = mask_size\n",
        "                train_step = jit(self.train_step)\n",
        "                val_step = jit(self.val_step)\n",
        "\n",
        "        if summary:\n",
        "            summary(self, data_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "KmTBoMwgArBw"
      },
      "outputs": [],
      "source": [
        "# @title Logging to WandB\n",
        "\n",
        "def visualize_lds(trainer, data_dict, aux):\n",
        "    data = data_dict[\"train_data\"]\n",
        "    true_model_params = data_dict[\"lds_params\"]\n",
        "    model = trainer.model\n",
        "    params = [trainer.params]\n",
        "    num_pts = 10\n",
        "    # We want to visualize the posterior marginals\n",
        "    inf_mus, inf_Sigmas, tgt_mus, tgt_Sigmas, true_mus, true_Sigmas = \\\n",
        "        get_marginals_and_targets(key_0, data, num_pts, model, params, true_model_params)\n",
        "    # Create the axis\n",
        "    fig, ax = plt.subplots()\n",
        "    # Plot each of the groups\n",
        "    tgt_points, tgt_lines = get_artists(ax, tgt_mus[0], tgt_Sigmas[0], key_0, num_pts,\n",
        "                                        alpha=0.3, linestyle=\"dotted\", label=\"current target\")\n",
        "    inf_points, inf_lines = get_artists(ax, inf_mus[0], inf_Sigmas[0], key_0, num_pts, label=\"inferred\")\n",
        "    true_points, true_lines = get_artists(ax, true_mus, true_Sigmas, \n",
        "                                          key_0, num_pts, alpha=0.2, label=\"true target\")\n",
        "    # The legend is too large and blocks most of the plot\n",
        "    # plt.legend()\n",
        "    # Relimit the axes\n",
        "    ax.relim()\n",
        "    # update ax.viewLim using the new dataLim\n",
        "    ax.autoscale_view()\n",
        "    fig.canvas.draw()\n",
        "    img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
        "    post_img = wandb.Image(img, caption=\"Posterior marginals versus targets\")\n",
        "    plt.close()\n",
        "    return {\n",
        "        \"Posterior marginals\": post_img\n",
        "    }\n",
        "\n",
        "def visualize_pendulum(trainer, aux):\n",
        "    # This assumes single sequence has shape (100, 24, 24, 1)\n",
        "    recon = aux[\"reconstruction\"][0][0]\n",
        "    # Show the sequence as a block of images\n",
        "    stacked = recon.reshape(10, 24 * 10, 24)\n",
        "    imgrid = stacked.swapaxes(0, 1).reshape(24 * 10, 24 * 10)\n",
        "    recon_img = wandb.Image(onp.array(imgrid), caption=\"Sample Reconstruction\")\n",
        "\n",
        "    fig = plt.figure()\n",
        "    mask = aux[\"mask\"][0]\n",
        "    post_sample = aux[\"posterior_samples\"][0][0]\n",
        "    top, bot = np.max(post_sample) + 5, np.min(post_sample) - 5\n",
        "    left, right = 0, post_sample.shape[0]\n",
        "    plt.imshow(mask[None], cmap=\"gray\", alpha=.4, vmin=0, vmax=1,\n",
        "               extent=(left, right, top, bot))\n",
        "    plt.plot(post_sample)\n",
        "    fig.canvas.draw()\n",
        "    img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
        "    post_img = wandb.Image(img, caption=\"Posterior Sample\")\n",
        "    plt.close()\n",
        "    return {\n",
        "        \"Reconstruction\": recon_img, \n",
        "        \"Posterior Sample\": post_img\n",
        "    }\n",
        "\n",
        "def get_group_name(run_params):\n",
        "    p = run_params\n",
        "    run_type = \"\" if p[\"inference_method\"] in [\"EM\", \"GT\", \"SMC\"] else \"_\" + p[\"run_type\"]\n",
        "    if p[\"dataset\"] == \"pendulum\":\n",
        "        dataset_summary = \"pendulum\"\n",
        "    elif p[\"dataset\"] == \"lds\":\n",
        "        d = p[\"dataset_params\"]\n",
        "        dataset_summary = \"lds_dims_{}_{}_noises_{}_{}\".format(\n",
        "            d[\"latent_dims\"], d[\"emission_dims\"], \n",
        "            d[\"dynamics_cov\"], d[\"emission_cov\"])\n",
        "    else:\n",
        "        dataset_summary = \"???\"\n",
        "\n",
        "    model_summary = \"_{}d_latent_\".format(p[\"latent_dims\"]) + p[\"inference_method\"]\n",
        "\n",
        "    group_tag = p.get(\"group_tag\") or \"\"\n",
        "    if group_tag != \"\": group_tag += \"_\"\n",
        "\n",
        "    group_name = (group_tag +\n",
        "        dataset_summary\n",
        "        + model_summary\n",
        "        + run_type\n",
        "    )\n",
        "    return group_name\n",
        "\n",
        "def validation_log_to_wandb(trainer, loss_out, data_dict):\n",
        "    p = trainer.train_params\n",
        "    if not p.get(\"log_to_wandb\"): return\n",
        "    \n",
        "    project_name = p[\"project_name\"]\n",
        "    group_name = get_group_name(p)\n",
        "\n",
        "    obj, aux = loss_out\n",
        "    elbo = -obj\n",
        "    kl = np.mean(aux[\"kl\"])\n",
        "    ell = np.mean(aux[\"ell\"])\n",
        "\n",
        "    model = trainer.model\n",
        "    prior = model.prior\n",
        "    D = prior.latent_dims\n",
        "    prior_params = trainer.params[\"prior_params\"]\n",
        "\n",
        "    visualizations = {}\n",
        "    if p[\"dataset\"] == \"pendulum\":\n",
        "        visualizations = visualize_pendulum(trainer, aux)\n",
        "        pred_ll = np.mean(aux[\"prediction_ll\"])\n",
        "        visualizations = {\n",
        "            \"Validation reconstruction\": visualizations[\"Reconstruction\"], \n",
        "            \"Validation posterior sample\": visualizations[\"Posterior Sample\"],\n",
        "            \"Validation prediction log likelihood\": pred_ll\n",
        "        }\n",
        "        \n",
        "    to_log = {\"Validation ELBO\": elbo, \"Validation KL\": kl, \"Validation likelihood\": ell,}\n",
        "    to_log.update(visualizations)\n",
        "    wandb.log(to_log)\n",
        "\n",
        "def log_to_wandb(trainer, loss_out, data_dict, grads):\n",
        "    p = trainer.train_params\n",
        "    if not p.get(\"log_to_wandb\"): return\n",
        "    \n",
        "    project_name = p[\"project_name\"]\n",
        "    group_name = get_group_name(p)\n",
        "\n",
        "    itr = len(trainer.train_losses) - 1\n",
        "    if len(trainer.train_losses) == 1:\n",
        "        wandb.init(project=project_name, group=group_name, config=p)\n",
        "        pprint(p)\n",
        "\n",
        "    obj, aux = loss_out\n",
        "    elbo = -obj\n",
        "    kl = np.mean(aux[\"kl\"])\n",
        "    ell = np.mean(aux[\"ell\"])\n",
        "\n",
        "    model = trainer.model\n",
        "    prior = model.prior\n",
        "    D = prior.latent_dims\n",
        "    prior_params = trainer.params[\"prior_params\"]\n",
        "    if (p.get(\"use_natural_grad\")):\n",
        "        Q = prior_params[\"Q\"]\n",
        "        A = prior_params[\"A\"]\n",
        "    else:\n",
        "        Q = lie_params_to_constrained(prior_params[\"Q\"], D)\n",
        "        A = prior_params[\"A\"]\n",
        "\n",
        "    eigs = eigh(Q)[0]\n",
        "    Q_cond_num = np.max(eigs) / np.min(eigs)\n",
        "    svs = svd(A)[1]\n",
        "    max_sv, min_sv = np.max(svs), np.min(svs)\n",
        "    A_cond_num = max_sv / min_sv\n",
        "\n",
        "    # Also log the prior params gradients\n",
        "    # prior_grads = grads[\"prior_params\"][\"sgd_params\"]\n",
        "    # prior_grads_norm = np.linalg.norm(\n",
        "    #     jax.tree_util.tree_leaves(tree_map(np.linalg.norm, prior_grads)))\n",
        "\n",
        "    visualizations = {}\n",
        "    if (itr % p[\"plot_interval\"] == 0):\n",
        "        if p[\"dataset\"] == \"lds\" and p.get(\"visualize_training\"):\n",
        "            visualizations = visualize_lds(trainer, data_dict, aux)\n",
        "        elif p[\"dataset\"] == \"pendulum\" and p.get(\"visualize_training\"):\n",
        "            visualizations = visualize_pendulum(trainer, aux)\n",
        "\n",
        "        # fig = plt.figure()\n",
        "        # prior_sample = prior.sample(prior_params, shape=(1,), key=jr.PRNGKey(0))[0]\n",
        "        # plt.plot(prior_sample)\n",
        "        # fig.canvas.draw()\n",
        "        # img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
        "        # prior_img = wandb.Image(img, caption=\"Prior Sample\")\n",
        "        # plt.close()\n",
        "        # visualizations[\"Prior sample\"] = prior_img\n",
        "    # Also log the learning rates\n",
        "    lr = p[\"learning_rate\"] \n",
        "    lr = lr if isinstance(lr, float) else lr(itr)\n",
        "    prior_lr = p[\"prior_learning_rate\"] \n",
        "    prior_lr = prior_lr if isinstance(prior_lr, float) else prior_lr(itr)\n",
        "\n",
        "    to_log = { \"ELBO\": elbo, \"KL\": kl, \"Likelihood\": ell, # \"Prior graident norm\": prior_grads_norm,\n",
        "               \"Max singular value of A\": max_sv, \"Min singular value of A\": min_sv,\n",
        "               \"Condition number of A\": A_cond_num, \"Condition number of Q\": Q_cond_num,\n",
        "               \"Learning rate\": lr, \"Prior learning rate\": prior_lr }\n",
        "    to_log.update(visualizations)\n",
        "    wandb.log(to_log)\n",
        "\n",
        "def save_params_to_wandb(trainer, data_dict):\n",
        "    file_name = \"parameters.pkl\"\n",
        "    with open(file_name, \"wb\") as f:\n",
        "        pkl.dump(trainer.past_params, f)\n",
        "        wandb.save(file_name, policy=\"now\")\n",
        "\n",
        "def on_error(data_dict, model_dict):\n",
        "    save_params_to_wandb(model_dict[\"trainer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "cellView": "form",
        "id": "ve7zlI0-P8tP"
      },
      "outputs": [],
      "source": [
        "# @title Specifics of SVAE training\n",
        "def svae_init(key, model, data, initial_params=None, **train_params):\n",
        "    init_params = model.init(key)\n",
        "    if (initial_params): init_params.update(initial_params)\n",
        "    \n",
        "    if (train_params[\"inference_method\"] == \"planet\"):\n",
        "        init_params[\"rec_params\"] = {\n",
        "            \"rec_params\": init_params[\"rec_params\"],\n",
        "            \"post_params\": init_params[\"post_params\"]\n",
        "        }\n",
        "    # Expand the posterior parameters by batch size\n",
        "    init_params[\"post_params\"] = vmap(lambda _: init_params[\"post_params\"])(data)\n",
        "    init_params[\"post_samples\"] = np.zeros((data.shape[0], \n",
        "                                            train_params.get(\"obj_samples\") or 1) \n",
        "                                             + model.posterior.shape)\n",
        "\n",
        "    learning_rate = train_params[\"learning_rate\"]\n",
        "    rec_opt = opt.adam(learning_rate=learning_rate)\n",
        "    rec_opt_state = rec_opt.init(init_params[\"rec_params\"])\n",
        "    dec_opt = opt.adam(learning_rate=learning_rate)\n",
        "    dec_opt_state = dec_opt.init(init_params[\"dec_params\"])\n",
        "\n",
        "    if (train_params.get(\"use_natural_grad\")):\n",
        "        prior_lr = None\n",
        "        prior_opt = None\n",
        "        prior_opt_state = None\n",
        "    else:\n",
        "        # Add the option of using an gradient optimizer for prior parameters\n",
        "        prior_lr = train_params.get(\"prior_learning_rate\") or learning_rate\n",
        "        prior_opt = opt.adam(learning_rate=prior_lr)\n",
        "        prior_opt_state = prior_opt.init(init_params[\"prior_params\"])\n",
        "\n",
        "    return (init_params, \n",
        "            (rec_opt, dec_opt, prior_opt), \n",
        "            (rec_opt_state, dec_opt_state, prior_opt_state))\n",
        "    \n",
        "def svae_loss(key, model, data_batch, model_params, **train_params):\n",
        "    batch_size = data_batch.shape[0]\n",
        "    # Axes specification for vmap\n",
        "    # We're just going to ignore this for now\n",
        "    params_in_axes = None\n",
        "    # params_in_axes = dict.fromkeys(model_params.keys(), None)\n",
        "    # params_in_axes[\"post_samples\"] = 0\n",
        "    result = vmap(partial(model.compute_objective, **train_params), \n",
        "                  in_axes=(0, 0, params_in_axes))(jr.split(key, batch_size), data_batch, model_params)\n",
        "    objs = result[\"objective\"]\n",
        "    post_params = result[\"posterior_params\"]\n",
        "    post_samples = result[\"posterior_samples\"]\n",
        "    # Need to compute sufficient stats if we want the natural gradient update\n",
        "    if (train_params.get(\"use_natural_grad\")):\n",
        "        post_suff_stats = vmap(model.posterior.sufficient_statistics)(post_params)\n",
        "        expected_post_suff_stats = tree_map(\n",
        "            lambda l: np.mean(l,axis=0), post_suff_stats)\n",
        "        result[\"sufficient_statistics\"] = expected_post_suff_stats\n",
        "    return -np.mean(objs), result\n",
        "\n",
        "def predict_forward(x, A, b, T):\n",
        "    def _step(carry, t):\n",
        "        carry = A @ carry + b\n",
        "        return carry, carry\n",
        "    return scan(_step, x, np.arange(T))[1]\n",
        "\n",
        "# Note: this is for pendulum data only\n",
        "def svae_val_loss(key, model, data_batch, model_params, **train_params):  \n",
        "    N, T = data_batch.shape[:2]\n",
        "    # We only care about the first 100 timesteps\n",
        "    T = T // 2\n",
        "    D = model.prior.latent_dims\n",
        "\n",
        "    # obs_data, pred_data = data_batch[:,:T//2], data_batch[:,T//2:]\n",
        "    obs_data = data_batch[:,:T]\n",
        "    obj, out_dict = svae_loss(key, model, obs_data, model_params, **train_params)\n",
        "    # Compute the prediction accuracy\n",
        "    prior_params = model_params[\"prior_params\"] \n",
        "    # Instead of this, we want to evaluate the expected log likelihood of the future observations\n",
        "    # under the posterior given the current set of observations\n",
        "    # So E_{q(x'|y)}[p(y'|x')] where the primes represent the future\n",
        "    post_params = out_dict[\"posterior_params\"]\n",
        "\n",
        "    posterior = model.posterior.distribution(post_params)\n",
        "    # J = posterior.filtered_precisions\n",
        "    # h = posterior.filtered_linear_potentials\n",
        "    Sigma_filtered = posterior.filtered_covariances # inv(J)\n",
        "    mu_filtered = posterior.filtered_means # np.einsum(\"...ij,...j->...i\", Sigma_filtered, h)\n",
        "    horizon = train_params[\"prediction_horizon\"] or 5\n",
        "\n",
        "    def _prediction_lls(data_id, key):\n",
        "        num_windows = T-horizon-1\n",
        "        pred_lls = vmap(_sliding_window_prediction_lls, in_axes=(None, None, None, 0, 0))(\n",
        "            mu_filtered[data_id], Sigma_filtered[data_id], obs_data[data_id],\n",
        "            np.arange(num_windows), jr.split(key, num_windows))\n",
        "        return pred_lls.mean()\n",
        "\n",
        "    # TODO: change this...!\n",
        "    def _sliding_window_prediction_lls(mu, Sigma, data, t, key):\n",
        "        # Build the posterior object on the future latent states \n",
        "        # (\"the posterior predictive distribution\")\n",
        "        # Convert unconstrained params to constrained dynamics parameters\n",
        "        prior_params_constrained = model.prior.get_dynamics_params(prior_params)\n",
        "        dynamics_params = {\n",
        "            \"m1\": mu[t],\n",
        "            \"Q1\": Sigma[t],\n",
        "            \"A\": prior_params_constrained[\"A\"],\n",
        "            \"b\": prior_params_constrained[\"b\"],\n",
        "            \"Q\": prior_params_constrained[\"Q\"]\n",
        "        }\n",
        "        tridiag_params = dynamics_to_tridiag(dynamics_params, horizon+1, D) # Note the +1\n",
        "        J, L, h = tridiag_params[\"J\"], tridiag_params[\"L\"], tridiag_params[\"h\"]\n",
        "        pred_posterior = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "        # Sample from it and evaluate the log likelihood\n",
        "        x_pred = pred_posterior.sample(seed=key)[1:]\n",
        "        likelihood_dist = model.decoder.apply(model_params[\"dec_params\"], x_pred)\n",
        "        return likelihood_dist.log_prob(\n",
        "            lax.dynamic_slice(data, (t+1, 0, 0, 0), (horizon,) + data.shape[1:])).sum(axis=(1, 2, 3))\n",
        "\n",
        "    # pred_lls = vmap(_prediction_lls)(\n",
        "    #     out_dict[\"posterior_params\"], data_batch, jr.split(key, N))\n",
        "    pred_lls = vmap(_prediction_lls)(np.arange(N), jr.split(key, N))\n",
        "    out_dict[\"prediction_ll\"] = pred_lls\n",
        "    return obj, out_dict\n",
        "\n",
        "def svae_update(params, grads, opts, opt_states, model, aux, **train_params):\n",
        "    rec_opt, dec_opt, prior_opt = opts\n",
        "    rec_opt_state, dec_opt_state, prior_opt_state = opt_states\n",
        "    rec_grad, dec_grad = grads[\"rec_params\"], grads[\"dec_params\"]\n",
        "    updates, rec_opt_state = rec_opt.update(rec_grad, rec_opt_state)\n",
        "    params[\"rec_params\"] = opt.apply_updates(params[\"rec_params\"], updates)\n",
        "    params[\"post_params\"] = aux[\"posterior_params\"]\n",
        "    params[\"post_samples\"] = aux[\"posterior_samples\"]\n",
        "    if train_params[\"run_type\"] == \"model_learning\":\n",
        "        # Update decoder\n",
        "        updates, dec_opt_state = dec_opt.update(dec_grad, dec_opt_state)\n",
        "        params[\"dec_params\"] = opt.apply_updates(params[\"dec_params\"], updates)\n",
        "\n",
        "        old_Q = deepcopy(params[\"prior_params\"][\"Q\"])\n",
        "        old_b = deepcopy(params[\"prior_params\"][\"b\"])\n",
        "\n",
        "        # Update prior parameters\n",
        "        if (train_params.get(\"use_natural_grad\")):\n",
        "            # Here we interpolate the sufficient statistics instead of the parameters\n",
        "            suff_stats = aux[\"sufficient_statistics\"]\n",
        "            lr = params.get(\"prior_learning_rate\") or 1\n",
        "            avg_suff_stats = params[\"prior_params\"][\"avg_suff_stats\"]\n",
        "            # Interpolate the sufficient statistics\n",
        "            params[\"prior_params\"][\"avg_suff_stats\"] = tree_map(lambda x,y : (1 - lr) * x + lr * y, \n",
        "                avg_suff_stats, suff_stats)\n",
        "            params[\"prior_params\"] = model.prior.m_step(params[\"prior_params\"])\n",
        "        else:\n",
        "            updates, prior_opt_state = prior_opt.update(grads[\"prior_params\"], prior_opt_state)\n",
        "            params[\"prior_params\"] = opt.apply_updates(params[\"prior_params\"], updates)\n",
        "        \n",
        "        if (train_params.get(\"constrain_prior\")):\n",
        "            # Revert Q and b to their previous values\n",
        "            params[\"prior_params\"][\"Q\"] = old_Q\n",
        "            params[\"prior_params\"][\"b\"] = old_b\n",
        "\n",
        "        if (train_params.get(\"constrain_dynamics\")):\n",
        "            # Scale A so that its maximum singular value does not exceed 1\n",
        "            params[\"prior_params\"][\"A\"] = truncate_singular_values(params[\"prior_params\"][\"A\"])\n",
        "            # params[\"prior_params\"][\"A\"] = scale_singular_values(params[\"prior_params\"][\"A\"])\n",
        "\n",
        "    return params, (rec_opt_state, dec_opt_state, prior_opt_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "8AdW1WXA4n1U"
      },
      "outputs": [],
      "source": [
        "# @title Model initialization and trainer\n",
        "def init_model(run_params, data_dict):\n",
        "    p = deepcopy(run_params)\n",
        "    d = p[\"dataset_params\"]\n",
        "    latent_dims = p[\"latent_dims\"]\n",
        "    input_shape = data_dict[\"train_data\"].shape[1:]\n",
        "    num_timesteps = input_shape[0]\n",
        "    data = data_dict[\"train_data\"]\n",
        "    seed = p[\"seed\"]\n",
        "    seed_model, seed_elbo, seed_ems, seed_rec = jr.split(seed, 4)\n",
        "\n",
        "    run_type = p[\"run_type\"]\n",
        "    recnet_class = globals()[p[\"recnet_class\"]]\n",
        "    decnet_class = globals()[p[\"decnet_class\"]]\n",
        "\n",
        "    if p[\"inference_method\"] == \"dkf\":\n",
        "        posterior = DKFPosterior(latent_dims, num_timesteps)\n",
        "    elif p[\"inference_method\"] == \"cdkf\":\n",
        "        posterior = CDKFPosterior(latent_dims, num_timesteps)\n",
        "    elif p[\"inference_method\"] == \"planet\":\n",
        "        posterior = PlaNetPosterior(p[\"posterior_architecture\"],\n",
        "                                    latent_dims, num_timesteps)\n",
        "    elif p[\"inference_method\"] == \"svae\":\n",
        "        # The parallel Kalman stuff only applies to SVAE\n",
        "        # Since RNN based methods are inherently sequential\n",
        "        posterior = LDSSVAEPosterior(latent_dims, num_timesteps, \n",
        "                                     use_parallel=p.get(\"use_parallel_kf\"))\n",
        "        \n",
        "    rec_net = recnet_class.from_params(**p[\"recnet_architecture\"])\n",
        "    dec_net = decnet_class.from_params(**p[\"decnet_architecture\"])\n",
        "    if p[\"inference_method\"] == \"planet\":\n",
        "        # Wrap the recognition network\n",
        "        rec_net = PlaNetRecognitionWrapper(rec_net)\n",
        "\n",
        "    if (p.get(\"use_natural_grad\")):\n",
        "        prior = LinearGaussianChainPrior(latent_dims, num_timesteps)\n",
        "    else:\n",
        "        prior = LieParameterizedLinearGaussianChainPrior(latent_dims, num_timesteps)\n",
        "\n",
        "    model = DeepLDS(\n",
        "        recognition=rec_net,\n",
        "        decoder=dec_net,\n",
        "        prior=prior,\n",
        "        posterior=posterior,\n",
        "        input_dummy=np.zeros(input_shape),\n",
        "        latent_dummy=np.zeros((num_timesteps, latent_dims))\n",
        "    )\n",
        "    \n",
        "    # TODO: Let's get the full linear version working first before moving on\n",
        "    # assert(run_params[\"run_type\"] == \"full_linear\")\n",
        "    if (run_type == \"inference_only\"):\n",
        "        p = data_dict[\"lds_params\"]\n",
        "        prior_params = { \"A\": p[\"A\"], \"b\": p[\"b\"], \n",
        "                        \"Q\": p[\"Q\"], \"m1\": p[\"m1\"], \"Q1\": p[\"Q1\"],\n",
        "                        \"avg_suff_stats\": p[\"avg_suff_stats\"]}\n",
        "        dec_params = fd.FrozenDict(\n",
        "            {\n",
        "                \"params\": {\n",
        "                    \"head_log_var_fn\":{\n",
        "                        \"Dense_0\":{\n",
        "                            \"bias\": inv_softplus(np.diag(p[\"R\"])),\n",
        "                            \"kernel\": np.zeros_like(p[\"C\"]).T\n",
        "                        }\n",
        "                    },\n",
        "                    \"head_mean_fn\":{\n",
        "                        \"Dense_0\":{\n",
        "                            \"bias\": p[\"d\"],\n",
        "                            \"kernel\": p[\"C\"].T\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            })\n",
        "        initial_params = { \"prior_params\": prior_params, \"dec_params\": dec_params }\n",
        "    else:\n",
        "        initial_params = None\n",
        "\n",
        "    # emission_params = emission.init(seed_ems, np.ones((num_latent_dims,)))\n",
        "    # Define the trainer object here\n",
        "    trainer = Trainer(model, train_params=run_params, init=svae_init, \n",
        "                      loss=svae_loss, \n",
        "                      val_loss=svae_val_loss, \n",
        "                      update=svae_update, initial_params=initial_params)\n",
        "\n",
        "    return {\n",
        "        # We don't actually need to include model here\n",
        "        # 'cause it's included in the trainer object\n",
        "        \"model\": model,\n",
        "        # \"emission_params\": emission_params\n",
        "        \"trainer\": trainer\n",
        "    }\n",
        "\n",
        "def start_trainer(model_dict, data_dict, run_params):\n",
        "    trainer = model_dict[\"trainer\"]\n",
        "    if run_params.get(\"log_to_wandb\"):\n",
        "        if run_params[\"dataset\"] == \"pendulum\":\n",
        "            summary = summarize_pendulum_run\n",
        "        else:\n",
        "            summary = save_params_to_wandb\n",
        "    else:\n",
        "        summary = None\n",
        "    trainer.train(data_dict,\n",
        "                  max_iters=run_params[\"max_iters\"],\n",
        "                  key=run_params[\"seed\"],\n",
        "                  callback=log_to_wandb, val_callback=validation_log_to_wandb,\n",
        "                  summary=summary)\n",
        "    return (trainer.model, trainer.params, trainer.train_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NSj6XUpPZRG"
      },
      "source": [
        "## Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "cellView": "form",
        "id": "ZeG1j7HgqWya"
      },
      "outputs": [],
      "source": [
        "# @title Sample from LDS\n",
        "def sample_lds_dataset(run_params):    \n",
        "    d = run_params[\"dataset_params\"]\n",
        "    \n",
        "    global data_dict\n",
        "    if data_dict is not None \\\n",
        "        and \"dataset_params\" in data_dict \\\n",
        "        and str(data_dict[\"dataset_params\"]) == str(fd.freeze(d)):\n",
        "        print(\"Using existing data.\")\n",
        "        print(\"Data MLL: \", data_dict[\"marginal_log_likelihood\"])\n",
        "        \n",
        "        # return jax.device_put(data_dict, jax.devices(\"gpu\")[0])\n",
        "        return data_dict\n",
        "\n",
        "    data_dict = {}\n",
        "\n",
        "    seed = d[\"seed\"]\n",
        "    emission_dims = d[\"emission_dims\"]\n",
        "    latent_dims = d[\"latent_dims\"]\n",
        "    emission_cov = d[\"emission_cov\"]\n",
        "    dynamics_cov = d[\"dynamics_cov\"]\n",
        "    num_timesteps = d[\"num_timesteps\"]\n",
        "    num_trials = d[\"num_trials\"]\n",
        "    seed_m1, seed_C, seed_d, seed_A, seed_sample = jr.split(seed, 5)\n",
        "\n",
        "    R = emission_cov * np.eye(emission_dims)\n",
        "    Q = dynamics_cov * np.eye(latent_dims)\n",
        "    C = jr.normal(seed_C, shape=(emission_dims, latent_dims))\n",
        "    d = jr.normal(seed_d, shape=(emission_dims,))\n",
        "\n",
        "    # Here we let Q1 = Q\n",
        "    lds = LDS(latent_dims, num_timesteps)\n",
        "    \n",
        "    params = {\n",
        "            \"m1\": jr.normal(key=seed_m1, shape=(latent_dims,)),\n",
        "            \"Q1\": Q,\n",
        "            \"Q\": Q,\n",
        "            \"A\": random_rotation(seed_A, latent_dims, theta=np.pi/20),\n",
        "            \"b\": np.zeros(latent_dims),\n",
        "            \"R\": R,\n",
        "            \"C\": C,\n",
        "            \"d\": d,\n",
        "        }\n",
        "    constrained = lds.get_constrained_params(params)\n",
        "    params[\"avg_suff_stats\"] = { \"Ex\": constrained[\"Ex\"], \n",
        "                                \"ExxT\": constrained[\"ExxT\"], \n",
        "                                \"ExnxT\": constrained[\"ExnxT\"] }\n",
        "\n",
        "    states, data = lds.sample(params, \n",
        "                              shape=(num_trials,), \n",
        "                              key=seed_sample)\n",
        "    \n",
        "    mll = vmap(lds.marginal_log_likelihood, in_axes=(None, 0))(params, data)\n",
        "    mll = np.mean(mll, axis=0)\n",
        "    print(\"Data MLL: \", mll)\n",
        "    \n",
        "    seed_val, _ = jr.split(seed_sample)\n",
        "    val_states, val_data = lds.sample(params, \n",
        "                              shape=(num_trials,), \n",
        "                              key=seed_val)\n",
        "\n",
        "    # data_dict[\"generative_model\"] = lds\n",
        "    data_dict[\"marginal_log_likelihood\"] = mll\n",
        "    data_dict[\"train_data\"] = data\n",
        "    data_dict[\"train_states\"] = states\n",
        "    data_dict[\"val_data\"] = val_data\n",
        "    data_dict[\"val_states\"] = val_states\n",
        "    data_dict[\"dataset_params\"] = fd.freeze(run_params[\"dataset_params\"])\n",
        "    data_dict[\"lds_params\"] = params\n",
        "    return data_dict\n",
        "    # return jax.device_put(data_dict, jax.devices(\"gpu\")[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "cellView": "form",
        "id": "MKj_UmfS6JdM"
      },
      "outputs": [],
      "source": [
        "# @title Testing the correctness of LDS m-step\n",
        "# seed = jr.PRNGKey(0)\n",
        "# emission_dims = 5\n",
        "# latent_dims = 3\n",
        "# emission_cov = 10.\n",
        "# dynamics_cov = .1\n",
        "# num_timesteps = 100\n",
        "# seed, seed_C, seed_d, seed_sample = jr.split(seed, 4)\n",
        "# R = emission_cov * np.eye(emission_dims)\n",
        "# Q = dynamics_cov * np.eye(latent_dims)\n",
        "# C = jr.normal(seed_C, shape=(emission_dims, latent_dims))\n",
        "# d = jr.normal(seed_d, shape=(emission_dims,))\n",
        "# # Here we let Q1 = Q\n",
        "# lds = LDS(latent_dims, num_timesteps)\n",
        "# params = lds.init(seed)\n",
        "# params.update(\n",
        "#     {\n",
        "#         \"Q1\": Q,\n",
        "#         \"Q\": Q,\n",
        "#         \"R\": R,\n",
        "#         \"C\": C,\n",
        "#         \"d\": d,\n",
        "#     }\n",
        "# )\n",
        "# states, data = lds.sample(params, \n",
        "#                           shape=(50,), \n",
        "#                           key=seed_sample)\n",
        "# post_params = vmap(lds.e_step, in_axes=(None, 0))(params, data)\n",
        "# posterior = LinearGaussianChainPosterior(latent_dims, num_timesteps)\n",
        "# suff_stats = vmap(posterior.sufficient_statistics)(post_params)\n",
        "# expected_suff_stats = jax.tree_util.tree_map(\n",
        "#         lambda l: np.mean(l,axis=0), suff_stats)\n",
        "# inferred_params = lds.m_step(params, expected_suff_stats)\n",
        "# for key in [\"m1\", \"Q1\", \"A\", \"Q\", \"b\"]:\n",
        "#     print(inferred_params[key] - params[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "cellView": "form",
        "id": "Qzp5Fb0CGtqk"
      },
      "outputs": [],
      "source": [
        "# @title Code for the pendulum dataset (~128 mb)\n",
        "\n",
        "\n",
        "# Modeling Irregular Time Series with Continuous Recurrent Units (CRUs)\n",
        "# Copyright (c) 2022 Robert Bosch GmbH\n",
        "# This program is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Affero General Public License as published\n",
        "# by the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "#\n",
        "# This program is distributed in the hope that it will be useful,\n",
        "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "# GNU Affero General Public License for more details.\n",
        "#\n",
        "# You should have received a copy of the GNU Affero General Public License\n",
        "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
        "#\n",
        "# This source code is derived from Pytorch RKN Implementation (https://github.com/ALRhub/rkn_share)\n",
        "# Copyright (c) 2021 Philipp Becker (Autonomous Learning Robots Lab @ KIT)\n",
        "# licensed under MIT License\n",
        "# cf. 3rd-party-licenses.txt file in the root directory of this source tree.\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "def add_img_noise(imgs, first_n_clean, random, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75, t_uu=1.0):\n",
        "    \"\"\"\n",
        "    :param imgs: Images to add noise to\n",
        "    :param first_n_clean: Keep first_n_images clean to allow the filter to burn in\n",
        "    :param random: np.random.RandomState used for sampling\n",
        "    :param r: \"correlation (over time) factor\" the smaller the more the noise is correlated\n",
        "    :param t_ll: lower bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_lu: upper bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_ul: lower bound of the interval the upper bound for each sequence is sampled from\n",
        "    :param t_uu: upper bound of the interval the upper bound for each sequence is sampled from\n",
        "    :return: noisy images, factors used to create them\n",
        "    \"\"\"\n",
        "\n",
        "    assert t_ll <= t_lu <= t_ul <= t_uu, \"Invalid bounds for noise generation\"\n",
        "    if len(imgs.shape) < 5:\n",
        "        imgs = onp.expand_dims(imgs, -1)\n",
        "    batch_size, seq_len = imgs.shape[:2]\n",
        "    factors = onp.zeros([batch_size, seq_len])\n",
        "    factors[:, 0] = random.uniform(low=0.0, high=1.0, size=batch_size)\n",
        "    for i in range(seq_len - 1):\n",
        "        factors[:, i + 1] = onp.clip(factors[:, i] + random.uniform(\n",
        "            low=-r, high=r, size=batch_size), a_min=0.0, a_max=1.0)\n",
        "\n",
        "    t1 = random.uniform(low=t_ll, high=t_lu, size=(batch_size, 1))\n",
        "    t2 = random.uniform(low=t_ul, high=t_uu, size=(batch_size, 1))\n",
        "\n",
        "    factors = (factors - t1) / (t2 - t1)\n",
        "    factors = onp.clip(factors, a_min=0.0, a_max=1.0)\n",
        "    factors = onp.reshape(factors, list(factors.shape) + [1, 1, 1])\n",
        "    factors[:, :first_n_clean] = 1.0\n",
        "    noisy_imgs = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        if imgs.dtype == onp.uint8:\n",
        "            noise = random.uniform(low=0.0, high=255, size=imgs.shape[1:])\n",
        "            noisy_imgs.append(\n",
        "                (factors[i] * imgs[i] + (1 - factors[i]) * noise).astype(onp.uint8))\n",
        "        else:\n",
        "            noise = random.uniform(low=0.0, high=1.1, size=imgs.shape[1:])\n",
        "            noisy_imgs.append(factors[i] * imgs[i] + (1 - factors[i]) * noise)\n",
        "\n",
        "    return onp.squeeze(onp.concatenate([onp.expand_dims(n, 0) for n in noisy_imgs], 0)), factors\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "def add_img_noise4(imgs, first_n_clean, random, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75, t_uu=1.0):\n",
        "    \"\"\"\n",
        "    :param imgs: Images to add noise to\n",
        "    :param first_n_clean: Keep first_n_images clean to allow the filter to burn in\n",
        "    :param random: np.random.RandomState used for sampling\n",
        "    :param r: \"correlation (over time) factor\" the smaller the more the noise is correlated\n",
        "    :param t_ll: lower bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_lu: upper bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_ul: lower bound of the interval the upper bound for each sequence is sampled from\n",
        "    :param t_uu: upper bound of the interval the upper bound for each sequence is sampled from\n",
        "    :return: noisy images, factors used to create them\n",
        "    \"\"\"\n",
        "\n",
        "    half_x = int(imgs.shape[2] / 2)\n",
        "    half_y = int(imgs.shape[3] / 2)\n",
        "    assert t_ll <= t_lu <= t_ul <= t_uu, \"Invalid bounds for noise generation\"\n",
        "    if len(imgs.shape) < 5:\n",
        "        imgs = np.expand_dims(imgs, -1)\n",
        "    batch_size, seq_len = imgs.shape[:2]\n",
        "    factors = np.zeros([batch_size, seq_len, 4])\n",
        "    factors[:, 0] = random.uniform(low=0.0, high=1.0, size=(batch_size, 4))\n",
        "    for i in range(seq_len - 1):\n",
        "        factors[:, i + 1] = np.clip(factors[:, i] + random.uniform(\n",
        "            low=-r, high=r, size=(batch_size, 4)), a_min=0.0, a_max=1.0)\n",
        "\n",
        "    t1 = random.uniform(low=t_ll, high=t_lu, size=(batch_size, 1, 4))\n",
        "    t2 = random.uniform(low=t_ul, high=t_uu, size=(batch_size, 1, 4))\n",
        "\n",
        "    factors = (factors - t1) / (t2 - t1)\n",
        "    factors = np.clip(factors, a_min=0.0, a_max=1.0)\n",
        "    factors = np.reshape(factors, list(factors.shape) + [1, 1, 1])\n",
        "    factors[:, :first_n_clean] = 1.0\n",
        "    noisy_imgs = []\n",
        "    qs = []\n",
        "    for i in range(batch_size):\n",
        "        if imgs.dtype == np.uint8:\n",
        "            qs.append(detect_pendulums(imgs[i], half_x, half_y))\n",
        "            noise = random.uniform(low=0.0, high=255, size=[\n",
        "                                   4, seq_len, half_x, half_y, imgs.shape[-1]]).astype(np.uint8)\n",
        "            curr = np.zeros(imgs.shape[1:], dtype=np.uint8)\n",
        "            curr[:, :half_x, :half_y] = (factors[i, :, 0] * imgs[i, :, :half_x, :half_y] + (\n",
        "                1 - factors[i, :, 0]) * noise[0]).astype(np.uint8)\n",
        "            curr[:, :half_x, half_y:] = (factors[i, :, 1] * imgs[i, :, :half_x, half_y:] + (\n",
        "                1 - factors[i, :, 1]) * noise[1]).astype(np.uint8)\n",
        "            curr[:, half_x:, :half_y] = (factors[i, :, 2] * imgs[i, :, half_x:, :half_y] + (\n",
        "                1 - factors[i, :, 2]) * noise[2]).astype(np.uint8)\n",
        "            curr[:, half_x:, half_y:] = (factors[i, :, 3] * imgs[i, :, half_x:, half_y:] + (\n",
        "                1 - factors[i, :, 3]) * noise[3]).astype(np.uint8)\n",
        "        else:\n",
        "            noise = random.uniform(low=0.0, high=1.0, size=[\n",
        "                                   4, seq_len, half_x, half_y, imgs.shape[-1]])\n",
        "            curr = np.zeros(imgs.shape[1:])\n",
        "            curr[:, :half_x, :half_y] = factors[i, :, 0] * imgs[i, :,\n",
        "                                                                :half_x, :half_y] + (1 - factors[i, :, 0]) * noise[0]\n",
        "            curr[:, :half_x, half_y:] = factors[i, :, 1] * imgs[i, :,\n",
        "                                                                :half_x, half_y:] + (1 - factors[i, :, 1]) * noise[1]\n",
        "            curr[:, half_x:, :half_y] = factors[i, :, 2] * imgs[i, :,\n",
        "                                                                half_x:, :half_y] + (1 - factors[i, :, 2]) * noise[2]\n",
        "            curr[:, half_x:, half_y:] = factors[i, :, 3] * imgs[i, :,\n",
        "                                                                half_x:, half_y:] + (1 - factors[i, :, 3]) * noise[3]\n",
        "        noisy_imgs.append(curr)\n",
        "\n",
        "    factors_ext = np.concatenate([np.squeeze(factors), np.zeros(\n",
        "        [factors.shape[0], factors.shape[1], 1])], -1)\n",
        "    q = np.concatenate([np.expand_dims(q, 0) for q in qs], 0)\n",
        "    f = np.zeros(q.shape)\n",
        "    for i in range(f.shape[0]):\n",
        "        for j in range(f.shape[1]):\n",
        "            for k in range(3):\n",
        "                f[i, j, k] = factors_ext[i, j, q[i, j, k]]\n",
        "\n",
        "    return np.squeeze(np.concatenate([np.expand_dims(n, 0) for n in noisy_imgs], 0)), f\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "def detect_pendulums(imgs, half_x, half_y):\n",
        "    qs = [imgs[:, :half_x, :half_y], imgs[:, :half_x, half_y:],\n",
        "          imgs[:, half_x:, :half_y], imgs[:, half_x:, half_y:]]\n",
        "\n",
        "    r_cts = np.array(\n",
        "        [np.count_nonzero(q[:, :, :, 0] > 5, axis=(-1, -2)) for q in qs]).T\n",
        "    g_cts = np.array(\n",
        "        [np.count_nonzero(q[:, :, :, 1] > 5, axis=(-1, -2)) for q in qs]).T\n",
        "    b_cts = np.array(\n",
        "        [np.count_nonzero(q[:, :, :, 2] > 5, axis=(-1, -2)) for q in qs]).T\n",
        "\n",
        "    cts = np.concatenate([np.expand_dims(c, 1)\n",
        "                         for c in [r_cts, g_cts, b_cts]], 1)\n",
        "\n",
        "    q_max = np.max(cts, -1)\n",
        "    q = np.argmax(cts, -1)\n",
        "    q[q_max < 10] = 4\n",
        "    return q\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "class Pendulum:\n",
        "\n",
        "    MAX_VELO_KEY = 'max_velo'\n",
        "    MAX_TORQUE_KEY = 'max_torque'\n",
        "    MASS_KEY = 'mass'\n",
        "    LENGTH_KEY = 'length'\n",
        "    GRAVITY_KEY = 'g'\n",
        "    FRICTION_KEY = 'friction'\n",
        "    DT_KEY = 'dt'\n",
        "    SIM_DT_KEY = 'sim_dt'\n",
        "    TRANSITION_NOISE_TRAIN_KEY = 'transition_noise_train'\n",
        "    TRANSITION_NOISE_TEST_KEY = 'transition_noise_test'\n",
        "\n",
        "    OBSERVATION_MODE_LINE = \"line\"\n",
        "    OBSERVATION_MODE_BALL = \"ball\"\n",
        "\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def __init__(self,\n",
        "                 img_size,\n",
        "                 observation_mode,\n",
        "                 generate_actions=False,\n",
        "                 transition_noise_std=0.0,\n",
        "                 observation_noise_std=0.0,\n",
        "                 pendulum_params=None,\n",
        "                 seed=0):\n",
        "\n",
        "        assert observation_mode == Pendulum.OBSERVATION_MODE_BALL or observation_mode == Pendulum.OBSERVATION_MODE_LINE\n",
        "        # Global Parameters\n",
        "        self.state_dim = 2\n",
        "        self.action_dim = 1\n",
        "        self.img_size = img_size\n",
        "        self.observation_dim = img_size ** 2\n",
        "        self.observation_mode = observation_mode\n",
        "\n",
        "        self.random = onp.random.RandomState(seed)\n",
        "\n",
        "        # image parameters\n",
        "        self.img_size_internal = 128\n",
        "        self.x0 = self.y0 = 64\n",
        "        self.plt_length = 55 if self.observation_mode == Pendulum.OBSERVATION_MODE_LINE else 50\n",
        "        self.plt_width = 8\n",
        "\n",
        "        self.generate_actions = generate_actions\n",
        "\n",
        "        # simulation parameters\n",
        "        if pendulum_params is None:\n",
        "            pendulum_params = self.pendulum_default_params()\n",
        "        self.max_velo = pendulum_params[Pendulum.MAX_VELO_KEY]\n",
        "        self.max_torque = pendulum_params[Pendulum.MAX_TORQUE_KEY]\n",
        "        self.dt = pendulum_params[Pendulum.DT_KEY]\n",
        "        self.mass = pendulum_params[Pendulum.MASS_KEY]\n",
        "        self.length = pendulum_params[Pendulum.LENGTH_KEY]\n",
        "        self.inertia = self.mass * self.length**2 / 3\n",
        "        self.g = pendulum_params[Pendulum.GRAVITY_KEY]\n",
        "        self.friction = pendulum_params[Pendulum.FRICTION_KEY]\n",
        "        self.sim_dt = pendulum_params[Pendulum.SIM_DT_KEY]\n",
        "\n",
        "        self.observation_noise_std = observation_noise_std\n",
        "        self.transition_noise_std = transition_noise_std\n",
        "\n",
        "        self.tranisition_covar_mat = onp.diag(\n",
        "            np.array([1e-8, self.transition_noise_std**2, 1e-8, 1e-8]))\n",
        "        self.observation_covar_mat = onp.diag(\n",
        "            [self.observation_noise_std**2, self.observation_noise_std**2])\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def sample_data_set(self, num_episodes, episode_length, full_targets):\n",
        "        states = onp.zeros((num_episodes, episode_length, self.state_dim))\n",
        "        actions = self._sample_action(\n",
        "            (num_episodes, episode_length, self.action_dim))\n",
        "        states[:, 0, :] = self._sample_init_state(num_episodes)\n",
        "        t = onp.zeros((num_episodes, episode_length))\n",
        "\n",
        "        for i in range(1, episode_length):\n",
        "            states[:, i, :], dt = self._get_next_states(\n",
        "                states[:, i - 1, :], actions[:, i - 1, :])\n",
        "            t[:, i:] += dt\n",
        "        states[..., 0] -= onp.pi\n",
        "\n",
        "        if self.observation_noise_std > 0.0:\n",
        "            observation_noise = self.random.normal(loc=0.0,\n",
        "                                                   scale=self.observation_noise_std,\n",
        "                                                   size=states.shape)\n",
        "        else:\n",
        "            observation_noise = onp.zeros(states.shape)\n",
        "\n",
        "        targets = self.pendulum_kinematic(states)\n",
        "        if self.observation_mode == Pendulum.OBSERVATION_MODE_LINE:\n",
        "            noisy_states = states + observation_noise\n",
        "            noisy_targets = self.pendulum_kinematic(noisy_states)\n",
        "        elif self.observation_mode == Pendulum.OBSERVATION_MODE_BALL:\n",
        "            noisy_targets = targets + observation_noise\n",
        "        imgs = self._generate_images(noisy_targets[..., :2])\n",
        "\n",
        "        return imgs, targets[..., :(4 if full_targets else 2)], states, noisy_targets[..., :(4 if full_targets else 2)], t/self.dt\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    @staticmethod\n",
        "    def pendulum_default_params():\n",
        "        return {\n",
        "            Pendulum.MAX_VELO_KEY: 8,\n",
        "            Pendulum.MAX_TORQUE_KEY: 10,\n",
        "            Pendulum.MASS_KEY: 1,\n",
        "            Pendulum.LENGTH_KEY: 1,\n",
        "            Pendulum.GRAVITY_KEY: 9.81,\n",
        "            Pendulum.FRICTION_KEY: 0,\n",
        "            Pendulum.DT_KEY: 0.05,\n",
        "            Pendulum.SIM_DT_KEY: 1e-4\n",
        "        }\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _sample_action(self, shape):\n",
        "        if self.generate_actions:\n",
        "            return self.random.uniform(-self.max_torque, self.max_torque, shape)\n",
        "        else:\n",
        "            return np.zeros(shape=shape)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _transition_function(self, states, actions):\n",
        "        dt = self.dt\n",
        "        n_steps = dt / self.sim_dt\n",
        "\n",
        "        if n_steps != np.round(n_steps):\n",
        "            #print(n_steps, 'Warning from Pendulum: dt does not match up')\n",
        "            n_steps = np.round(n_steps)\n",
        "\n",
        "        c = self.g * self.length * self.mass / self.inertia\n",
        "        for i in range(0, int(n_steps)):\n",
        "            velNew = states[..., 1:2] + self.sim_dt * (c * np.sin(states[..., 0:1])\n",
        "                                                       + actions / self.inertia\n",
        "                                                       - states[..., 1:2] * self.friction)\n",
        "            states = onp.concatenate(\n",
        "                (states[..., 0:1] + self.sim_dt * velNew, velNew), axis=1)\n",
        "        return states, dt\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _get_next_states(self, states, actions):\n",
        "        actions = np.maximum(-self.max_torque,\n",
        "                             np.minimum(actions, self.max_torque))\n",
        "\n",
        "        states, dt = self._transition_function(states, actions)\n",
        "        if self.transition_noise_std > 0.0:\n",
        "            states[:, 1] += self.random.normal(loc=0.0,\n",
        "                                               scale=self.transition_noise_std,\n",
        "                                               size=[len(states)])\n",
        "\n",
        "        states[:, 0] = ((states[:, 0]) % (2 * np.pi))\n",
        "        return states, dt\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def get_ukf_smothing(self, obs):\n",
        "        batch_size, seq_length = obs.shape[:2]\n",
        "        succ = np.zeros(batch_size, dtype=np.bool)\n",
        "        means = np.zeros([batch_size, seq_length, 4])\n",
        "        covars = np.zeros([batch_size, seq_length, 4, 4])\n",
        "        fail_ct = 0\n",
        "        for i in range(batch_size):\n",
        "            if i % 10 == 0:\n",
        "                print(i)\n",
        "            try:\n",
        "                means[i], covars[i] = self.ukf.filter(obs[i])\n",
        "                succ[i] = True\n",
        "            except:\n",
        "                fail_ct += 1\n",
        "        print(fail_ct / batch_size, \"failed\")\n",
        "\n",
        "        return means[succ], covars[succ], succ\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _sample_init_state(self, nr_epochs):\n",
        "        return onp.concatenate((self.random.uniform(0, 2 * np.pi, (nr_epochs, 1)), np.zeros((nr_epochs, 1))), 1)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def add_observation_noise(self, imgs, first_n_clean, r=0.2, t_ll=0.1, t_lu=0.4, t_ul=0.6, t_uu=0.9):\n",
        "        return add_img_noise(imgs, first_n_clean, self.random, r, t_ll, t_lu, t_ul, t_uu)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _get_task_space_pos(self, joint_states):\n",
        "        task_space_pos = onp.zeros(list(joint_states.shape[:-1]) + [2])\n",
        "        task_space_pos[..., 0] = np.sin(joint_states[..., 0]) * self.length\n",
        "        task_space_pos[..., 1] = np.cos(joint_states[..., 0]) * self.length\n",
        "        return task_space_pos\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _generate_images(self, ts_pos):\n",
        "        imgs = onp.zeros(shape=list(ts_pos.shape)[\n",
        "                        :-1] + [self.img_size, self.img_size], dtype=np.uint8)\n",
        "        for seq_idx in range(ts_pos.shape[0]):\n",
        "            for idx in range(ts_pos.shape[1]):\n",
        "                imgs[seq_idx, idx] = self._generate_single_image(\n",
        "                    ts_pos[seq_idx, idx])\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _generate_single_image(self, pos):\n",
        "        x1 = pos[0] * (self.plt_length / self.length) + self.x0\n",
        "        y1 = pos[1] * (self.plt_length / self.length) + self.y0\n",
        "        img = Image.new('F', (self.img_size_internal,\n",
        "                        self.img_size_internal), 0.0)\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        if self.observation_mode == Pendulum.OBSERVATION_MODE_LINE:\n",
        "            draw.line([(self.x0, self.y0), (x1, y1)],\n",
        "                      fill=1.0, width=self.plt_width)\n",
        "        elif self.observation_mode == Pendulum.OBSERVATION_MODE_BALL:\n",
        "            x_l = x1 - self.plt_width\n",
        "            x_u = x1 + self.plt_width\n",
        "            y_l = y1 - self.plt_width\n",
        "            y_u = y1 + self.plt_width\n",
        "            draw.ellipse((x_l, y_l, x_u, y_u), fill=1.0)\n",
        "\n",
        "        img = img.resize((self.img_size, self.img_size),\n",
        "                         resample=Image.ANTIALIAS)\n",
        "        img_as_array = onp.asarray(img)\n",
        "        img_as_array = onp.clip(img_as_array, 0, 1)\n",
        "        return 255.0 * img_as_array\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _kf_transition_function(self, state, noise):\n",
        "        nSteps = self.dt / self.sim_dt\n",
        "\n",
        "        if nSteps != np.round(nSteps):\n",
        "            print('Warning from Pendulum: dt does not match up')\n",
        "            nSteps = np.round(nSteps)\n",
        "\n",
        "        c = self.g * self.length * self.mass / self.inertia\n",
        "        for i in range(0, int(nSteps)):\n",
        "            velNew = state[1] + self.sim_dt * \\\n",
        "                (c * np.sin(state[0]) - state[1] * self.friction)\n",
        "            state = onp.array([state[0] + self.sim_dt * velNew, velNew])\n",
        "        state[0] = state[0] % (2 * np.pi)\n",
        "        state[1] = state[1] + noise[1]\n",
        "        return state\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def pendulum_kinematic_single(self, js):\n",
        "        theta, theat_dot = js\n",
        "        x = np.sin(theta)\n",
        "        y = np.cos(theta)\n",
        "        x_dot = theat_dot * y\n",
        "        y_dot = theat_dot * -x\n",
        "        return onp.array([x, y, x_dot, y_dot]) * self.length\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def pendulum_kinematic(self, js_batch):\n",
        "        theta = js_batch[..., :1]\n",
        "        theta_dot = js_batch[..., 1:]\n",
        "        x = np.sin(theta)\n",
        "        y = np.cos(theta)\n",
        "        x_dot = theta_dot * y\n",
        "        y_dot = theta_dot * -x\n",
        "        return onp.concatenate([x, y, x_dot, y_dot], axis=-1)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def inverse_pendulum_kinematics(self, ts_batch):\n",
        "        x = ts_batch[..., :1]\n",
        "        y = ts_batch[..., 1:2]\n",
        "        x_dot = ts_batch[..., 2:3]\n",
        "        y_dot = ts_batch[..., 3:]\n",
        "        val = x / y\n",
        "        theta = np.arctan2(x, y)\n",
        "        theta_dot_outer = 1 / (1 + val**2)\n",
        "        theta_dot_inner = (x_dot * y - y_dot * x) / y**2\n",
        "        return onp.concatenate([theta, theta_dot_outer * theta_dot_inner], axis=-1)\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and modified\n",
        "def generate_pendulums(file_path, task, \n",
        "                       num_train_trials=200, num_test_trials=100,\n",
        "                       impute_rate=0.5, seq_len=100, file_tag=\"\"):\n",
        "    \n",
        "    if task == 'interpolation':\n",
        "        pend_params = Pendulum.pendulum_default_params()\n",
        "        pend_params[Pendulum.FRICTION_KEY] = 0.1\n",
        "        n = seq_len\n",
        "\n",
        "        pendulum = Pendulum(24, observation_mode=Pendulum.OBSERVATION_MODE_LINE,\n",
        "                            transition_noise_std=0.1, observation_noise_std=1e-5,\n",
        "                            seed=42, pendulum_params=pend_params)\n",
        "        rng = pendulum.random\n",
        "\n",
        "        train_obs, _, _, _, train_ts = pendulum.sample_data_set(\n",
        "            num_train_trials, n, full_targets=False)\n",
        "        train_obs = onp.expand_dims(train_obs, -1)\n",
        "        train_targets = train_obs.copy()\n",
        "        train_obs_valid = rng.rand(\n",
        "            train_obs.shape[0], train_obs.shape[1], 1) > impute_rate\n",
        "        train_obs_valid[:, :5] = True\n",
        "        train_obs[onp.logical_not(onp.squeeze(train_obs_valid))] = 0\n",
        "\n",
        "        test_obs, _, _, _, test_ts = pendulum.sample_data_set(\n",
        "            num_test_trials, n, full_targets=False)\n",
        "        test_obs = onp.expand_dims(test_obs, -1)\n",
        "        test_targets = test_obs.copy()\n",
        "        test_obs_valid = rng.rand(\n",
        "            test_obs.shape[0], test_obs.shape[1], 1) > impute_rate\n",
        "        test_obs_valid[:, :5] = True\n",
        "        test_obs[onp.logical_not(onp.squeeze(test_obs_valid))] = 0\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            os.makedirs(file_path)\n",
        "        onp.savez_compressed(os.path.join(file_path, f\"pend_interpolation_ir{impute_rate}\"),\n",
        "                            train_obs=train_obs, train_targets=train_targets, train_obs_valid=train_obs_valid, train_ts=train_ts,\n",
        "                            test_obs=test_obs, test_targets=test_targets, test_obs_valid=test_obs_valid, test_ts=test_ts)\n",
        "    \n",
        "    elif task == 'regression':\n",
        "        pend_params = Pendulum.pendulum_default_params()\n",
        "        pend_params[Pendulum.FRICTION_KEY] = 0.1\n",
        "        pend_params[Pendulum.DT_KEY] = 0.01\n",
        "        n = seq_len\n",
        "\n",
        "        pendulum = Pendulum(24, observation_mode=Pendulum.OBSERVATION_MODE_LINE,\n",
        "                            transition_noise_std=0.1, observation_noise_std=1e-5,\n",
        "                            seed=42, pendulum_params=pend_params)\n",
        "\n",
        "        train_obs, train_targets, _, _, train_ts = pendulum.sample_data_set(\n",
        "            num_train_trials, n, full_targets=False)\n",
        "        # train_obs, _ = pendulum.add_observation_noise(train_obs, first_n_clean=5, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75,\n",
        "        #                                               t_uu=1.0)\n",
        "        train_obs = onp.expand_dims(train_obs, -1)\n",
        "\n",
        "        test_obs, test_targets, _, _, test_ts = pendulum.sample_data_set(\n",
        "            num_test_trials, n, full_targets=False)\n",
        "        # test_obs, _ = pendulum.add_observation_noise(test_obs, first_n_clean=5, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75,\n",
        "        #                                              t_uu=1.0)\n",
        "        test_obs = onp.expand_dims(test_obs, -1)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            os.makedirs(file_path)\n",
        "        onp.savez_compressed(os.path.join(file_path, f\"pend_regression\"+file_tag+\".npz\"),\n",
        "                            train_obs=train_obs, train_targets=train_targets, train_ts=train_ts,\n",
        "                            test_obs=test_obs, test_targets=test_targets, test_ts=test_ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Roo84N4NN9-"
      },
      "source": [
        "The full dataset is (2000, 100, 24, 24, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "RaD7Wl1f9mWx"
      },
      "outputs": [],
      "source": [
        "# @title Create the pendulum dataset (uncomment this block!)\n",
        "# Takes about 2 minutes\n",
        "# generate_pendulums(\"pendulum\", \"regression\", seq_len=200)\n",
        "# generate_pendulums(\"pendulum\", \"regression\", \n",
        "#                    num_train_trials=0, num_test_trials=100, \n",
        "#                    seq_len=400, file_tag=\"_longer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "5Op7ol3UKP0g"
      },
      "outputs": [],
      "source": [
        "# @title Load the pendulum dataset\n",
        "def load_pendulum(run_params, log=False):    \n",
        "    d = run_params[\"dataset_params\"]\n",
        "    train_trials = d[\"train_trials\"]\n",
        "    val_trials = d[\"val_trials\"]\n",
        "    noise_scale = d[\"emission_cov\"] ** 0.5\n",
        "    key_train, key_val, key_pred = jr.split(d[\"seed\"], 3)\n",
        "\n",
        "    data = np.load(\"pendulum/pend_regression.npz\")\n",
        "\n",
        "    def _process_data(data, key):\n",
        "        processed = data[:, ::2] / 255.0\n",
        "        processed += jr.normal(key=key, shape=processed.shape) * noise_scale\n",
        "        # return np.clip(processed, 0, 1)\n",
        "        return processed # We are not cliping the data anymore!\n",
        "\n",
        "    # Take subset, subsample every 2 frames, normalize to [0, 1]\n",
        "    train_data = _process_data(data[\"train_obs\"][:train_trials], key_train)\n",
        "    train_states = data[\"train_targets\"][:train_trials, ::2]\n",
        "    # val_data = _process_data(data[\"test_obs\"][:val_trials], key_val)\n",
        "    data = np.load(\"pendulum/pend_regression_longer.npz\")\n",
        "    val_data = _process_data(data[\"test_obs\"][:val_trials], key_pred)\n",
        "    val_states = data[\"test_targets\"][:val_trials, ::2]\n",
        "\n",
        "    print(\"Full dataset:\", data[\"train_obs\"].shape)\n",
        "    print(\"Subset:\", train_data.shape)\n",
        "    return {\n",
        "        \"train_data\": train_data,\n",
        "        \"val_data\": val_data,\n",
        "        \"train_states\": train_states,\n",
        "        \"val_states\": val_states,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU2AZWhAPjux"
      },
      "source": [
        "# Run experiments!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "_MograqHxqqs"
      },
      "outputs": [],
      "source": [
        "if (\"data_dict\" not in globals()): data_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "cellView": "form",
        "id": "DXDxYDcDrCBG"
      },
      "outputs": [],
      "source": [
        "# @title Define network architecture parameters\n",
        "\n",
        "linear_recnet_architecture = {\n",
        "    \"diagonal_covariance\": False,\n",
        "    \"input_rank\": 1,\n",
        "    \"head_mean_params\": { \"features\": [] },\n",
        "    \"head_var_params\": { \"features\": [] },\n",
        "    \"eps\": 1e-3,\n",
        "    \"cov_init\": 2,\n",
        "}\n",
        "\n",
        "BiRNN_recnet_architecture = {\n",
        "    \"input_rank\": 1,\n",
        "    \"input_type\": \"MLP\",\n",
        "    \"input_params\":{ \"features\": [20,] },\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"head_dyn_params\": { \"features\": [20,] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "planet_posterior_architecture = {\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "linear_decnet_architecture = {\n",
        "    \"diagonal_covariance\": True,\n",
        "    \"input_rank\": 1,\n",
        "    \"head_mean_params\": { \"features\": [] },\n",
        "    \"head_var_params\": { \"features\": [] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "CNN_layers = [\n",
        "            {\"features\": 32, \"kernel_size\": (3, 3), \"strides\": (1, 1) },\n",
        "            {\"features\": 64, \"kernel_size\": (3, 3), \"strides\": (2, 2) },\n",
        "            {\"features\": 32, \"kernel_size\": (3, 3), \"strides\": (2, 2) }\n",
        "]\n",
        "\n",
        "CNN_recnet_architecture = {\n",
        "    \"input_rank\": 3,\n",
        "    \"trunk_type\": \"CNN\",\n",
        "    \"trunk_params\": {\n",
        "        \"layer_params\": CNN_layers\n",
        "    },\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "CNN_BiRNN_recnet_architecture = {\n",
        "    \"input_rank\": 3,\n",
        "    \"input_type\": \"CNN\",\n",
        "    \"input_params\":{\n",
        "        \"layer_params\": CNN_layers\n",
        "    },\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"head_dyn_params\": { \"features\": [20,] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "DCNN_decnet_architecture = {\n",
        "    \"input_shape\": (6, 6, 32),\n",
        "    \"layer_params\": [\n",
        "        { \"features\": 64, \"kernel_size\": (3, 3), \"strides\": (2, 2) },\n",
        "        { \"features\": 32, \"kernel_size\": (3, 3), \"strides\": (2, 2) },\n",
        "        { \"features\": 2, \"kernel_size\": (3, 3) }\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "92Lh9HVoqqZO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run parameter expanders\n",
        "def get_lr(params, max_iters):\n",
        "    base_lr = params[\"base_lr\"]\n",
        "    prior_base_lr = params[\"prior_base_lr\"]\n",
        "    lr = base_lr\n",
        "    prior_lr = prior_base_lr\n",
        "    pprint(params)\n",
        "    if params[\"lr_decay\"]:\n",
        "        print(\"Using learning rate decay!\")\n",
        "        lr = opt.exponential_decay(init_value=base_lr, \n",
        "                                     transition_steps=max_iters,\n",
        "                                     decay_rate=0.99, \n",
        "                                   transition_begin=.8*max_iters, staircase=False)\n",
        "        # This is kind of a different scheme but whatever...\n",
        "        if params[\"prior_lr_warmup\"]:\n",
        "            prior_lr = opt.cosine_onecycle_schedule(max_iters, prior_base_lr, 0.5)\n",
        "    else:\n",
        "        lr = base_lr\n",
        "        if params[\"prior_lr_warmup\"]: \n",
        "            prior_lr = opt.linear_schedule(0, prior_base_lr, .2 * max_iters, 0)\n",
        "    return lr, prior_lr\n",
        "\n",
        "def expand_lds_parameters(params):\n",
        "    num_timesteps = params.get(\"num_timesteps\") or 200\n",
        "    train_trials = { \"small\": 10, \"medium\": 100, \"large\": 1000 }\n",
        "    batch_sizes = {\"small\": 10, \"medium\": 10, \"large\": 20 }\n",
        "    emission_noises = { \"small\": 10., \"medium\": 1., \"large\": .1 }\n",
        "    dynamics_noises = { \"small\": 0.01, \"medium\": .1, \"large\": .1 }\n",
        "    max_iters = 8000\n",
        "\n",
        "    # Modify all the architectures according to the parameters given\n",
        "    D, H, N = params[\"latent_dims\"], params[\"rnn_dims\"], params[\"emission_dims\"]\n",
        "    inf_params = {}\n",
        "    if (params[\"inference_method\"] == \"svae\"):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(linear_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "    elif (params[\"inference_method\"] in [\"dkf\", \"cdkf\"]):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianBiRNN\"\n",
        "        architecture = deepcopy(BiRNN_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "        architecture[\"rnn_dim\"] = H\n",
        "    elif (params[\"inference_method\"] == \"planet\"):\n",
        "        # Here we're considering the filtering setting as default\n",
        "        # Most likely we're not even going to use this so it should be fine\n",
        "        # If we want smoothing then we can probably just replace these with the\n",
        "        # BiRNN version\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(linear_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = H\n",
        "        post_arch = deepcopy(planet_posterior_architecture)\n",
        "        post_arch[\"input_dim\"] = H\n",
        "        post_arch[\"rnn_dim\"] = H\n",
        "        post_arch[\"output_dim\"] = D\n",
        "        inf_params[\"posterior_architecture\"] = post_arch\n",
        "        inf_params[\"sample_kl\"] = True # PlaNet doesn't have built-in suff-stats\n",
        "    else:\n",
        "        print(\"Inference method not found: \" + params[\"inference_method\"])\n",
        "        assert(False)\n",
        "    decnet_architecture = deepcopy(linear_decnet_architecture)\n",
        "    decnet_architecture[\"output_dim\"] = N\n",
        "    inf_params[\"decnet_class\"] = \"GaussianEmission\"\n",
        "    inf_params[\"decnet_architecture\"] = decnet_architecture\n",
        "    inf_params[\"recnet_architecture\"] = architecture\n",
        "\n",
        "    lr, prior_lr = get_lr(params, max_iters)\n",
        "\n",
        "    extended_params = {\n",
        "        \"project_name\": \"SVAE-LDS-Final\",\n",
        "        \"log_to_wandb\": True,\n",
        "        \"dataset\": \"lds\",\n",
        "        \"dataset_params\": {\n",
        "            \"seed\": key_0,\n",
        "            \"num_trials\": train_trials[params[\"dataset_size\"]],\n",
        "            \"num_timesteps\": num_timesteps,\n",
        "            \"emission_cov\": emission_noises[params[\"snr\"]],\n",
        "            \"dynamics_cov\": dynamics_noises[params[\"snr\"]],\n",
        "            \"latent_dims\": D,\n",
        "            \"emission_dims\": N,\n",
        "        },\n",
        "        # Implementation choice\n",
        "        \"use_parallel_kf\": False,\n",
        "        # Training specifics\n",
        "        \"max_iters\": max_iters,\n",
        "        \"elbo_samples\": 1,\n",
        "        \"sample_kl\": False,\n",
        "        \"batch_size\": batch_sizes[params[\"dataset_size\"]],\n",
        "        \"record_params\": lambda i: i % 100 == 0,\n",
        "        \"plot_interval\": 100,\n",
        "        \"learning_rate\": lr, \n",
        "        \"prior_learning_rate\": prior_lr\n",
        "    }\n",
        "    extended_params.update(inf_params)\n",
        "    # This allows us to override ANY of the above...!\n",
        "    extended_params.update(params)\n",
        "    return extended_params\n",
        "\n",
        "def expand_pendulum_parameters(params):\n",
        "    train_trials = { \"small\": 20, \"medium\": 100, \"large\": 2000 }\n",
        "    batch_sizes = {\"small\": 10, \"medium\": 10, \"large\": 40 }\n",
        "    # Not a very good validation split (mostly because we're doing one full batch for val)\n",
        "    val_trials = { \"small\": 4, \"medium\": 20, \"large\": 200 }\n",
        "    noise_scales = { \"small\": 1., \"medium\": .1, \"large\": .01 }\n",
        "    max_iters = 20000\n",
        "\n",
        "    # Modify all the architectures according to the parameters given\n",
        "    D, H = params[\"latent_dims\"], params[\"rnn_dims\"]\n",
        "    inf_params = {}\n",
        "    if (params[\"inference_method\"] == \"svae\"):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(CNN_recnet_architecture)\n",
        "        architecture[\"trunk_params\"][\"output_dim\"] = D\n",
        "        architecture[\"output_dim\"] = D\n",
        "    elif (params[\"inference_method\"] in [\"dkf\", \"cdkf\"]):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianBiRNN\"\n",
        "        architecture = deepcopy(CNN_BiRNN_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "        architecture[\"rnn_dim\"] = H\n",
        "        architecture[\"input_params\"][\"output_dim\"] = H\n",
        "    elif (params[\"inference_method\"] == \"planet\"):\n",
        "        # Here we're considering the filtering setting as default\n",
        "        # Most likely we're not even going to use this so it should be fine\n",
        "        # If we want smoothing then we can probably just replace these with the\n",
        "        # BiRNN version\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(CNN_recnet_architecture)\n",
        "        architecture[\"trunk_params\"][\"output_dim\"] = H\n",
        "        architecture[\"output_dim\"] = H\n",
        "        post_arch = deepcopy(planet_posterior_architecture)\n",
        "        post_arch[\"input_dim\"] = H\n",
        "        post_arch[\"rnn_dim\"] = H\n",
        "        post_arch[\"output_dim\"] = D\n",
        "        inf_params[\"posterior_architecture\"] = post_arch\n",
        "        inf_params[\"sample_kl\"] = True # PlaNet doesn't have built-in suff-stats\n",
        "    else:\n",
        "        print(\"Inference method not found: \" + params[\"inference_method\"])\n",
        "        assert(False)\n",
        "    inf_params[\"recnet_architecture\"] = architecture\n",
        "    \n",
        "    lr, prior_lr = get_lr(params, max_iters)\n",
        "\n",
        "    extended_params = {\n",
        "        \"project_name\": \"SVAE-Pendulum-Final\",\n",
        "        \"log_to_wandb\": True,\n",
        "        \"dataset\": \"pendulum\",\n",
        "        # Must be model learning\n",
        "        \"run_type\": \"model_learning\",\n",
        "        \"decnet_class\": \"GaussianDCNNEmission\",\n",
        "        \"decnet_architecture\": DCNN_decnet_architecture,\n",
        "        \"dataset_params\": {\n",
        "            \"seed\": key_0,\n",
        "            \"train_trials\": train_trials[params[\"dataset_size\"]],\n",
        "            \"val_trials\": val_trials[params[\"dataset_size\"]],\n",
        "            \"emission_cov\": noise_scales[params[\"snr\"]]\n",
        "        },\n",
        "        # Implementation choice\n",
        "        \"use_parallel_kf\": False,\n",
        "        # Training specifics\n",
        "        \"max_iters\": max_iters,\n",
        "        \"elbo_samples\": 1,\n",
        "        \"sample_kl\": False,\n",
        "        \"batch_size\": batch_sizes[params[\"dataset_size\"]],\n",
        "        \"record_params\": lambda i: i % 100 == 0,\n",
        "        \"plot_interval\": 200,\n",
        "        \"mask_type\": \"potential\" if params[\"inference_method\"] == \"svae\" else \"data\",\n",
        "        \"learning_rate\": lr, \n",
        "        \"prior_learning_rate\": prior_lr,\n",
        "        \"use_validation\": True,\n",
        "        \"constrain_dynamics\": True,\n",
        "        \"prediction_horizon\": 5,\n",
        "    }\n",
        "    extended_params.update(inf_params)\n",
        "    # This allows us to override ANY of the above...!\n",
        "    extended_params.update(params)\n",
        "    return extended_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpwzMT9YQSMT"
      },
      "source": [
        "## Run the LDS experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5saocWweLzt"
      },
      "outputs": [],
      "source": [
        "# # @title LDS run parameters\n",
        "# run_params = {\n",
        "#     # Most important: inference method\n",
        "#     \"inference_method\": \"svae\",\n",
        "#     \"use_parallel_kf\": True,\n",
        "#     # Relevant dimensions\n",
        "#     \"latent_dims\": 3,\n",
        "#     \"rnn_dims\": 10,\n",
        "#     \"seed\": jr.PRNGKey(0),\n",
        "#     \"dataset_size\": \"small\", # \"small\", \"medium\", \"large\"\n",
        "#     \"snr\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "#     \"use_natural_grad\": False,\n",
        "#     \"constrain_prior\": True,\n",
        "#     # ---------------------------------------------\n",
        "#     \"constrain_dynamics\": True, # Truncates the singular values of A\n",
        "#     # ---------------------------------------------\n",
        "#     # We set it to true for since it's extra work to compute the sufficient stats \n",
        "#     # from smoothed potentials in the current parallel KF\n",
        "#     \"sample_kl\": False,\n",
        "#     \"base_lr\": 1e-3,\n",
        "#     \"prior_base_lr\": 1e-3, # Set to 0 for debugging\n",
        "#     \"prior_lr_warmup\": True,\n",
        "#     \"lr_decay\": False,\n",
        "#     \"group_tag\": \"\",\n",
        "#     # The only LDS-specific entries\n",
        "#     \"emission_dims\": 5,\n",
        "#     \"num_timesteps\": 10000,\n",
        "#     \"run_type\": \"model_learning\", # \"inference_only\"\n",
        "#     \"log_to_wandb\": False,\n",
        "#     \"visualize_training\": False,\n",
        "#     \"max_iters\": 2000\n",
        "# }\n",
        "\n",
        "# # run_variations = {\n",
        "# #     \"num_timesteps\": [50, 100, 200, 400, 800, 1000]\n",
        "# # }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqhimlvNt39G"
      },
      "outputs": [],
      "source": [
        "# jax.config.update(\"jax_debug_nans\", True)\n",
        "# jax.config.update(\"jax_enable_x64\", False)\n",
        "\n",
        "# results = experiment_scheduler(run_params, \n",
        "#                     #  run_variations=run_variations,\n",
        "#                      dataset_getter=sample_lds_dataset, \n",
        "#                      model_getter=init_model, \n",
        "#                      train_func=start_trainer,\n",
        "#                      params_expander=expand_lds_parameters,\n",
        "#                     #  on_error=on_error,\n",
        "#                      continue_on_error=False,\n",
        "#                      )\n",
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P97PoE4yJsMV"
      },
      "outputs": [],
      "source": [
        "# model = results[1][0][\"model\"]\n",
        "# params = results[1][0][\"trainer\"].params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UoVwAwpQYdx"
      },
      "outputs": [],
      "source": [
        "# mean = np.array([np.mean(np.array(exp[\"trainer\"].time_spent[1:])) for exp in results[1]])\n",
        "# std = np.array([np.std(np.array(exp[\"trainer\"].time_spent[1:])) for exp in results[1]])\n",
        "# plt.plot([50, 100, 200, 400, 800, 1000], mean+std)\n",
        "# plt.plot([50, 100, 200, 400, 800, 1000], mean-std)\n",
        "# assert False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLOSSwKQ9vl3"
      },
      "source": [
        "## Dealing with float32 imprecision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4jST1ldPGfO"
      },
      "outputs": [],
      "source": [
        "# # Code for unpacking and examining results\n",
        "# model = results[1][0][\"model\"]\n",
        "# params = results[1][0][\"trainer\"].params\n",
        "\n",
        "# latent_dim = 20\n",
        "# seq_len = 200\n",
        "\n",
        "# use_x64 = True\n",
        "# # Switch to x64\n",
        "# if (use_x64):\n",
        "#     jax.config.update(\"jax_enable_x64\", True)\n",
        "#     to_64 = lambda x: jax.tree_map(lambda y: np.array(y, dtype=np.float64), x)\n",
        "# else:\n",
        "#     jax.config.update(\"jax_enable_x64\", False)\n",
        "#     to_64 = lambda x: x\n",
        "\n",
        "# data = to_64(data_dict[\"train_data\"][4])\n",
        "# prior_params = to_64(params[\"prior_params\"])\n",
        "# potentials = to_64(model.recognition.apply(params[\"rec_params\"], data))\n",
        "# Sigmas = solve(potentials[\"J\"], np.eye(latent_dim)[None])\n",
        "# mus = vmap(solve)(potentials[\"J\"], potentials[\"h\"])\n",
        "# potentials[\"mu\"] = mus\n",
        "# potentials[\"Sigma\"] = Sigmas\n",
        "\n",
        "# prior_para = ParallelLieParameterizedLinearGaussianChain(latent_dim, seq_len)\n",
        "# posterior_para = ParallelLDSSVAEPosterior_Mean(latent_dim, seq_len)\n",
        "# model.posterior = posterior_para\n",
        "# prior_params_para = prior_para.get_constrained_params(prior_params)\n",
        "# post_params_para = posterior_para.infer(prior_params_para, potentials)\n",
        "\n",
        "# if (use_x64):\n",
        "#     kl_x64 = model.kl_posterior_prior(post_params_para, prior_para.get_constrained_params(prior_params))\n",
        "#     print(\"KL with x64:\", kl_x64)\n",
        "# else:\n",
        "#     kl_x32 = model.kl_posterior_prior(post_params_para, prior_para.get_constrained_params(prior_params))\n",
        "#     print(\"KL with x32:\", kl_x32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li5AH0eRa3in"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_para = posterior_para.distribution(post_params_para)\n",
        "# posterior_dist_para.log_prob(posterior_dist_para.expected_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI-YukjSbqj6"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_para.entropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_caqLPKFcnio"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_old.entropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLvURJK7bDg9"
      },
      "outputs": [],
      "source": [
        "# err = np.mean(posterior_dist_para.expected_states_squared \n",
        "#             - posterior_dist_old.expected_states_squared, axis=(1,2))\n",
        "# plt.plot(err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QOiE80IZ9vY"
      },
      "outputs": [],
      "source": [
        "# posterior_old = ParallelLDSSVAEPosterior(latent_dim, seq_len)\n",
        "# post_params_old = posterior_old.infer(prior_params_para, potentials)\n",
        "# posterior_dist_old = posterior_old.distribution(post_params_old)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mfPPecNavsk"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_old.log_prob(posterior_dist_old.expected_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHtj6ZwJaGKh"
      },
      "outputs": [],
      "source": [
        "# plt.plot(post_params_old[\"mu_filtered\"] - post_params_para[\"mu_filtered\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZP9aMCP3k6l"
      },
      "outputs": [],
      "source": [
        "# Sigmas = solve(potentials[\"J\"], np.eye(latent_dim)[None])\n",
        "# mus = vmap(solve)(potentials[\"J\"], potentials[\"h\"])\n",
        "# p = prior_params_para\n",
        "# params = { \"m1\": p[\"m1\"], \"Q1\": p[\"Q1\"], \n",
        "#           \"A\": p[\"A\"], \"b\": p[\"b\"], \"Q\": p[\"Q\"], \n",
        "#           \"mus\": mus, \"Sigmas\": Sigmas }\n",
        "# np.save(\"faulty_params.npy\", params, allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II5Q019TDHkb"
      },
      "outputs": [],
      "source": [
        "# def log_normalizer(p):\n",
        "#     Q, A, b = p[\"Q\"][None], p[\"A\"][None], p[\"b\"][None]\n",
        "#     AT = (p[\"A\"].T)[None]\n",
        "\n",
        "#     I = np.eye(Q.shape[-1])\n",
        "\n",
        "#     Sigma_filtered, mu_filtered = p[\"Sigma_filtered\"][:-1], p[\"mu_filtered\"][:-1]\n",
        "#     Sigma = Q + A @ Sigma_filtered @ AT\n",
        "#     mu = (A[0] @ mu_filtered.T).T + b\n",
        "#     # Append the first element\n",
        "#     Sigma_pred = np.concatenate([p[\"Q1\"][None], Sigma])\n",
        "#     mu_pred = np.concatenate([p[\"m1\"][None], mu])\n",
        "#     mu_rec, Sigma_rec = p[\"mu\"], p[\"Sigma\"]\n",
        "\n",
        "#     def log_Z_single(mu_pred, Sigma_pred, mu_rec, Sigma_rec):\n",
        "#         return MVN(loc=mu_pred, covariance_matrix=Sigma_pred+Sigma_rec).log_prob(mu_rec)\n",
        "\n",
        "#     log_Z = vmap(log_Z_single)(mu_pred, Sigma_pred, mu_rec, Sigma_rec)\n",
        "#     return np.sum(log_Z)\n",
        "\n",
        "# log_normalizer(post_params_para) - post_params_para[\"log_Z\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbHRN57av7Hk"
      },
      "outputs": [],
      "source": [
        "# def log_prob(self, p, data):\n",
        "\n",
        "#     A = self._dynamics_matrix #params[\"A\"]\n",
        "#     Q = self._dynamics_noise_covariance #params[\"Q\"]\n",
        "#     Q1 = self._initial_covariance #params[\"Q1\"]\n",
        "#     m1 = self._initial_mean #params[\"m1\"]\n",
        "\n",
        "#     num_batch_dims = len(data.shape) - 2\n",
        "\n",
        "#     t1 = np.sum(\n",
        "#         MVN(loc=np.einsum(\"ij,...tj->...ti\", A, data[...,:-1,:]), \n",
        "#             covariance_matrix=Q).log_prob(data[...,1:,:])\n",
        "#         )\n",
        "    \n",
        "#     t2 = MVN(loc=m1, covariance_matrix=Q1).log_prob(data[...,0,:])\n",
        "\n",
        "#     # Add the observation potentials\n",
        "#     # t3_ = - 0.5 * np.einsum(\"...ti,tij,...tj->...t\", data, self._emissions_precisions, data)\n",
        "#     # t3 = np.sum(t3_)\n",
        "#     # t4_ = + np.einsum(\"...ti,ti->...t\", data, self._emissions_linear_potentials)\n",
        "#     # t4 = np.sum(t4_)\n",
        "#     t3 = 0\n",
        "#     t4 = np.sum(MVN(loc=self._emissions_means, \n",
        "#                   covariance_matrix=self._emissions_covariances).log_prob(data))\n",
        "#     # print(t4)\n",
        "#     # Add the log normalizer\n",
        "#     # t5 = -self._log_normalizer\n",
        "#     # t5_ = np.ones(seq_len) * t5 / seq_len\n",
        "#     t5 = -log_normalizer(p)\n",
        "\n",
        "#     s1 = t3 + t4 + t5\n",
        "#     # s1_ = np.sum(t3_ + t4_ + t5_)\n",
        "\n",
        "#     # return s1, (t1, t2, t3, t4, t5, s1, s1_)\n",
        "#     return t1 + t2 + s1, (t1, t2, t3, t4, t5, s1)\n",
        "\n",
        "# if (use_x64):\n",
        "#     dist = posterior_para.distribution(post_params_para)\n",
        "#     lp, x64_trace_ll = log_prob(dist, post_params_para, dist.expected_states)\n",
        "#     print(\"Log prob computed stepwise with x64:\", lp)\n",
        "# else:\n",
        "#     dist = posterior_para.distribution(post_params_para)\n",
        "#     lp, x32_trace_ll = log_prob(dist, post_params_para, dist.expected_states)\n",
        "#     print(\"Log prob computed stepwise with x32:\", lp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liSyGmnRlE9l"
      },
      "outputs": [],
      "source": [
        "# def kl_posterior_prior(posterior, prior, posterior_params, prior_params):\n",
        "#     posterior = posterior.distribution(posterior_params)\n",
        "#     prior = prior.distribution(prior_params)\n",
        "#     Ex = posterior.expected_states\n",
        "#     ExxT = posterior.expected_states_squared\n",
        "#     ExnxT = posterior.expected_states_next_states\n",
        "#     Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "#     Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "#     # J, L = prior_params[\"J\"], prior_params[\"L\"]\n",
        "#     p = dynamics_to_tridiag(prior_params, Ex.shape[0], Ex.shape[1])\n",
        "#     J, L = p[\"J\"], p[\"L\"]\n",
        "\n",
        "#     p = dynamics_to_tridiag(posterior_params, Ex.shape[0], Ex.shape[1])\n",
        "#     J_post, L_post = p[\"J\"] + potentials[\"J\"], p[\"L\"]\n",
        "\n",
        "#     t1 = -prior.log_prob(Ex) \n",
        "#     # t1 = 0#-log_prob(prior, prior_params, Ex)[0]\n",
        "#     t2 = 0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "#     t3 = np.einsum(\"tij,tij->\", L, Sigmatnt)\n",
        "#     # t4 = -posterior.log_prob(Ex) \n",
        "#     t4 = -log_prob(posterior, posterior_params, Ex)[0]\n",
        "#     t5 = 0.5 * np.einsum(\"tij,tij->\", J_post, Sigmatt)\n",
        "#     t6 = np.einsum(\"tij,tij->\", L_post, Sigmatnt)\n",
        "\n",
        "#     # print(t1, t2, t3, t4, t5, t6)\n",
        "\n",
        "#     cross_entropy = t1\n",
        "#     cross_entropy += t2 #0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "#     cross_entropy += t3\n",
        "#     cross_entropy -= t4+t5+t6\n",
        "\n",
        "#     return cross_entropy, (t1, t2, t3, t4, t5, t6, t1 - t4, t2 - t5, t3 - t6)\n",
        "\n",
        "# if (use_x64):\n",
        "#     x64_kl, x64_trace = kl_posterior_prior(posterior_para, prior_para, post_params_para, prior_params_para)\n",
        "#     print(\"KL computed stepwise with x64:\", x64_kl)\n",
        "# else:\n",
        "#     x32_kl, x32_trace = kl_posterior_prior(posterior_para, prior_para, post_params_para, prior_params_para)\n",
        "#     print(\"KL computed stepwise with x32:\", x32_kl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kjE9yc1eNIA"
      },
      "outputs": [],
      "source": [
        "# print(\"KL computed stepwise with x32:\", x32_kl)\n",
        "# print(\"KL computed stepwise with x64:\", x64_kl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2LaiyyRuBRD"
      },
      "outputs": [],
      "source": [
        "# pprint([x64_trace[i] - x32_trace[i] for i in range(len(x64_trace))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ELK6UUjTnNr"
      },
      "source": [
        "## Pendulum experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "JjtwI49pGk7U",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Pendulum run params\n",
        "run_params = {\n",
        "    # Most important: inference method\n",
        "    \"inference_method\": \"svae\",#\"svae\",\n",
        "    \"use_parallel_kf\": True,\n",
        "    # Relevant dimensions\n",
        "    \"latent_dims\": 5,\n",
        "    \"rnn_dims\": 10,\n",
        "    \"seed\": jr.PRNGKey(0),\n",
        "    \"dataset_size\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "    \"snr\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "    \"use_natural_grad\": False,\n",
        "    \"constrain_prior\": True,\n",
        "    \"base_lr\": 1e-2,\n",
        "    \"prior_base_lr\": 1e-3,\n",
        "    \"prior_lr_warmup\": True,\n",
        "    \"lr_decay\": False,\n",
        "    \"group_tag\": \"re-running pendulum\",\n",
        "    # The only pendulum-specific entry, will be overridden by params expander\n",
        "    \"mask_size\": 40,\n",
        "    # \"plot_interval\": 1,\n",
        "    \"mask_start\": 0,#1000,\n",
        "    \"sample_kl\": False,\n",
        "    \"log_to_wandb\": True,\n",
        "    \"max_iters\": 100,\n",
        "}\n",
        "\n",
        "# methods = {\n",
        "#     # \"inference_method\": [\"svae\", \"cdkf\", \"planet\", \"dkf\"],\n",
        "#     # \"mask_start\": [0, 2000, 2000, 2000]\n",
        "#     \"inference_method\": [\"planet\", \"svae\"],\n",
        "#     \"mask_start\": [2000, 0],\n",
        "#     \"use_natural_grad\": [False, False],\n",
        "#     \"constrain_prior\": [True, True]\n",
        "# }\n",
        "\n",
        "seeds = {\n",
        "    \"seed\": [jr.PRNGKey(i) for i in range(3)]\n",
        "}\n",
        "\n",
        "run_variations = seeds#dict_product(seeds, methods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d85f48240ce64a2b893cb50a1cec116e",
            "7f0b6061d6034471af50c424c7817ba1",
            "bd47a97cfdef4c2796b97f053b9dd25e",
            "898e14a1a56e4dfd924136fd62b3fe32",
            "34649ef7020842baa7d3c09612758fa2",
            "78d5898b0d904ab4b98bda2bd7101f07",
            "9ae07192f8624684b66ce5216b303c35",
            "91f68732127b40bdb2441ac71dca13b6"
          ]
        },
        "id": "7cd8Vsd5q9Dl",
        "outputId": "cbae0703-9843-40cc-f041-32a880d0b329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of runs: 1\n",
            "Base paramerters:\n",
            "{'base_lr': 0.01,\n",
            " 'constrain_prior': True,\n",
            " 'dataset_size': 'medium',\n",
            " 'group_tag': 're-running pendulum',\n",
            " 'inference_method': 'svae',\n",
            " 'latent_dims': 5,\n",
            " 'log_to_wandb': True,\n",
            " 'lr_decay': False,\n",
            " 'mask_size': 40,\n",
            " 'mask_start': 0,\n",
            " 'max_iters': 100,\n",
            " 'prior_base_lr': 0.001,\n",
            " 'prior_lr_warmup': True,\n",
            " 'rnn_dims': 10,\n",
            " 'sample_kl': False,\n",
            " 'seed': DeviceArray([0, 0], dtype=uint32),\n",
            " 'snr': 'medium',\n",
            " 'use_natural_grad': False,\n",
            " 'use_parallel_kf': True}\n",
            "##########################################\n",
            "Starting run #0\n",
            "##########################################\n",
            "{'base_lr': 0.01,\n",
            " 'constrain_prior': True,\n",
            " 'dataset_size': 'medium',\n",
            " 'group_tag': 're-running pendulum',\n",
            " 'inference_method': 'svae',\n",
            " 'latent_dims': 5,\n",
            " 'log_to_wandb': True,\n",
            " 'lr_decay': False,\n",
            " 'mask_size': 40,\n",
            " 'mask_start': 0,\n",
            " 'max_iters': 100,\n",
            " 'prior_base_lr': 0.001,\n",
            " 'prior_lr_warmup': True,\n",
            " 'rnn_dims': 10,\n",
            " 'sample_kl': False,\n",
            " 'seed': DeviceArray([0, 0], dtype=uint32),\n",
            " 'snr': 'medium',\n",
            " 'use_natural_grad': False,\n",
            " 'use_parallel_kf': True}\n",
            "Loading dataset!\n",
            "Full dataset: (0, 400, 24, 24, 1)\n",
            "Subset: (100, 100, 24, 24, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LP: 41821.133:   0%|          | 0/100 [00:37<?, ?it/s]    "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230120_061315-2sqvzxuu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/matthew9671/SVAE-Pendulum-Final/runs/2sqvzxuu\" target=\"_blank\">gallant-fog-76</a></strong> to <a href=\"https://wandb.ai/matthew9671/SVAE-Pendulum-Final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href=\"https://wandb.ai/matthew9671/SVAE-Pendulum-Final\" target=\"_blank\">https://wandb.ai/matthew9671/SVAE-Pendulum-Final</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href=\"https://wandb.ai/matthew9671/SVAE-Pendulum-Final/runs/2sqvzxuu\" target=\"_blank\">https://wandb.ai/matthew9671/SVAE-Pendulum-Final/runs/2sqvzxuu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP: 41821.133:   1%|          | 1/100 [00:39<1:05:04, 39.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'base_lr': 0.01,\n",
            " 'batch_size': 10,\n",
            " 'constrain_dynamics': True,\n",
            " 'constrain_prior': True,\n",
            " 'dataset': 'pendulum',\n",
            " 'dataset_params': {'emission_cov': 0.1,\n",
            "                    'seed': DeviceArray([0, 0], dtype=uint32),\n",
            "                    'train_trials': 100,\n",
            "                    'val_trials': 20},\n",
            " 'dataset_size': 'medium',\n",
            " 'decnet_architecture': {'input_shape': (6, 6, 32),\n",
            "                         'layer_params': [{'features': 64,\n",
            "                                           'kernel_size': (3, 3),\n",
            "                                           'strides': (2, 2)},\n",
            "                                          {'features': 32,\n",
            "                                           'kernel_size': (3, 3),\n",
            "                                           'strides': (2, 2)},\n",
            "                                          {'features': 2,\n",
            "                                           'kernel_size': (3, 3)}]},\n",
            " 'decnet_class': 'GaussianDCNNEmission',\n",
            " 'elbo_samples': 1,\n",
            " 'group_tag': 're-running pendulum',\n",
            " 'inference_method': 'svae',\n",
            " 'latent_dims': 5,\n",
            " 'learning_rate': 0.01,\n",
            " 'log_to_wandb': True,\n",
            " 'lr_decay': False,\n",
            " 'mask_size': 40,\n",
            " 'mask_start': 0,\n",
            " 'mask_type': 'potential',\n",
            " 'max_iters': 100,\n",
            " 'plot_interval': 200,\n",
            " 'prediction_horizon': 5,\n",
            " 'prior_base_lr': 0.001,\n",
            " 'prior_learning_rate': <function polynomial_schedule.<locals>.schedule at 0x7f6a68ba41f0>,\n",
            " 'prior_lr_warmup': True,\n",
            " 'project_name': 'SVAE-Pendulum-Final',\n",
            " 'recnet_architecture': {'cov_init': 1,\n",
            "                         'eps': 0.0001,\n",
            "                         'head_mean_params': {'features': [20, 20]},\n",
            "                         'head_var_params': {'features': [20, 20]},\n",
            "                         'input_rank': 3,\n",
            "                         'output_dim': 5,\n",
            "                         'trunk_params': {'layer_params': [{'features': 32,\n",
            "                                                            'kernel_size': (3,\n",
            "                                                                            3),\n",
            "                                                            'strides': (1, 1)},\n",
            "                                                           {'features': 64,\n",
            "                                                            'kernel_size': (3,\n",
            "                                                                            3),\n",
            "                                                            'strides': (2, 2)},\n",
            "                                                           {'features': 32,\n",
            "                                                            'kernel_size': (3,\n",
            "                                                                            3),\n",
            "                                                            'strides': (2, 2)}],\n",
            "                                          'output_dim': 5},\n",
            "                         'trunk_type': 'CNN'},\n",
            " 'recnet_class': 'GaussianRecognition',\n",
            " 'record_params': <function expand_pendulum_parameters.<locals>.<lambda> at 0x7f6a68ba4280>,\n",
            " 'rnn_dims': 10,\n",
            " 'run_type': 'model_learning',\n",
            " 'sample_kl': False,\n",
            " 'seed': DeviceArray([0, 0], dtype=uint32),\n",
            " 'snr': 'medium',\n",
            " 'use_natural_grad': False,\n",
            " 'use_parallel_kf': True,\n",
            " 'use_validation': True}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LP: 21293.977: 100%|██████████| 100/100 [01:15<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='1.602 MB of 1.616 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.991011…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d85f48240ce64a2b893cb50a1cec116e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Condition number of A</td><td>▁▂▃▃▃▃▄▃▄▅▅▆▇▇▇▇█▇▇█▇▇▇▇▇█▇▇▇▆▆▅▅▅▅▄▃▄▄▄</td></tr><tr><td>Condition number of Q</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ELBO</td><td>▁▂▁▁▁▂▂▂▃▃▅▅▆▇██████████████████████████</td></tr><tr><td>KL</td><td>▁▁▂▂▃▃▃▃▃▂▃█▃▂▃▃▂▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Learning rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Likelihood</td><td>▁▂▁▁▁▂▂▂▃▃▅▅▆▇██████████████████████████</td></tr><tr><td>Max singular value of A</td><td>▃▆▄▅▅▄▅▄▃▅▄▄▆▄▅▄▆▄▃▃▅▄▃▂▄▆▄▄▄▃▄▃▅▄▅▁▁█▃▃</td></tr><tr><td>Min singular value of A</td><td>██▆▇▆▆▅▆▅▄▄▃▂▂▂▂▂▂▁▁▂▂▂▂▂▁▁▂▂▃▃▄▄▄▄▅▅▅▅▅</td></tr><tr><td>Prior learning rate</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Validation ELBO</td><td>▁▃▆███████</td></tr><tr><td>Validation KL</td><td>▄█▇▃▇▃▂▁▁▁</td></tr><tr><td>Validation likelihood</td><td>▁▃▆███████</td></tr><tr><td>Validation prediction log likelihood</td><td>▁▃▆▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Condition number of A</td><td>1.00001</td></tr><tr><td>Condition number of Q</td><td>1.0</td></tr><tr><td>ELBO</td><td>-21293.97656</td></tr><tr><td>KL</td><td>89.42857</td></tr><tr><td>Learning rate</td><td>0.01</td></tr><tr><td>Likelihood</td><td>-21204.54883</td></tr><tr><td>Max singular value of A</td><td>1.0</td></tr><tr><td>Min singular value of A</td><td>0.99999</td></tr><tr><td>Prior learning rate</td><td>2e-05</td></tr><tr><td>Validation ELBO</td><td>-21312.83398</td></tr><tr><td>Validation KL</td><td>89.37078</td></tr><tr><td>Validation likelihood</td><td>-21223.45898</td></tr><tr><td>Validation prediction log likelihood</td><td>-214.17508</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gallant-fog-76</strong> at: <a href=\"https://wandb.ai/matthew9671/SVAE-Pendulum-Final/runs/2sqvzxuu\" target=\"_blank\">https://wandb.ai/matthew9671/SVAE-Pendulum-Final/runs/2sqvzxuu</a><br/>Synced 5 W&B file(s), 20 media file(s), 0 artifact file(s) and 2 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230120_061315-2sqvzxuu/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Run the pendulum experiments\n",
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "jax.config.update(\"jax_enable_x64\", False)\n",
        "\n",
        "results = experiment_scheduler(run_params, \n",
        "                    #  run_variations=run_variations,\n",
        "                     dataset_getter=load_pendulum, \n",
        "                     model_getter=init_model, \n",
        "                     train_func=start_trainer,\n",
        "                     params_expander=expand_pendulum_parameters,\n",
        "                     on_error=on_error,\n",
        "                     continue_on_error=True)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run path: matthew9671/SVAE-Pendulum-Final/zvbh2nrm"
      ],
      "metadata": {
        "id": "bAd00_V1V_Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzngmpgUw4cb"
      },
      "source": [
        "# Pendulum Diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XbEGFOLU2i4"
      },
      "source": [
        "- [x] Visualize the samples from the predictive posterior\n",
        "- [x] Implement a shorter horizon prediction \n",
        "- [x] See how the svae performs as prediction horizon gets longer\n",
        "- [x] What about the other frameworks? -- best SVAE beats the cDKF...!\n",
        "- [x] Test the linear decoding for the entire dataset\n",
        "- [x] Package the diagnostics code into helper functions\n",
        "- [x] Also include the linear decoding accuracies (MSE) analysis\n",
        "\n",
        "TODOS:\n",
        "- [ ] Investigate why the sliding window prediction is still not giving us what we what\n",
        "- [ ] Pick one best run for all methods and plot the uncertainty estimates learned by them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5mX32LKg6xXj"
      },
      "outputs": [],
      "source": [
        "# @title Load the run from WandB\n",
        "data_dict = {}\n",
        "\n",
        "def load_run(project_path, run_name):\n",
        "    api = wandb.Api()\n",
        "    \n",
        "    try:\n",
        "        os.remove(\"parameters.pkl\")\n",
        "    except:\n",
        "        pass\n",
        "    with open(wandb.restore(\"parameters.pkl\", project_path+run_name).name, \"rb\") as f:\n",
        "        d = pkl.load(f)\n",
        "    params = d[-1]\n",
        "\n",
        "    # Get the configs for that specific run\n",
        "    run = api.run(project_path+run_name)\n",
        "    run_params = deepcopy(run.config)\n",
        "    # Get the dataset and the model object\n",
        "\n",
        "    # run_params[\"seed\"] = np.array(run_params[\"seed\"], dtype=np.uint32)\n",
        "    # For some unknown reason we're getting an int instead of a PRNGKey object \n",
        "    run_params[\"seed\"] = jr.PRNGKey(run_params[\"seed\"]) if isinstance(run_params[\"seed\"], int) \\\n",
        "        else np.array(run_params[\"seed\"], dtype=np.uint32)\n",
        "    # For some old runs the architecture logged is incorrect\n",
        "    # del(run_params[\"recnet_architecture\"][\"head_mean_params\"][\"features\"][-1])\n",
        "    # del(run_params[\"recnet_architecture\"][\"head_var_params\"][\"features\"][-1])\n",
        "    run_params[\"dataset_params\"][\"seed\"] = np.array(\n",
        "        run_params[\"dataset_params\"][\"seed\"], dtype=np.uint32)\n",
        "    \n",
        "    global data_dict\n",
        "\n",
        "    data_dict = load_pendulum(run_params)\n",
        "    model_dict = init_model(run_params, data_dict)\n",
        "    model = model_dict[\"model\"]\n",
        "    return run_params, params, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "LA1iJuvv4o39"
      },
      "outputs": [],
      "source": [
        "def predict_multiple(run_params, model_params, model, data, key, num_samples=100):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        Posterior mean (T, D)\n",
        "        Posterior covariance (T, D, D)\n",
        "        Predictions (N, T//2, D)\n",
        "        Expected prediction log likelihood (lower bound of MLL) (T//2, D)\n",
        "    \"\"\"\n",
        "    out = model.elbo(key, data, model_params, **run_params)\n",
        "    post_params = out[\"posterior_params\"]\n",
        "    posterior = model.posterior.distribution(post_params)\n",
        "    T, D = model.prior.seq_len // 2, model.prior.latent_dims\n",
        "    # Get the final mean and covariance\n",
        "    post_mean, post_covariance = posterior.mean(), posterior.covariance()\n",
        "    mu, Sigma = post_mean[T-1], post_covariance[T-1]\n",
        "    # Build the posterior object on the future latent states \n",
        "    # (\"the posterior predictive distribution\")\n",
        "    # Convert unconstrained params to constrained dynamics parameters\n",
        "    dynamics = model.prior.get_dynamics_params(model_params[\"prior_params\"])\n",
        "    pred_posterior = LinearGaussianChain.from_stationary_dynamics(\n",
        "        mu, Sigma, dynamics[\"A\"], dynamics[\"b\"], dynamics[\"Q\"], T+1) # Note the +1\n",
        "\n",
        "    # Sample from it and evaluate the log likelihood\n",
        "    x_preds = pred_posterior.sample(seed=key, sample_shape=(num_samples,))[:,1:]\n",
        "\n",
        "    def pred_ll(x_pred):\n",
        "        likelihood_dist = model.decoder.apply(model_params[\"dec_params\"], x_pred)\n",
        "        return likelihood_dist.log_prob(data[T:])\n",
        "\n",
        "    pred_lls = vmap(pred_ll)(x_preds)\n",
        "    # This assumes the pendulum dataset\n",
        "    # Which has 3d observations (width, height, channels)\n",
        "    pred_lls = pred_lls.sum(axis=(2, 3, 4))\n",
        "    # We want to estimate \\log p_tilde(y2|y1) = \\log \\int p(y2|x2) q(x2|y1) dx2\n",
        "    # ~ \\log E_q(x2|y1)[p(y2|x2)] \n",
        "    # So instead of mean we compute the logsumexp\n",
        "    pred_lls = logsumexp(pred_lls, axis=0) - np.log(num_samples)\n",
        "    # Keep only the first 10 samples\n",
        "    return post_mean, post_covariance, x_preds[:10], pred_lls\n",
        "\n",
        "# Note: ideally we want to compute the latent states for a bunch of training data\n",
        "# for computing the linear regression weights\n",
        "# But we also want to compute the prediction log likelihoods for test data\n",
        "\n",
        "def get_latents_and_predictions(run_params, model_params, model, data_dict):\n",
        "    \n",
        "    key = jr.PRNGKey(42)\n",
        "    \n",
        "    # Try to decode true states linearly from model encodings\n",
        "    num_predictions = 10\n",
        "    num_examples = 20\n",
        "    num_frames = 100\n",
        "\n",
        "    def encode(data):\n",
        "        out = model.elbo(jr.PRNGKey(0), data, model_params, **run_params)\n",
        "        post_params = out[\"posterior_params\"]\n",
        "        post_dist = model.posterior.distribution(post_params)\n",
        "        return post_dist.mean()\n",
        "\n",
        "    train_data = data_dict[\"train_data\"][:,:num_frames]\n",
        "    Ex = vmap(encode)(train_data)\n",
        "    # Figure out the linear regression weights which decodes true states\n",
        "    states = data_dict[\"train_states\"][:]\n",
        "    # states = targets[:,::2] # We subsampled the data during training to make pendulum swing faster\n",
        "    # Compute the true angles and angular velocities\n",
        "    train_thetas = np.arctan2(states[:,:,0], states[:,:,1])\n",
        "    train_omegas = train_thetas[:,1:]-train_thetas[:,:-1]\n",
        "    thetas = train_thetas.flatten()\n",
        "    omegas = train_omegas.flatten()\n",
        "    # Fit the learned representations to the true states\n",
        "    D = model.prior.latent_dims\n",
        "    xs_theta = Ex.reshape((-1, D))\n",
        "    xs_omega = Ex[:,1:].reshape((-1, D))\n",
        "    W_theta, _, _, _ = np.linalg.lstsq(xs_theta, thetas)\n",
        "    W_omega, _, _, _ = np.linalg.lstsq(xs_omega, omegas)\n",
        "\n",
        "    # Evaluate mse on test data\n",
        "    test_states = data_dict[\"val_states\"][:, :num_frames]\n",
        "    test_data = data_dict[\"val_data\"][:, :num_frames]\n",
        "    thetas = np.arctan2(test_states[:,:,0], test_states[:,:,1])\n",
        "    omegas = thetas[:,1:]-thetas[:,:-1]\n",
        "\n",
        "    test_mean, test_cov, test_preds, test_pred_lls = vmap(predict_multiple, \n",
        "        in_axes=(None, None, None, 0, None, None))\\\n",
        "        (run_params, model_params, model, test_data[:], key, num_predictions)\n",
        "    pred_thetas = np.einsum(\"i,...i->...\", W_theta, test_mean)\n",
        "    theta_mse = np.mean((pred_thetas - thetas) ** 2)\n",
        "    pred_omegas = np.einsum(\"i,...i->...\", W_omega, test_mean[:,1:])\n",
        "    omega_mse = np.mean((pred_omegas - omegas) ** 2)\n",
        "\n",
        "    return {\n",
        "        \"latent_mean\": test_mean,\n",
        "        \"latent_covariance\": test_cov,\n",
        "        \"prediction_lls\": test_pred_lls,\n",
        "        \"predictions\": test_preds,\n",
        "        \"w_theta\": W_theta,\n",
        "        \"w_omega\": W_omega,\n",
        "        \"theta_mse\": theta_mse,\n",
        "        \"omega_mse\": omega_mse,\n",
        "    }\n",
        "\n",
        "def evaluate_run(project_path, run_name):\n",
        "\n",
        "    # Load the run from wandb\n",
        "    run_params, model_params, model = load_run(project_path, run_name)\n",
        "    run_params = deepcopy(run_params)\n",
        "    run_params[\"mask_size\"] = 0\n",
        "\n",
        "    return get_latents_and_predictions(run_params, model_params, model, data_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxCueh0pIQK_"
      },
      "outputs": [],
      "source": [
        "project_path = \"matthew9671/SVAE-Pendulum-Final/\"\n",
        "result = evaluate_run(project_path, \"zvbh2nrm\", key_0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = np.load(\"results.npy\", allow_pickle=True).item()"
      ],
      "metadata": {
        "id": "io-aJSDp0G_V"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a pendulum state prediction plot"
      ],
      "metadata": {
        "id": "xvmCOOvPSKVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize a sample prediction\n",
        "# out_dist = model.decoder.apply(model_params[\"dec_params\"], np.concatenate([Ex[0, :50], x_preds[0, 0]]))\n",
        "# y_decoded = out_dist.mean()\n",
        "# plt.figure()\n",
        "# plot_img_grid(y_decoded)"
      ],
      "metadata": {
        "id": "Ufd585O3TV5O"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_id = 0 #if \"test_id\" not in globals() else test_id + 1\n",
        "# plt.plot(Ex_test[test_id] @ W_theta, label=\"decoded\")\n",
        "# plt.plot(thetas[test_id], label=\"true\")\n",
        "# plt.title(\"Linear decoding of pendulum angle (test sequence #{})\".format(test_id))\n",
        "# plt.legend()\n",
        "# plt.figure()\n",
        "# plt.plot(Ex_test[test_id] @ W_omega, label=\"decoded\")\n",
        "# plt.plot(omegas[test_id], label=\"true\")\n",
        "# plt.title(\"Linear decoding of pendulum velocity (test sequence #{})\".format(test_id))\n",
        "# plt.legend()"
      ],
      "metadata": {
        "id": "GPGf6D5iKiC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_theta = result[\"w_theta\"]\n",
        "W_omega = result[\"w_omega\"]\n",
        "preds = result[\"predictions\"]\n",
        "\n",
        "preds.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eGj7HfTavYx",
        "outputId": "331eb0c6-a88f-4e14-d545-c2ced555164e"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 10, 50, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets = data_dict[\"val_states\"][:, :100]\n",
        "thetas = np.arctan2(targets[:,:,0], targets[:,:,1])\n",
        "omegas = thetas[:,1:]-thetas[:,:-1]"
      ],
      "metadata": {
        "id": "re_QBjp7bhjy"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "plt.plot(thetas[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "5zkXsTPodVPV",
        "outputId": "8937b0d5-d546-46b1-af9f-92e8161b139d"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f6a08e5ca90>]"
            ]
          },
          "metadata": {},
          "execution_count": 185
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_theta = np.einsum(\"...i,i->...\", preds, W_theta)\n",
        "pred_omega = np.einsum(\"...i,i->...\", preds, W_omega)\n",
        "theta_norm = np.sum(W_theta ** 2) ** .5\n",
        "omega_norm = np.sum(W_omega ** 2) ** .5\n",
        "latent_cov_theta = np.einsum(\"...ij,i,j->...\", result[\"latent_covariance\"], W_theta, W_theta) \n",
        "latent_cov_omega = np.einsum(\"...ij,i,j->...\", result[\"latent_covariance\"], \n",
        "                       W_omega, W_omega) "
      ],
      "metadata": {
        "id": "hW3HAXw1dg4M"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We probably want to plot the posterior distribution for the first part and \n",
        "i = 10\n",
        "\n",
        "mean = result[\"latent_mean\"][i] @ W_theta\n",
        "plt.plot(mean)\n",
        "std = latent_cov_theta[i] ** .5\n",
        "plt.fill_between(np.arange(mean.shape[0]),mean+std, mean-std, alpha=.2)\n",
        "\n",
        "mean, std = np.mean(pred_theta[i], axis=0), np.std(pred_theta[i], axis=0) \n",
        "plt.plot(np.arange(50, 100), mean)\n",
        "plt.fill_between(np.arange(50, 100),mean+std, mean-std, alpha=.2)\n",
        "plt.plot(thetas[i])\n",
        "# for j in range(pred_theta.shape[1]):\n",
        "#     plt.plot(np.arange(50, 100), pred_theta[i][j], c=\"grey\")\n",
        "plt.figure()\n",
        "\n",
        "mean = result[\"latent_mean\"][i] @ W_omega\n",
        "plt.plot(mean)\n",
        "std = latent_cov_omega[i] ** .5\n",
        "plt.fill_between(np.arange(mean.shape[0]),mean+std, mean-std, alpha=.2)\n",
        "mean, std = np.mean(pred_omega[i], axis=0), np.std(pred_omega[i], axis=0) \n",
        "plt.plot(np.arange(50, 100), mean)\n",
        "plt.fill_between(np.arange(50, 100),mean+std, mean-std, alpha=.2)\n",
        "plt.plot(omegas[i])\n",
        "# for j in range(pred_omega.shape[1]):\n",
        "#     plt.plot(np.arange(50, 100), pred_omega[i][j], c=\"grey\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "fsD81Yytd2Xs",
        "outputId": "d79bd6bd-41fa-47e5-a311-2e60e04fd6f4"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f69fb50f3a0>]"
            ]
          },
          "metadata": {},
          "execution_count": 188
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a prediction log likelihood falloff plot"
      ],
      "metadata": {
        "id": "-WOdimoQTWR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(-np.mean(data_dict[\"train_data\"][i][50:], axis=(1, 2, 3)))"
      ],
      "metadata": {
        "id": "2kc4IMVSZia_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_ll = result[\"prediction_lls\"]"
      ],
      "metadata": {
        "id": "etz7-P9Rns7N"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 3\n",
        "mu = np.mean(pred_ll, axis=0)\n",
        "std = np.std(pred_ll, axis=0)\n",
        "plt.plot(mu)\n",
        "plt.fill_between(np.arange(mu.shape[0]), mu+std, mu-std, alpha=.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "eT_2nda9YGE7",
        "outputId": "bb207062-a411-466e-aa0a-60280d244219"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7f69fbcf1670>"
            ]
          },
          "metadata": {},
          "execution_count": 190
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pendulum summary function"
      ],
      "metadata": {
        "id": "25cgotxST5sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_pendulum_run(trainer, data_dict):\n",
        "    file_name = \"parameters.pkl\"\n",
        "    with open(file_name, \"wb\") as f:\n",
        "        pkl.dump(trainer.past_params, f)\n",
        "        wandb.save(file_name, policy=\"now\")\n",
        "    # Compute predictions on test set\n",
        "    results = get_latents_and_predictions(\n",
        "        trainer.train_params, trainer.params, trainer.model, data_dict)\n",
        "    file_name = \"results.npy\"\n",
        "    np.save(file_name, results, allow_pickle=True)\n",
        "    wandb.save(file_name, policy=\"now\")"
      ],
      "metadata": {
        "id": "yI4NjKQXT5cB"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Archive"
      ],
      "metadata": {
        "id": "L6fbLo70TQWf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns5x_wrtKc1W"
      },
      "outputs": [],
      "source": [
        "# result = evaluate_run(project_path, \"cdt4gir1\", key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8foTbjOSMiq2"
      },
      "outputs": [],
      "source": [
        "# _ = evaluate_run(project_path, \"dgudjrut\", key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsHe74D5FFVK"
      },
      "outputs": [],
      "source": [
        "# del all_results\n",
        "if (\"all_results\" not in globals()): all_results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uVctTR3JcBN"
      },
      "outputs": [],
      "source": [
        "api = wandb.Api()\n",
        "latent_dims = 5\n",
        "runs = api.runs(\"matthew9671/SVAE-Pendulum-ICML-3\", filters={\"State\": \"finished\", \n",
        "                                                             \"config.inference_method\": \"svae\",\n",
        "                                                             \"config.latent_dims\": latent_dims})\n",
        "svae_runs = []\n",
        "for run in runs:\n",
        "    svae_runs.append(run.id)\n",
        "runs = api.runs(\"matthew9671/SVAE-Pendulum-ICML-3\", filters={\"State\": \"finished\", \n",
        "                                                             \"config.inference_method\": \"cdkf\",\n",
        "                                                             \"config.latent_dims\": latent_dims})\n",
        "cdkf_runs = []\n",
        "for run in runs:\n",
        "    cdkf_runs.append(run.id)\n",
        "runs = api.runs(\"matthew9671/SVAE-Pendulum-ICML-3\", filters={\"State\": \"finished\", \n",
        "                                                             \"config.inference_method\": \"planet\",\n",
        "                                                             \"config.latent_dims\": latent_dims})\n",
        "planet_runs = []\n",
        "for run in runs:\n",
        "    planet_runs.append(run.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKPe7m3kD70V"
      },
      "outputs": [],
      "source": [
        "ax = plt.subplot()\n",
        "project_path = \"matthew9671/SVAE-Pendulum-ICML-3/\"\n",
        "# svae_runs = [\"v6sbb9xh\", \"xpf9s9ie\", \"1jxw27wp\", \"yo3fprzr\"]\n",
        "# cdkf_runs = [\"ik6i6igs\"] #\"cdt4gir1\", \"mpc58ktj\", \"dgudjrut\", \"4nctqeby\", \"cc8s8xxs\"]\n",
        "key = key_0\n",
        "for run_name in svae_runs + cdkf_runs:# + planet_runs:\n",
        "    print(\"Loading run \" + run_name)\n",
        "    key = jr.split(key)[1]\n",
        "    if (run_name not in all_results):\n",
        "        all_results[run_name] = evaluate_run(project_path, run_name, key)\n",
        "    result = all_results[run_name]\n",
        "    mean_pred_lls = result[\"long_horizon_pred_lls\"].mean(axis=0)\n",
        "    if (mean_pred_lls.mean() < -100 and run_name in svae_runs):\n",
        "        print(\"Run \" + run_name + \" gives really bad likelihoods!\")\n",
        "    if (run_name in svae_runs):\n",
        "        ax.plot(mean_pred_lls, color=\"blue\", alpha=.3)\n",
        "    elif (run_name in cdkf_runs):\n",
        "        ax.plot(mean_pred_lls, color=\"red\", alpha=.3)\n",
        "    elif (run_name in planet_runs):\n",
        "        # pass\n",
        "        ax.plot(mean_pred_lls, color=\"green\", alpha=.3)\n",
        "ax.plot(0, label=\"svae\", color=\"blue\")\n",
        "ax.plot(0, label=\"cdkf\", color=\"red\")\n",
        "ax.plot(0, label=\"planet\", color=\"green\")\n",
        "ax.set_title(\"Prediction log likelihood vs. horizon\")\n",
        "# ax.set_ylim(-100, 150)\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjH4oyyKkE-D"
      },
      "outputs": [],
      "source": [
        "svae_thetas = []\n",
        "svae_omegas = []\n",
        "for run_name in svae_runs:\n",
        "    svae_thetas.append(all_results[run_name][\"theta_mse\"])\n",
        "    svae_omegas.append(all_results[run_name][\"omega_mse\"])\n",
        "cdkf_thetas = []\n",
        "cdkf_omegas = []\n",
        "for run_name in cdkf_runs:\n",
        "    cdkf_thetas.append(all_results[run_name][\"theta_mse\"])\n",
        "    cdkf_omegas.append(all_results[run_name][\"omega_mse\"])\n",
        "planet_thetas = []\n",
        "planet_omegas = []\n",
        "for run_name in planet_runs:\n",
        "    planet_thetas.append(all_results[run_name][\"theta_mse\"])\n",
        "    planet_omegas.append(all_results[run_name][\"omega_mse\"])\n",
        "\n",
        "svae_thetas = np.array(svae_thetas)\n",
        "cdkf_thetas = np.array(cdkf_thetas)\n",
        "planet_thetas = np.array(planet_thetas)\n",
        "svae_omegas = np.array(svae_omegas) * 100\n",
        "cdkf_omegas = np.array(cdkf_omegas) * 100\n",
        "planet_omegas = np.array(planet_omegas) * 100\n",
        "bar_width = .2\n",
        "\n",
        "plt.scatter(np.zeros_like(svae_thetas)-bar_width/2, svae_thetas, s=2, color=\"blue\")\n",
        "plt.bar(-bar_width/2, svae_thetas.mean(), width=bar_width, color=\"blue\", alpha=.3, label=\"svae\")\n",
        "plt.scatter(np.zeros_like(cdkf_thetas)+bar_width/2, cdkf_thetas, s=2, color=\"red\")\n",
        "plt.bar(bar_width/2, cdkf_thetas.mean(), width=bar_width, color=\"red\", alpha=.3, label=\"cdkf\")\n",
        "plt.scatter(np.zeros_like(planet_thetas)+bar_width*3/2, planet_thetas, s=2, color=\"green\")\n",
        "plt.bar(bar_width*3/2, planet_thetas.mean(), width=bar_width, color=\"green\", alpha=.3, label=\"planet\")\n",
        "\n",
        "plt.scatter(np.zeros_like(svae_omegas)+1-bar_width/2, svae_omegas, s=2, color=\"blue\")\n",
        "plt.bar(1-bar_width/2, svae_omegas.mean(), width=bar_width, color=\"blue\", alpha=.3)\n",
        "plt.scatter(np.zeros_like(cdkf_omegas)+1+bar_width/2, cdkf_omegas, s=2, color=\"red\")\n",
        "plt.bar(1+bar_width/2, cdkf_omegas.mean(), width=bar_width, color=\"red\", alpha=.3)\n",
        "plt.scatter(np.zeros_like(planet_omegas)+1+bar_width*3/2, planet_omegas, s=2, color=\"green\")\n",
        "plt.bar(1+bar_width*3/2, planet_omegas.mean(), width=bar_width, color=\"green\", alpha=.3)\n",
        "plt.title(\"MSE for linear decoding of true pendulum state\")\n",
        "plt.xticks([0, 1], [\"theta\", \"omega\"])\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iychu_ZEV6Z7"
      },
      "outputs": [],
      "source": [
        "# @title Turns out the correlation between runs comes from the fluctuating averge pixel intensity of the image\n",
        "i = 1 # data id\n",
        "result = all_results[\"v6sbb9xh\"]\n",
        "plt.plot(result[\"long_horizon_pred_lls\"][i])\n",
        "# plt.plot(result[\"predictions\"][i][0])\n",
        "result = all_results[\"xpf9s9ie\"]\n",
        "plt.plot(result[\"long_horizon_pred_lls\"][i])\n",
        "# plt.plot(result[\"predictions\"][i][0])\n",
        "obs_mean = data_dict[\"train_data\"][i,50:100].sum(axis=(1, 2, 3))\n",
        "plt.plot((obs_mean - obs_mean.mean()) * -5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7tG1ohb4wl-"
      },
      "outputs": [],
      "source": [
        "# @title SVAE run names\n",
        "# run_name = \"391tsihg\" # 5d\n",
        "# run_name = \"vs2dkdje\" # 3d\n",
        "# run_name = \"zp5manco\" # 2d\n",
        "# 5d sinusoidal\n",
        "# run_name = \"0q0c0hbw\" \n",
        "# run_name = \"0vogdb8f\"\n",
        "# run_name = \"5b8cefgf\" # ICML-2 hopeful-sweep\n",
        "# run_name = \"7ntuood6\" # ICML-2 dainty-sweep (best prediction I've seen so far)\n",
        "# run_name = \"xpf9s9ie\" # ICML-3 good-sweep-21\n",
        "run_name = \"v6sbb9xh\" # ICML-3 rose-sweep-20 (good prediction)\n",
        "# \"1jxw27wp\" # ICML-3 solar-sweep\n",
        "# \"yo3fprzr\" # ICML-3 dry-sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJi79Ias4IP_"
      },
      "outputs": [],
      "source": [
        "# @title CDKF run names\n",
        "# run_name = \"cy1j7jyg\" # ICML-2 fancy-sweep\n",
        "run_name = \"dgudjrut\" # ICML-3 quiet-sweep\n",
        "run_name = \"4nctqeby\" # ICML-3 honest-sweep\n",
        "# \"cdt4gir1\" tough-sweep\n",
        "# \"mpc58ktj\" young-sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okamp0foh-Yg"
      },
      "outputs": [],
      "source": [
        "# If sampling from the prior becomes problematic, run this to truncate the singular values of A\n",
        "# prior_params[\"A\"] = truncate_singular_values(prior_params[\"A\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlSFRedYCyUR"
      },
      "outputs": [],
      "source": [
        "# @title Sample from the prior and visualize its decoding\n",
        "key = key_0\n",
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "prior_sample = model.prior.sample(prior_params, shape=(1,), key=key)[0]\n",
        "plt.plot(prior_sample)\n",
        "plt.figure()\n",
        "# plot_pcs(prior_sample, 2)\n",
        "out_dist = model.decoder.apply(dec_params, prior_sample)\n",
        "y_decoded = out_dist.mean()\n",
        "plt.figure()\n",
        "plot_img_grid(y_decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPgdXO-fDWue"
      },
      "outputs": [],
      "source": [
        "key = jr.split(key)[0]\n",
        "data_id = 3#jr.choice(key, 100)\n",
        "data = data_dict[\"train_data\"][data_id]\n",
        "# states = targets[data_id]\n",
        "# angles = np.arctan2(states[:,0], states[:,1])\n",
        "plot_img_grid(data)\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sZcQduzMC5m-"
      },
      "outputs": [],
      "source": [
        "# @title Visualize the mean predicted trajectory from the model\n",
        "# This might be the wrong thing to do, because the prior dynamics might not be accurate for\n",
        "# specific observation sequences\n",
        "temp_params = deepcopy(run_params)\n",
        "temp_params[\"mask_size\"] = 0\n",
        "out = model.elbo(jr.PRNGKey(0), data, params, **temp_params)\n",
        "post_params = out[\"posterior_params\"]\n",
        "post_dist = model.posterior.distribution(post_params)\n",
        "Ex = post_dist.mean()\n",
        "A = prior_params[\"A\"]\n",
        "b = prior_params[\"b\"]\n",
        "T = 100\n",
        "Ex_pred = predict_forward(Ex[T//2-1], A, b, T//2)\n",
        "hs = plt.plot(Ex)\n",
        "hs_ = plt.plot(np.arange(T//2-1, T), np.concatenate([Ex[T//2-1][None], Ex_pred]), linestyle=\":\")\n",
        "plt.title(\"Posterior and predictions\")\n",
        "for i in range(len(hs)):\n",
        "    hs_[i].set_color(hs[i].get_color())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDmP8-yldd8U"
      },
      "outputs": [],
      "source": [
        "out_dist = model.decoder.apply(dec_params, np.concatenate([Ex[:T//2], Ex_pred]))\n",
        "y_decoded = out_dist.mean()\n",
        "plt.figure()\n",
        "plot_img_grid(y_decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0aed57nAXHoP"
      },
      "outputs": [],
      "source": [
        "# @title Visualize multiple possible future paths predicted by the model...!\n",
        "\n",
        "\n",
        "jax.config.update(\"jax_debug_nans\", False)\n",
        "train_data = data_dict[\"train_data\"][:20,:100]\n",
        "x_preds, svae_pred_lls = vmap(predict_multiple, in_axes=(0, None, None))\\\n",
        "    (train_data, key_0, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhKaOqzTZHRW"
      },
      "outputs": [],
      "source": [
        "# D = model.prior.latent_dims\n",
        "# offset = 100\n",
        "\n",
        "# plt.figure(figsize=(20, 10))\n",
        "# for i in range(D):\n",
        "#     plt.plot(Ex[:,i] + i * offset, color=colors[i])\n",
        "#     plt.plot(np.arange(50, 100), x_preds[data_id,:,:,i].T + i * offset, color=colors[i], linestyle=\":\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEaCclVd0AZv"
      },
      "source": [
        "## Look at how well the physical state can be decoded from the latent representations linearly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnfLNMgc-eRl"
      },
      "outputs": [],
      "source": [
        "temp_params = deepcopy(run_params)\n",
        "temp_params[\"mask_size\"] = 0\n",
        "\n",
        "targets = np.load(\"pendulum/pend_regression.npz\")[\"train_targets\"][:100]\n",
        "\n",
        "def encode(data):\n",
        "    out = model.elbo(jr.PRNGKey(0), data, params, **temp_params)\n",
        "    post_params = out[\"posterior_params\"]\n",
        "    post_dist = model.posterior.distribution(post_params)\n",
        "    return post_dist.mean()\n",
        "\n",
        "all_latents_train = vmap(encode)(data_dict[\"train_data\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qsufn5KZYUN1"
      },
      "outputs": [],
      "source": [
        "states = targets[:,::2]\n",
        "train_thetas = np.arctan2(states[:,:,0], states[:,:,1])\n",
        "train_omegas = train_thetas[:,1:]-train_thetas[:,:-1]\n",
        "thetas = train_thetas.flatten()\n",
        "omegas = train_omegas.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ofB0o2F_qOS"
      },
      "outputs": [],
      "source": [
        "D = 5\n",
        "xs_theta = all_latents_train.reshape((-1, D))\n",
        "xs_omega = all_latents_train[:,1:].reshape((-1, D))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es8t-Udfasa8"
      },
      "outputs": [],
      "source": [
        "W_theta, _, _, _ = np.linalg.lstsq(xs_theta, thetas)\n",
        "W_omega, _, _, _ = np.linalg.lstsq(xs_omega, omegas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPZckeRTDEbx"
      },
      "outputs": [],
      "source": [
        "test_id = 0 if \"test_id\" not in globals() else test_id + 1\n",
        "plt.plot(all_latents_train[test_id] @ W_theta, label=\"decoded\")\n",
        "plt.plot(train_thetas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum angle (train sequence #{})\".format(test_id))\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(all_latents_train[test_id] @ W_omega, label=\"decoded\")\n",
        "plt.plot(train_omegas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum velocity (train sequence #{})\".format(test_id))\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQAY3_BZA4Px"
      },
      "outputs": [],
      "source": [
        "test_targets = np.load(\"pendulum/pend_regression_longer.npz\")[\"test_targets\"][:20, ::2][:, :100]\n",
        "test_data = data_dict[\"val_data\"][:, :100]#np.load(\"pendulum/pend_regression.npz\")[\"test_obs\"][:, ::2]\n",
        "thetas = np.arctan2(test_targets[:,:,0], test_targets[:,:,1])\n",
        "omegas = thetas[:,1:]-thetas[:,:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmcd1UabBcZg"
      },
      "outputs": [],
      "source": [
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "all_latents_test = vmap(encode)(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QOzPyLqZ78y"
      },
      "outputs": [],
      "source": [
        "test_id = 0 if \"test_id\" not in globals() else test_id + 1\n",
        "plt.plot(all_latents_test[test_id] @ W_theta, label=\"decoded\")\n",
        "plt.plot(thetas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum angle (test sequence #{})\".format(test_id))\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(all_latents_test[test_id] @ W_omega, label=\"decoded\")\n",
        "plt.plot(omegas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum velocity (test sequence #{})\".format(test_id))\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxjbHKWcPSmd"
      },
      "source": [
        "## Evaluate the sliding window prediction log likelihoods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvNpnf-wPXsk"
      },
      "outputs": [],
      "source": [
        "def prediction_lls(post_params):\n",
        "    posterior = model.posterior.distribution(post_params)\n",
        "    # Get the final mean and covariance\n",
        "    mu, Sigma = posterior.mean()[-1], posterior.covariance()[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAuuK-FGRUW2"
      },
      "outputs": [],
      "source": [
        "obj, out_dict = svae_loss(key, model, data_dict[\"train_data\"][:10], params, **temp_params)\n",
        "post_params = out_dict[\"posterior_params\"]\n",
        "posterior = model.posterior.distribution(post_params)\n",
        "J = posterior.filtered_precisions\n",
        "h = posterior.filtered_linear_potentials\n",
        "Sigma_filtered = inv(J)\n",
        "mu_filtered = np.einsum(\"...ij,...j->...i\", Sigma_filtered, h)\n",
        "data_batch = data_dict[\"train_data\"]\n",
        "horizon = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ1y0BMfSNWf"
      },
      "outputs": [],
      "source": [
        "def pred_ll(data_id, key):\n",
        "    num_windows = T-horizon-1\n",
        "    pred_lls = vmap(sliding_window_prediction_lls, in_axes=(None, None, None, 0, 0))(\n",
        "        mu_filtered[data_id], Sigma_filtered[data_id], data_batch[data_id],\n",
        "        np.arange(num_windows), jr.split(key, num_windows))\n",
        "    return pred_lls.mean(axis=0)\n",
        "\n",
        "def sliding_window_prediction_lls(mu, Sigma, data, t, key):\n",
        "    # Build the posterior object on the future latent states \n",
        "    # (\"the posterior predictive distribution\")\n",
        "    # Convert unconstrained params to constrained dynamics parameters\n",
        "    prior_params_constrained = model.prior.get_dynamics_params(prior_params)\n",
        "    dynamics_params = {\n",
        "        \"m1\": mu[t],\n",
        "        \"Q1\": Sigma[t],\n",
        "        \"A\": prior_params_constrained[\"A\"],\n",
        "        \"b\": prior_params_constrained[\"b\"],\n",
        "        \"Q\": prior_params_constrained[\"Q\"]\n",
        "    }\n",
        "    tridiag_params = dynamics_to_tridiag(dynamics_params, horizon+1, D) # Note the +1\n",
        "    J, L, h = tridiag_params[\"J\"], tridiag_params[\"L\"], tridiag_params[\"h\"]\n",
        "    pred_posterior = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "    # Sample from it and evaluate the log likelihood\n",
        "    x_pred = pred_posterior.sample(seed=key)[1:]\n",
        "    likelihood_dist = model.decoder.apply(dec_params, x_pred)\n",
        "    return likelihood_dist.log_prob(\n",
        "        lax.dynamic_slice(data, (t+1, 0, 0, 0), (horizon,) + data.shape[1:])).sum(axis=(1, 2, 3))\n",
        "\n",
        "num_windows = T-horizon-1\n",
        "pred_lls = vmap(pred_ll)(np.arange(10), jr.split(key_0, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoDd5GdpUAqi"
      },
      "outputs": [],
      "source": [
        "plt.plot(pred_lls.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD3BXGWQ6ko_"
      },
      "source": [
        "# What is going on with the dynamics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtChfl-eG5qm"
      },
      "outputs": [],
      "source": [
        "theta = 2 * np.pi / 100\n",
        "lds_params = {\n",
        "    \"m1\": np.zeros(2),\n",
        "    \"Q1\": np.eye(2),\n",
        "    \"A\": np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]]),\n",
        "    \"Q\": np.eye(2) / 100,\n",
        "    \"b\": np.zeros(2)\n",
        "}\n",
        "prior = LinearGaussianChain(2, 100)\n",
        "prior_dist = prior.distribution(prior.get_constrained_params(lds_params))\n",
        "plt.plot(prior_dist.sample(seed=key_0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEvsYfef6kdW"
      },
      "outputs": [],
      "source": [
        "temp_params = deepcopy(run_params)\n",
        "temp_params[\"mask_size\"] = 0\n",
        "_, aux = svae_loss(key_0, model, data_dict[\"train_data\"][:10], params, **temp_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRmFHEyb67Un"
      },
      "outputs": [],
      "source": [
        "pp = deepcopy(params[\"prior_params\"])\n",
        "suff_stats = aux[\"sufficient_statistics\"]\n",
        "pp[\"avg_suff_stats\"] = suff_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YMXhGbU7RmK"
      },
      "outputs": [],
      "source": [
        "fit_prior_params = model.prior.m_step(pp)\n",
        "# fit_prior_params[\"A\"] = scale_singular_values(fit_prior_params[\"A\"])\n",
        "# fit_prior_params[\"A\"] = truncate_singular_values(fit_prior_params[\"A\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qU7e1QiDMvy"
      },
      "outputs": [],
      "source": [
        "key = key_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnFVPlKBBhz0"
      },
      "outputs": [],
      "source": [
        "m1 = Q = fit_prior_params[\"m1\"]\n",
        "Q1 = fit_prior_params[\"Q1\"]\n",
        "Q = fit_prior_params[\"Q\"]\n",
        "A = fit_prior_params[\"A\"]\n",
        "b = fit_prior_params[\"b\"]\n",
        "\n",
        "\n",
        "x = jr.multivariate_normal(key=key, mean=m1, cov=Q1)\n",
        "xs = []\n",
        "for i in range(200):\n",
        "    xs.append(x)\n",
        "    key, _ = jr.split(key)\n",
        "    noise = jr.multivariate_normal(key=key, mean=np.zeros_like(x), cov=Q)\n",
        "    x = A @ x + b + noise\n",
        "\n",
        "plt.plot(np.array(xs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Beq52DGTBr0U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHEwqDfA8Awl"
      },
      "outputs": [],
      "source": [
        "prior = LinearGaussianChain(model.prior.latent_dims, model.prior.seq_len)\n",
        "prior_dist = prior.distribution(prior.get_constrained_params(fit_prior_params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT-OnbBX9SC7"
      },
      "outputs": [],
      "source": [
        "Q = fit_prior_params[\"Q\"]\n",
        "A = fit_prior_params[\"A\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uACoUB3T89M_"
      },
      "outputs": [],
      "source": [
        "sample = prior_dist.sample(seed=key_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISTJrxMD9Mq6"
      },
      "outputs": [],
      "source": [
        "plt.plot(sample)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "njW9wRLEPG7T",
        "8Xevru2BSSSZ",
        "U6ImmaouPD-G",
        "CpwzMT9YQSMT",
        "aLOSSwKQ9vl3",
        "KEaCclVd0AZv",
        "uxjbHKWcPSmd",
        "rD3BXGWQ6ko_"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d85f48240ce64a2b893cb50a1cec116e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f0b6061d6034471af50c424c7817ba1",
              "IPY_MODEL_bd47a97cfdef4c2796b97f053b9dd25e"
            ],
            "layout": "IPY_MODEL_898e14a1a56e4dfd924136fd62b3fe32"
          }
        },
        "7f0b6061d6034471af50c424c7817ba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34649ef7020842baa7d3c09612758fa2",
            "placeholder": "​",
            "style": "IPY_MODEL_78d5898b0d904ab4b98bda2bd7101f07",
            "value": "1.602 MB of 1.616 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "bd47a97cfdef4c2796b97f053b9dd25e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ae07192f8624684b66ce5216b303c35",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91f68732127b40bdb2441ac71dca13b6",
            "value": 0.9910118962279346
          }
        },
        "898e14a1a56e4dfd924136fd62b3fe32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34649ef7020842baa7d3c09612758fa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78d5898b0d904ab4b98bda2bd7101f07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ae07192f8624684b66ce5216b303c35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91f68732127b40bdb2441ac71dca13b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}