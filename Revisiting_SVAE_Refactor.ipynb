{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthew9671/MattLab/blob/main/Revisiting_SVAE_Refactor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFyEe0Xi_84F"
      },
      "source": [
        "- [x] Implement and test Kalman filtering and smoothing with parallel scan\n",
        "- [ ] Make parallel scan KF work with non-zero biases\n",
        "- [x] Write analysis code for pendulum\n",
        "  - [x] Evaluate predictive accuracy\n",
        "  - [x] Do linear regression from latents to angle and velocity "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6u_EWGy0phmX"
      },
      "outputs": [],
      "source": [
        "# This reloads files not modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BJs02uuMM79",
        "outputId": "c343f11a-a2df-48d2-f542-4c31c8a03424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# @title Download stuff \n",
        "import os\n",
        "# Download and install the relevant libraries\n",
        "!pip install -q git+https://github.com/google/flax\n",
        "!pip install -q wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UbqX5CKc11Uc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde29ab1-bc28-4249-b904-7a8362c0b128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing dynamax\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dynamax\n",
            "  Cloning https://github.com/probml/dynamax.git to /tmp/pip-install-986r77bk/dynamax_10219299e75d49e9ac4ba8a19dc1dc43\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/probml/dynamax.git /tmp/pip-install-986r77bk/dynamax_10219299e75d49e9ac4ba8a19dc1dc43\n",
            "  Resolved https://github.com/probml/dynamax.git to commit 3d190b5b89990afe8d7bb43c08c1353f61555eb7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from dynamax) (4.4.0)\n",
            "Collecting jaxtyping\n",
            "  Downloading jaxtyping-0.2.11-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.8/dist-packages (from dynamax) (0.17.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.8/dist-packages (from dynamax) (0.1.4)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.8/dist-packages (from dynamax) (0.3.25+cuda11.cudnn805)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from dynamax) (1.0.2)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.8/dist-packages (from dynamax) (0.3.25)\n",
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.8/dist-packages (from dynamax) (1.0.3)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.15->dynamax) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.15->dynamax) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.15->dynamax) (1.21.6)\n",
            "Collecting typeguard>=2.13.3\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax->dynamax) (1.3.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from optax->dynamax) (0.1.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->dynamax) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->dynamax) (1.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->dynamax) (1.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->dynamax) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->dynamax) (0.4.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->dynamax) (0.1.8)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability->dynamax) (2.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax->dynamax) (0.12.0)\n",
            "Building wheels for collected packages: dynamax\n",
            "  Building wheel for dynamax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dynamax: filename=dynamax-0.1.0+134.g3d190b5-py3-none-any.whl size=147460 sha256=a994e24a32327989411e081371dae26335a7a527fe5541dd329418dce56d0bc3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i7wchjpg/wheels/52/e1/7c/4664778646c92bb31957a1dac9e17ef231020b69e0b2e4d113\n",
            "Successfully built dynamax\n",
            "Installing collected packages: typeguard, jaxtyping, dynamax\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 2.7.1\n",
            "    Uninstalling typeguard-2.7.1:\n",
            "      Successfully uninstalled typeguard-2.7.1\n",
            "Successfully installed dynamax-0.1.0+134.g3d190b5 jaxtyping-0.2.11 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import dynamax\n",
        "except ModuleNotFoundError:\n",
        "    print('installing dynamax')\n",
        "    !pip install git+https://github.com/probml/dynamax.git#egg=dynamax\n",
        "    import dynamax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njW9wRLEPG7T"
      },
      "source": [
        "# Set everything up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "metadata": {
        "cellView": "form",
        "id": "b1ikkl1ULTEB"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "# Misc\n",
        "import os\n",
        "from importlib import reload\n",
        "import numpy as onp\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "from tqdm import trange\n",
        "import copy, traceback\n",
        "from pprint import pprint\n",
        "from copy import deepcopy\n",
        "import pickle as pkl\n",
        "\n",
        "# for logging\n",
        "import wandb\n",
        "# Debug\n",
        "import pdb\n",
        "# Jax\n",
        "import jax\n",
        "from jax import vmap, lax, jit, value_and_grad\n",
        "import jax.numpy as np\n",
        "import jax.scipy as scipy\n",
        "import jax.random as jr\n",
        "key_0 = jr.PRNGKey(0) # Convenience\n",
        "from jax.lax import scan, stop_gradient\n",
        "from jax.tree_util import tree_map\n",
        "# optax\n",
        "import optax as opt\n",
        "# Flax\n",
        "import flax.linen as nn\n",
        "from flax.linen import Conv, ConvTranspose\n",
        "from flax.core import frozen_dict as fd\n",
        "\n",
        "# Tensorflow probability\n",
        "import tensorflow_probability.substrates.jax as tfp\n",
        "import tensorflow_probability.substrates.jax.distributions as tfd\n",
        "from tensorflow_probability.python.internal import reparameterization\n",
        "MVN = tfd.MultivariateNormalFullCovariance\n",
        "\n",
        "# Dynamax (central to our implementation)\n",
        "from dynamax.linear_gaussian_ssm.inference import make_lgssm_params, lgssm_smoother\n",
        "# from dynamax.linear_gaussian_ssm.parallel_inference import lgssm_smoother as parallel_lgssm_smoother\n",
        "from dynamax.utils.utils import psd_solve\n",
        "\n",
        "# Common math functions\n",
        "from flax.linen import softplus, sigmoid\n",
        "from jax.scipy.special import logsumexp\n",
        "from jax.scipy.linalg import solve_triangular\n",
        "from jax.numpy.linalg import eigh, cholesky, svd, inv, solve\n",
        "\n",
        "# For typing in neural network utils\n",
        "from typing import (NamedTuple, Any, Callable, Sequence, Iterable, List, Optional, Tuple,\n",
        "                    Set, Type, Union, TypeVar, Generic, Dict)\n",
        "\n",
        "# For making the pendulum dataset\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "\n",
        "# For making nice visualizations\n",
        "from sklearn.decomposition import PCA\n",
        "from IPython.display import clear_output, HTML\n",
        "from matplotlib import animation, rc\n",
        "import seaborn as sns\n",
        "color_names = [\"windows blue\",\n",
        "                \"red\",\n",
        "                \"amber\",\n",
        "                \"faded green\",\n",
        "                \"dusty purple\",\n",
        "                \"orange\",\n",
        "                \"clay\",\n",
        "                \"pink\",\n",
        "                \"greyish\",\n",
        "                \"mint\",\n",
        "                \"light cyan\",\n",
        "                \"steel blue\",\n",
        "                \"forest green\",\n",
        "                \"pastel purple\",\n",
        "                \"salmon\",\n",
        "                \"dark brown\",\n",
        "               \"violet\",\n",
        "               \"mauve\",\n",
        "               \"ocean\",\n",
        "               \"ugly yellow\"]\n",
        "colors = sns.xkcd_palette(color_names)\n",
        "\n",
        "# Get rid of the check types warning\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "class CheckTypesFilter(logging.Filter):\n",
        "    def filter(self, record):\n",
        "        return \"check_types\" not in record.getMessage()\n",
        "logger.addFilter(CheckTypesFilter())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {
        "cellView": "form",
        "id": "2MjuUxHxR5O0"
      },
      "outputs": [],
      "source": [
        "# @title Misc helpers\n",
        "def get_value(x):\n",
        "    try:\n",
        "        return x.val.val.primal\n",
        "    except:\n",
        "        try:\n",
        "            return x.val.val\n",
        "        except:\n",
        "            try:\n",
        "                return x.val\n",
        "            except:\n",
        "                return x  # Oh well.\n",
        "\n",
        "def plot_img_grid(recon):\n",
        "    plt.figure(figsize=(8,8))\n",
        "    # Show the sequence as a block of images\n",
        "    stacked = recon.reshape(10, 24 * 10, 24)\n",
        "    imgrid = stacked.swapaxes(0, 1).reshape(24 * 10, 24 * 10)\n",
        "    plt.imshow(imgrid, vmin=0, vmax=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {
        "cellView": "form",
        "id": "FGleKPUALeEd"
      },
      "outputs": [],
      "source": [
        "# @title Math helpers\n",
        "def softplus(x):\n",
        "    return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0)\n",
        "\n",
        "def inv_softplus(x, eps=1e-4):\n",
        "    return np.log(np.exp(x - eps) - 1)\n",
        "\n",
        "def vectorize_pytree(*args):\n",
        "    \"\"\"\n",
        "    Flatten an arbitrary PyTree into a vector.\n",
        "    :param args:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    flat_tree, _ = jax.tree_util.tree_flatten(args)\n",
        "    flat_vs = [x.flatten() for x in flat_tree]\n",
        "    return np.concatenate(flat_vs, axis=0)\n",
        "\n",
        "# converts an (n(n+1)/2,) vector of Lie parameters\n",
        "# to an (n, n) matrix\n",
        "def lie_params_to_constrained(out_flat, dim, eps=1e-4):\n",
        "    D, A = out_flat[:dim], out_flat[dim:]\n",
        "    # ATTENTION: we changed this!\n",
        "    # D = np.maximum(softplus(D), eps)\n",
        "    D = softplus(D) + eps\n",
        "    # Build a skew-symmetric matrix\n",
        "    S = np.zeros((dim, dim))\n",
        "    i1, i2 = np.tril_indices(dim - 1)\n",
        "    S = S.at[i1+1, i2].set(A)\n",
        "    S = S.T\n",
        "    S = S.at[i1+1, i2].set(-A)\n",
        "\n",
        "    O = scipy.linalg.expm(S)\n",
        "    J = O.T @ np.diag(D) @ O\n",
        "    return J\n",
        "\n",
        "# converts an (n, n) matrix \n",
        "# to an (n, n) matrix with singular values in (0, 1)\n",
        "def get_constrained_dynamics(A):\n",
        "    dim = A.shape[0]\n",
        "    diag = np.diag(A)\n",
        "    diag = sigmoid(diag)\n",
        "    # Build a skew-symmetric matrix\n",
        "    S = np.zeros((dim, dim))\n",
        "    i1, i2 = np.tril_indices(dim - 1)\n",
        "    S = S.at[i1+1, i2].set(A[i1+1, i2])\n",
        "    S = S.T\n",
        "    S = S.at[i1+1, i2].set(-A[i1+1, i2])\n",
        "    U = scipy.linalg.expm(S)\n",
        "\n",
        "    S = np.zeros((dim, dim))\n",
        "    i1, i2 = np.tril_indices(dim - 1)\n",
        "    S = S.at[i1+1, i2].set(A.T[i1+1, i2])\n",
        "    S = S.T\n",
        "    S = S.at[i1+1, i2].set(-A.T[i1+1, i2])\n",
        "    V = scipy.linalg.expm(S)\n",
        "\n",
        "    A = U @ np.diag(diag) @ V\n",
        "    return A, U, V\n",
        "\n",
        "def scale_singular_values(A):\n",
        "    _, s, _ = svd(A)\n",
        "    return A / (np.maximum(1, np.max(s)))\n",
        "\n",
        "def truncate_singular_values(A):\n",
        "    eps = 1e-3\n",
        "    u, s, vt = svd(A)\n",
        "    return u @ np.diag(np.clip(s, eps, 1)) @ vt\n",
        "\n",
        "# Assume that h has a batch shape here\n",
        "def sample_info_gaussian(seed, J, h):\n",
        "    # Avoid inversion.\n",
        "    # see https://github.com/mattjj/pybasicbayes/blob/master/pybasicbayes/util/stats.py#L117-L122\n",
        "    L = np.linalg.cholesky(J)\n",
        "    x = jr.normal(key=seed, shape=h.shape)\n",
        "    return solve_triangular(L,x.T,lower=True,trans='T').T \\\n",
        "        + np.linalg.solve(J,h.T).T\n",
        "\n",
        "def sample_info_gaussian_old(seed, J, h):\n",
        "    cov = np.linalg.inv(J)\n",
        "    loc = np.einsum(\"...ij,...j->...i\", cov, h)\n",
        "    return tfp.distributions.MultivariateNormalFullCovariance(\n",
        "        loc=loc, covariance_matrix=cov).sample(sample_shape=(), seed=seed)\n",
        "\n",
        "def random_rotation(seed, n, theta=None):\n",
        "    key1, key2 = jr.split(seed)\n",
        "\n",
        "    if theta is None:\n",
        "        # Sample a random, slow rotation\n",
        "        theta = 0.5 * np.pi * jr.uniform(key1)\n",
        "\n",
        "    if n == 1:\n",
        "        return jr.uniform(key1) * np.eye(1)\n",
        "\n",
        "    rot = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
        "    out = np.eye(n)\n",
        "    out = out.at[:2, :2].set(rot)\n",
        "    q = np.linalg.qr(jr.uniform(key2, shape=(n, n)))[0]\n",
        "    return q.dot(out).dot(q.T)\n",
        "\n",
        "# Computes ATQ-1A in a way that's guaranteed to be symmetric\n",
        "def inv_quad_form(Q, A):\n",
        "    sqrt_Q = np.linalg.cholesky(Q)\n",
        "    trm = solve_triangular(sqrt_Q, A, lower=True, check_finite=False)\n",
        "    return trm.T @ trm\n",
        "\n",
        "def inv_symmetric(Q):\n",
        "    sqrt_Q = np.linalg.cholesky(Q)\n",
        "    sqrt_Q_inv = np.linalg.inv(sqrt_Q)\n",
        "    return sqrt_Q_inv.T @ sqrt_Q_inv\n",
        "\n",
        "# Converts from (A, b, Q) to (J, L, h)\n",
        "def dynamics_to_tridiag(dynamics_params, T, D):\n",
        "    Q1, m1, A, Q, b = dynamics_params[\"Q1\"], \\\n",
        "        dynamics_params[\"m1\"], dynamics_params[\"A\"], \\\n",
        "        dynamics_params[\"Q\"], dynamics_params[\"b\"]\n",
        "    # diagonal blocks of precision matrix\n",
        "    J = np.zeros((T, D, D))\n",
        "    J = J.at[0].add(inv_symmetric(Q1))\n",
        "\n",
        "    J = J.at[:-1].add(inv_quad_form(Q, A))\n",
        "    J = J.at[1:].add(inv_symmetric(Q))\n",
        "    # lower diagonal blocks of precision matrix\n",
        "    L = -np.linalg.solve(Q, A)\n",
        "    L = np.tile(L[None, :, :], (T - 1, 1, 1))\n",
        "    # linear potential\n",
        "    h = np.zeros((T, D)) \n",
        "    h = h.at[0].add(np.linalg.solve(Q1, m1))\n",
        "    h = h.at[:-1].add(-np.dot(A.T, np.linalg.solve(Q, b)))\n",
        "    h = h.at[1:].add(np.linalg.solve(Q, b))\n",
        "    return { \"J\": J, \"L\": L, \"h\": h }\n",
        "\n",
        "# Helper function: solve a linear regression given expected sufficient statistics\n",
        "def fit_linear_regression(Ex, Ey, ExxT, EyxT, EyyT, En):\n",
        "    big_ExxT = np.row_stack([np.column_stack([ExxT, Ex]),\n",
        "                            np.concatenate( [Ex.T, np.array([En])])])\n",
        "    big_EyxT = np.column_stack([EyxT, Ey])\n",
        "    Cd = np.linalg.solve(big_ExxT, big_EyxT.T).T\n",
        "    C, d = Cd[:, :-1], Cd[:, -1]\n",
        "    R = (EyyT - 2 * Cd @ big_EyxT.T + Cd @ big_ExxT @ Cd.T) / En\n",
        "\n",
        "    # Manually symmetrize R\n",
        "    R = (R + R.T) / 2\n",
        "    return C, d, R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "metadata": {
        "cellView": "form",
        "id": "NeeYgySA0hPT"
      },
      "outputs": [],
      "source": [
        "# @title Experiment scheduler\n",
        "LINE_SEP = \"#\" * 42\n",
        "\n",
        "def dict_len(d):\n",
        "    if (type(d) == list):\n",
        "        return len(d)\n",
        "    else:\n",
        "        return dict_len(d[list(d.keys())[0]])\n",
        "\n",
        "def dict_map(d, func):\n",
        "    if type(d) == list:\n",
        "        return func(d)\n",
        "    elif type(d) == dict:\n",
        "        r = copy.deepcopy(d)\n",
        "        for key in d.keys():\n",
        "            r[key] = dict_map(r[key], func)\n",
        "            # Ignore all the Nones\n",
        "            if r[key] is None:\n",
        "                r.pop(key)\n",
        "        if len(r.keys()) == 0:\n",
        "            # There's no content\n",
        "            return None\n",
        "        else:\n",
        "            return r\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def dict_product(d1, d2):\n",
        "    l1, l2 = dict_len(d1), dict_len(d2)\n",
        "    def expand_list(d):\n",
        "        result = []\n",
        "        for item in d:\n",
        "            result.append(item)\n",
        "            result.extend([None] * (l2-1))\n",
        "        return result\n",
        "    def multiply_list(d):\n",
        "        return d * l1\n",
        "    result = dict_map(d1, expand_list)\n",
        "    additions = dict_map(d2, multiply_list)\n",
        "    return dict_update(result, additions)\n",
        "\n",
        "def dict_get(d, id):\n",
        "    return dict_map(d, lambda l: l[id])\n",
        "\n",
        "def dict_update(d, u):\n",
        "    if d is None:\n",
        "        d = dict()\n",
        "    for key in u.keys():\n",
        "        if type(u[key]) == dict:\n",
        "            d.update({\n",
        "                key: dict_update(d.get(key), u[key])\n",
        "            })\n",
        "        else:\n",
        "            d.update({key: u[key]})\n",
        "    return d\n",
        "\n",
        "# A standardized function that structures and schedules experiments\n",
        "# Can chain multiple variations of experiment parameters together\n",
        "def experiment_scheduler(run_params, dataset_getter, model_getter, train_func, \n",
        "                         logger_func=None, err_logger_func=None, \n",
        "                         run_variations=None, params_expander=None,\n",
        "                         on_error=None, continue_on_error=True, use_wandb=True):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        run_params: dict{\"dataset_params\"} \n",
        "            A large dictionary containing all relevant parameters to the run \n",
        "        dataset_getter: run_params -> dict{\"train_data\", [\"generative_model\"]}\n",
        "            A function that loads/samples a dataset\n",
        "        model_getter: run_params, data_dict -> model\n",
        "            A function that creates a model given parameters. Note that the model\n",
        "            could depend on the specifics of the dataset/generative model as well\n",
        "        train_func: model, data, run_params -> results\n",
        "            A function that contains the training loop. \n",
        "            TODO: later we might wanna open up this pipeline and customize further!\n",
        "        (optional) logger_func: results, run_params -> ()\n",
        "            A function that logs the current run.\n",
        "        (optional) err_logger_func: message, run_params -> ()\n",
        "            A function that is called when the run fails.\n",
        "        (optional) run_variations: dict{}\n",
        "            A nested dictionary where the leaves are lists of different parameters.\n",
        "            None means no change from parameters of the last run.\n",
        "        (optional) params_expander: dict{} -> dict{}\n",
        "            Turns high level parameters into specific low level parameters.\n",
        "    returns:\n",
        "        all_results: List<result>\n",
        "            A list containing results from all runs. Failed runs are indicated\n",
        "            with a None value.\n",
        "    \"\"\"\n",
        "    params_expander = params_expander or (lambda d: d)\n",
        "\n",
        "    num_runs = dict_len(run_variations) if run_variations else 1\n",
        "    params = copy.deepcopy(run_params)\n",
        "    print(\"Total number of runs: {}\".format(num_runs))\n",
        "    print(\"Base paramerters:\")\n",
        "    pprint(params)\n",
        "\n",
        "    global data_dict\n",
        "    all_results = []\n",
        "    all_models = []\n",
        "\n",
        "    def _single_run(data_out, model_out):\n",
        "        print(\"Loading dataset!\")\n",
        "        data_dict = dataset_getter(curr_params)\n",
        "        data_out.append(data_dict)\n",
        "        # Make a new model\n",
        "        model_dict = model_getter(curr_params, data_dict)\n",
        "        model_out.append(model_dict)\n",
        "        all_models.append(model_dict)\n",
        "        results = train_func(model_dict, data_dict, curr_params)\n",
        "        all_results.append(results)\n",
        "        if logger_func:\n",
        "            logger_func(results, curr_params, data_dict)\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        print(LINE_SEP)\n",
        "        print(\"Starting run #{}\".format(run))\n",
        "        print(LINE_SEP)\n",
        "        curr_variation = dict_get(run_variations, run)\n",
        "        if curr_variation is None:\n",
        "            if (run != 0):\n",
        "                print(\"Variation #{} is a duplicate, skipping run.\".format(run))\n",
        "                continue\n",
        "            curr_params = params_expander(params)\n",
        "        else:\n",
        "            print(\"Current parameter variation:\")\n",
        "            pprint(curr_variation)\n",
        "            curr_params = dict_update(params, curr_variation)\n",
        "            curr_params = params_expander(curr_params)\n",
        "            print(\"Current full parameters:\")\n",
        "            pprint(curr_params)\n",
        "            if curr_variation.get(\"dataset_params\"):\n",
        "                reload_data = True\n",
        "        # Hack to get the values even when they err out\n",
        "        data_out = []\n",
        "        model_out = []\n",
        "        if not continue_on_error:\n",
        "            _single_run(data_out, model_out)\n",
        "        else:\n",
        "            try:\n",
        "                _single_run(data_out, model_out)\n",
        "                if use_wandb: wandb.finish()\n",
        "            except:\n",
        "                all_results.append(None)\n",
        "                if (on_error): \n",
        "                    try:\n",
        "                        on_error(data_out[0], model_out[0])\n",
        "                    except:\n",
        "                        pass # Oh well...\n",
        "                print(\"Run errored out due to some the following reason:\")\n",
        "                traceback.print_exc()\n",
        "                if use_wandb: wandb.finish(exit_code=1)\n",
        "    return all_results, all_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OPHhcbZObuu"
      },
      "source": [
        "## Define the base SVAE object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {
        "id": "9Pj8Xn9jNBz1"
      },
      "outputs": [],
      "source": [
        "class SVAE:\n",
        "    def __init__(self,\n",
        "                 recognition=None, decoder=None, prior=None, posterior=None,\n",
        "                 input_dummy=None, latent_dummy=None):\n",
        "        \"\"\"\n",
        "        rec_net, dec_net, prior are all objects that take in parameters\n",
        "        rec_net.apply(params, data) returns Gaussian potentials (parameters)\n",
        "        dec_net.apply(params, latents) returns probability distributions\n",
        "        prior : SVAEPrior\n",
        "        \"\"\"\n",
        "        self.recognition = recognition\n",
        "        self.decoder = decoder\n",
        "        self.prior = prior\n",
        "        self.posterior = posterior\n",
        "        self.input_dummy = input_dummy\n",
        "        self.latent_dummy = latent_dummy\n",
        "\n",
        "    def init(self, key=None):\n",
        "        if key is None:\n",
        "            key = jr.PRNGKey(0)\n",
        "\n",
        "        rec_key, dec_key, prior_key, post_key = jr.split(key, 4)\n",
        "\n",
        "        return {\n",
        "            \"rec_params\": self.recognition.init(rec_key, self.input_dummy),\n",
        "            \"dec_params\": self.decoder.init(dec_key, self.latent_dummy),\n",
        "            \"prior_params\": self.prior.init(prior_key),\n",
        "            \"post_params\": self.posterior.init(post_key)\n",
        "        }\n",
        "\n",
        "    def kl_posterior_prior(self, posterior_params, prior_params, \n",
        "                           samples=None):\n",
        "        posterior = self.posterior.distribution(posterior_params)\n",
        "        prior = self.prior.distribution(prior_params)\n",
        "        if samples is None:\n",
        "            return posterior.kl_divergence(prior)\n",
        "        else:\n",
        "            return np.mean(posterior.log_prob(samples) - prior.log_prob(samples))\n",
        "\n",
        "    def elbo(self, key, data, model_params, sample_kl=False, **params):\n",
        "        rec_params = model_params[\"rec_params\"]\n",
        "        dec_params = model_params[\"dec_params\"]\n",
        "        prior_params = self.prior.get_constrained_params(model_params[\"prior_params\"])\n",
        "\n",
        "        # Mask out a large window of states\n",
        "        mask_size = params.get(\"mask_size\")\n",
        "        T = data.shape[0]\n",
        "        D = self.prior.latent_dims\n",
        "\n",
        "        mask = onp.ones((T,))\n",
        "        key, dropout_key = jr.split(key)\n",
        "        if mask_size:\n",
        "            # Potential dropout...!\n",
        "            # Use a trick to generate the mask without indexing with a tracer\n",
        "            start_id = jr.choice(dropout_key, T - mask_size + 1)\n",
        "            mask = np.array(np.arange(T) >= start_id) \\\n",
        "                 * np.array(np.arange(T) < start_id + mask_size)\n",
        "            mask = 1 - mask\n",
        "            if params.get(\"mask_type\") == \"potential\":\n",
        "                # This only works with svaes\n",
        "                potential = self.recognition.apply(rec_params, data)\n",
        "                # Uninformative potential\n",
        "                infinity = 1e5\n",
        "                uninf_potential = {\"mu\": np.zeros((T, D)), \n",
        "                                   \"Sigma\": np.tile(np.eye(D) * infinity, (T, 1, 1))}\n",
        "                # Replace masked parts with uninformative potentials\n",
        "                potential = tree_map(\n",
        "                    lambda t1, t2: np.einsum(\"i,i...->i...\", mask[:t1.shape[0]], t1) \n",
        "                                 + np.einsum(\"i,i...->i...\", 1-mask[:t2.shape[0]], t2), \n",
        "                    potential, \n",
        "                    uninf_potential)\n",
        "            else:\n",
        "                potential = self.recognition.apply(rec_params, \n",
        "                                                   np.einsum(\"t...,t->t...\", data, mask))\n",
        "        else:\n",
        "            # Don't do any masking\n",
        "            potential = self.recognition.apply(rec_params, data)\n",
        "\n",
        "        # Update: it makes more sense that inference is done in the posterior object\n",
        "        posterior_params = self.posterior.infer(prior_params, potential)\n",
        "        \n",
        "        # Take samples under the posterior\n",
        "        num_samples = params.get(\"obj_samples\") or 1\n",
        "        samples = self.posterior.sample(posterior_params, (num_samples,), key)\n",
        "        # and compute average ll\n",
        "\n",
        "        def likelihood_outputs(latent):\n",
        "            likelihood_dist = self.decoder.apply(dec_params, latent)\n",
        "            return likelihood_dist.mean(), likelihood_dist.log_prob(data)\n",
        "\n",
        "        mean, ells = vmap(likelihood_outputs)(samples)\n",
        "        # Take average over samples then sum the rest\n",
        "        ell = np.sum(np.mean(ells, axis=0))\n",
        "        # Compute kl from posterior to prior\n",
        "        if sample_kl:\n",
        "            kl = self.kl_posterior_prior(posterior_params, prior_params, \n",
        "                                         samples=samples)\n",
        "        else:\n",
        "            kl = self.kl_posterior_prior(posterior_params, prior_params)\n",
        "\n",
        "        elbo = ell - kl\n",
        "\n",
        "        return {\n",
        "            \"elbo\": elbo,\n",
        "            \"ell\": ell,\n",
        "            \"kl\": kl,\n",
        "            \"posterior_params\": posterior_params,\n",
        "            \"posterior_samples\": samples,\n",
        "            \"reconstruction\": mean,\n",
        "            \"mask\": mask\n",
        "        }\n",
        "\n",
        "    def compute_objective(self, key, data, model_params, **params):\n",
        "        results = self.elbo(key, data, model_params, **params)\n",
        "        results[\"objective\"] = results[\"elbo\"]\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {
        "cellView": "form",
        "id": "7ckaLRUL1QVb"
      },
      "outputs": [],
      "source": [
        "# @title The DeepLDS object (implements custom kl function)\n",
        "class DeepLDS(SVAE):\n",
        "    def kl_posterior_prior(self, posterior_params, prior_params, \n",
        "                           samples=None):\n",
        "        posterior = self.posterior.distribution(posterior_params)\n",
        "        prior = self.prior.distribution(prior_params)\n",
        "        if samples is None:\n",
        "            Ex = posterior.expected_states\n",
        "            ExxT = posterior.expected_states_squared\n",
        "            ExnxT = posterior.expected_states_next_states\n",
        "            Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "            Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "            J, L = prior_params[\"J\"], prior_params[\"L\"]\n",
        "\n",
        "            cross_entropy = -prior.log_prob(Ex)\n",
        "            cross_entropy += 0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "            cross_entropy += np.einsum(\"tij,tij->\", L, Sigmatnt)\n",
        "            return cross_entropy - posterior.entropy()\n",
        "            \n",
        "        else:\n",
        "            return np.mean(posterior.log_prob(samples) - prior.log_prob(samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "metadata": {
        "id": "Nre5PK2MPt2N"
      },
      "outputs": [],
      "source": [
        "# @title SVAE Prior object\n",
        "class SVAEPrior:\n",
        "    def init(self, key):\n",
        "        \"\"\"\n",
        "        Returns the initial prior parameters.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def distribution(self, prior_params):\n",
        "        \"\"\"\n",
        "        Returns a tfp distribution object\n",
        "        Takes constrained params\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def m_step(self, prior_params, posterior, post_params):\n",
        "        \"\"\"\n",
        "        Returns updated prior parameters.\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def sample(self, params, shape, key):\n",
        "        return self.distribution(\n",
        "            self.get_constrained_params(params)).sample(sample_shape=shape, seed=key)\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        return deepcopy(params)\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOPpeMzjj42H"
      },
      "source": [
        "## Information form (deprecated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {
        "cellView": "form",
        "id": "NHkdKpcxOi1S"
      },
      "outputs": [],
      "source": [
        "# @title MVN tridiag object (taken from ssm)\n",
        "from tensorflow_probability.python.internal import reparameterization\n",
        "\n",
        "def block_tridiag_mvn_log_normalizer(J_diag, J_lower_diag, h):\n",
        "    \"\"\" TODO\n",
        "    \"\"\"\n",
        "    # extract dimensions\n",
        "    num_timesteps, dim = J_diag.shape[:2]\n",
        "\n",
        "    # Pad the L's with one extra set of zeros for the last predict step\n",
        "    J_lower_diag_pad = np.concatenate((J_lower_diag, np.zeros((1, dim, dim))), axis=0)\n",
        "\n",
        "    def marginalize(carry, t):\n",
        "        Jp, hp, lp = carry\n",
        "\n",
        "        # Condition\n",
        "        Jc = J_diag[t] + Jp\n",
        "        hc = h[t] + hp\n",
        "\n",
        "        # Predict -- Cholesky approach seems unstable!\n",
        "        # sqrt_Jc = np.linalg.cholesky(Jc)\n",
        "        # trm1 = solve_triangular(sqrt_Jc, hc, lower=True)\n",
        "        # trm2 = solve_triangular(sqrt_Jc, J_lower_diag_pad[t].T, lower=True)\n",
        "        # log_Z = 0.5 * dim * np.log(2 * np.pi)\n",
        "        # log_Z += -np.sum(np.log(np.diag(sqrt_Jc)))  # sum these terms only to get approx log|J|\n",
        "        # log_Z += 0.5 * np.dot(trm1.T, trm1)\n",
        "        # Jp = -np.dot(trm2.T, trm2)\n",
        "        # hp = -np.dot(trm2.T, trm1)\n",
        "\n",
        "        # Alternative predict step:\n",
        "        log_Z = 0.5 * dim * np.log(2 * np.pi)\n",
        "        log_Z += -0.5 * np.linalg.slogdet(Jc)[1]\n",
        "        log_Z += 0.5 * np.dot(hc, np.linalg.solve(Jc, hc))\n",
        "        Jp = -np.dot(J_lower_diag_pad[t], np.linalg.solve(Jc, J_lower_diag_pad[t].T))\n",
        "        # Jp = (Jp + Jp.T) * .5   # Manual symmetrization\n",
        "        hp = -np.dot(J_lower_diag_pad[t], np.linalg.solve(Jc, hc))\n",
        "\n",
        "        new_carry = Jp, hp, lp + log_Z\n",
        "        return new_carry, (Jc, hc)\n",
        "\n",
        "    # Initialize\n",
        "    Jp0 = np.zeros((dim, dim))\n",
        "    hp0 = np.zeros((dim,))\n",
        "    (_, _, log_Z), (filtered_Js, filtered_hs) = lax.scan(marginalize, (Jp0, hp0, 0), np.arange(num_timesteps))\n",
        "    return log_Z, (filtered_Js, filtered_hs)\n",
        "\n",
        "class MultivariateNormalBlockTridiag(tfd.Distribution):\n",
        "    \"\"\"\n",
        "    The Gaussian linear dynamical system's posterior distribution over latent states\n",
        "    is a multivariate normal distribution whose _precision_ matrix is\n",
        "    block tridiagonal.\n",
        "\n",
        "        x | y ~ N(\\mu, \\Sigma)\n",
        "\n",
        "    where\n",
        "\n",
        "        \\Sigma^{-1} = J = [[J_{0,0},   J_{0,1},   0,       0,      0],\n",
        "                           [J_{1,0},   J_{1,1},   J_{1,2}, 0,      0],\n",
        "                           [0,         J_{2,1},   J_{2,2}, \\ddots, 0],\n",
        "                           [0,         0,         \\ddots,  \\ddots,  ],\n",
        "\n",
        "    is block tridiagonal, and J_{t, t+1} = J_{t+1, t}^T.\n",
        "\n",
        "    The pdf is\n",
        "\n",
        "        p(x) = exp \\{-1/2 x^T J x + x^T h - \\log Z(J, h) \\}\n",
        "             = exp \\{- 1/2 \\sum_{t=1}^T x_t^T J_{t,t} x_t\n",
        "                     - \\sum_{t=1}^{T-1} x_{t+1}^T J_{t+1,t} x_t\n",
        "                     + \\sum_{t=1}^T x_t^T h_t\n",
        "                     -\\log Z(J, h)\\}\n",
        "\n",
        "    where J = \\Sigma^{-1} and h = \\Sigma^{-1} \\mu = J \\mu.\n",
        "\n",
        "    Using exponential family tricks we know that\n",
        "\n",
        "        E[x_t] = \\grad_{h_t} \\log Z(J, h)\n",
        "        E[x_t x_t^T] = -2 \\grad_{J_{t,t}} \\log Z(J, h)\n",
        "        E[x_{t+1} x_t^T] = -\\grad_{J_{t+1,t}} \\log Z(J, h)\n",
        "\n",
        "    These are the expectations we need for EM.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 precision_diag_blocks,\n",
        "                 precision_lower_diag_blocks,\n",
        "                 linear_potential,\n",
        "                 log_normalizer,\n",
        "                 filtered_precisions,\n",
        "                 filtered_linear_potentials,\n",
        "                 expected_states,\n",
        "                 expected_states_squared,\n",
        "                 expected_states_next_states,\n",
        "                 validate_args=False,\n",
        "                 allow_nan_stats=True,\n",
        "                 name=\"MultivariateNormalBlockTridiag\",\n",
        "             ) -> None:\n",
        "\n",
        "        self._precision_diag_blocks = precision_diag_blocks\n",
        "        self._precision_lower_diag_blocks = precision_lower_diag_blocks\n",
        "        self._linear_potential = linear_potential\n",
        "        self._log_normalizer = log_normalizer\n",
        "        self._filtered_precisions = filtered_precisions\n",
        "        self._filtered_linear_potentials = filtered_linear_potentials\n",
        "        self._expected_states = expected_states\n",
        "        self._expected_states_squared = expected_states_squared\n",
        "        self._expected_states_next_states = expected_states_next_states\n",
        "\n",
        "        # We would detect the dtype dynamically but that would break vmap\n",
        "        # see https://github.com/tensorflow/probability/issues/1271\n",
        "        dtype = np.float32\n",
        "        super(MultivariateNormalBlockTridiag, self).__init__(\n",
        "            dtype=dtype,\n",
        "            validate_args=validate_args,\n",
        "            allow_nan_stats=allow_nan_stats,\n",
        "            reparameterization_type=reparameterization.NOT_REPARAMETERIZED,\n",
        "            parameters=dict(precision_diag_blocks=self._precision_diag_blocks,\n",
        "                            precision_lower_diag_blocks=self._precision_lower_diag_blocks,\n",
        "                            linear_potential=self._linear_potential,\n",
        "                            log_normalizer=self._log_normalizer,\n",
        "                            filtered_precisions=self._filtered_precisions,\n",
        "                            filtered_linear_potentials=self._filtered_linear_potentials,\n",
        "                            expected_states=self._expected_states,\n",
        "                            expected_states_squared=self._expected_states_squared,\n",
        "                            expected_states_next_states=self._expected_states_next_states),\n",
        "            name=name,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _parameter_properties(cls, dtype, num_classes=None):\n",
        "        # pylint: disable=g-long-lambda\n",
        "        return dict(\n",
        "            precision_diag_blocks=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            precision_lower_diag_blocks=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            linear_potential=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "            log_normalizer=tfp.internal.parameter_properties.ParameterProperties(event_ndims=0),\n",
        "            filtered_precisions=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            filtered_linear_potentials=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "            expected_states=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "            expected_states_squared=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            expected_states_next_states=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def infer(cls,\n",
        "              precision_diag_blocks,\n",
        "              precision_lower_diag_blocks,\n",
        "              linear_potential):\n",
        "        assert precision_diag_blocks.ndim == 3\n",
        "        num_timesteps, dim = precision_diag_blocks.shape[:2]\n",
        "        assert precision_diag_blocks.shape[2] == dim\n",
        "        assert precision_lower_diag_blocks.shape == (num_timesteps - 1, dim, dim)\n",
        "        assert linear_potential.shape == (num_timesteps, dim)\n",
        "\n",
        "        # Run message passing code to get the log normalizer, the filtering potentials,\n",
        "        # and the expected values of x. Technically, the natural parameters are -1/2 J\n",
        "        # so we need to do a little correction of the gradients to get the expectations.\n",
        "        f = value_and_grad(block_tridiag_mvn_log_normalizer, argnums=(0, 1, 2), has_aux=True)\n",
        "        (log_normalizer, (filtered_precisions, filtered_linear_potentials)), grads = \\\n",
        "            f(precision_diag_blocks, precision_lower_diag_blocks, linear_potential)\n",
        "\n",
        "        # Manually symmetrize ExxT due to numerical issues...!!!\n",
        "        # Correct for the -1/2 J -> J implementation\n",
        "        expected_states_squared = - grads[0] - np.swapaxes(grads[0], -2, -1)\n",
        "        expected_states_next_states = -grads[1]\n",
        "        expected_states = grads[2]\n",
        "\n",
        "        return cls(precision_diag_blocks,\n",
        "                   precision_lower_diag_blocks,\n",
        "                   linear_potential,\n",
        "                   log_normalizer,\n",
        "                   filtered_precisions,\n",
        "                   filtered_linear_potentials,\n",
        "                   expected_states,\n",
        "                   expected_states_squared,\n",
        "                   expected_states_next_states)\n",
        "\n",
        "    @classmethod\n",
        "    def infer_from_precision_and_mean(cls,\n",
        "                                      precision_diag_blocks,\n",
        "                                      precision_lower_diag_blocks,\n",
        "                                      mean):\n",
        "        assert precision_diag_blocks.ndim == 3\n",
        "        num_timesteps, dim = precision_diag_blocks.shape[:2]\n",
        "        assert precision_diag_blocks.shape[2] == dim\n",
        "        assert precision_lower_diag_blocks.shape == (num_timesteps - 1, dim, dim)\n",
        "        assert mean.shape == (num_timesteps, dim)\n",
        "\n",
        "        # Convert the mean to the linear potential\n",
        "        linear_potential = np.einsum('tij,tj->ti', precision_diag_blocks, mean)\n",
        "        linear_potential = linear_potential.at[:-1].add(\n",
        "            np.einsum('tji,tj->ti', precision_lower_diag_blocks, mean[1:]))\n",
        "        linear_potential = linear_potential.at[1:].add(\n",
        "            np.einsum('tij,tj->ti', precision_lower_diag_blocks, mean[:-1]))\n",
        "\n",
        "        # Call the constructor above\n",
        "        return cls.infer(precision_diag_blocks,\n",
        "                         precision_lower_diag_blocks,\n",
        "                         linear_potential)\n",
        "\n",
        "    # Properties to get private class variables\n",
        "    @property\n",
        "    def precision_diag_blocks(self):\n",
        "        return self._precision_diag_blocks\n",
        "\n",
        "    @property\n",
        "    def precision_lower_diag_blocks(self):\n",
        "        return self._precision_lower_diag_blocks\n",
        "\n",
        "    @property\n",
        "    def linear_potential(self):\n",
        "        return self._linear_potential\n",
        "\n",
        "    @property\n",
        "    def log_normalizer(self):\n",
        "        return self._log_normalizer\n",
        "\n",
        "    @property\n",
        "    def filtered_precisions(self):\n",
        "        return self._filtered_precisions\n",
        "\n",
        "    @property\n",
        "    def filtered_linear_potentials(self):\n",
        "        return self._filtered_linear_potentials\n",
        "\n",
        "    @property\n",
        "    def filtered_covariances(self):\n",
        "        return inv(self._filtered_precisions)\n",
        "\n",
        "    @property\n",
        "    def filtered_means(self):\n",
        "        # TODO: this is bad numerically\n",
        "        return np.einsum(\"...ij,...j->...i\", self.filtered_covariances, \n",
        "                         self.filtered_linear_potentials)\n",
        "\n",
        "    @property\n",
        "    def expected_states(self):\n",
        "        return self._expected_states\n",
        "\n",
        "    @property\n",
        "    def expected_states_squared(self):\n",
        "        return self._expected_states_squared\n",
        "\n",
        "    @property\n",
        "    def expected_states_next_states(self):\n",
        "        return self._expected_states_next_states\n",
        "\n",
        "    def _log_prob(self, data, **kwargs):\n",
        "        lp = -0.5 * np.einsum('...ti,...tij,...tj->...', data, self._precision_diag_blocks, data)\n",
        "        lp += -np.einsum('...ti,...tij,...tj->...', data[...,1:,:], self._precision_lower_diag_blocks, data[...,:-1,:])\n",
        "        lp += np.einsum('...ti,...ti->...', data, self._linear_potential)\n",
        "        lp -= self.log_normalizer\n",
        "        return lp\n",
        "\n",
        "    def _mean(self):\n",
        "        return self.expected_states\n",
        "\n",
        "    def _covariance(self):\n",
        "        \"\"\"\n",
        "        NOTE: This computes the _marginal_ covariance Cov[x_t] for each t\n",
        "        \"\"\"\n",
        "        Ex = self._expected_states\n",
        "        ExxT = self._expected_states_squared\n",
        "        return ExxT - np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "\n",
        "    def _sample_n(self, n, seed=None):\n",
        "        filtered_Js = self._filtered_precisions\n",
        "        filtered_hs = self._filtered_linear_potentials\n",
        "        J_lower_diag = self._precision_lower_diag_blocks\n",
        "\n",
        "        def sample_single(seed, filtered_Js, filtered_hs, J_lower_diag):\n",
        "\n",
        "            def _sample_info_gaussian(seed, J, h, sample_shape=()):\n",
        "                # TODO: avoid inversion.\n",
        "                # see https://github.com/mattjj/pybasicbayes/blob/master/pybasicbayes/util/stats.py#L117-L122\n",
        "                # L = np.linalg.cholesky(J)\n",
        "                # x = np.random.randn(h.shape[0])\n",
        "                # return scipy.linalg.solve_triangular(L,x,lower=True,trans='T') \\\n",
        "                #     + dpotrs(L,h,lower=True)[0]\n",
        "                cov = np.linalg.inv(J)\n",
        "                loc = np.einsum(\"...ij,...j->...i\", cov, h)\n",
        "                return tfp.distributions.MultivariateNormalFullCovariance(\n",
        "                    loc=loc, covariance_matrix=cov).sample(sample_shape=sample_shape, seed=seed)\n",
        "\n",
        "            def _step(carry, inpt):\n",
        "                x_next, seed = carry\n",
        "                Jf, hf, L = inpt\n",
        "\n",
        "                # Condition on the next observation\n",
        "                Jc = Jf\n",
        "                hc = hf - np.einsum('ni,ij->nj', x_next, L)\n",
        "\n",
        "                # Split the seed\n",
        "                seed, this_seed = jr.split(seed)\n",
        "                x = _sample_info_gaussian(this_seed, Jc, hc)\n",
        "                return (x, seed), x\n",
        "\n",
        "            # Initialize with sample of last timestep and sample in reverse\n",
        "            seed_T, seed = jr.split(seed)\n",
        "            x_T = _sample_info_gaussian(seed_T, filtered_Js[-1], filtered_hs[-1], sample_shape=(n,))\n",
        "            inputs = (filtered_Js[:-1][::-1], filtered_hs[:-1][::-1], J_lower_diag[::-1])\n",
        "            _, x_rev = lax.scan(_step, (x_T, seed), inputs)\n",
        "\n",
        "            # Reverse and concatenate the last time-step's sample\n",
        "            x = np.concatenate((x_rev[::-1], x_T[None, ...]), axis=0)\n",
        "\n",
        "            # Transpose to be (num_samples, num_timesteps, dim)\n",
        "            return np.transpose(x, (1, 0, 2))\n",
        "\n",
        "        # TODO: Handle arbitrary batch shapes\n",
        "        if filtered_Js.ndim == 4:\n",
        "            # batch mode\n",
        "            samples = vmap(sample_single)(seed, filtered_Js, filtered_hs, J_lower_diag)\n",
        "            # Transpose to be (num_samples, num_batches, num_timesteps, dim)\n",
        "            samples = np.transpose(samples, (1, 0, 2, 3))\n",
        "        else:\n",
        "            # non-batch mode\n",
        "            samples = sample_single(seed, filtered_Js, filtered_hs, J_lower_diag)\n",
        "        return samples\n",
        "\n",
        "    def _entropy(self):\n",
        "        \"\"\"\n",
        "        Compute the entropy\n",
        "\n",
        "            H[X] = -E[\\log p(x)]\n",
        "                 = -E[-1/2 x^T J x + x^T h - log Z(J, h)]\n",
        "                 = 1/2 <J, E[x x^T] - <h, E[x]> + log Z(J, h)\n",
        "        \"\"\"\n",
        "        Ex = self._expected_states\n",
        "        ExxT = self._expected_states_squared\n",
        "        ExnxT = self._expected_states_next_states\n",
        "        J_diag = self._precision_diag_blocks\n",
        "        J_lower_diag = self._precision_lower_diag_blocks\n",
        "        h = self._linear_potential\n",
        "\n",
        "        entropy = 0.5 * np.sum(J_diag * ExxT)\n",
        "        entropy += np.sum(J_lower_diag * ExnxT)\n",
        "        entropy -= np.sum(h * Ex)\n",
        "        entropy += self.log_normalizer\n",
        "        return entropy\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        children = (self._precision_diag_blocks,\n",
        "                    self._precision_lower_diag_blocks,\n",
        "                    self._linear_potential,\n",
        "                    self._log_normalizer,\n",
        "                    self._filtered_precisions,\n",
        "                    self._filtered_linear_potentials,\n",
        "                    self._expected_states,\n",
        "                    self._expected_states_squared,\n",
        "                    self._expected_states_next_states)\n",
        "        aux_data = None\n",
        "        return children, aux_data\n",
        "\n",
        "    @classmethod\n",
        "    def tree_unflatten(cls, aux_data, children):\n",
        "        return cls(*children)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bss7PlIgQRDp"
      },
      "source": [
        "## Important distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {
        "cellView": "form",
        "id": "KZ0Avt4Ad8G-"
      },
      "outputs": [],
      "source": [
        "# @title Linear Gaussian chain distribution object\n",
        "# As a prior distribution, we only need to be able to 1) Evaluate log prob 2) sample\n",
        "# As a posterior distribution, we also need to figure out the sufficient stats (Ex, ExxT, ExnxT)\n",
        "class LinearGaussianChain:\n",
        "    def __init__(self, dynamics_matrix, dynamics_bias, noise_covariance,\n",
        "                 expected_states, expected_states_squared, expected_states_next_states):\n",
        "        \"\"\"\n",
        "        params: dictionary containing the following keys:\n",
        "            A:  (seq_len, dim, dim)\n",
        "            Q:  (seq_len, dim, dim)\n",
        "            b:  (seq_len, dim)\n",
        "        \"\"\"\n",
        "        self._dynamics_matrix = dynamics_matrix\n",
        "        self._dynamics_bias = dynamics_bias\n",
        "        self._noise_covariance = noise_covariance\n",
        "        self._expected_states = expected_states\n",
        "        self._expected_states_squared = expected_states_squared\n",
        "        self._expected_states_next_states = expected_states_next_states\n",
        "\n",
        "    @classmethod\n",
        "    def from_stationary_dynamics(cls, m1, Q1, A, b, Q, T):\n",
        "        dynamics_matrix = np.tile(A[None], (T, 1, 1))\n",
        "        dynamics_bias = np.concatenate([m1[None], \n",
        "                                        np.tile(b[None], (T-1, 1))])\n",
        "        noise_covariance = np.concatenate([Q1[None], \n",
        "                                           np.tile(Q[None], (T-1, 1, 1))])\n",
        "        return cls.from_nonstationary_dynamics(dynamics_matrix, dynamics_bias, noise_covariance)\n",
        "    \n",
        "    @classmethod\n",
        "    def from_nonstationary_dynamics(cls, dynamics_matrix, dynamics_bias, noise_covariance):\n",
        "        # Compute the means and covariances via parallel scan\n",
        "        init_elems = (dynamics_matrix, dynamics_bias, noise_covariance)\n",
        "\n",
        "        @vmap\n",
        "        def assoc_op(elem1, elem2):\n",
        "            A1, b1, Q1 = elem1\n",
        "            A2, b2, Q2 = elem2\n",
        "            return A2 @ A1, A2 @ b1 + b2, A2 @ Q1 @ A2.T + Q2\n",
        "\n",
        "        _, Ex, covariances = lax.associative_scan(assoc_op, init_elems)\n",
        "        expected_states = Ex\n",
        "        expected_states_squared = covariances + np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "        expected_states_next_states = np.einsum(\"...ij,...jk->...ik\", \n",
        "            covariances[:-1], dynamics_matrix[1:]) + np.einsum(\"...i,...j->...ji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "        return cls(dynamics_matrix, dynamics_bias, noise_covariance,\n",
        "                   expected_states, expected_states_squared, expected_states_next_states)\n",
        "\n",
        "    @property\n",
        "    def mean(self):\n",
        "        return self._expected_states\n",
        "\n",
        "    @property\n",
        "    def covariance(self):\n",
        "        Ex = self._expected_states\n",
        "        ExxT = self._expected_states_squared\n",
        "        return ExxT - np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "        \n",
        "    @property\n",
        "    def expected_states(self):\n",
        "        return self._expected_states\n",
        "\n",
        "    @property\n",
        "    def expected_states_squared(self):\n",
        "        return self._expected_states_squared\n",
        "\n",
        "    @property\n",
        "    def expected_states_next_states(self):\n",
        "        return self._expected_states_next_states\n",
        "\n",
        "    # Works with batched distributions and arguments...!\n",
        "    def log_prob(self, xs):\n",
        "\n",
        "        @partial(np.vectorize, signature=\"(t,d,d),(t,d),(t,d,d),(t,d)->()\")\n",
        "        def log_prob_single(A, b, Q, x):\n",
        "            ll = MVN(loc=b[0], covariance_matrix=Q[0]).log_prob(x[0])\n",
        "            ll += MVN(loc=np.einsum(\"tij,tj->ti\", A[1:], x[:-1]) + b[1:], \n",
        "                      covariance_matrix=Q[1:]).log_prob(x[1:]).sum()\n",
        "            return ll\n",
        "\n",
        "        return log_prob_single(self._dynamics_matrix,\n",
        "                               self._dynamics_bias, \n",
        "                               self._noise_covariance, xs)\n",
        "        \n",
        "    # Only supports 0d and 1d sample shapes\n",
        "    # Does not support sampling with batched object\n",
        "    def sample(self, seed, sample_shape=()):\n",
        "\n",
        "        @partial(np.vectorize, signature=\"(n),(t,d,d),(t,d),(t,d,d)->(t,d)\")\n",
        "        def sample_single(key, A, b, Q):\n",
        "\n",
        "            biases = MVN(loc=b, covariance_matrix=Q).sample(seed=key)\n",
        "            init_elems = (A, biases)\n",
        "\n",
        "            @vmap\n",
        "            def assoc_op(elem1, elem2):\n",
        "                A1, b1 = elem1\n",
        "                A2, b2 = elem2\n",
        "                return A2 @ A1, A2 @ b1 + b2\n",
        "\n",
        "            _, sample = lax.associative_scan(assoc_op, init_elems)\n",
        "            return sample\n",
        "        \n",
        "        if (len(sample_shape) == 0):\n",
        "            return sample_single(seed, self._dynamics_matrix,\n",
        "                                 self._dynamics_bias, \n",
        "                                 self._noise_covariance)\n",
        "        elif (len(sample_shape) == 1):\n",
        "            return sample_single(jr.split(seed, sample_shape[0]),\n",
        "                                 self._dynamics_matrix[None],\n",
        "                                 self._dynamics_bias[None],\n",
        "                                 self._noise_covariance[None]) \n",
        "        else:\n",
        "            raise Exception(\"More than one sample dimensions are not supported!\")\n",
        "\n",
        "    def entropy(self):\n",
        "        \"\"\"\n",
        "        Compute the entropy\n",
        "\n",
        "            H[X] = -E[\\log p(x)]\n",
        "                 = -E[-1/2 x^T J x + x^T h - log Z(J, h)]\n",
        "                 = 1/2 <J, E[x x^T] - <h, E[x]> + log Z(J, h)\n",
        "        \"\"\"\n",
        "        Ex = self.expected_states\n",
        "        ExxT = self.expected_states_squared\n",
        "        ExnxT = self.expected_states_next_states\n",
        "        \n",
        "        dim = Ex.shape[-1]        \n",
        "        Q_inv = solve(self._noise_covariance, np.eye(dim)[None])\n",
        "        A = self._dynamics_matrix\n",
        "\n",
        "        J_lower_diag = np.einsum(\"til,tlj->tij\", -Q_inv[1:], A[1:])\n",
        "        ATQinvA = np.einsum(\"tji,tjl,tlk->tik\", A[1:], Q_inv[1:], A[1:])\n",
        "        J_diag = Q_inv.at[:-1].add(ATQinvA)\n",
        "\n",
        "        Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "        Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "        trm1 = 0.5 * np.sum(J_diag * Sigmatt)\n",
        "        trm2 = np.sum(J_lower_diag * Sigmatnt)\n",
        "\n",
        "        return trm1 + trm2 - self.log_prob(Ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {
        "cellView": "form",
        "id": "lBVe4-YTOQLp"
      },
      "outputs": [],
      "source": [
        "# @title Testing the correctness of the linear Gaussian chain\n",
        "# jax.config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "# D = 3\n",
        "# T = 200\n",
        "\n",
        "# mu1 = np.zeros(D)\n",
        "# Q1 = np.eye(D)\n",
        "# # TODO: A could be a sensitive parameter\n",
        "# # A = jnp.linspace(0.001, 1.0, D)\n",
        "# A = random_rotation(key_0, D, np.pi / 20)\n",
        "# b = jr.normal(key_0, shape=(D,))#np.zeros(D)\n",
        "# Q = np.eye(D)\n",
        "\n",
        "# C = np.eye(D)\n",
        "# d = np.zeros(D)\n",
        "\n",
        "# dist = LinearGaussianChain.from_stationary_dynamics(mu1, Q1, A, b, Q, T)\n",
        "# dist.entropy()\n",
        "# # Sigmas = np.tile(0.01 * np.eye(D), (T, 1, 1))\n",
        "\n",
        "# p = dynamics_to_tridiag({\"m1\": mu1, \"Q1\": Q1, \"A\": A, \"b\": b, \"Q\": Q}, T, D)\n",
        "# dist_ = MultivariateNormalBlockTridiag.infer(p[\"J\"], p[\"L\"], p[\"h\"])\n",
        "\n",
        "# ExnxT_ = dist_.expected_states_next_states\n",
        "# ExnxT = dist.expected_states_next_states\n",
        "# print(ExnxT - ExnxT_)\n",
        "# print(dist_.entropy() - dist.entropy())\n",
        "# print(dist.expected_states_squared - dist_.expected_states_squared)\n",
        "# posterior = LinearGaussianChain.from_stationary_dynamics(mu1, Q1, A, b, Q, T)\n",
        "# key_1 = jr.split(key_0)[0]\n",
        "# A1 = random_rotation(key_1, D, np.pi / 20)\n",
        "# b1 = jr.normal(key_1, shape=(D,))#np.zeros(D)\n",
        "# prior_params = dynamics_to_tridiag({\"m1\": mu1, \"Q1\": Q1, \"A\": A1, \"b\": b1, \"Q\": Q}, T, D)\n",
        "# prior = LinearGaussianChain.from_stationary_dynamics(mu1, Q1, A1, b1, Q, T)\n",
        "# Ex = posterior.expected_states\n",
        "# ExxT = posterior.expected_states_squared\n",
        "# ExnxT = posterior.expected_states_next_states\n",
        "# Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "# Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "# J, L = prior_params[\"J\"], prior_params[\"L\"]\n",
        "# cross_entropy = -prior.log_prob(Ex)\n",
        "# cross_entropy += 0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "# cross_entropy += np.einsum(\"tij,tij->\", L, Sigmatnt)\n",
        "# print(\"Closed form KL:\", cross_entropy - posterior.entropy())\n",
        "# samples = posterior.sample(key_1, (100,))\n",
        "# print(\"Sampled KL:\", np.mean(posterior.log_prob(samples) - prior.log_prob(samples)))\n",
        "# # return np.mean(posterior.log_prob(samples) - prior.log_prob(samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {
        "id": "6D7qIBFpRyH8"
      },
      "outputs": [],
      "source": [
        "# @title Linear Gaussian chain prior\n",
        "# This is a linear Gaussian chain\n",
        "class LinearGaussianChainPrior(SVAEPrior):\n",
        "\n",
        "    def __init__(self, latent_dims, seq_len):\n",
        "        self.latent_dims = latent_dims\n",
        "        # The only annoying thing is that we have to specify the sequence length\n",
        "        # ahead of time\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return (self.seq_len, self.latent_dims)\n",
        "\n",
        "    # Must be the full set of constrained parameters!\n",
        "    def distribution(self, params):\n",
        "        As, bs, Qs = params[\"As\"], params[\"bs\"], params[\"Qs\"]\n",
        "        Ex, ExxT, ExnxT = params[\"Ex\"], params[\"ExxT\"], params[\"ExnxT\"]\n",
        "        return LinearGaussianChain(As, bs, Qs, Ex, ExxT, ExnxT)\n",
        "\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        params = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"Q1\": np.eye(D),\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": np.eye(D)\n",
        "        }\n",
        "        constrained = self.get_constrained_params(params)\n",
        "        return params\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        return params\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        tridiag = dynamics_to_tridiag(params, self.seq_len, self.latent_dims)\n",
        "        p.update(tridiag)\n",
        "        dist = LinearGaussianChain.from_stationary_dynamics(p[\"m1\"], p[\"Q1\"], \n",
        "                                         p[\"A\"], p[\"b\"], p[\"Q\"], self.seq_len)\n",
        "        p.update({\n",
        "            \"As\": dist._dynamics_matrix,\n",
        "            \"bs\": dist._dynamics_bias,\n",
        "            \"Qs\": dist._noise_covariance,\n",
        "            \"Ex\": dist.expected_states,\n",
        "            \"ExxT\": dist.expected_states_squared,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    # This is pretty much deprecated since we're using sgd\n",
        "    def m_step(self, prior_params):\n",
        "        suff_stats = prior_params[\"avg_suff_stats\"]\n",
        "        ExxT = suff_stats[\"ExxT\"]\n",
        "        ExnxT = suff_stats[\"ExnxT\"]\n",
        "        Ex = suff_stats[\"Ex\"]\n",
        "        seq_len = Ex.shape[0]\n",
        "        # Update the initials\n",
        "        m1 = Ex[0]\n",
        "        Q1 = ExxT[0] - np.outer(m1, m1)\n",
        "        D = self.latent_dims\n",
        "        A, b, Q = fit_linear_regression(Ex[:-1].sum(axis=0), \n",
        "                                        Ex[1:].sum(axis=0), \n",
        "                                        ExxT[:-1].sum(axis=0), \n",
        "                                        ExnxT.sum(axis=0), \n",
        "                                        ExxT[1:].sum(axis=0), \n",
        "                                        seq_len - 1)\n",
        "        out = { \"m1\": m1, \"Q1\": Q1, \"A\": A, \"b\": b, \"Q\": Q }\n",
        "        out[\"avg_suff_stats\"] = deepcopy(suff_stats)\n",
        "        return out\n",
        "\n",
        "# This is a bit clumsy but it's the best we can do without using some sophisticated way\n",
        "# Of marking the constrained/optimized parameters vs. unconstrained parameters\n",
        "class LieParameterizedLinearGaussianChainPrior(LinearGaussianChainPrior):\n",
        "    def init(self, key):\n",
        "        D = self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        # Equivalent to the unit matrix\n",
        "        Q_flat = np.concatenate([np.ones(D) * inv_softplus(1), np.zeros((D*(D-1)//2))])\n",
        "        params = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"Q1\": Q_flat,\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": Q_flat\n",
        "        }\n",
        "        return params\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        return {\n",
        "            \"m1\": params[\"m1\"],\n",
        "            \"Q1\": lie_params_to_constrained(params[\"Q1\"], self.latent_dims),\n",
        "            \"A\": params[\"A\"],\n",
        "            \"b\": params[\"b\"],\n",
        "            \"Q\": lie_params_to_constrained(params[\"Q\"], self.latent_dims)   \n",
        "        }\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        D = self.latent_dims\n",
        "        p = self.get_dynamics_params(params)\n",
        "        return super().get_constrained_params(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "metadata": {
        "id": "6c7K9dJWzyG8"
      },
      "outputs": [],
      "source": [
        "# @title Linear Gaussian chain posteriors\n",
        "\n",
        "# The infer function for the DKF version just uses the posterior params \n",
        "class CDKFPosterior(LinearGaussianChainPrior):\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        params = {\n",
        "            \"As\": np.zeros((T, D, D)), \n",
        "            \"bs\": np.zeros((T, D)),\n",
        "            \"Qs\": np.tile(np.eye(D)[None], (T, 1, 1))\n",
        "        }\n",
        "        return self.get_constrained_params(params)\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        dist = LinearGaussianChain.from_nonstationary_dynamics(p[\"As\"], p[\"bs\"], p[\"Qs\"])\n",
        "        p.update({\n",
        "            \"Ex\": dist.expected_states,\n",
        "            \"ExxT\": dist.expected_states_squared,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    def sufficient_statistics(self, params):\n",
        "        return {\n",
        "            \"Ex\": params[\"Ex\"],\n",
        "            \"ExxT\": params[\"ExxT\"],\n",
        "            \"ExnxT\": params[\"ExnxT\"]\n",
        "        }\n",
        "\n",
        "    def infer(self, prior_params, posterior_params):\n",
        "        return self.get_constrained_params(posterior_params)\n",
        "\n",
        "class DKFPosterior(CDKFPosterior):\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        # The DKF produces a factored posterior\n",
        "        # So the dynamics matrix is zeroed out\n",
        "        p[\"As\"] *= 0\n",
        "        dist = LinearGaussianChain.from_nonstationary_dynamics(p[\"As\"], p[\"bs\"], p[\"Qs\"])\n",
        "        p.update({\n",
        "            \"Ex\": dist.expected_states,\n",
        "            \"ExxT\": dist.expected_states_squared,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "metadata": {
        "cellView": "form",
        "id": "Rbe3PsAIHliY"
      },
      "outputs": [],
      "source": [
        "# @title PlaNet type posterior\n",
        "# The infer function for the DKF version just uses the posterior params \n",
        "# TODO: Put the dummies in the params dictionary as well\n",
        "class PlaNetPosterior(DKFPosterior):\n",
        "    def __init__(self, network_params, latent_dims, seq_len):\n",
        "        super().__init__(latent_dims, seq_len)\n",
        "        self.network = StochasticRNNCell.from_params(**network_params)\n",
        "        self.input_dim = network_params[\"input_dim\"]      # u\n",
        "        self.latent_dim = network_params[\"rnn_dim\"]       # h\n",
        "        self.output_dim = network_params[\"output_dim\"]    # x\n",
        "\n",
        "    def init(self, key):\n",
        "        input_dummy = np.zeros((self.input_dim,))\n",
        "        latent_dummy = np.zeros((self.latent_dim,))\n",
        "        output_dummy = np.zeros((self.output_dim,))\n",
        "        rnn_params = self.network.init(key, input_dummy, latent_dummy, output_dummy)\n",
        "        return {\n",
        "            \"rnn_params\": rnn_params,\n",
        "            \"input_dummy\": input_dummy,\n",
        "            \"latent_dummy\": latent_dummy,\n",
        "            \"output_dummy\": output_dummy,\n",
        "        }\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        # All of the information is stored in the second argument already\n",
        "        return params\n",
        "\n",
        "    def distribution(self, params):\n",
        "        return DeepAutoregressiveDynamics(self.network, params)\n",
        "        \n",
        "    # These are just dummies\n",
        "    def sufficient_statistics(self, params):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        return {\n",
        "            \"Ex\": np.zeros((T, D)),\n",
        "            \"ExxT\": np.zeros((T, D, D)),\n",
        "            \"ExnxT\": np.zeros((T-1, D, D))\n",
        "        }\n",
        "\n",
        "# We only need to be able to 1) Evaluate log prob 2) sample\n",
        "# The tricky thing here is evaluating the \n",
        "class DeepAutoregressiveDynamics:\n",
        "\n",
        "    def __init__(self, network, params):\n",
        "        self.cell = network\n",
        "        self.params = params[\"network_params\"]\n",
        "        self.inputs = params[\"network_input\"]\n",
        "        # self.input_dummy = params[\"network_params\"][\"input_dummy\"]\n",
        "        # self.latent_dummy = params[\"network_params\"][\"latent_dummy\"]\n",
        "        # self.output_dummy = params[\"network_params\"][\"output_dummy\"]\n",
        "        self._mean = None\n",
        "        self._covariance = None\n",
        "\n",
        "    def mean(self):\n",
        "        if (self._mean is None):\n",
        "            self.compute_mean_and_cov()\n",
        "        return self._mean\n",
        "\n",
        "    def covariance(self):\n",
        "        if (self._covariance is None):\n",
        "            self.compute_mean_and_cov()\n",
        "        return self._covariance\n",
        "\n",
        "    @property\n",
        "    def filtered_covariances(self):\n",
        "        return self.covariance()\n",
        "\n",
        "    @property\n",
        "    def filtered_means(self):\n",
        "        return self.mean()\n",
        "        \n",
        "    def compute_mean_and_cov(self):\n",
        "        num_samples = 25\n",
        "        samples = self.sample((num_samples,), key_0)\n",
        "        Ex = np.mean(samples, axis=0)\n",
        "        self._mean = Ex\n",
        "        ExxT = np.einsum(\"s...ti,s...tj->s...tij\", samples, samples).mean(axis=0)\n",
        "        self._covariance = ExxT - np.einsum(\"...ti,...tj->...tij\", Ex, Ex)\n",
        "\n",
        "    # TODO: make this work properly with a batched distribution object\n",
        "    def log_prob(self, xs):\n",
        "        params = self.params\n",
        "        def log_prob_single(x_):\n",
        "            def _log_prob_step(carry, i):\n",
        "                h, prev_x = carry\n",
        "                x, u = x_[i], self.inputs[i]\n",
        "                h, (cov, mean) = self.cell.apply(params[\"rnn_params\"], h, prev_x, u)\n",
        "                pred_dist = tfd.MultivariateNormalFullCovariance(loc=mean, \n",
        "                                                            covariance_matrix=cov)\n",
        "                log_prob = pred_dist.log_prob(x)\n",
        "                carry = h, x\n",
        "                return carry, log_prob\n",
        "            # Assuming these are zero arrays already\n",
        "            init = (params[\"latent_dummy\"], params[\"output_dummy\"])\n",
        "            _, log_probs = scan(_log_prob_step, init, np.arange(x_.shape[0]))\n",
        "            return np.sum(log_probs, axis=0)\n",
        "        return vmap(log_prob_single)(xs)\n",
        "\n",
        "    # TODO: make this work with a batched distribution object\n",
        "    # Only supports rank 0 and 1 sample shapes\n",
        "    # Output: ([num_samples,] [batch_size,] seq_len, event_dim)\n",
        "    def sample(self, sample_shape, seed):\n",
        "        def _sample_single(key, params, inputs):\n",
        "            def _sample_step(carry, u):\n",
        "                key, h, x = carry\n",
        "                key, new_key = jr.split(key)\n",
        "                h, (cov, mean) = self.cell.apply(params[\"rnn_params\"], h, x, u)\n",
        "                sample = jr.multivariate_normal(key, mean, cov)\n",
        "                carry = new_key, h, sample\n",
        "                output = sample\n",
        "                return carry, output\n",
        "\n",
        "            init = (key, params[\"latent_dummy\"], params[\"output_dummy\"])\n",
        "            _, sample = scan(_sample_step, init, inputs)\n",
        "            return sample\n",
        "\n",
        "        if (len(self.inputs.shape) == 2):\n",
        "            if (len(sample_shape) == 0):\n",
        "                return _sample_single(seed, self.params, self.inputs)\n",
        "            else:\n",
        "                assert (len(sample_shape) == 1)\n",
        "                return vmap(_sample_single, in_axes=(0, None, None))(jr.split(seed, sample_shape[0]),\n",
        "                                            self.params, self.inputs)\n",
        "        else:\n",
        "            # This is a batched distribution object\n",
        "            assert(len(self.inputs.shape) == 3)\n",
        "            batch_size = self.inputs.shape[0]\n",
        "            if (len(sample_shape) == 0):\n",
        "                return vmap(_sample_single)(\n",
        "                            jr.split(seed, batch_size), \n",
        "                            self.params,\n",
        "                            self.inputs\n",
        "                        )\n",
        "            else:\n",
        "                assert (len(sample_shape) == 1)\n",
        "                return vmap(\n",
        "                        lambda s:vmap(_sample_single)(\n",
        "                            jr.split(s, batch_size), \n",
        "                            self.params,\n",
        "                            self.inputs\n",
        "                        )\n",
        "                    )(jr.split(seed, sample_shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {
        "cellView": "form",
        "id": "qDsz18BcyDcf"
      },
      "outputs": [],
      "source": [
        "# @title LDS object (might wanna refactor this)\n",
        "\n",
        "# Takes a linear Gaussian chain as its base\n",
        "class LDS(LinearGaussianChainPrior):\n",
        "    def __init__(self, latent_dims, seq_len, base=None, posterior=None):\n",
        "        super().__init__(latent_dims, seq_len)\n",
        "        self.posterior = posterior or LDSSVAEPosterior(latent_dims, seq_len)\n",
        "        self.base = base or LinearGaussianChainPrior(latent_dims, seq_len) # Slightly redundant...\n",
        "\n",
        "    # Takes unconstrained params\n",
        "    def sample(self, params, shape, key):\n",
        "        latents = self.base.sample(params, shape, key)\n",
        "        sample_shape = latents.shape[:-1]\n",
        "        key, _ = jr.split(key)\n",
        "        C, d, R = params[\"C\"], params[\"d\"], params[\"R\"]\n",
        "        obs_noise = tfd.MultivariateNormalFullCovariance(loc=d, covariance_matrix=R)\\\n",
        "            .sample(sample_shape=sample_shape, seed=key)\n",
        "        obs = np.einsum(\"ij,...tj->...ti\", C, latents) + obs_noise\n",
        "        return latents, obs\n",
        "\n",
        "    # Should work with any batch dimension\n",
        "    def log_prob(self, params, states, data):\n",
        "        latent_dist = self.base.distribution(self.base.get_constrained_params(params))\n",
        "        latent_ll = latent_dist.log_prob(states)\n",
        "        C, d, R = params[\"C\"], params[\"d\"], params[\"R\"]\n",
        "        # Gets around batch dimensions\n",
        "        noise = tfd.MultivariateNormalFullCovariance(loc=d, covariance_matrix=R)\n",
        "        obs_ll = noise.log_prob(data - np.einsum(\"ij,...tj->...ti\", C, states))\n",
        "        return latent_ll + obs_ll.sum(axis=-1)\n",
        "\n",
        "    # Assumes single data points\n",
        "    def e_step(self, params, data):\n",
        "        # Shorthand names for parameters\n",
        "        C, d, R = params[\"C\"], params[\"d\"], params[\"R\"]\n",
        "\n",
        "        J = np.dot(C.T, np.linalg.solve(R, C))\n",
        "        J = np.tile(J[None, :, :], (self.seq_len, 1, 1))\n",
        "        # linear potential\n",
        "        h = np.dot(data - d, np.linalg.solve(R, C))\n",
        "\n",
        "        Sigma = solve(J, np.eye(self.latent_dims)[None])\n",
        "        mu = vmap(solve)(J, h)\n",
        "\n",
        "        return self.posterior.infer(self.base.get_constrained_params(params), {\"J\": J, \"h\": h, \n",
        "                                                                    \"mu\": mu, \"Sigma\": Sigma})\n",
        "        \n",
        "    # Also assumes single data points\n",
        "    def marginal_log_likelihood(self, params, data):\n",
        "        posterior = self.posterior.distribution(self.e_step(params, data))\n",
        "        states = posterior.mean()\n",
        "        prior_ll = self.log_prob(params, states, data)\n",
        "        posterior_ll = posterior.log_prob(states)\n",
        "        # This is numerically unstable!\n",
        "        lps = prior_ll - posterior_ll\n",
        "        return lps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGISYI1CN6cv"
      },
      "source": [
        "## Making a mean parameter posterior object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 347,
      "metadata": {
        "cellView": "form",
        "id": "il0xUSa3nn0K"
      },
      "outputs": [],
      "source": [
        "# @title Parallel Kalman filtering and smoothing\n",
        "\n",
        "def _make_associative_sampling_elements(params, key, filtered_means, filtered_covariances):\n",
        "    \"\"\"Preprocess filtering output to construct input for smoothing assocative scan.\"\"\"\n",
        "\n",
        "    F = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "\n",
        "    def _last_sampling_element(key, m, P):\n",
        "        return np.zeros_like(P), MVN(m, P).sample(seed=key)\n",
        "\n",
        "    def _generic_sampling_element(params, key, m, P):\n",
        "\n",
        "        eps = 1e-3\n",
        "        P += np.eye(dims) * eps\n",
        "        Pp = F @ P @ F.T + Q\n",
        "\n",
        "        FP = F @ P\n",
        "        E  = psd_solve(Pp, FP).T\n",
        "        g  = m - E @ F @ m\n",
        "        L  = P - E @ Pp @ E.T\n",
        "\n",
        "        L = (L + L.T) * .5 + np.eye(dims) * eps # Add eps to the crucial covariance matrix\n",
        "\n",
        "        h = MVN(g, L).sample(seed=key)\n",
        "        return E, h\n",
        "\n",
        "    num_timesteps = len(filtered_means)\n",
        "    dims = filtered_means.shape[-1]\n",
        "    keys = jr.split(key, num_timesteps)\n",
        "    last_elems = _last_sampling_element(keys[-1], filtered_means[-1], \n",
        "                                        filtered_covariances[-1])\n",
        "    generic_elems = vmap(_generic_sampling_element, (None, 0, 0, 0))(\n",
        "        params, keys[:-1], filtered_means[:-1], filtered_covariances[:-1]\n",
        "        )\n",
        "    combined_elems = tuple(np.append(gen_elm, last_elm[None,:], axis=0)\n",
        "                           for gen_elm, last_elm in zip(generic_elems, last_elems))\n",
        "    return combined_elems\n",
        "\n",
        "def _make_associative_filtering_elements(params, potentials):\n",
        "    \"\"\"Preprocess observations to construct input for filtering assocative scan.\"\"\"\n",
        "\n",
        "    F = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "    Q1 = params[\"Q1\"]\n",
        "    P0 = Q1\n",
        "    P1 = Q1\n",
        "    m1 = params[\"m1\"]\n",
        "    dim = Q.shape[0]\n",
        "    H = np.eye(dim)\n",
        "\n",
        "    def _first_filtering_element(params, mu, Sigma):\n",
        "\n",
        "        y, R = mu, Sigma\n",
        "\n",
        "        S = H @ Q @ H.T + R\n",
        "        CF, low = scipy.linalg.cho_factor(S)\n",
        "\n",
        "        S1 = H @ P1 @ H.T + R\n",
        "        K1 = psd_solve(S1, H @ P1).T\n",
        "\n",
        "        A = np.zeros_like(F)\n",
        "        b = m1 + K1 @ (y - H @ m1)\n",
        "        C = P1 - K1 @ S1 @ K1.T\n",
        "        eta = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), y)\n",
        "        J = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), H @ F)\n",
        "\n",
        "        logZ = -MVN(loc=np.zeros_like(y), covariance_matrix=H @ P0 @ H.T + R).log_prob(y)\n",
        "\n",
        "        return A, b, C, J, eta, logZ\n",
        "\n",
        "\n",
        "    def _generic_filtering_element(params, mu, Sigma):\n",
        "\n",
        "        y, R = mu, Sigma\n",
        "\n",
        "        S = H @ Q @ H.T + R\n",
        "        CF, low = scipy.linalg.cho_factor(S)\n",
        "        K = scipy.linalg.cho_solve((CF, low), H @ Q).T\n",
        "        A = F - K @ H @ F\n",
        "        b = K @ y\n",
        "        C = Q - K @ H @ Q\n",
        "\n",
        "        eta = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), y)\n",
        "        J = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), H @ F)\n",
        "\n",
        "        logZ = -MVN(loc=np.zeros_like(y), covariance_matrix=S).log_prob(y)\n",
        "\n",
        "        return A, b, C, J, eta, logZ\n",
        "\n",
        "    mus, Sigmas = potentials[\"mu\"], potentials[\"Sigma\"]\n",
        "\n",
        "    first_elems = _first_filtering_element(params, mus[0], Sigmas[0])\n",
        "    generic_elems = vmap(_generic_filtering_element, (None, 0, 0))(params, mus[1:], Sigmas[1:])\n",
        "    combined_elems = tuple(np.concatenate((first_elm[None,...], gen_elm))\n",
        "                           for first_elm, gen_elm in zip(first_elems, generic_elems))\n",
        "    return combined_elems\n",
        "\n",
        "def lgssm_filter(params, emissions):\n",
        "    \"\"\"A parallel version of the lgssm filtering algorithm.\n",
        "    See S. Särkkä and Á. F. García-Fernández (2021) - https://arxiv.org/abs/1905.13002.\n",
        "    Note: This function does not yet handle `inputs` to the system.\n",
        "    \"\"\"\n",
        "\n",
        "    initial_elements = _make_associative_filtering_elements(params, emissions)\n",
        "\n",
        "    @vmap\n",
        "    def filtering_operator(elem1, elem2):\n",
        "        A1, b1, C1, J1, eta1, logZ1 = elem1\n",
        "        A2, b2, C2, J2, eta2, logZ2 = elem2\n",
        "        dim = A1.shape[0]\n",
        "        I = np.eye(dim)\n",
        "\n",
        "        I_C1J2 = I + C1 @ J2\n",
        "        temp = scipy.linalg.solve(I_C1J2.T, A2.T).T\n",
        "        A = temp @ A1\n",
        "        b = temp @ (b1 + C1 @ eta2) + b2\n",
        "        C = temp @ C1 @ A2.T + C2\n",
        "\n",
        "        I_J2C1 = I + J2 @ C1\n",
        "        temp = scipy.linalg.solve(I_J2C1.T, A1).T\n",
        "\n",
        "        eta = temp @ (eta2 - J2 @ b1) + eta1\n",
        "        J = temp @ J2 @ A1 + J1\n",
        "\n",
        "        # mu = scipy.linalg.solve(J2, eta2)\n",
        "        # t2 = - eta2 @ mu + (b1 - mu) @ scipy.linalg.solve(I_J2C1, (J2 @ b1 - eta2))\n",
        "\n",
        "        mu = np.linalg.solve(C1, b1)\n",
        "        t1 = (b1 @ mu - (eta2 + mu) @ np.linalg.solve(I_C1J2, C1 @ eta2 + b1))\n",
        "\n",
        "        logZ = (logZ1 + logZ2 + 0.5 * np.linalg.slogdet(I_C1J2)[1] + 0.5 * t1)\n",
        "\n",
        "        return A, b, C, J, eta, logZ\n",
        "\n",
        "    _, filtered_means, filtered_covs, _, _, logZ = lax.associative_scan(\n",
        "                                                filtering_operator, initial_elements\n",
        "                                                )\n",
        "\n",
        "    return {\n",
        "        \"marginal_logliks\": -logZ,\n",
        "        \"marginal_loglik\": -logZ[-1],\n",
        "        \"filtered_means\": filtered_means, \n",
        "        \"filtered_covariances\": filtered_covs\n",
        "    }\n",
        "\n",
        "def _make_associative_smoothing_elements(params, filtered_means, filtered_covariances):\n",
        "    \"\"\"Preprocess filtering output to construct input for smoothing assocative scan.\"\"\"\n",
        "\n",
        "    F = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "\n",
        "    def _last_smoothing_element(m, P):\n",
        "        return np.zeros_like(P), m, P\n",
        "\n",
        "    def _generic_smoothing_element(params, m, P):\n",
        "\n",
        "        Pp = F @ P @ F.T + Q\n",
        "\n",
        "        E  = psd_solve(Pp, F @ P).T\n",
        "        g  = m - E @ F @ m\n",
        "        L  = P - E @ Pp @ E.T\n",
        "        return E, g, L\n",
        "\n",
        "    last_elems = _last_smoothing_element(filtered_means[-1], filtered_covariances[-1])\n",
        "    generic_elems = vmap(_generic_smoothing_element, (None, 0, 0))(\n",
        "        params, filtered_means[:-1], filtered_covariances[:-1]\n",
        "        )\n",
        "    combined_elems = tuple(np.append(gen_elm, last_elm[None,:], axis=0)\n",
        "                           for gen_elm, last_elm in zip(generic_elems, last_elems))\n",
        "    return combined_elems\n",
        "\n",
        "\n",
        "def parallel_lgssm_smoother(params, emissions):\n",
        "    \"\"\"A parallel version of the lgssm smoothing algorithm.\n",
        "    See S. Särkkä and Á. F. García-Fernández (2021) - https://arxiv.org/abs/1905.13002.\n",
        "    Note: This function does not yet handle `inputs` to the system.\n",
        "    \"\"\"\n",
        "    filtered_posterior = lgssm_filter(params, emissions)\n",
        "    filtered_means = filtered_posterior[\"filtered_means\"]\n",
        "    filtered_covs = filtered_posterior[\"filtered_covariances\"]\n",
        "    initial_elements = _make_associative_smoothing_elements(params, filtered_means, filtered_covs)\n",
        "\n",
        "    @vmap\n",
        "    def smoothing_operator(elem1, elem2):\n",
        "        E1, g1, L1 = elem1\n",
        "        E2, g2, L2 = elem2\n",
        "\n",
        "        E = E2 @ E1\n",
        "        g = E2 @ g1 + g2\n",
        "        L = E2 @ L1 @ E2.T + L2\n",
        "\n",
        "        return E, g, L\n",
        "\n",
        "    _, smoothed_means, smoothed_covs, *_ = lax.associative_scan(\n",
        "                                                smoothing_operator, initial_elements, reverse=True\n",
        "                                                )\n",
        "    return {\n",
        "        \"marginal_loglik\": filtered_posterior[\"marginal_loglik\"],\n",
        "        \"filtered_means\": filtered_means,\n",
        "        \"filtered_covariances\": filtered_covs,\n",
        "        \"smoothed_means\": smoothed_means,\n",
        "        \"smoothed_covariances\": smoothed_covs\n",
        "    }\n",
        "\n",
        "def lgssm_log_normalizer(dynamics_params, mu_filtered, Sigma_filtered, potentials):\n",
        "    p = dynamics_params\n",
        "    Q, A, b = p[\"Q\"][None], p[\"A\"][None], p[\"b\"][None]\n",
        "    AT = (p[\"A\"].T)[None]\n",
        "\n",
        "    I = np.eye(Q.shape[-1])\n",
        "\n",
        "    Sigma_filtered, mu_filtered = Sigma_filtered[:-1], mu_filtered[:-1]\n",
        "    Sigma = Q + A @ Sigma_filtered @ AT\n",
        "    mu = (A[0] @ mu_filtered.T).T + b\n",
        "    # Append the first element\n",
        "    Sigma_pred = np.concatenate([p[\"Q1\"][None], Sigma])\n",
        "    mu_pred = np.concatenate([p[\"m1\"][None], mu])\n",
        "    mu_rec, Sigma_rec = potentials[\"mu\"], potentials[\"Sigma\"]\n",
        "\n",
        "    def log_Z_single(mu_pred, Sigma_pred, mu_rec, Sigma_rec):\n",
        "        return MVN(loc=mu_pred, covariance_matrix=Sigma_pred+Sigma_rec).log_prob(mu_rec)\n",
        "\n",
        "    log_Z = vmap(log_Z_single)(mu_pred, Sigma_pred, mu_rec, Sigma_rec)\n",
        "    return np.sum(log_Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "metadata": {
        "cellView": "form",
        "id": "_zQepN9hRCdG"
      },
      "outputs": [],
      "source": [
        "# @title Parallel linear Gaussian state space model object\n",
        "def lgssm_log_normalizer(dynamics_params, mu_filtered, Sigma_filtered, potentials):\n",
        "    p = dynamics_params\n",
        "    Q, A, b = p[\"Q\"][None], p[\"A\"][None], p[\"b\"][None]\n",
        "    AT = (p[\"A\"].T)[None]\n",
        "\n",
        "    I = np.eye(Q.shape[-1])\n",
        "\n",
        "    Sigma_filtered, mu_filtered = Sigma_filtered[:-1], mu_filtered[:-1]\n",
        "    Sigma = Q + A @ Sigma_filtered @ AT\n",
        "    mu = (A[0] @ mu_filtered.T).T + b\n",
        "    # Append the first element\n",
        "    Sigma_pred = np.concatenate([p[\"Q1\"][None], Sigma])\n",
        "    mu_pred = np.concatenate([p[\"m1\"][None], mu])\n",
        "    mu_rec, Sigma_rec = potentials[\"mu\"], potentials[\"Sigma\"]\n",
        "\n",
        "    def log_Z_single(mu_pred, Sigma_pred, mu_rec, Sigma_rec):\n",
        "        return MVN(loc=mu_pred, covariance_matrix=Sigma_pred+Sigma_rec).log_prob(mu_rec)\n",
        "\n",
        "    log_Z = vmap(log_Z_single)(mu_pred, Sigma_pred, mu_rec, Sigma_rec)\n",
        "    return np.sum(log_Z)\n",
        "\n",
        "class LinearGaussianSSM(tfd.Distribution):\n",
        "    def __init__(self,\n",
        "                 initial_mean,\n",
        "                 initial_covariance,\n",
        "                 dynamics_matrix,\n",
        "                 dynamics_bias,\n",
        "                 dynamics_noise_covariance,\n",
        "                 emissions_means,\n",
        "                 emissions_covariances,\n",
        "                 log_normalizer,\n",
        "                 filtered_means,\n",
        "                 filtered_covariances,\n",
        "                 smoothed_means,\n",
        "                 smoothed_covariances,\n",
        "                 smoothed_cross,\n",
        "                 validate_args=False,\n",
        "                 allow_nan_stats=True,\n",
        "                 name=\"LinearGaussianSSM\",\n",
        "             ) -> None:\n",
        "        # Dynamics\n",
        "        self._initial_mean = initial_mean\n",
        "        self._initial_covariance = initial_covariance\n",
        "        self._dynamics_matrix = dynamics_matrix\n",
        "        self._dynamics_bias = dynamics_bias\n",
        "        self._dynamics_noise_covariance = dynamics_noise_covariance\n",
        "        # Emissions\n",
        "        self._emissions_means = emissions_means\n",
        "        self._emissions_covariances = emissions_covariances\n",
        "        # Filtered\n",
        "        self._log_normalizer = log_normalizer\n",
        "        self._filtered_means = filtered_means\n",
        "        self._filtered_covariances = filtered_covariances\n",
        "        # Smoothed\n",
        "        self._smoothed_means = smoothed_means\n",
        "        self._smoothed_covariances = smoothed_covariances\n",
        "        self._smoothed_cross = smoothed_cross\n",
        "\n",
        "        # We would detect the dtype dynamically but that would break vmap\n",
        "        # see https://github.com/tensorflow/probability/issues/1271\n",
        "        dtype = np.float32\n",
        "        super(LinearGaussianSSM, self).__init__(\n",
        "            dtype=dtype,\n",
        "            validate_args=validate_args,\n",
        "            allow_nan_stats=allow_nan_stats,\n",
        "            reparameterization_type=reparameterization.NOT_REPARAMETERIZED,\n",
        "            parameters=dict(initial_mean=self._initial_mean,\n",
        "                            initial_covariance=self._initial_covariance,\n",
        "                            dynamics_matrix=self._dynamics_matrix,\n",
        "                            dynamics_bias=self._dynamics_bias,\n",
        "                            dynamics_noise_covariance=self._dynamics_noise_covariance,\n",
        "                            emissions_means=self._emissions_means,\n",
        "                            emissions_covariances=self._emissions_covariances,\n",
        "                            log_normalizer=self._log_normalizer,\n",
        "                            filtered_means=self._filtered_means,\n",
        "                            filtered_covariances=self._filtered_covariances,\n",
        "                            smoothed_means=self._smoothed_means,\n",
        "                            smoothed_covariances=self._smoothed_covariances,\n",
        "                            smoothed_cross=self._smoothed_cross),\n",
        "            name=name,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _parameter_properties(cls, dtype, num_classes=None):\n",
        "        # pylint: disable=g-long-lambda\n",
        "        return dict(initial_mean=tfp.internal.parameter_properties.ParameterProperties(event_ndims=1),\n",
        "                    initial_covariance=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    dynamics_matrix=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    dynamics_bias=tfp.internal.parameter_properties.ParameterProperties(event_ndims=1),\n",
        "                    dynamics_noise_covariance=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    emissions_means=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    emissions_covariances=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "                    log_normalizer=tfp.internal.parameter_properties.ParameterProperties(event_ndims=0),\n",
        "                    filtered_means=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    filtered_covariances=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "                    smoothed_means=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    smoothed_covariances=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "                    smoothed_cross=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3)\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def infer_from_dynamics_and_potential(cls, dynamics_params, emissions_potentials):\n",
        "        p = dynamics_params\n",
        "        mus, Sigmas = emissions_potentials[\"mu\"], emissions_potentials[\"Sigma\"]\n",
        "\n",
        "        dim = mus.shape[-1]\n",
        "        C = np.eye(dim)\n",
        "        d = np.zeros(dim)\n",
        "\n",
        "        params = make_lgssm_params(p[\"m1\"], p[\"Q1\"], p[\"A\"], p[\"Q\"], C, Sigmas, \n",
        "                                   dynamics_bias=p[\"b\"], emissions_bias=d)\n",
        "\n",
        "        smoothed = lgssm_smoother(params, mus)._asdict()\n",
        "        \n",
        "        # Compute ExxT\n",
        "        A, Q = dynamics_params[\"A\"], dynamics_params[\"Q\"]\n",
        "        filtered_cov = smoothed[\"filtered_covariances\"]\n",
        "        filtered_mean = smoothed[\"smoothed_means\"]\n",
        "        smoothed_cov = smoothed[\"smoothed_covariances\"]\n",
        "        smoothed_mean = smoothed[\"smoothed_means\"]\n",
        "        G = vmap(lambda C: psd_solve(Q + A @ C @ A.T, A @ C).T)(filtered_cov)\n",
        "\n",
        "        # Compute the smoothed expectation of z_t z_{t+1}^T\n",
        "        smoothed_cross = vmap(\n",
        "            lambda Gt, mean, next_mean, next_cov: Gt @ next_cov + np.outer(mean, next_mean))\\\n",
        "            (G[:-1], smoothed_mean[:-1], smoothed_mean[1:], smoothed_cov[1:])\n",
        "\n",
        "        log_Z = lgssm_log_normalizer(dynamics_params, \n",
        "                                     smoothed[\"filtered_means\"], \n",
        "                                     smoothed[\"filtered_covariances\"], \n",
        "                                     emissions_potentials)\n",
        "\n",
        "        return cls(dynamics_params[\"m1\"],\n",
        "                   dynamics_params[\"Q1\"],\n",
        "                   dynamics_params[\"A\"],\n",
        "                   dynamics_params[\"b\"],\n",
        "                   dynamics_params[\"Q\"],\n",
        "                   emissions_potentials[\"mu\"],\n",
        "                   emissions_potentials[\"Sigma\"],\n",
        "                   log_Z, # smoothed[\"marginal_loglik\"],\n",
        "                   smoothed[\"filtered_means\"],\n",
        "                   smoothed[\"filtered_covariances\"],\n",
        "                   smoothed[\"smoothed_means\"],\n",
        "                   smoothed[\"smoothed_covariances\"],\n",
        "                   smoothed_cross)\n",
        "        \n",
        "    # Properties to get private class variables\n",
        "    @property\n",
        "    def log_normalizer(self):\n",
        "        return self._log_normalizer\n",
        "\n",
        "    @property\n",
        "    def filtered_means(self):\n",
        "        return self._filtered_means\n",
        "\n",
        "    @property\n",
        "    def filtered_covariances(self):\n",
        "        return self._filtered_covariances\n",
        "\n",
        "    @property\n",
        "    def smoothed_means(self):\n",
        "        return self._smoothed_means\n",
        "\n",
        "    @property\n",
        "    def smoothed_covariances(self):\n",
        "        return self._smoothed_covariances\n",
        "\n",
        "    @property\n",
        "    def expected_states(self):\n",
        "        return self._smoothed_means\n",
        "\n",
        "    @property\n",
        "    def expected_states_squared(self):\n",
        "        Ex = self._smoothed_means\n",
        "        return self._smoothed_covariances + np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "\n",
        "    @property\n",
        "    def expected_states_next_states(self):\n",
        "        return self._smoothed_cross\n",
        "\n",
        "    def _mean(self):\n",
        "        return self.smoothed_means\n",
        "\n",
        "    def _covariance(self):\n",
        "        return self.smoothed_covariances\n",
        "    \n",
        "    # TODO: currently this function does not depend on the dynamics bias\n",
        "    def _log_prob(self, data, **kwargs):\n",
        "        A = self._dynamics_matrix #params[\"A\"]\n",
        "        Q = self._dynamics_noise_covariance #params[\"Q\"]\n",
        "        Q1 = self._initial_covariance #params[\"Q1\"]\n",
        "        m1 = self._initial_mean #params[\"m1\"]\n",
        "\n",
        "        num_batch_dims = len(data.shape) - 2\n",
        "\n",
        "        ll = np.sum(\n",
        "            MVN(loc=np.einsum(\"ij,...tj->...ti\", A, data[...,:-1,:]), \n",
        "                covariance_matrix=Q).log_prob(data[...,1:,:])\n",
        "            )\n",
        "        ll += MVN(loc=m1, covariance_matrix=Q1).log_prob(data[...,0,:])\n",
        "\n",
        "        # Add the observation potentials\n",
        "        # ll += - 0.5 * np.einsum(\"...ti,tij,...tj->...\", data, self._emissions_precisions, data) \\\n",
        "        #       + np.einsum(\"...ti,ti->...\", data, self._emissions_linear_potentials)\n",
        "        ll += np.sum(MVN(loc=self._emissions_means, \n",
        "                  covariance_matrix=self._emissions_covariances).log_prob(data), axis=-1)\n",
        "        # Add the log normalizer\n",
        "        ll -= self._log_normalizer\n",
        "\n",
        "        return ll\n",
        "\n",
        "    def _sample_n(self, n, seed=None):\n",
        "\n",
        "        F = self._dynamics_matrix\n",
        "        b = self._dynamics_bias\n",
        "        Q = self._dynamics_noise_covariance\n",
        "        \n",
        "        def sample_single(\n",
        "            key,\n",
        "            filtered_means,\n",
        "            filtered_covariances\n",
        "        ):\n",
        "\n",
        "            initial_elements = _make_associative_sampling_elements(\n",
        "                { \"A\": F, \"b\": b, \"Q\": Q }, key, filtered_means, filtered_covariances)\n",
        "\n",
        "            @vmap\n",
        "            def sampling_operator(elem1, elem2):\n",
        "                E1, h1 = elem1\n",
        "                E2, h2 = elem2\n",
        "\n",
        "                E = E2 @ E1\n",
        "                h = E2 @ h1 + h2\n",
        "                return E, h\n",
        "\n",
        "            _, sample = \\\n",
        "                lax.associative_scan(sampling_operator, initial_elements, reverse=True)\n",
        "                \n",
        "            return sample\n",
        "\n",
        "        # TODO: Handle arbitrary batch shapes\n",
        "        if self._filtered_covariances.ndim == 4:\n",
        "            # batch mode\n",
        "            samples = vmap(vmap(sample_single, in_axes=(None, 0, 0)), in_axes=(0, None, None))\\\n",
        "                (jr.split(seed, n), self._filtered_means, self._filtered_covariances)\n",
        "            # Transpose to be (num_samples, num_batches, num_timesteps, dim)\n",
        "            # samples = np.transpose(samples, (1, 0, 2, 3))\n",
        "        else:\n",
        "            # non-batch mode\n",
        "            samples = vmap(sample_single, in_axes=(0, None, None))\\\n",
        "                (jr.split(seed, n), self._filtered_means, self._filtered_covariances)\n",
        "        return samples\n",
        "\n",
        "    def _entropy(self):\n",
        "        \"\"\"\n",
        "        Compute the entropy\n",
        "\n",
        "            H[X] = -E[\\log p(x)]\n",
        "                 = -E[-1/2 x^T J x + x^T h - log Z(J, h)]\n",
        "                 = 1/2 <J, E[x x^T] - <h, E[x]> + log Z(J, h)\n",
        "        \"\"\"\n",
        "        Ex = self.expected_states\n",
        "        ExxT = self.expected_states_squared\n",
        "        ExnxT = self.expected_states_next_states\n",
        "        p = dynamics_to_tridiag(\n",
        "            {\n",
        "                \"m1\": self._initial_mean,\n",
        "                \"Q1\": self._initial_covariance,\n",
        "                \"A\": self._dynamics_matrix,\n",
        "                \"b\": self._dynamics_bias,\n",
        "                \"Q\": self._dynamics_noise_covariance,\n",
        "            }, Ex.shape[0], Ex.shape[1]\n",
        "        )\n",
        "        J_diag = p[\"J\"] + solve(self._emissions_covariances, np.eye(Ex.shape[-1])[None])\n",
        "        J_lower_diag = p[\"L\"]\n",
        "\n",
        "        Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "        Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "        entropy = 0.5 * np.sum(J_diag * Sigmatt)\n",
        "        entropy += np.sum(J_lower_diag * Sigmatnt)\n",
        "        return entropy - self.log_prob(Ex)\n",
        "\n",
        "class ParallelLinearGaussianSSM(LinearGaussianSSM):\n",
        "    def __init__(self, *args, **kwargs) -> None:\n",
        "        kwargs[\"name\"] = \"ParallelLinearGaussianSSM\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def infer_from_dynamics_and_potential(cls, dynamics_params, emissions_potentials):\n",
        "        # p = dynamics_params\n",
        "        # mus, Sigmas = emissions_potentials[\"mu\"], emissions_potentials[\"Sigma\"]\n",
        "\n",
        "        # dim = mus.shape[-1]\n",
        "        # C = np.eye(dim)\n",
        "        # d = np.zeros(dim)\n",
        "\n",
        "        smoothed = parallel_lgssm_smoother(dynamics_params, emissions_potentials)\n",
        "        \n",
        "        # Compute ExxT\n",
        "        A, Q = dynamics_params[\"A\"], dynamics_params[\"Q\"]\n",
        "        filtered_cov = smoothed[\"filtered_covariances\"]\n",
        "        filtered_mean = smoothed[\"smoothed_means\"]\n",
        "        smoothed_cov = smoothed[\"smoothed_covariances\"]\n",
        "        smoothed_mean = smoothed[\"smoothed_means\"]\n",
        "        G = vmap(lambda C: psd_solve(Q + A @ C @ A.T, A @ C).T)(filtered_cov)\n",
        "\n",
        "        # Compute the smoothed expectation of z_t z_{t+1}^T\n",
        "        smoothed_cross = vmap(\n",
        "            lambda Gt, mean, next_mean, next_cov: Gt @ next_cov + np.outer(mean, next_mean))\\\n",
        "            (G[:-1], smoothed_mean[:-1], smoothed_mean[1:], smoothed_cov[1:])\n",
        "\n",
        "        log_Z = lgssm_log_normalizer(dynamics_params, \n",
        "                                     smoothed[\"filtered_means\"], \n",
        "                                     smoothed[\"filtered_covariances\"], \n",
        "                                     emissions_potentials)\n",
        "\n",
        "        return cls(dynamics_params[\"m1\"],\n",
        "                   dynamics_params[\"Q1\"],\n",
        "                   dynamics_params[\"A\"],\n",
        "                   dynamics_params[\"b\"],\n",
        "                   dynamics_params[\"Q\"],\n",
        "                   emissions_potentials[\"mu\"],\n",
        "                   emissions_potentials[\"Sigma\"],\n",
        "                   log_Z, # smoothed[\"marginal_loglik\"],\n",
        "                   smoothed[\"filtered_means\"],\n",
        "                   smoothed[\"filtered_covariances\"],\n",
        "                   smoothed[\"smoothed_means\"],\n",
        "                   smoothed[\"smoothed_covariances\"],\n",
        "                   smoothed_cross)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {
        "id": "HOdprEnnTpSN"
      },
      "outputs": [],
      "source": [
        "# @title Parallel versions of the same priors and posteriors\n",
        "\n",
        "# Super simple because all the machinary is already taken care of\n",
        "class LDSSVAEPosterior(SVAEPrior):\n",
        "    def __init__(self, latent_dims, seq_len, use_parallel=False):\n",
        "        self.latent_dims = latent_dims\n",
        "        # The only annoying thing is that we have to specify the sequence length\n",
        "        # ahead of time\n",
        "        self.seq_len = seq_len\n",
        "        self.dist = ParallelLinearGaussianSSM if use_parallel else LinearGaussianSSM\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return (self.seq_len, self.latent_dims)\n",
        "\n",
        "    # Must be the full set of constrained parameters!\n",
        "    def distribution(self, p):\n",
        "        m1, Q1, A, b, Q, mus, Sigmas = p[\"m1\"], p[\"Q1\"], p[\"A\"], p[\"b\"], p[\"Q\"], p[\"mu\"], p[\"Sigma\"]\n",
        "        log_Z, mu_filtered, Sigma_filtered = p[\"log_Z\"], p[\"mu_filtered\"], p[\"Sigma_filtered\"]\n",
        "        mu_smoothed, Sigma_smoothed, ExnxT = p[\"mu_smoothed\"], p[\"Sigma_smoothed\"], p[\"ExnxT\"]\n",
        "        return self.dist(m1, Q1, A, b, Q, mus, Sigmas, \n",
        "                             log_Z, mu_filtered, Sigma_filtered, \n",
        "                             mu_smoothed, Sigma_smoothed, ExnxT)\n",
        "\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        p = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"Q1\": np.eye(D),\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": np.eye(D),\n",
        "            \"Sigma\": np.tile(np.eye(D)[None], (T, 1, 1)),\n",
        "            \"mu\": np.zeros((T, D))\n",
        "        }\n",
        "\n",
        "        dist = self.dist.infer_from_dynamics_and_potential(p, \n",
        "                                    {\"mu\": p[\"mu\"], \"Sigma\": p[\"Sigma\"]})\n",
        "        \n",
        "        p.update({\n",
        "            \"log_Z\": dist.log_normalizer,\n",
        "            \"mu_filtered\": dist.filtered_means,\n",
        "            \"Sigma_filtered\": dist.filtered_covariances,\n",
        "            \"mu_smoothed\": dist.smoothed_means,\n",
        "            \"Sigma_smoothed\": dist.smoothed_covariances,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        return params\n",
        "\n",
        "    def infer(self, prior_params, potential_params):\n",
        "        p = {\n",
        "            \"m1\": prior_params[\"m1\"],\n",
        "            \"Q1\": prior_params[\"Q1\"],\n",
        "            \"A\": prior_params[\"A\"],\n",
        "            \"b\": prior_params[\"b\"],\n",
        "            \"Q\": prior_params[\"Q\"],\n",
        "            \"Sigma\": potential_params[\"Sigma\"],\n",
        "            \"mu\": potential_params[\"mu\"]\n",
        "        }\n",
        "\n",
        "        dist = self.dist.infer_from_dynamics_and_potential(prior_params, \n",
        "                                    {\"mu\": p[\"mu\"], \"Sigma\": p[\"Sigma\"]})\n",
        "        p.update({\n",
        "            \"log_Z\": dist.log_normalizer,\n",
        "            \"mu_filtered\": dist.filtered_means,\n",
        "            \"Sigma_filtered\": dist.filtered_covariances,\n",
        "            \"mu_smoothed\": dist.smoothed_means,\n",
        "            \"Sigma_smoothed\": dist.smoothed_covariances,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtvHU_dlOklC"
      },
      "source": [
        "## Define neural network architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {
        "id": "40wvAwfoOn_E"
      },
      "outputs": [],
      "source": [
        "# @title Neural network utils\n",
        "\n",
        "PRNGKey = Any\n",
        "Shape = Iterable[int]\n",
        "Dtype = Any  # this could be a real type?\n",
        "Array = Any\n",
        "\n",
        "# Note: the last layer output does not have a relu activation!\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Define a simple fully connected MLP with ReLU activations.\n",
        "    \"\"\"\n",
        "    features: Sequence[int]\n",
        "    kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.he_normal()\n",
        "    bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.zeros\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for feat in self.features[:-1]:\n",
        "            x = nn.relu(nn.Dense(feat, \n",
        "                kernel_init=self.kernel_init,\n",
        "                bias_init=self.bias_init,)(x))\n",
        "        x = nn.Dense(self.features[-1], \n",
        "            kernel_init=self.kernel_init, \n",
        "            bias_init=self.bias_init)(x)\n",
        "        return x\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    \"\"\"\n",
        "    A layer which passes the input through unchanged.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "class Static(nn.Module):\n",
        "    \"\"\"\n",
        "    A layer which just returns some static parameters.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "    kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.lecun_normal()\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        kernel = self.param('kernel',\n",
        "                            self.kernel_init,\n",
        "                            (self.features, ))\n",
        "        return kernel\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"A simple CNN model.\"\"\"\n",
        "    input_rank : int = None   \n",
        "    output_dim : int = None\n",
        "    layer_params : Sequence[dict] = None\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for params in self.layer_params:\n",
        "            x = nn.relu(Conv(**params)(x))\n",
        "        # No activations at the output\n",
        "        x = nn.Dense(features=self.output_dim)(x.flatten())\n",
        "        return x\n",
        "\n",
        "class DCNN(nn.Module):\n",
        "    \"\"\"A simple DCNN model.\"\"\"   \n",
        "\n",
        "    input_shape: Sequence[int] = None\n",
        "    layer_params: Sequence[dict] = None\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        input_features = onp.prod(onp.array(self.input_shape))\n",
        "        x = nn.Dense(features=input_features)(x)\n",
        "        x = x.reshape(self.input_shape)\n",
        "        # Note that the last layer doesn't have an activation\n",
        "        for params in self.layer_params:\n",
        "            x = ConvTranspose(**params)(nn.relu(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "metadata": {
        "id": "Ja-LJ88rybCV"
      },
      "outputs": [],
      "source": [
        "# @title Potential networks (outputs potentials on single observations)\n",
        "class PotentialNetwork(nn.Module):\n",
        "    def __call__(self, inputs):\n",
        "        Sigma, mu = self._generate_distribution_parameters(inputs)\n",
        "        # J, h = solve(Sigma, np.eye(mu.shape[-1])[None]), solve(Sigma, mu)\n",
        "        # if (len(J.shape) == 3):\n",
        "        #     seq_len, latent_dims, _ = J.shape\n",
        "        #     # lower diagonal blocks of precision matrix\n",
        "        #     L = np.zeros((seq_len-1, latent_dims, latent_dims))\n",
        "        # elif (len(J.shape) == 4):\n",
        "        #     batch_size, seq_len, latent_dims, _ = J.shape\n",
        "        #     # lower diagonal blocks of precision matrix\n",
        "        #     L = np.zeros((batch_size, seq_len-1, latent_dims, latent_dims))\n",
        "        # else:\n",
        "        #     L = np.zeros(tuple())\n",
        "        return {#\"J\": J, \"L\": L, \"h\": h, \n",
        "                \"Sigma\": Sigma, \"mu\": mu}\n",
        "\n",
        "    def _generate_distribution_parameters(self, inputs):\n",
        "        if (len(inputs.shape) == self.input_rank + 2):\n",
        "            # We have both a batch dimension and a time dimension\n",
        "            # and we have to vmap over both...!\n",
        "            return vmap(vmap(self._call_single, 0), 0)(inputs)\n",
        "        elif (len(inputs.shape) == self.input_rank + 1):\n",
        "            return vmap(self._call_single)(inputs)\n",
        "        elif (len(inputs.shape) == self.input_rank):\n",
        "            return self._call_single(inputs)\n",
        "        else:\n",
        "            # error\n",
        "            return None\n",
        "\n",
        "    def _call_single(self, inputs):\n",
        "        pass\n",
        "\n",
        "# A new, more general implementation of the Gaussian recognition network\n",
        "# Uses mean parameterization which works better empirically\n",
        "class GaussianRecognition(PotentialNetwork):\n",
        "\n",
        "    use_diag : int = None\n",
        "    input_rank : int = None\n",
        "    latent_dims : int = None\n",
        "    trunk_fn : nn.Module = None\n",
        "    head_mean_fn : nn.Module = None\n",
        "    head_log_var_fn : nn.Module = None\n",
        "    eps : float = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_params(cls, input_rank=1, input_dim=None, output_dim=None, \n",
        "                    trunk_type=\"Identity\", trunk_params=None, \n",
        "                    head_mean_type=\"MLP\", head_mean_params=None,\n",
        "                    head_var_type=\"MLP\", head_var_params=None, diagonal_covariance=False,\n",
        "                    cov_init=1, eps=1e-3): \n",
        "\n",
        "        if trunk_type == \"Identity\":\n",
        "            trunk_params = { \"features\": input_dim }\n",
        "        if head_mean_type == \"MLP\":\n",
        "            head_mean_params[\"features\"] += [output_dim]\n",
        "        if head_var_type == \"MLP\":\n",
        "            if (diagonal_covariance):\n",
        "                head_var_params[\"features\"] += [output_dim]\n",
        "            else:\n",
        "                head_var_params[\"features\"] += [output_dim * (output_dim + 1) // 2]\n",
        "            head_var_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_var_params[\"bias_init\"] = nn.initializers.constant(cov_init)\n",
        "\n",
        "        trunk_fn = globals()[trunk_type](**trunk_params)\n",
        "        head_mean_fn = globals()[head_mean_type](**head_mean_params)\n",
        "        head_log_var_fn = globals()[head_var_type](**head_var_params)\n",
        "\n",
        "        return cls(diagonal_covariance, input_rank, output_dim, trunk_fn, \n",
        "                   head_mean_fn, head_log_var_fn, eps)\n",
        "\n",
        "    def _call_single(self, inputs):\n",
        "        # Apply the trunk.\n",
        "        trunk_output = self.trunk_fn(inputs)\n",
        "        # Get the mean.\n",
        "        mu = self.head_mean_fn(trunk_output)\n",
        "        # Get the covariance parameters and build a full matrix from it.\n",
        "        var_output_flat = self.head_log_var_fn(trunk_output)\n",
        "        if self.use_diag:\n",
        "            Sigma = np.diag(softplus(var_output_flat) + self.eps)\n",
        "        else:\n",
        "            Sigma = lie_params_to_constrained(var_output_flat, self.latent_dims, self.eps)\n",
        "        # h = np.linalg.solve(Sigma, mu)\n",
        "        # J = np.linalg.inv(Sigma)\n",
        "        # lower diagonal blocks of precision matrix\n",
        "        return (Sigma, mu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 352,
      "metadata": {
        "id": "gmeCrsbCy96H"
      },
      "outputs": [],
      "source": [
        "# @title Posterior networks (outputs full posterior for entire sequence)\n",
        "# Outputs Gaussian distributions for the entire sequence at once\n",
        "class PosteriorNetwork(PotentialNetwork):\n",
        "    def __call__(self, inputs):\n",
        "        As, bs, Qs = self._generate_distribution_parameters(inputs)\n",
        "        return {\"As\": As, \"bs\": bs, \"Qs\": Qs}\n",
        "\n",
        "    def _generate_distribution_parameters(self, inputs):\n",
        "        is_batched = (len(inputs.shape) == self.input_rank+2)\n",
        "        if is_batched:\n",
        "            return vmap(self._call_single, in_axes=0)(inputs)\n",
        "        else:\n",
        "            assert(len(inputs.shape) == self.input_rank+1)\n",
        "            return self._call_single(inputs)\n",
        "\n",
        "class GaussianBiRNN(PosteriorNetwork):\n",
        "    \n",
        "    input_rank : int = None\n",
        "    rnn_dim : int = None\n",
        "    output_dim : int = None\n",
        "    forward_RNN : nn.Module = None\n",
        "    backward_RNN : nn.Module = None\n",
        "    input_fn : nn.Module = None\n",
        "    trunk_fn: nn.Module = None\n",
        "    head_mean_fn : nn.Module = None\n",
        "    head_log_var_fn : nn.Module = None\n",
        "    head_dyn_fn : nn.Module = None\n",
        "    eps : float = None\n",
        "        \n",
        "    @classmethod\n",
        "    def from_params(cls, input_rank=1, cell_type=nn.GRUCell,\n",
        "                    input_dim=None, rnn_dim=None, output_dim=None, \n",
        "                    input_type=\"MLP\", input_params=None,\n",
        "                    trunk_type=\"Identity\", trunk_params=None, \n",
        "                    head_mean_type=\"MLP\", head_mean_params=None,\n",
        "                    head_var_type=\"MLP\", head_var_params=None,\n",
        "                    head_dyn_type=\"MLP\", head_dyn_params=None,\n",
        "                    cov_init=1, eps=1e-4): \n",
        "\n",
        "        forward_RNN = nn.scan(cell_type, variable_broadcast=\"params\", \n",
        "                                             split_rngs={\"params\": False})()\n",
        "        backward_RNN = nn.scan(cell_type, variable_broadcast=\"params\", \n",
        "                                               split_rngs={\"params\": False}, reverse=True)()\n",
        "        if trunk_type == \"Identity\":\n",
        "            trunk_params = { \"features\": rnn_dim }\n",
        "        if input_type == \"MLP\":\n",
        "            input_params[\"features\"] += [rnn_dim]\n",
        "        if head_mean_type == \"MLP\":\n",
        "            head_mean_params[\"features\"] += [output_dim]\n",
        "        if head_var_type == \"MLP\":\n",
        "            head_var_params[\"features\"] += [output_dim * (output_dim + 1) // 2]\n",
        "            head_var_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_var_params[\"bias_init\"] = nn.initializers.constant(cov_init)\n",
        "        if head_dyn_type == \"MLP\":\n",
        "            head_dyn_params[\"features\"] += [output_dim ** 2,]\n",
        "            head_dyn_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_dyn_params[\"bias_init\"] = nn.initializers.zeros\n",
        "\n",
        "        trunk_fn = globals()[trunk_type](**trunk_params)\n",
        "        input_fn = globals()[input_type](**input_params)\n",
        "        head_mean_fn = globals()[head_mean_type](**head_mean_params)\n",
        "        head_log_var_fn = globals()[head_var_type](**head_var_params)\n",
        "        head_dyn_fn = globals()[head_dyn_type](**head_dyn_params)\n",
        "\n",
        "        return cls(input_rank, rnn_dim, output_dim, \n",
        "                   forward_RNN, backward_RNN, \n",
        "                   input_fn, trunk_fn, \n",
        "                   head_mean_fn, head_log_var_fn, head_dyn_fn, eps)\n",
        "\n",
        "    # Applied the BiRNN to a single sequence of inputs\n",
        "    def _call_single(self, inputs):\n",
        "        output_dim = self.output_dim\n",
        "        \n",
        "        inputs = vmap(self.input_fn)(inputs)\n",
        "        init_carry_forward = np.zeros((self.rnn_dim,))\n",
        "        _, out_forward = self.forward_RNN(init_carry_forward, inputs)\n",
        "        init_carry_backward = np.zeros((self.rnn_dim,))\n",
        "        _, out_backward = self.backward_RNN(init_carry_backward, inputs)\n",
        "        # Concatenate the forward and backward outputs\n",
        "        out_combined = np.concatenate([out_forward, out_backward], axis=-1)\n",
        "        \n",
        "        # Get the mean.\n",
        "        # vmap over the time dimension\n",
        "        b = vmap(self.head_mean_fn)(out_combined)\n",
        "\n",
        "        # Get the variance output and reshape it.\n",
        "        # vmap over the time dimension\n",
        "        var_output_flat = vmap(self.head_log_var_fn)(out_combined)\n",
        "        Q = vmap(lie_params_to_constrained, in_axes=(0, None, None))\\\n",
        "            (var_output_flat, output_dim, self.eps)\n",
        "        dynamics_flat = vmap(self.head_dyn_fn)(out_combined)\n",
        "        A = dynamics_flat.reshape((-1, output_dim, output_dim))\n",
        "\n",
        "        return (A, b, Q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "metadata": {
        "cellView": "form",
        "id": "caU8H-hMEB_f"
      },
      "outputs": [],
      "source": [
        "# @title Special architectures for PlaNet\n",
        "class PlaNetRecognitionWrapper:\n",
        "    def __init__(self, rec_net):\n",
        "        self.rec_net = rec_net\n",
        "\n",
        "    def init(self, key, *inputs):\n",
        "        return self.rec_net.init(key, *inputs)\n",
        "    \n",
        "    def apply(self, params, x):\n",
        "        return {\n",
        "            \"network_input\": self.rec_net.apply(params[\"rec_params\"], x)[\"h\"],\n",
        "            \"network_params\": params[\"post_params\"],\n",
        "        }\n",
        "\n",
        "class StochasticRNNCell(nn.Module):\n",
        "\n",
        "    output_dim : int = None\n",
        "    rnn_cell : nn.Module = None\n",
        "    trunk_fn: nn.Module = None\n",
        "    head_mean_fn : nn.Module = None\n",
        "    head_log_var_fn : nn.Module = None\n",
        "    eps : float = None\n",
        "        \n",
        "    @classmethod\n",
        "    def from_params(cls, cell_type=nn.GRUCell,\n",
        "                    rnn_dim=None, output_dim=None, \n",
        "                    trunk_type=\"Identity\", trunk_params=None, \n",
        "                    head_mean_type=\"MLP\", head_mean_params=None,\n",
        "                    head_var_type=\"MLP\", head_var_params=None,\n",
        "                    cov_init=1, eps=1e-4, **kwargs): \n",
        "\n",
        "        rnn_cell = cell_type()\n",
        "\n",
        "        if trunk_type == \"Identity\":\n",
        "            trunk_params = { \"features\": rnn_dim }\n",
        "        if head_mean_type == \"MLP\":\n",
        "            head_mean_params[\"features\"] += [output_dim]\n",
        "        if head_var_type == \"MLP\":\n",
        "            head_var_params[\"features\"] += [output_dim * (output_dim + 1) // 2]\n",
        "            head_var_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_var_params[\"bias_init\"] = nn.initializers.constant(cov_init)\n",
        "\n",
        "        trunk_fn = globals()[trunk_type](**trunk_params)\n",
        "        head_mean_fn = globals()[head_mean_type](**head_mean_params)\n",
        "        head_log_var_fn = globals()[head_var_type](**head_var_params)\n",
        "\n",
        "        return cls(output_dim, rnn_cell, trunk_fn, head_mean_fn, head_log_var_fn, eps)\n",
        "\n",
        "    # h: latent state that's carried to the next\n",
        "    # x: last sample\n",
        "    # u: input at this timestep\n",
        "    def __call__(self, h, x, u):\n",
        "        h, out = self.rnn_cell(h, np.concatenate([x, u]))\n",
        "        out = self.trunk_fn(out)\n",
        "        mean, cov_flat = self.head_mean_fn(out), self.head_log_var_fn(out)\n",
        "        cov = lie_params_to_constrained(cov_flat, self.output_dim, self.eps)\n",
        "        return h, (cov, mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "metadata": {
        "id": "_DU0-NEWx_Il"
      },
      "outputs": [],
      "source": [
        "# @title Emission network (outputs distribution instead of parameters)\n",
        "\n",
        "# This is largely for convenience\n",
        "class GaussianEmission(GaussianRecognition):\n",
        "    def __call__(self, inputs):\n",
        "        J, h = self._generate_distribution_parameters(inputs)\n",
        "        # TODO: inverting J is pretty bad numerically, perhaps save Cholesky instead?\n",
        "        if (len(J.shape) == 3):\n",
        "            Sigma = vmap(inv)(J)\n",
        "            mu = np.einsum(\"tij,tj->ti\", Sigma, h)\n",
        "        elif (len(J.shape) == 2):\n",
        "            Sigma = inv(J)\n",
        "            mu = np.linalg.solve(J, h)\n",
        "        else:\n",
        "            # Error\n",
        "            return None\n",
        "        return tfd.MultivariateNormalFullCovariance(\n",
        "            loc=mu, covariance_matrix=Sigma)\n",
        "        \n",
        "class GaussianDCNNEmission(PotentialNetwork):\n",
        "\n",
        "    input_rank : int = None\n",
        "    network : nn.Module = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_params(cls, **params):\n",
        "        network = DCNN(**params)\n",
        "        return cls(1, network)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        out = self._generate_distribution_parameters(inputs)\n",
        "        mu = out[\"mu\"]\n",
        "        # Adding a constant to prevent the model from getting too crazy\n",
        "        sigma = out[\"sigma\"] + 1e-4\n",
        "        return tfd.Normal(loc=mu, scale=sigma)\n",
        "\n",
        "    def _call_single(self, x):\n",
        "        out_raw = self.network(x)\n",
        "        mu_raw, sigma_raw = np.split(out_raw, 2, axis=-1)\n",
        "        # Get rid of the Sigmoid\n",
        "        # mu = sigmoid(mu_raw)\n",
        "        mu = mu_raw\n",
        "        sigma = softplus(sigma_raw)\n",
        "        # sigma = np.ones_like(mu) * 0.1\n",
        "        return { \"mu\": mu, \"sigma\": sigma }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xevru2BSSSZ"
      },
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 355,
      "metadata": {
        "id": "28XHvF41SVWK"
      },
      "outputs": [],
      "source": [
        "# @title Visualization/animation helpers\n",
        "\n",
        "# Returns a random projection matrix from ND to 2D\n",
        "def random_projection(seed, N):\n",
        "    key1, key2 = jr.split(seed, 2)\n",
        "    v1 = jr.normal(key1, (N,))\n",
        "    v2 = jr.normal(key2, (N,))\n",
        "\n",
        "    v1 /= np.linalg.norm(v1)\n",
        "    v2 -= v1 * np.dot(v1, v2)\n",
        "    v2 /= np.linalg.norm(v2)\n",
        "\n",
        "    return np.stack([v1, v2])\n",
        "\n",
        "def get_gaussian_draw_params(mu, Sigma, proj_seed=None):\n",
        "\n",
        "    Sigma = (Sigma + Sigma.T) * .5\n",
        "\n",
        "    if (mu.shape[0] > 2):\n",
        "        P = random_projection(proj_seed, mu.shape[0])\n",
        "        mu = P @ mu\n",
        "    angles = np.hstack([np.arange(0, 2*np.pi, 0.01), 0])\n",
        "    circle = np.vstack([np.sin(angles), np.cos(angles)])\n",
        "    min_eig = np.min(eigh(Sigma)[0])\n",
        "    eps = 1e-6\n",
        "    if (min_eig <= eps): Sigma += np.eye(Sigma.shape[0]) * eps\n",
        "    L = np.linalg.cholesky(Sigma)\n",
        "    u, svs, vt = svd(P @ L)\n",
        "    ellipse = np.dot(u * svs, circle) * 2\n",
        "    return (mu[0], mu[1]), (ellipse[0, :] + mu[0], ellipse[1, :] + mu[1])\n",
        "\n",
        "def plot_gaussian_2D(mu, Sigma, proj_seed=None, ax=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Helper function to plot 2D Gaussian contours\n",
        "    \"\"\"\n",
        "    (px, py), (exs, eys) = get_gaussian_draw_params(mu, Sigma, proj_seed)\n",
        "\n",
        "    ax = plt.gca() if ax is None else ax\n",
        "    point = ax.plot(px, py, marker='D', **kwargs)\n",
        "    line, = ax.plot(exs, eys, **kwargs)\n",
        "    return (point, line)\n",
        "\n",
        "def get_artists(ax, mus, Sigmas, proj_seed, num_pts, **draw_params):\n",
        "    point_artists = []\n",
        "    line_artists = []\n",
        "\n",
        "    for j in range(num_pts):\n",
        "        mean_params, cov_params = get_gaussian_draw_params(mus[j], \n",
        "                                                           Sigmas[j], \n",
        "                                                           proj_seed)\n",
        "        point = ax.plot(mean_params[0], mean_params[1], marker='D', \n",
        "                        color=colors[j], **draw_params)[0]\n",
        "        line = ax.plot(cov_params[0], cov_params[1], \n",
        "                       color=colors[j], **draw_params)[0]\n",
        "        point_artists.append(point)\n",
        "        line_artists.append(line)\n",
        "    return point_artists, line_artists\n",
        "\n",
        "def update_draw_params(point_artists, line_artists, mus, Sigmas, proj_seed, num_pts):\n",
        "    for i in range(num_pts):\n",
        "        mean_params, cov_params = get_gaussian_draw_params(mus[i], Sigmas[i], proj_seed)\n",
        "        point_artists[i].set_data(mean_params[0], mean_params[1])\n",
        "        line_artists[i].set_data(cov_params[0], cov_params[1])\n",
        "\n",
        "# Some animation helpers\n",
        "def animate_gaussians(inf_mus, inf_Sigmas, \n",
        "                      tgt_mus, tgt_Sigmas, \n",
        "                      true_mus, true_Sigmas,\n",
        "                      num_pts,\n",
        "                      proj_seed=None, x_lim=None, y_lim=None, **kwargs):\n",
        "    proj_seed = jr.PRNGKey(0) if proj_seed is None else proj_seed\n",
        "    print(\"Animating Gaussian blobs...!\")\n",
        "    # First set up the figure, the axis, and the plot element we want to animate\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlim(-10, 10)\n",
        "    ax.set_ylim(-10, 10)\n",
        "    plt.close()\n",
        "    \n",
        "\n",
        "    tgt_points, tgt_lines = get_artists(ax, tgt_mus[0], tgt_Sigmas[0], proj_seed, num_pts,\n",
        "                                        alpha=0.3, linestyle=\"dotted\")\n",
        "    inf_points, inf_lines = get_artists(ax, inf_mus[0], inf_Sigmas[0], proj_seed, num_pts)\n",
        "    true_points, true_lines = get_artists(ax, true_mus, true_Sigmas, \n",
        "                                          proj_seed, num_pts, alpha=0.2)\n",
        "\n",
        "    artists = tgt_points + tgt_lines + inf_points + inf_lines + true_points + true_lines\n",
        "\n",
        "    T = len(inf_mus)\n",
        "\n",
        "    # animation function. This is called sequentially  \n",
        "    def animate(i):\n",
        "        update_draw_params(tgt_points, tgt_lines, tgt_mus[i], tgt_Sigmas[i], proj_seed, num_pts)\n",
        "        update_draw_params(inf_points, inf_lines, inf_mus[i], inf_Sigmas[i], proj_seed, num_pts)\n",
        "        clear_output(wait=True)\n",
        "        print(\"Processing frame #{}/{}\".format(i+1, T))\n",
        "        return artists\n",
        "    \n",
        "    if x_lim is not None:\n",
        "        ax.set_xlim(x_lim)\n",
        "    if y_lim is not None:\n",
        "        ax.set_ylim(y_lim)\n",
        "\n",
        "    anim = animation.FuncAnimation(fig, animate, \n",
        "                                frames=T, interval=50, blit=True)\n",
        "    print(\"Frames created! Displaying animation in output cell...\")\n",
        "    # Note: below is the part which makes it work on Colab\n",
        "    rc('animation', html='jshtml')\n",
        "    return anim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "metadata": {
        "cellView": "form",
        "id": "EUG5JgVpTkC3"
      },
      "outputs": [],
      "source": [
        "# @title Helper for computing posterior marginals\n",
        "def get_emission_matrices(dec_params):\n",
        "    eps = 1e-4\n",
        "    dec_mean_params = dec_params[\"params\"][\"head_mean_fn\"][\"Dense_0\"]\n",
        "    dec_cov_params = dec_params[\"params\"][\"head_log_var_fn\"][\"Dense_0\"]\n",
        "    C_, d_ = dec_mean_params[\"kernel\"].T, dec_mean_params[\"bias\"]\n",
        "    R_ = np.diag(softplus(dec_cov_params[\"bias\"]) + eps)\n",
        "    return { \"C\": C_, \"d\": d_, \"R\": R_ }\n",
        "\n",
        "def get_marginals_and_targets(seed, data, num_points, model, \n",
        "                              past_params, true_model_params):\n",
        "    N, T = data.shape[:2]\n",
        "    rand_sample = jr.permutation(seed, onp.arange(N * T))[:num_points]\n",
        "    trials = rand_sample // T\n",
        "    times = rand_sample % T\n",
        "\n",
        "    # Compute a linear transformation in the latent space that will attempt to \n",
        "    # align the learned posterior to the true posterior\n",
        "    C, d = true_model_params[\"C\"], true_model_params[\"d\"]\n",
        "    CTC = C.T @ C\n",
        "    C_pinv = np.linalg.solve(CTC, C.T)\n",
        "\n",
        "    def align_latents(mus, Sigmas, p):\n",
        "        emissions_matrices = get_emission_matrices(p)\n",
        "        C_, d_ = emissions_matrices[\"C\"], emissions_matrices[\"d\"]\n",
        "        P = C_pinv @ C_\n",
        "        mus = np.einsum(\"ij,nj->ni\", P, mus) + (C_pinv @ (d_ - d))[None,:]\n",
        "        Sigmas = np.einsum(\"ij,njk,kl->nil\", P, Sigmas, P.T)\n",
        "        return mus, Sigmas\n",
        "\n",
        "    def posterior_mean_and_cov(post_params, t):\n",
        "        dist = model.posterior.distribution(post_params)\n",
        "        return dist.mean()[t], dist.covariance()[t]\n",
        "\n",
        "    def gaussian_posterior_mean_and_cov(post_params, t):\n",
        "        dist = true_lds.posterior.distribution(post_params)\n",
        "        return dist.mean()[t], dist.covariance()[t]\n",
        "\n",
        "    index_into_leaves = lambda l: l[trials]\n",
        "    \n",
        "    inf_mus = []\n",
        "    inf_Sigmas = []\n",
        "    tgt_mus = []\n",
        "    tgt_Sigmas = []\n",
        "    \n",
        "    true_lds = LDS(model.prior.latent_dims, T)\n",
        "    true_post_params = vmap(true_lds.e_step, in_axes=(None, 0))\\\n",
        "        (true_model_params, data[trials])\n",
        "    true_mus, true_Sigmas = vmap(gaussian_posterior_mean_and_cov)(true_post_params, times)\n",
        "\n",
        "    # TODO: this is temporary! Only for testing parallel KF!\n",
        "    base = LieParameterizedLinearGaussianChainPrior(model.prior.latent_dims, T)\n",
        "    model_lds = LDS(model.prior.latent_dims, T, base=base)\n",
        "\n",
        "    for i in range(len(past_params)):\n",
        "        model_params = past_params[i]\n",
        "        post_params = jax.tree_util.tree_map(index_into_leaves, \n",
        "                                             model_params[\"post_params\"])\n",
        "        dec_params = model_params[\"dec_params\"]\n",
        "        prior_params = deepcopy(model_params[\"prior_params\"])\n",
        "        # Compute posterior marginals\n",
        "        mus, Sigmas = vmap(posterior_mean_and_cov)(post_params, times)\n",
        "        mus, Sigmas = align_latents(mus, Sigmas, dec_params)\n",
        "        inf_mus.append(mus)\n",
        "        inf_Sigmas.append(Sigmas)\n",
        "\n",
        "        # Infer true posterior under current model params\n",
        "        prior_params.update(get_emission_matrices(dec_params))\n",
        "        tgt_post_params = vmap(model_lds.e_step, in_axes=(None, 0))(prior_params, data[trials])\n",
        "        mus, Sigmas = vmap(gaussian_posterior_mean_and_cov)(tgt_post_params, times)\n",
        "        mus, Sigmas = align_latents(mus, Sigmas, dec_params)\n",
        "\n",
        "        tgt_mus.append(mus)\n",
        "        tgt_Sigmas.append(Sigmas)\n",
        "    return inf_mus, inf_Sigmas, tgt_mus, tgt_Sigmas, true_mus, true_Sigmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "metadata": {
        "id": "aTYdrgHMdJct"
      },
      "outputs": [],
      "source": [
        "# Trying to figure out the proper way of plotting the projection\n",
        "# Of high dimensional Gaussians\n",
        "# key = jr.split(key)[1]\n",
        "# dim = 5\n",
        "# Q = jr.uniform(key, shape=(dim, dim))\n",
        "# A = Q @ Q.T\n",
        "# L = cholesky(A)\n",
        "# P = random_projection(key_0, dim)\n",
        "# plot_gaussian_2D(np.zeros(dim), A, key_0)\n",
        "# points = jr.normal(key_0, (400, dim))\n",
        "# points /= np.sum(points ** 2, axis=1, keepdims=True) ** .5\n",
        "# points = P @ L @ points.T\n",
        "# plt.scatter(points[0, :], points[1, :], s=1)\n",
        "# u, svs, vt = svd(P @ L)\n",
        "# plt.scatter(u[0][0] * svs[0], u[1][0] * svs[0])\n",
        "# plt.scatter(u[0][1] * svs[1], u[1][1] * svs[1])\n",
        "# # plt.scatter(u[0][0] * np.sqrt(svs[0]), u[1][0] * np.sqrt(svs[0]))\n",
        "# # plt.scatter(u[0][1] * svs[1], u[1][1] * svs[1])\n",
        "# # P = random_projection(key_0, 3)\n",
        "# angles = np.hstack([np.arange(0, 2*np.pi, 0.01), 0])\n",
        "# circle = np.vstack([np.sin(angles), np.cos(angles)])\n",
        "# ellipse = np.dot(u * svs, circle) * 2\n",
        "# plt.plot(ellipse[0,:], ellipse[1,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ImmaouPD-G"
      },
      "source": [
        "## Define training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {
        "cellView": "form",
        "id": "cPqoyXb6PgpV"
      },
      "outputs": [],
      "source": [
        "# @title Trainer object \n",
        "from time import time\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    model: a pytree node\n",
        "    loss (key, params, model, data, **train_params) -> (loss, aux)\n",
        "        Returns a loss (a single float) and an auxillary output (e.g. posterior)\n",
        "    init (key, model, data, **train_params) -> (params, opts)\n",
        "        Returns the initial parameters and optimizers to go with those parameters\n",
        "    update (params, grads, opts, model, aux, **train_params) -> (params, opts)\n",
        "        Returns updated parameters, optimizers\n",
        "    \"\"\"\n",
        "    def __init__(self, model, \n",
        "                 train_params=None, \n",
        "                 init=None, \n",
        "                 loss=None, \n",
        "                 val_loss=None,\n",
        "                 update=None,\n",
        "                 initial_params=None):\n",
        "        # Trainer state\n",
        "        self.params = initial_params\n",
        "        self.model = model\n",
        "        self.past_params = []\n",
        "        self.time_spent = []\n",
        "\n",
        "        if train_params is None:\n",
        "            train_params = dict()\n",
        "\n",
        "        self.train_params = train_params\n",
        "\n",
        "        if init is not None:\n",
        "            self.init = init\n",
        "        if loss is not None:\n",
        "            self.loss = loss\n",
        "\n",
        "        self.val_loss = val_loss or self.loss\n",
        "        if update is not None: \n",
        "            self.update = update\n",
        "\n",
        "    # @partial(jit, static_argnums=(0,))\n",
        "    def train_step(self, key, params, data, opt_states):\n",
        "        model = self.model\n",
        "        results = \\\n",
        "            jax.value_and_grad(\n",
        "                lambda params: partial(self.loss, **self.train_params)(key, model, data, params), has_aux=True)(params)\n",
        "        (loss, aux), grads = results\n",
        "        params, opts = self.update(params, grads, self.opts, opt_states, model, aux, **self.train_params)\n",
        "        return params, opts, (loss, aux), grads\n",
        "\n",
        "    # @partial(jit, static_argnums=(0,))\n",
        "    def val_step(self, key, params, data):\n",
        "        return self.val_loss(key, self.model, data, params, **self.train_params)\n",
        "\n",
        "    # def test_step(self, key, params, model, data):\n",
        "    #     loss_out = self.loss(key, params, model, data)\n",
        "    #     return loss_out\n",
        "\n",
        "    \"\"\"\n",
        "    Callback: a function that takes training iterations and relevant parameter\n",
        "        And logs to WandB\n",
        "    \"\"\"\n",
        "    def train(self, data_dict, max_iters, \n",
        "              callback=None, val_callback=None, \n",
        "              summary=None, key=None,\n",
        "              early_stop_start=5000, \n",
        "              max_lose_streak=1000):\n",
        "\n",
        "        if key is None:\n",
        "            key = jr.PRNGKey(0)\n",
        "\n",
        "        model = self.model\n",
        "        train_data = data_dict[\"train_data\"]\n",
        "        batch_size = self.train_params.get(\"batch_size\") or train_data.shape[0]\n",
        "        num_batches = train_data.shape[0] // batch_size\n",
        "\n",
        "        init_key, key = jr.split(key, 2)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.params, self.opts, self.opt_states = self.init(init_key, model, \n",
        "                                                       train_data[:batch_size], \n",
        "                                                       self.params,\n",
        "                                                       **self.train_params)\n",
        "        self.train_losses = []\n",
        "        self.test_losses = []\n",
        "        self.val_losses = []\n",
        "        self.past_params = []\n",
        "\n",
        "        pbar = trange(max_iters)\n",
        "        pbar.set_description(\"[jit compling...]\")\n",
        "        \n",
        "        mask_start = self.train_params.get(\"mask_start\")\n",
        "        if (mask_start):\n",
        "            mask_size = self.train_params[\"mask_size\"]\n",
        "            self.train_params[\"mask_size\"] = 0\n",
        "\n",
        "        train_step = jit(self.train_step)\n",
        "        val_step = jit(self.val_step)\n",
        "\n",
        "        best_loss = None\n",
        "        best_itr = 0\n",
        "        val_loss = None\n",
        "\n",
        "        for itr in pbar:\n",
        "            train_key, val_key, key = jr.split(key, 3)\n",
        "\n",
        "            batch_id = itr % num_batches\n",
        "            batch_start = batch_id * batch_size\n",
        "\n",
        "            t = time()\n",
        "            # Training step\n",
        "            # ----------------------------------------\n",
        "            step_results = train_step(train_key, self.params, \n",
        "                           train_data[batch_start:batch_start+batch_size], self.opt_states)\n",
        "            self.params, self.opt_states, loss_out, grads = \\\n",
        "                jax.tree_map(lambda x: x.block_until_ready(), step_results)\n",
        "            # ----------------------------------------\n",
        "            dt = time() - t\n",
        "            self.time_spent.append(dt)\n",
        "\n",
        "            loss, aux = loss_out\n",
        "            self.train_losses.append(loss)\n",
        "            pbar.set_description(\"LP: {:.3f}\".format(loss))\n",
        "\n",
        "            if batch_id == num_batches - 1:\n",
        "                # We're at the end of an epoch\n",
        "                # We could randomly shuffle the data\n",
        "                # train_data = jr.permutation(key, train_data)\n",
        "                if (self.train_params.get(\"use_validation\")):\n",
        "                    val_loss_out = val_step(val_key, self.params, data_dict[\"val_data\"])\n",
        "                    if (val_callback): val_callback(self, val_loss_out, data_dict)\n",
        "                    val_loss, _ = val_loss_out\n",
        "                    \n",
        "            if not self.train_params.get(\"use_validation\") or val_loss is None:\n",
        "                curr_loss = loss\n",
        "            else:\n",
        "                curr_loss = val_loss\n",
        "\n",
        "            if itr >= early_stop_start:\n",
        "                if best_loss is None or curr_loss < best_loss:\n",
        "                    best_itr = itr\n",
        "                    best_loss = curr_loss\n",
        "                if curr_loss > best_loss and itr - best_itr > max_lose_streak:\n",
        "                    print(\"Early stopping!\")\n",
        "                    break\n",
        "\n",
        "            if (callback): callback(self, loss_out, data_dict, grads)\n",
        "\n",
        "            # Record parameters\n",
        "            record_params = self.train_params.get(\"record_params\")\n",
        "            if record_params and record_params(itr):\n",
        "                curr_params = deepcopy(self.params)\n",
        "                curr_params[\"iteration\"] = itr\n",
        "                self.past_params.append(curr_params)\n",
        "\n",
        "            if (mask_start and itr == mask_start):\n",
        "                self.train_params[\"mask_size\"] = mask_size\n",
        "                train_step = jit(self.train_step)\n",
        "                val_step = jit(self.val_step)\n",
        "\n",
        "        if summary:\n",
        "            summary(self, data_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "metadata": {
        "id": "KmTBoMwgArBw"
      },
      "outputs": [],
      "source": [
        "# @title Logging to WandB\n",
        "\n",
        "def visualize_lds(trainer, data_dict, aux):\n",
        "    data = data_dict[\"train_data\"]\n",
        "    true_model_params = data_dict[\"lds_params\"]\n",
        "    model = trainer.model\n",
        "    params = [trainer.params]\n",
        "    num_pts = 10\n",
        "    # We want to visualize the posterior marginals\n",
        "    inf_mus, inf_Sigmas, tgt_mus, tgt_Sigmas, true_mus, true_Sigmas = \\\n",
        "        get_marginals_and_targets(key_0, data, num_pts, model, params, true_model_params)\n",
        "    # Create the axis\n",
        "    fig, ax = plt.subplots()\n",
        "    # Plot each of the groups\n",
        "    tgt_points, tgt_lines = get_artists(ax, tgt_mus[0], tgt_Sigmas[0], key_0, num_pts,\n",
        "                                        alpha=0.3, linestyle=\"dotted\", label=\"current target\")\n",
        "    inf_points, inf_lines = get_artists(ax, inf_mus[0], inf_Sigmas[0], key_0, num_pts, label=\"inferred\")\n",
        "    true_points, true_lines = get_artists(ax, true_mus, true_Sigmas, \n",
        "                                          key_0, num_pts, alpha=0.2, label=\"true target\")\n",
        "    # The legend is too large and blocks most of the plot\n",
        "    # plt.legend()\n",
        "    # Relimit the axes\n",
        "    ax.relim()\n",
        "    # update ax.viewLim using the new dataLim\n",
        "    ax.autoscale_view()\n",
        "    fig.canvas.draw()\n",
        "    img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
        "    post_img = wandb.Image(img, caption=\"Posterior marginals versus targets\")\n",
        "    plt.close()\n",
        "    return {\n",
        "        \"Posterior marginals\": post_img\n",
        "    }\n",
        "\n",
        "def visualize_pendulum(trainer, aux):\n",
        "    # This assumes single sequence has shape (100, 24, 24, 1)\n",
        "    recon = aux[\"reconstruction\"][0][0]\n",
        "    # Show the sequence as a block of images\n",
        "    stacked = recon.reshape(10, 24 * 10, 24)\n",
        "    imgrid = stacked.swapaxes(0, 1).reshape(24 * 10, 24 * 10)\n",
        "    recon_img = wandb.Image(onp.array(imgrid), caption=\"Sample Reconstruction\")\n",
        "\n",
        "    fig = plt.figure()\n",
        "    mask = aux[\"mask\"][0]\n",
        "    post_sample = aux[\"posterior_samples\"][0][0]\n",
        "    top, bot = np.max(post_sample) + 5, np.min(post_sample) - 5\n",
        "    left, right = 0, post_sample.shape[0]\n",
        "    plt.imshow(mask[None], cmap=\"gray\", alpha=.4, vmin=0, vmax=1,\n",
        "               extent=(left, right, top, bot))\n",
        "    plt.plot(post_sample)\n",
        "    fig.canvas.draw()\n",
        "    img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
        "    post_img = wandb.Image(img, caption=\"Posterior Sample\")\n",
        "    plt.close()\n",
        "    return {\n",
        "        \"Reconstruction\": recon_img, \n",
        "        \"Posterior Sample\": post_img\n",
        "    }\n",
        "\n",
        "def get_group_name(run_params):\n",
        "    p = run_params\n",
        "    run_type = \"\" if p[\"inference_method\"] in [\"EM\", \"GT\", \"SMC\"] else \"_\" + p[\"run_type\"]\n",
        "    if p[\"dataset\"] == \"pendulum\":\n",
        "        dataset_summary = \"pendulum\"\n",
        "    elif p[\"dataset\"] == \"lds\":\n",
        "        d = p[\"dataset_params\"]\n",
        "        dataset_summary = \"lds_dims_{}_{}_noises_{}_{}\".format(\n",
        "            d[\"latent_dims\"], d[\"emission_dims\"], \n",
        "            d[\"dynamics_cov\"], d[\"emission_cov\"])\n",
        "    else:\n",
        "        dataset_summary = \"???\"\n",
        "\n",
        "    model_summary = \"_{}d_latent_\".format(p[\"latent_dims\"]) + p[\"inference_method\"]\n",
        "\n",
        "    group_tag = p.get(\"group_tag\") or \"\"\n",
        "    if group_tag != \"\": group_tag += \"_\"\n",
        "\n",
        "    group_name = (group_tag +\n",
        "        dataset_summary\n",
        "        + model_summary\n",
        "        + run_type\n",
        "    )\n",
        "    return group_name\n",
        "\n",
        "def validation_log_to_wandb(trainer, loss_out, data_dict):\n",
        "    p = trainer.train_params\n",
        "    if not p.get(\"log_to_wandb\"): return\n",
        "    \n",
        "    project_name = p[\"project_name\"]\n",
        "    group_name = get_group_name(p)\n",
        "\n",
        "    obj, aux = loss_out\n",
        "    elbo = -obj\n",
        "    kl = np.mean(aux[\"kl\"])\n",
        "    ell = np.mean(aux[\"ell\"])\n",
        "\n",
        "    model = trainer.model\n",
        "    prior = model.prior\n",
        "    D = prior.latent_dims\n",
        "    prior_params = trainer.params[\"prior_params\"]\n",
        "\n",
        "    visualizations = {}\n",
        "    if p[\"dataset\"] == \"pendulum\":\n",
        "        visualizations = visualize_pendulum(trainer, aux)\n",
        "        pred_ll = np.mean(aux[\"prediction_ll\"])\n",
        "        visualizations = {\n",
        "            \"Validation reconstruction\": visualizations[\"Reconstruction\"], \n",
        "            \"Validation posterior sample\": visualizations[\"Posterior Sample\"],\n",
        "            \"Validation prediction log likelihood\": pred_ll\n",
        "        }\n",
        "        \n",
        "    to_log = {\"Validation ELBO\": elbo, \"Validation KL\": kl, \"Validation likelihood\": ell,}\n",
        "    to_log.update(visualizations)\n",
        "    wandb.log(to_log)\n",
        "\n",
        "def log_to_wandb(trainer, loss_out, data_dict, grads):\n",
        "    p = trainer.train_params\n",
        "    if not p.get(\"log_to_wandb\"): return\n",
        "    \n",
        "    project_name = p[\"project_name\"]\n",
        "    group_name = get_group_name(p)\n",
        "\n",
        "    itr = len(trainer.train_losses) - 1\n",
        "    if len(trainer.train_losses) == 1:\n",
        "        wandb.init(project=project_name, group=group_name, config=p)\n",
        "        pprint(p)\n",
        "\n",
        "    obj, aux = loss_out\n",
        "    elbo = -obj\n",
        "    kl = np.mean(aux[\"kl\"])\n",
        "    ell = np.mean(aux[\"ell\"])\n",
        "\n",
        "    model = trainer.model\n",
        "    prior = model.prior\n",
        "    D = prior.latent_dims\n",
        "    prior_params = trainer.params[\"prior_params\"]\n",
        "    if (p.get(\"use_natural_grad\")):\n",
        "        Q = prior_params[\"Q\"]\n",
        "        A = prior_params[\"A\"]\n",
        "    else:\n",
        "        Q = lie_params_to_constrained(prior_params[\"Q\"], D)\n",
        "        A = prior_params[\"A\"]\n",
        "\n",
        "    eigs = eigh(Q)[0]\n",
        "    Q_cond_num = np.max(eigs) / np.min(eigs)\n",
        "    svs = svd(A)[1]\n",
        "    max_sv, min_sv = np.max(svs), np.min(svs)\n",
        "    A_cond_num = max_sv / min_sv\n",
        "\n",
        "    # Also log the prior params gradients\n",
        "    # prior_grads = grads[\"prior_params\"][\"sgd_params\"]\n",
        "    # prior_grads_norm = np.linalg.norm(\n",
        "    #     jax.tree_util.tree_leaves(tree_map(np.linalg.norm, prior_grads)))\n",
        "\n",
        "    visualizations = {}\n",
        "    if (itr % p[\"plot_interval\"] == 0):\n",
        "        if p[\"dataset\"] == \"lds\" and p.get(\"visualize_training\"):\n",
        "            visualizations = visualize_lds(trainer, data_dict, aux)\n",
        "        elif p[\"dataset\"] == \"pendulum\" and p.get(\"visualize_training\"):\n",
        "            visualizations = visualize_pendulum(trainer, aux)\n",
        "\n",
        "        # fig = plt.figure()\n",
        "        # prior_sample = prior.sample(prior_params, shape=(1,), key=jr.PRNGKey(0))[0]\n",
        "        # plt.plot(prior_sample)\n",
        "        # fig.canvas.draw()\n",
        "        # img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
        "        # prior_img = wandb.Image(img, caption=\"Prior Sample\")\n",
        "        # plt.close()\n",
        "        # visualizations[\"Prior sample\"] = prior_img\n",
        "    # Also log the learning rates\n",
        "    lr = p[\"learning_rate\"] \n",
        "    lr = lr if isinstance(lr, float) else lr(itr)\n",
        "    prior_lr = p[\"prior_learning_rate\"] \n",
        "    prior_lr = prior_lr if isinstance(prior_lr, float) else prior_lr(itr)\n",
        "\n",
        "    to_log = { \"ELBO\": elbo, \"KL\": kl, \"Likelihood\": ell, # \"Prior graident norm\": prior_grads_norm,\n",
        "               \"Max singular value of A\": max_sv, \"Min singular value of A\": min_sv,\n",
        "               \"Condition number of A\": A_cond_num, \"Condition number of Q\": Q_cond_num,\n",
        "               \"Learning rate\": lr, \"Prior learning rate\": prior_lr }\n",
        "    to_log.update(visualizations)\n",
        "    wandb.log(to_log)\n",
        "\n",
        "def save_params_to_wandb(trainer, data_dict):\n",
        "    file_name = \"parameters.pkl\"\n",
        "    with open(file_name, \"wb\") as f:\n",
        "        pkl.dump(trainer.past_params, f)\n",
        "        wandb.save(file_name, policy=\"now\")\n",
        "\n",
        "def on_error(data_dict, model_dict):\n",
        "    save_params_to_wandb(model_dict[\"trainer\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Summary functions\n",
        "\n",
        "def predict_multiple(run_params, model_params, model, data, T, key, num_samples=100):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        Posterior mean (T, D)\n",
        "        Posterior covariance (T, D, D)\n",
        "        Predictions (N, T//2, D)\n",
        "        Expected prediction log likelihood (lower bound of MLL) (T//2, D)\n",
        "    \"\"\"\n",
        "    seq_len = model.prior.seq_len\n",
        "    D = model.prior.latent_dims\n",
        "    model.prior.seq_len = T\n",
        "    model.posterior.seq_len = T\n",
        "\n",
        "    out = model.elbo(key, data[:T], model_params, **run_params)\n",
        "    post_params = out[\"posterior_params\"]\n",
        "    posterior = model.posterior.distribution(post_params)\n",
        "    \n",
        "    # Get the final mean and covariance\n",
        "    post_mean, post_covariance = posterior.mean, posterior.covariance\n",
        "    mu, Sigma = post_mean[T-1], post_covariance[T-1]\n",
        "    # Build the posterior object on the future latent states \n",
        "    # (\"the posterior predictive distribution\")\n",
        "    # Convert unconstrained params to constrained dynamics parameters\n",
        "    dynamics = model.prior.get_dynamics_params(model_params[\"prior_params\"])\n",
        "    pred_posterior = LinearGaussianChain.from_stationary_dynamics(\n",
        "        mu, Sigma, dynamics[\"A\"], dynamics[\"b\"], dynamics[\"Q\"], seq_len-T+1) # Note the +1\n",
        "\n",
        "    # Sample from it and evaluate the log likelihood\n",
        "    x_preds = pred_posterior.sample(seed=key, sample_shape=(num_samples,))\n",
        "\n",
        "    def pred_ll(x_pred):\n",
        "        likelihood_dist = model.decoder.apply(model_params[\"dec_params\"], x_pred)\n",
        "        return likelihood_dist.log_prob(data[T:])\n",
        "\n",
        "    pred_lls = vmap(pred_ll)(x_preds[:,1:])\n",
        "    # This assumes the pendulum dataset\n",
        "    # Which has 3d observations (width, height, channels)\n",
        "    pred_lls = pred_lls.sum(axis=(2, 3, 4))\n",
        "    # We want to estimate \\log p_tilde(y2|y1) = \\log \\int p(y2|x2) q(x2|y1) dx2\n",
        "    # ~ \\log E_q(x2|y1)[p(y2|x2)] \n",
        "    # So instead of mean we compute the logsumexp\n",
        "    pred_lls = logsumexp(pred_lls, axis=0) - np.log(num_samples)\n",
        "    \n",
        "    # Revert the model sequence length\n",
        "    model.prior.seq_len = seq_len\n",
        "    model.posterior.seq_len = seq_len\n",
        "\n",
        "    # Keep only the first 10 samples\n",
        "    return post_mean, post_covariance, x_preds[:10], pred_lls\n",
        "\n",
        "def get_latents_and_predictions(run_params, model_params, model, data_dict):\n",
        "    \n",
        "    key = jr.PRNGKey(42)\n",
        "\n",
        "    # Try to decode true states linearly from model encodings\n",
        "    num_predictions = 10\n",
        "    num_examples = 20\n",
        "    num_frames = 100\n",
        "\n",
        "    def encode(data):\n",
        "        out = model.elbo(jr.PRNGKey(0), data, model_params, **run_params)\n",
        "        post_params = out[\"posterior_params\"]\n",
        "        post_dist = model.posterior.distribution(post_params)\n",
        "        return post_dist.mean, post_dist.covariance\n",
        "\n",
        "    train_data = data_dict[\"train_data\"][:,:num_frames]\n",
        "    Ex, _ = vmap(encode)(train_data)\n",
        "    # Figure out the linear regression weights which decodes true states\n",
        "    states = data_dict[\"train_states\"][:]\n",
        "    # states = targets[:,::2] # We subsampled the data during training to make pendulum swing faster\n",
        "    # Compute the true angles and angular velocities\n",
        "    train_thetas = np.arctan2(states[:,:,0], states[:,:,1])\n",
        "    train_omegas = train_thetas[:,1:]-train_thetas[:,:-1]\n",
        "    thetas = train_thetas.flatten()\n",
        "    omegas = train_omegas.flatten()\n",
        "    # Fit the learned representations to the true states\n",
        "    D = model.prior.latent_dims\n",
        "    xs_theta = Ex.reshape((-1, D))\n",
        "    xs_omega = Ex[:,1:].reshape((-1, D))\n",
        "    W_theta, _, _, _ = np.linalg.lstsq(xs_theta, thetas)\n",
        "    W_omega, _, _, _ = np.linalg.lstsq(xs_omega, omegas)\n",
        "\n",
        "    # Evaluate mse on test data\n",
        "    test_states = data_dict[\"val_states\"][:, :num_frames]\n",
        "    test_data = data_dict[\"val_data\"][:, :num_frames]\n",
        "    thetas = np.arctan2(test_states[:,:,0], test_states[:,:,1])\n",
        "    omegas = thetas[:,1:]-thetas[:,:-1]\n",
        "\n",
        "    partial_mean, partial_cov, test_preds, test_pred_lls = vmap(predict_multiple, \n",
        "        in_axes=(None, None, None, 0, None, None, None))\\\n",
        "        (run_params, model_params, model, test_data, num_frames//2, key, num_predictions)\n",
        "    test_mean, test_cov = vmap(encode)(test_data)\n",
        "    pred_thetas = np.einsum(\"i,...i->...\", W_theta, test_mean)\n",
        "    theta_mse = np.mean((pred_thetas - thetas) ** 2)\n",
        "    pred_omegas = np.einsum(\"i,...i->...\", W_omega, test_mean[:,1:])\n",
        "    omega_mse = np.mean((pred_omegas - omegas) ** 2)\n",
        "\n",
        "    return {\n",
        "        \"latent_mean\": test_mean,\n",
        "        \"latent_covariance\": test_cov,\n",
        "        \"latent_mean_partial\": partial_mean,\n",
        "        \"latent_covariance_partial\": partial_cov,\n",
        "        \"prediction_lls\": test_pred_lls,\n",
        "        \"predictions\": test_preds,\n",
        "        \"w_theta\": W_theta,\n",
        "        \"w_omega\": W_omega,\n",
        "        \"theta_mse\": theta_mse,\n",
        "        \"omega_mse\": omega_mse,\n",
        "    }\n",
        "\n",
        "def summarize_pendulum_run(trainer, data_dict):\n",
        "    file_name = \"parameters.pkl\"\n",
        "    with open(file_name, \"wb\") as f:\n",
        "        pkl.dump(trainer.past_params, f)\n",
        "        wandb.save(file_name, policy=\"now\")\n",
        "    # Compute predictions on test set\n",
        "    # Set the mask size to 0 for summary!!\n",
        "    run_params = deepcopy(trainer.train_params)\n",
        "    run_params[\"mask_size\"] = 0\n",
        "    results = get_latents_and_predictions(\n",
        "        run_params, trainer.params, trainer.model, data_dict)\n",
        "    file_name = \"results.npy\"\n",
        "    np.save(file_name, results, allow_pickle=True)\n",
        "    wandb.save(file_name, policy=\"now\")"
      ],
      "metadata": {
        "id": "dEiXxL9s_qI9"
      },
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "metadata": {
        "cellView": "form",
        "id": "ve7zlI0-P8tP"
      },
      "outputs": [],
      "source": [
        "# @title Specifics of SVAE training\n",
        "def svae_init(key, model, data, initial_params=None, **train_params):\n",
        "    init_params = model.init(key)\n",
        "    if (initial_params): init_params.update(initial_params)\n",
        "    \n",
        "    if (train_params[\"inference_method\"] == \"planet\"):\n",
        "        init_params[\"rec_params\"] = {\n",
        "            \"rec_params\": init_params[\"rec_params\"],\n",
        "            \"post_params\": init_params[\"post_params\"]\n",
        "        }\n",
        "    # Expand the posterior parameters by batch size\n",
        "    init_params[\"post_params\"] = vmap(lambda _: init_params[\"post_params\"])(data)\n",
        "    init_params[\"post_samples\"] = np.zeros((data.shape[0], \n",
        "                                            train_params.get(\"obj_samples\") or 1) \n",
        "                                             + model.posterior.shape)\n",
        "\n",
        "    learning_rate = train_params[\"learning_rate\"]\n",
        "    rec_opt = opt.adam(learning_rate=learning_rate)\n",
        "    rec_opt_state = rec_opt.init(init_params[\"rec_params\"])\n",
        "    dec_opt = opt.adam(learning_rate=learning_rate)\n",
        "    dec_opt_state = dec_opt.init(init_params[\"dec_params\"])\n",
        "\n",
        "    if (train_params.get(\"use_natural_grad\")):\n",
        "        prior_lr = None\n",
        "        prior_opt = None\n",
        "        prior_opt_state = None\n",
        "    else:\n",
        "        # Add the option of using an gradient optimizer for prior parameters\n",
        "        prior_lr = train_params.get(\"prior_learning_rate\") or learning_rate\n",
        "        prior_opt = opt.adam(learning_rate=prior_lr)\n",
        "        prior_opt_state = prior_opt.init(init_params[\"prior_params\"])\n",
        "\n",
        "    return (init_params, \n",
        "            (rec_opt, dec_opt, prior_opt), \n",
        "            (rec_opt_state, dec_opt_state, prior_opt_state))\n",
        "    \n",
        "def svae_loss(key, model, data_batch, model_params, **train_params):\n",
        "    batch_size = data_batch.shape[0]\n",
        "    # Axes specification for vmap\n",
        "    # We're just going to ignore this for now\n",
        "    params_in_axes = None\n",
        "    # params_in_axes = dict.fromkeys(model_params.keys(), None)\n",
        "    # params_in_axes[\"post_samples\"] = 0\n",
        "    result = vmap(partial(model.compute_objective, **train_params), \n",
        "                  in_axes=(0, 0, params_in_axes))(jr.split(key, batch_size), data_batch, model_params)\n",
        "    objs = result[\"objective\"]\n",
        "    post_params = result[\"posterior_params\"]\n",
        "    post_samples = result[\"posterior_samples\"]\n",
        "    # Need to compute sufficient stats if we want the natural gradient update\n",
        "    if (train_params.get(\"use_natural_grad\")):\n",
        "        post_suff_stats = vmap(model.posterior.sufficient_statistics)(post_params)\n",
        "        expected_post_suff_stats = tree_map(\n",
        "            lambda l: np.mean(l,axis=0), post_suff_stats)\n",
        "        result[\"sufficient_statistics\"] = expected_post_suff_stats\n",
        "    return -np.mean(objs), result\n",
        "\n",
        "def predict_forward(x, A, b, T):\n",
        "    def _step(carry, t):\n",
        "        carry = A @ carry + b\n",
        "        return carry, carry\n",
        "    return scan(_step, x, np.arange(T))[1]\n",
        "\n",
        "# Note: this is for pendulum data only\n",
        "def svae_val_loss(key, model, data_batch, model_params, **train_params):  \n",
        "    N, T = data_batch.shape[:2]\n",
        "    # We only care about the first 100 timesteps\n",
        "    T = T // 2\n",
        "    D = model.prior.latent_dims\n",
        "\n",
        "    # obs_data, pred_data = data_batch[:,:T//2], data_batch[:,T//2:]\n",
        "    obs_data = data_batch[:,:T]\n",
        "    obj, out_dict = svae_loss(key, model, obs_data, model_params, **train_params)\n",
        "    # Compute the prediction accuracy\n",
        "    prior_params = model_params[\"prior_params\"] \n",
        "    # Instead of this, we want to evaluate the expected log likelihood of the future observations\n",
        "    # under the posterior given the current set of observations\n",
        "    # So E_{q(x'|y)}[p(y'|x')] where the primes represent the future\n",
        "    post_params = out_dict[\"posterior_params\"]\n",
        "    horizon = train_params[\"prediction_horizon\"] or 5\n",
        "\n",
        "    # def _prediction_lls(data_id, key):\n",
        "    #     num_windows = T-horizon-1\n",
        "    #     pred_lls = vmap(_sliding_window_prediction_lls, in_axes=(None, None, None, 0, 0))(\n",
        "    #         mu_filtered[data_id], Sigma_filtered[data_id], obs_data[data_id],\n",
        "    #         np.arange(num_windows), jr.split(key, num_windows))\n",
        "    #     return pred_lls.mean()\n",
        "\n",
        "    # # TODO: change this...!\n",
        "    # def _sliding_window_prediction_lls(mu, Sigma, data, t, key):\n",
        "    #     # Build the posterior object on the future latent states \n",
        "    #     # (\"the posterior predictive distribution\")\n",
        "    #     # Convert unconstrained params to constrained dynamics parameters\n",
        "    #     prior_params_constrained = model.prior.get_dynamics_params(prior_params)\n",
        "    #     dynamics_params = {\n",
        "    #         \"m1\": mu[t],\n",
        "    #         \"Q1\": Sigma[t],\n",
        "    #         \"A\": prior_params_constrained[\"A\"],\n",
        "    #         \"b\": prior_params_constrained[\"b\"],\n",
        "    #         \"Q\": prior_params_constrained[\"Q\"]\n",
        "    #     }\n",
        "    #     tridiag_params = dynamics_to_tridiag(dynamics_params, horizon+1, D) # Note the +1\n",
        "    #     J, L, h = tridiag_params[\"J\"], tridiag_params[\"L\"], tridiag_params[\"h\"]\n",
        "    #     pred_posterior = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "    #     # Sample from it and evaluate the log likelihood\n",
        "    #     x_pred = pred_posterior.sample(seed=key)[1:]\n",
        "    #     likelihood_dist = model.decoder.apply(model_params[\"dec_params\"], x_pred)\n",
        "    #     return likelihood_dist.log_prob(\n",
        "    #         lax.dynamic_slice(data, (t+1, 0, 0, 0), (horizon,) + data.shape[1:])).sum(axis=(1, 2, 3))\n",
        "    _, _, _, pred_lls = vmap(predict_multiple, in_axes=(None, None, None, 0, None, 0, None))\\\n",
        "        (train_params, model_params, model, obs_data, T-horizon, jr.split(key, N), 10)\n",
        "    # pred_lls = vmap(_prediction_lls)(np.arange(N), jr.split(key, N))\n",
        "    out_dict[\"prediction_ll\"] = pred_lls\n",
        "    return obj, out_dict\n",
        "\n",
        "def svae_update(params, grads, opts, opt_states, model, aux, **train_params):\n",
        "    rec_opt, dec_opt, prior_opt = opts\n",
        "    rec_opt_state, dec_opt_state, prior_opt_state = opt_states\n",
        "    rec_grad, dec_grad = grads[\"rec_params\"], grads[\"dec_params\"]\n",
        "    updates, rec_opt_state = rec_opt.update(rec_grad, rec_opt_state)\n",
        "    params[\"rec_params\"] = opt.apply_updates(params[\"rec_params\"], updates)\n",
        "    params[\"post_params\"] = aux[\"posterior_params\"]\n",
        "    params[\"post_samples\"] = aux[\"posterior_samples\"]\n",
        "    if train_params[\"run_type\"] == \"model_learning\":\n",
        "        # Update decoder\n",
        "        updates, dec_opt_state = dec_opt.update(dec_grad, dec_opt_state)\n",
        "        params[\"dec_params\"] = opt.apply_updates(params[\"dec_params\"], updates)\n",
        "\n",
        "        old_Q = deepcopy(params[\"prior_params\"][\"Q\"])\n",
        "        old_b = deepcopy(params[\"prior_params\"][\"b\"])\n",
        "\n",
        "        # Update prior parameters\n",
        "        if (train_params.get(\"use_natural_grad\")):\n",
        "            # Here we interpolate the sufficient statistics instead of the parameters\n",
        "            suff_stats = aux[\"sufficient_statistics\"]\n",
        "            lr = params.get(\"prior_learning_rate\") or 1\n",
        "            avg_suff_stats = params[\"prior_params\"][\"avg_suff_stats\"]\n",
        "            # Interpolate the sufficient statistics\n",
        "            params[\"prior_params\"][\"avg_suff_stats\"] = tree_map(lambda x,y : (1 - lr) * x + lr * y, \n",
        "                avg_suff_stats, suff_stats)\n",
        "            params[\"prior_params\"] = model.prior.m_step(params[\"prior_params\"])\n",
        "        else:\n",
        "            updates, prior_opt_state = prior_opt.update(grads[\"prior_params\"], prior_opt_state)\n",
        "            params[\"prior_params\"] = opt.apply_updates(params[\"prior_params\"], updates)\n",
        "        \n",
        "        if (train_params.get(\"constrain_prior\")):\n",
        "            # Revert Q and b to their previous values\n",
        "            params[\"prior_params\"][\"Q\"] = old_Q\n",
        "            params[\"prior_params\"][\"b\"] = old_b\n",
        "\n",
        "        if (train_params.get(\"constrain_dynamics\")):\n",
        "            # Scale A so that its maximum singular value does not exceed 1\n",
        "            params[\"prior_params\"][\"A\"] = truncate_singular_values(params[\"prior_params\"][\"A\"])\n",
        "            # params[\"prior_params\"][\"A\"] = scale_singular_values(params[\"prior_params\"][\"A\"])\n",
        "\n",
        "    return params, (rec_opt_state, dec_opt_state, prior_opt_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "metadata": {
        "id": "8AdW1WXA4n1U",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Model initialization and trainer\n",
        "def init_model(run_params, data_dict):\n",
        "    p = deepcopy(run_params)\n",
        "    d = p[\"dataset_params\"]\n",
        "    latent_dims = p[\"latent_dims\"]\n",
        "    input_shape = data_dict[\"train_data\"].shape[1:]\n",
        "    num_timesteps = input_shape[0]\n",
        "    data = data_dict[\"train_data\"]\n",
        "    seed = p[\"seed\"]\n",
        "    seed_model, seed_elbo, seed_ems, seed_rec = jr.split(seed, 4)\n",
        "\n",
        "    run_type = p[\"run_type\"]\n",
        "    recnet_class = globals()[p[\"recnet_class\"]]\n",
        "    decnet_class = globals()[p[\"decnet_class\"]]\n",
        "\n",
        "    if p[\"inference_method\"] == \"dkf\":\n",
        "        posterior = DKFPosterior(latent_dims, num_timesteps)\n",
        "    elif p[\"inference_method\"] == \"cdkf\":\n",
        "        posterior = CDKFPosterior(latent_dims, num_timesteps)\n",
        "    elif p[\"inference_method\"] == \"planet\":\n",
        "        posterior = PlaNetPosterior(p[\"posterior_architecture\"],\n",
        "                                    latent_dims, num_timesteps)\n",
        "    elif p[\"inference_method\"] == \"svae\":\n",
        "        # The parallel Kalman stuff only applies to SVAE\n",
        "        # Since RNN based methods are inherently sequential\n",
        "        posterior = LDSSVAEPosterior(latent_dims, num_timesteps, \n",
        "                                     use_parallel=p.get(\"use_parallel_kf\"))\n",
        "        \n",
        "    rec_net = recnet_class.from_params(**p[\"recnet_architecture\"])\n",
        "    dec_net = decnet_class.from_params(**p[\"decnet_architecture\"])\n",
        "    if p[\"inference_method\"] == \"planet\":\n",
        "        # Wrap the recognition network\n",
        "        rec_net = PlaNetRecognitionWrapper(rec_net)\n",
        "\n",
        "    if (p.get(\"use_natural_grad\")):\n",
        "        prior = LinearGaussianChainPrior(latent_dims, num_timesteps)\n",
        "    else:\n",
        "        prior = LieParameterizedLinearGaussianChainPrior(latent_dims, num_timesteps)\n",
        "\n",
        "    model = DeepLDS(\n",
        "        recognition=rec_net,\n",
        "        decoder=dec_net,\n",
        "        prior=prior,\n",
        "        posterior=posterior,\n",
        "        input_dummy=np.zeros(input_shape),\n",
        "        latent_dummy=np.zeros((num_timesteps, latent_dims))\n",
        "    )\n",
        "    \n",
        "    # TODO: Let's get the full linear version working first before moving on\n",
        "    # assert(run_params[\"run_type\"] == \"full_linear\")\n",
        "    if (run_type == \"inference_only\"):\n",
        "        p = data_dict[\"lds_params\"]\n",
        "        prior_params = { \"A\": p[\"A\"], \"b\": p[\"b\"], \n",
        "                        \"Q\": p[\"Q\"], \"m1\": p[\"m1\"], \"Q1\": p[\"Q1\"],\n",
        "                        \"avg_suff_stats\": p[\"avg_suff_stats\"]}\n",
        "        dec_params = fd.FrozenDict(\n",
        "            {\n",
        "                \"params\": {\n",
        "                    \"head_log_var_fn\":{\n",
        "                        \"Dense_0\":{\n",
        "                            \"bias\": inv_softplus(np.diag(p[\"R\"])),\n",
        "                            \"kernel\": np.zeros_like(p[\"C\"]).T\n",
        "                        }\n",
        "                    },\n",
        "                    \"head_mean_fn\":{\n",
        "                        \"Dense_0\":{\n",
        "                            \"bias\": p[\"d\"],\n",
        "                            \"kernel\": p[\"C\"].T\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            })\n",
        "        initial_params = { \"prior_params\": prior_params, \"dec_params\": dec_params }\n",
        "    else:\n",
        "        initial_params = None\n",
        "\n",
        "    # emission_params = emission.init(seed_ems, np.ones((num_latent_dims,)))\n",
        "    # Define the trainer object here\n",
        "    trainer = Trainer(model, train_params=run_params, init=svae_init, \n",
        "                      loss=svae_loss, \n",
        "                      val_loss=svae_val_loss, \n",
        "                      update=svae_update, initial_params=initial_params)\n",
        "\n",
        "    return {\n",
        "        # We don't actually need to include model here\n",
        "        # 'cause it's included in the trainer object\n",
        "        \"model\": model,\n",
        "        # \"emission_params\": emission_params\n",
        "        \"trainer\": trainer\n",
        "    }\n",
        "\n",
        "def start_trainer(model_dict, data_dict, run_params):\n",
        "    trainer = model_dict[\"trainer\"]\n",
        "    if run_params.get(\"log_to_wandb\"):\n",
        "        if run_params[\"dataset\"] == \"pendulum\":\n",
        "            summary = summarize_pendulum_run\n",
        "        else:\n",
        "            summary = save_params_to_wandb\n",
        "    else:\n",
        "        summary = None\n",
        "    trainer.train(data_dict,\n",
        "                  max_iters=run_params[\"max_iters\"],\n",
        "                  key=run_params[\"seed\"],\n",
        "                  callback=log_to_wandb, val_callback=validation_log_to_wandb,\n",
        "                  summary=summary)\n",
        "    return (trainer.model, trainer.params, trainer.train_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NSj6XUpPZRG"
      },
      "source": [
        "## Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "metadata": {
        "cellView": "form",
        "id": "ZeG1j7HgqWya"
      },
      "outputs": [],
      "source": [
        "# @title Sample from LDS\n",
        "def sample_lds_dataset(run_params):    \n",
        "    d = run_params[\"dataset_params\"]\n",
        "    \n",
        "    global data_dict\n",
        "    if data_dict is not None \\\n",
        "        and \"dataset_params\" in data_dict \\\n",
        "        and str(data_dict[\"dataset_params\"]) == str(fd.freeze(d)):\n",
        "        print(\"Using existing data.\")\n",
        "        print(\"Data MLL: \", data_dict[\"marginal_log_likelihood\"])\n",
        "        \n",
        "        # return jax.device_put(data_dict, jax.devices(\"gpu\")[0])\n",
        "        return data_dict\n",
        "\n",
        "    data_dict = {}\n",
        "\n",
        "    seed = d[\"seed\"]\n",
        "    emission_dims = d[\"emission_dims\"]\n",
        "    latent_dims = d[\"latent_dims\"]\n",
        "    emission_cov = d[\"emission_cov\"]\n",
        "    dynamics_cov = d[\"dynamics_cov\"]\n",
        "    num_timesteps = d[\"num_timesteps\"]\n",
        "    num_trials = d[\"num_trials\"]\n",
        "    seed_m1, seed_C, seed_d, seed_A, seed_sample = jr.split(seed, 5)\n",
        "\n",
        "    R = emission_cov * np.eye(emission_dims)\n",
        "    Q = dynamics_cov * np.eye(latent_dims)\n",
        "    C = jr.normal(seed_C, shape=(emission_dims, latent_dims))\n",
        "    d = jr.normal(seed_d, shape=(emission_dims,))\n",
        "\n",
        "    # Here we let Q1 = Q\n",
        "    lds = LDS(latent_dims, num_timesteps)\n",
        "    \n",
        "    params = {\n",
        "            \"m1\": jr.normal(key=seed_m1, shape=(latent_dims,)),\n",
        "            \"Q1\": Q,\n",
        "            \"Q\": Q,\n",
        "            \"A\": random_rotation(seed_A, latent_dims, theta=np.pi/20),\n",
        "            \"b\": np.zeros(latent_dims),\n",
        "            \"R\": R,\n",
        "            \"C\": C,\n",
        "            \"d\": d,\n",
        "        }\n",
        "    constrained = lds.get_constrained_params(params)\n",
        "    params[\"avg_suff_stats\"] = { \"Ex\": constrained[\"Ex\"], \n",
        "                                \"ExxT\": constrained[\"ExxT\"], \n",
        "                                \"ExnxT\": constrained[\"ExnxT\"] }\n",
        "\n",
        "    states, data = lds.sample(params, \n",
        "                              shape=(num_trials,), \n",
        "                              key=seed_sample)\n",
        "    \n",
        "    mll = vmap(lds.marginal_log_likelihood, in_axes=(None, 0))(params, data)\n",
        "    mll = np.mean(mll, axis=0)\n",
        "    print(\"Data MLL: \", mll)\n",
        "    \n",
        "    seed_val, _ = jr.split(seed_sample)\n",
        "    val_states, val_data = lds.sample(params, \n",
        "                              shape=(num_trials,), \n",
        "                              key=seed_val)\n",
        "\n",
        "    # data_dict[\"generative_model\"] = lds\n",
        "    data_dict[\"marginal_log_likelihood\"] = mll\n",
        "    data_dict[\"train_data\"] = data\n",
        "    data_dict[\"train_states\"] = states\n",
        "    data_dict[\"val_data\"] = val_data\n",
        "    data_dict[\"val_states\"] = val_states\n",
        "    data_dict[\"dataset_params\"] = fd.freeze(run_params[\"dataset_params\"])\n",
        "    data_dict[\"lds_params\"] = params\n",
        "    return data_dict\n",
        "    # return jax.device_put(data_dict, jax.devices(\"gpu\")[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {
        "cellView": "form",
        "id": "MKj_UmfS6JdM"
      },
      "outputs": [],
      "source": [
        "# @title Testing the correctness of LDS m-step\n",
        "# seed = jr.PRNGKey(0)\n",
        "# emission_dims = 5\n",
        "# latent_dims = 3\n",
        "# emission_cov = 10.\n",
        "# dynamics_cov = .1\n",
        "# num_timesteps = 100\n",
        "# seed, seed_C, seed_d, seed_sample = jr.split(seed, 4)\n",
        "# R = emission_cov * np.eye(emission_dims)\n",
        "# Q = dynamics_cov * np.eye(latent_dims)\n",
        "# C = jr.normal(seed_C, shape=(emission_dims, latent_dims))\n",
        "# d = jr.normal(seed_d, shape=(emission_dims,))\n",
        "# # Here we let Q1 = Q\n",
        "# lds = LDS(latent_dims, num_timesteps)\n",
        "# params = lds.init(seed)\n",
        "# params.update(\n",
        "#     {\n",
        "#         \"Q1\": Q,\n",
        "#         \"Q\": Q,\n",
        "#         \"R\": R,\n",
        "#         \"C\": C,\n",
        "#         \"d\": d,\n",
        "#     }\n",
        "# )\n",
        "# states, data = lds.sample(params, \n",
        "#                           shape=(50,), \n",
        "#                           key=seed_sample)\n",
        "# post_params = vmap(lds.e_step, in_axes=(None, 0))(params, data)\n",
        "# posterior = LinearGaussianChainPosterior(latent_dims, num_timesteps)\n",
        "# suff_stats = vmap(posterior.sufficient_statistics)(post_params)\n",
        "# expected_suff_stats = jax.tree_util.tree_map(\n",
        "#         lambda l: np.mean(l,axis=0), suff_stats)\n",
        "# inferred_params = lds.m_step(params, expected_suff_stats)\n",
        "# for key in [\"m1\", \"Q1\", \"A\", \"Q\", \"b\"]:\n",
        "#     print(inferred_params[key] - params[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {
        "cellView": "form",
        "id": "Qzp5Fb0CGtqk"
      },
      "outputs": [],
      "source": [
        "# @title Code for the pendulum dataset (~128 mb)\n",
        "\n",
        "\n",
        "# Modeling Irregular Time Series with Continuous Recurrent Units (CRUs)\n",
        "# Copyright (c) 2022 Robert Bosch GmbH\n",
        "# This program is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Affero General Public License as published\n",
        "# by the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "#\n",
        "# This program is distributed in the hope that it will be useful,\n",
        "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "# GNU Affero General Public License for more details.\n",
        "#\n",
        "# You should have received a copy of the GNU Affero General Public License\n",
        "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
        "#\n",
        "# This source code is derived from Pytorch RKN Implementation (https://github.com/ALRhub/rkn_share)\n",
        "# Copyright (c) 2021 Philipp Becker (Autonomous Learning Robots Lab @ KIT)\n",
        "# licensed under MIT License\n",
        "# cf. 3rd-party-licenses.txt file in the root directory of this source tree.\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "def add_img_noise(imgs, first_n_clean, random, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75, t_uu=1.0):\n",
        "    \"\"\"\n",
        "    :param imgs: Images to add noise to\n",
        "    :param first_n_clean: Keep first_n_images clean to allow the filter to burn in\n",
        "    :param random: np.random.RandomState used for sampling\n",
        "    :param r: \"correlation (over time) factor\" the smaller the more the noise is correlated\n",
        "    :param t_ll: lower bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_lu: upper bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_ul: lower bound of the interval the upper bound for each sequence is sampled from\n",
        "    :param t_uu: upper bound of the interval the upper bound for each sequence is sampled from\n",
        "    :return: noisy images, factors used to create them\n",
        "    \"\"\"\n",
        "\n",
        "    assert t_ll <= t_lu <= t_ul <= t_uu, \"Invalid bounds for noise generation\"\n",
        "    if len(imgs.shape) < 5:\n",
        "        imgs = onp.expand_dims(imgs, -1)\n",
        "    batch_size, seq_len = imgs.shape[:2]\n",
        "    factors = onp.zeros([batch_size, seq_len])\n",
        "    factors[:, 0] = random.uniform(low=0.0, high=1.0, size=batch_size)\n",
        "    for i in range(seq_len - 1):\n",
        "        factors[:, i + 1] = onp.clip(factors[:, i] + random.uniform(\n",
        "            low=-r, high=r, size=batch_size), a_min=0.0, a_max=1.0)\n",
        "\n",
        "    t1 = random.uniform(low=t_ll, high=t_lu, size=(batch_size, 1))\n",
        "    t2 = random.uniform(low=t_ul, high=t_uu, size=(batch_size, 1))\n",
        "\n",
        "    factors = (factors - t1) / (t2 - t1)\n",
        "    factors = onp.clip(factors, a_min=0.0, a_max=1.0)\n",
        "    factors = onp.reshape(factors, list(factors.shape) + [1, 1, 1])\n",
        "    factors[:, :first_n_clean] = 1.0\n",
        "    noisy_imgs = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        if imgs.dtype == onp.uint8:\n",
        "            noise = random.uniform(low=0.0, high=255, size=imgs.shape[1:])\n",
        "            noisy_imgs.append(\n",
        "                (factors[i] * imgs[i] + (1 - factors[i]) * noise).astype(onp.uint8))\n",
        "        else:\n",
        "            noise = random.uniform(low=0.0, high=1.1, size=imgs.shape[1:])\n",
        "            noisy_imgs.append(factors[i] * imgs[i] + (1 - factors[i]) * noise)\n",
        "\n",
        "    return onp.squeeze(onp.concatenate([onp.expand_dims(n, 0) for n in noisy_imgs], 0)), factors\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "def add_img_noise4(imgs, first_n_clean, random, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75, t_uu=1.0):\n",
        "    \"\"\"\n",
        "    :param imgs: Images to add noise to\n",
        "    :param first_n_clean: Keep first_n_images clean to allow the filter to burn in\n",
        "    :param random: np.random.RandomState used for sampling\n",
        "    :param r: \"correlation (over time) factor\" the smaller the more the noise is correlated\n",
        "    :param t_ll: lower bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_lu: upper bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_ul: lower bound of the interval the upper bound for each sequence is sampled from\n",
        "    :param t_uu: upper bound of the interval the upper bound for each sequence is sampled from\n",
        "    :return: noisy images, factors used to create them\n",
        "    \"\"\"\n",
        "\n",
        "    half_x = int(imgs.shape[2] / 2)\n",
        "    half_y = int(imgs.shape[3] / 2)\n",
        "    assert t_ll <= t_lu <= t_ul <= t_uu, \"Invalid bounds for noise generation\"\n",
        "    if len(imgs.shape) < 5:\n",
        "        imgs = np.expand_dims(imgs, -1)\n",
        "    batch_size, seq_len = imgs.shape[:2]\n",
        "    factors = np.zeros([batch_size, seq_len, 4])\n",
        "    factors[:, 0] = random.uniform(low=0.0, high=1.0, size=(batch_size, 4))\n",
        "    for i in range(seq_len - 1):\n",
        "        factors[:, i + 1] = np.clip(factors[:, i] + random.uniform(\n",
        "            low=-r, high=r, size=(batch_size, 4)), a_min=0.0, a_max=1.0)\n",
        "\n",
        "    t1 = random.uniform(low=t_ll, high=t_lu, size=(batch_size, 1, 4))\n",
        "    t2 = random.uniform(low=t_ul, high=t_uu, size=(batch_size, 1, 4))\n",
        "\n",
        "    factors = (factors - t1) / (t2 - t1)\n",
        "    factors = np.clip(factors, a_min=0.0, a_max=1.0)\n",
        "    factors = np.reshape(factors, list(factors.shape) + [1, 1, 1])\n",
        "    factors[:, :first_n_clean] = 1.0\n",
        "    noisy_imgs = []\n",
        "    qs = []\n",
        "    for i in range(batch_size):\n",
        "        if imgs.dtype == np.uint8:\n",
        "            qs.append(detect_pendulums(imgs[i], half_x, half_y))\n",
        "            noise = random.uniform(low=0.0, high=255, size=[\n",
        "                                   4, seq_len, half_x, half_y, imgs.shape[-1]]).astype(np.uint8)\n",
        "            curr = np.zeros(imgs.shape[1:], dtype=np.uint8)\n",
        "            curr[:, :half_x, :half_y] = (factors[i, :, 0] * imgs[i, :, :half_x, :half_y] + (\n",
        "                1 - factors[i, :, 0]) * noise[0]).astype(np.uint8)\n",
        "            curr[:, :half_x, half_y:] = (factors[i, :, 1] * imgs[i, :, :half_x, half_y:] + (\n",
        "                1 - factors[i, :, 1]) * noise[1]).astype(np.uint8)\n",
        "            curr[:, half_x:, :half_y] = (factors[i, :, 2] * imgs[i, :, half_x:, :half_y] + (\n",
        "                1 - factors[i, :, 2]) * noise[2]).astype(np.uint8)\n",
        "            curr[:, half_x:, half_y:] = (factors[i, :, 3] * imgs[i, :, half_x:, half_y:] + (\n",
        "                1 - factors[i, :, 3]) * noise[3]).astype(np.uint8)\n",
        "        else:\n",
        "            noise = random.uniform(low=0.0, high=1.0, size=[\n",
        "                                   4, seq_len, half_x, half_y, imgs.shape[-1]])\n",
        "            curr = np.zeros(imgs.shape[1:])\n",
        "            curr[:, :half_x, :half_y] = factors[i, :, 0] * imgs[i, :,\n",
        "                                                                :half_x, :half_y] + (1 - factors[i, :, 0]) * noise[0]\n",
        "            curr[:, :half_x, half_y:] = factors[i, :, 1] * imgs[i, :,\n",
        "                                                                :half_x, half_y:] + (1 - factors[i, :, 1]) * noise[1]\n",
        "            curr[:, half_x:, :half_y] = factors[i, :, 2] * imgs[i, :,\n",
        "                                                                half_x:, :half_y] + (1 - factors[i, :, 2]) * noise[2]\n",
        "            curr[:, half_x:, half_y:] = factors[i, :, 3] * imgs[i, :,\n",
        "                                                                half_x:, half_y:] + (1 - factors[i, :, 3]) * noise[3]\n",
        "        noisy_imgs.append(curr)\n",
        "\n",
        "    factors_ext = np.concatenate([np.squeeze(factors), np.zeros(\n",
        "        [factors.shape[0], factors.shape[1], 1])], -1)\n",
        "    q = np.concatenate([np.expand_dims(q, 0) for q in qs], 0)\n",
        "    f = np.zeros(q.shape)\n",
        "    for i in range(f.shape[0]):\n",
        "        for j in range(f.shape[1]):\n",
        "            for k in range(3):\n",
        "                f[i, j, k] = factors_ext[i, j, q[i, j, k]]\n",
        "\n",
        "    return np.squeeze(np.concatenate([np.expand_dims(n, 0) for n in noisy_imgs], 0)), f\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "def detect_pendulums(imgs, half_x, half_y):\n",
        "    qs = [imgs[:, :half_x, :half_y], imgs[:, :half_x, half_y:],\n",
        "          imgs[:, half_x:, :half_y], imgs[:, half_x:, half_y:]]\n",
        "\n",
        "    r_cts = np.array(\n",
        "        [np.count_nonzero(q[:, :, :, 0] > 5, axis=(-1, -2)) for q in qs]).T\n",
        "    g_cts = np.array(\n",
        "        [np.count_nonzero(q[:, :, :, 1] > 5, axis=(-1, -2)) for q in qs]).T\n",
        "    b_cts = np.array(\n",
        "        [np.count_nonzero(q[:, :, :, 2] > 5, axis=(-1, -2)) for q in qs]).T\n",
        "\n",
        "    cts = np.concatenate([np.expand_dims(c, 1)\n",
        "                         for c in [r_cts, g_cts, b_cts]], 1)\n",
        "\n",
        "    q_max = np.max(cts, -1)\n",
        "    q = np.argmax(cts, -1)\n",
        "    q[q_max < 10] = 4\n",
        "    return q\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "class Pendulum:\n",
        "\n",
        "    MAX_VELO_KEY = 'max_velo'\n",
        "    MAX_TORQUE_KEY = 'max_torque'\n",
        "    MASS_KEY = 'mass'\n",
        "    LENGTH_KEY = 'length'\n",
        "    GRAVITY_KEY = 'g'\n",
        "    FRICTION_KEY = 'friction'\n",
        "    DT_KEY = 'dt'\n",
        "    SIM_DT_KEY = 'sim_dt'\n",
        "    TRANSITION_NOISE_TRAIN_KEY = 'transition_noise_train'\n",
        "    TRANSITION_NOISE_TEST_KEY = 'transition_noise_test'\n",
        "\n",
        "    OBSERVATION_MODE_LINE = \"line\"\n",
        "    OBSERVATION_MODE_BALL = \"ball\"\n",
        "\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def __init__(self,\n",
        "                 img_size,\n",
        "                 observation_mode,\n",
        "                 generate_actions=False,\n",
        "                 transition_noise_std=0.0,\n",
        "                 observation_noise_std=0.0,\n",
        "                 pendulum_params=None,\n",
        "                 seed=0):\n",
        "\n",
        "        assert observation_mode == Pendulum.OBSERVATION_MODE_BALL or observation_mode == Pendulum.OBSERVATION_MODE_LINE\n",
        "        # Global Parameters\n",
        "        self.state_dim = 2\n",
        "        self.action_dim = 1\n",
        "        self.img_size = img_size\n",
        "        self.observation_dim = img_size ** 2\n",
        "        self.observation_mode = observation_mode\n",
        "\n",
        "        self.random = onp.random.RandomState(seed)\n",
        "\n",
        "        # image parameters\n",
        "        self.img_size_internal = 128\n",
        "        self.x0 = self.y0 = 64\n",
        "        self.plt_length = 55 if self.observation_mode == Pendulum.OBSERVATION_MODE_LINE else 50\n",
        "        self.plt_width = 8\n",
        "\n",
        "        self.generate_actions = generate_actions\n",
        "\n",
        "        # simulation parameters\n",
        "        if pendulum_params is None:\n",
        "            pendulum_params = self.pendulum_default_params()\n",
        "        self.max_velo = pendulum_params[Pendulum.MAX_VELO_KEY]\n",
        "        self.max_torque = pendulum_params[Pendulum.MAX_TORQUE_KEY]\n",
        "        self.dt = pendulum_params[Pendulum.DT_KEY]\n",
        "        self.mass = pendulum_params[Pendulum.MASS_KEY]\n",
        "        self.length = pendulum_params[Pendulum.LENGTH_KEY]\n",
        "        self.inertia = self.mass * self.length**2 / 3\n",
        "        self.g = pendulum_params[Pendulum.GRAVITY_KEY]\n",
        "        self.friction = pendulum_params[Pendulum.FRICTION_KEY]\n",
        "        self.sim_dt = pendulum_params[Pendulum.SIM_DT_KEY]\n",
        "\n",
        "        self.observation_noise_std = observation_noise_std\n",
        "        self.transition_noise_std = transition_noise_std\n",
        "\n",
        "        self.tranisition_covar_mat = onp.diag(\n",
        "            np.array([1e-8, self.transition_noise_std**2, 1e-8, 1e-8]))\n",
        "        self.observation_covar_mat = onp.diag(\n",
        "            [self.observation_noise_std**2, self.observation_noise_std**2])\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def sample_data_set(self, num_episodes, episode_length, full_targets):\n",
        "        states = onp.zeros((num_episodes, episode_length, self.state_dim))\n",
        "        actions = self._sample_action(\n",
        "            (num_episodes, episode_length, self.action_dim))\n",
        "        states[:, 0, :] = self._sample_init_state(num_episodes)\n",
        "        t = onp.zeros((num_episodes, episode_length))\n",
        "\n",
        "        for i in range(1, episode_length):\n",
        "            states[:, i, :], dt = self._get_next_states(\n",
        "                states[:, i - 1, :], actions[:, i - 1, :])\n",
        "            t[:, i:] += dt\n",
        "        states[..., 0] -= onp.pi\n",
        "\n",
        "        if self.observation_noise_std > 0.0:\n",
        "            observation_noise = self.random.normal(loc=0.0,\n",
        "                                                   scale=self.observation_noise_std,\n",
        "                                                   size=states.shape)\n",
        "        else:\n",
        "            observation_noise = onp.zeros(states.shape)\n",
        "\n",
        "        targets = self.pendulum_kinematic(states)\n",
        "        if self.observation_mode == Pendulum.OBSERVATION_MODE_LINE:\n",
        "            noisy_states = states + observation_noise\n",
        "            noisy_targets = self.pendulum_kinematic(noisy_states)\n",
        "        elif self.observation_mode == Pendulum.OBSERVATION_MODE_BALL:\n",
        "            noisy_targets = targets + observation_noise\n",
        "        imgs = self._generate_images(noisy_targets[..., :2])\n",
        "\n",
        "        return imgs, targets[..., :(4 if full_targets else 2)], states, noisy_targets[..., :(4 if full_targets else 2)], t/self.dt\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    @staticmethod\n",
        "    def pendulum_default_params():\n",
        "        return {\n",
        "            Pendulum.MAX_VELO_KEY: 8,\n",
        "            Pendulum.MAX_TORQUE_KEY: 10,\n",
        "            Pendulum.MASS_KEY: 1,\n",
        "            Pendulum.LENGTH_KEY: 1,\n",
        "            Pendulum.GRAVITY_KEY: 9.81,\n",
        "            Pendulum.FRICTION_KEY: 0,\n",
        "            Pendulum.DT_KEY: 0.05,\n",
        "            Pendulum.SIM_DT_KEY: 1e-4\n",
        "        }\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _sample_action(self, shape):\n",
        "        if self.generate_actions:\n",
        "            return self.random.uniform(-self.max_torque, self.max_torque, shape)\n",
        "        else:\n",
        "            return np.zeros(shape=shape)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _transition_function(self, states, actions):\n",
        "        dt = self.dt\n",
        "        n_steps = dt / self.sim_dt\n",
        "\n",
        "        if n_steps != np.round(n_steps):\n",
        "            #print(n_steps, 'Warning from Pendulum: dt does not match up')\n",
        "            n_steps = np.round(n_steps)\n",
        "\n",
        "        c = self.g * self.length * self.mass / self.inertia\n",
        "        for i in range(0, int(n_steps)):\n",
        "            velNew = states[..., 1:2] + self.sim_dt * (c * np.sin(states[..., 0:1])\n",
        "                                                       + actions / self.inertia\n",
        "                                                       - states[..., 1:2] * self.friction)\n",
        "            states = onp.concatenate(\n",
        "                (states[..., 0:1] + self.sim_dt * velNew, velNew), axis=1)\n",
        "        return states, dt\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _get_next_states(self, states, actions):\n",
        "        actions = np.maximum(-self.max_torque,\n",
        "                             np.minimum(actions, self.max_torque))\n",
        "\n",
        "        states, dt = self._transition_function(states, actions)\n",
        "        if self.transition_noise_std > 0.0:\n",
        "            states[:, 1] += self.random.normal(loc=0.0,\n",
        "                                               scale=self.transition_noise_std,\n",
        "                                               size=[len(states)])\n",
        "\n",
        "        states[:, 0] = ((states[:, 0]) % (2 * np.pi))\n",
        "        return states, dt\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def get_ukf_smothing(self, obs):\n",
        "        batch_size, seq_length = obs.shape[:2]\n",
        "        succ = np.zeros(batch_size, dtype=np.bool)\n",
        "        means = np.zeros([batch_size, seq_length, 4])\n",
        "        covars = np.zeros([batch_size, seq_length, 4, 4])\n",
        "        fail_ct = 0\n",
        "        for i in range(batch_size):\n",
        "            if i % 10 == 0:\n",
        "                print(i)\n",
        "            try:\n",
        "                means[i], covars[i] = self.ukf.filter(obs[i])\n",
        "                succ[i] = True\n",
        "            except:\n",
        "                fail_ct += 1\n",
        "        print(fail_ct / batch_size, \"failed\")\n",
        "\n",
        "        return means[succ], covars[succ], succ\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _sample_init_state(self, nr_epochs):\n",
        "        return onp.concatenate((self.random.uniform(0, 2 * np.pi, (nr_epochs, 1)), np.zeros((nr_epochs, 1))), 1)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def add_observation_noise(self, imgs, first_n_clean, r=0.2, t_ll=0.1, t_lu=0.4, t_ul=0.6, t_uu=0.9):\n",
        "        return add_img_noise(imgs, first_n_clean, self.random, r, t_ll, t_lu, t_ul, t_uu)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _get_task_space_pos(self, joint_states):\n",
        "        task_space_pos = onp.zeros(list(joint_states.shape[:-1]) + [2])\n",
        "        task_space_pos[..., 0] = np.sin(joint_states[..., 0]) * self.length\n",
        "        task_space_pos[..., 1] = np.cos(joint_states[..., 0]) * self.length\n",
        "        return task_space_pos\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _generate_images(self, ts_pos):\n",
        "        imgs = onp.zeros(shape=list(ts_pos.shape)[\n",
        "                        :-1] + [self.img_size, self.img_size], dtype=np.uint8)\n",
        "        for seq_idx in range(ts_pos.shape[0]):\n",
        "            for idx in range(ts_pos.shape[1]):\n",
        "                imgs[seq_idx, idx] = self._generate_single_image(\n",
        "                    ts_pos[seq_idx, idx])\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _generate_single_image(self, pos):\n",
        "        x1 = pos[0] * (self.plt_length / self.length) + self.x0\n",
        "        y1 = pos[1] * (self.plt_length / self.length) + self.y0\n",
        "        img = Image.new('F', (self.img_size_internal,\n",
        "                        self.img_size_internal), 0.0)\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        if self.observation_mode == Pendulum.OBSERVATION_MODE_LINE:\n",
        "            draw.line([(self.x0, self.y0), (x1, y1)],\n",
        "                      fill=1.0, width=self.plt_width)\n",
        "        elif self.observation_mode == Pendulum.OBSERVATION_MODE_BALL:\n",
        "            x_l = x1 - self.plt_width\n",
        "            x_u = x1 + self.plt_width\n",
        "            y_l = y1 - self.plt_width\n",
        "            y_u = y1 + self.plt_width\n",
        "            draw.ellipse((x_l, y_l, x_u, y_u), fill=1.0)\n",
        "\n",
        "        img = img.resize((self.img_size, self.img_size),\n",
        "                         resample=Image.ANTIALIAS)\n",
        "        img_as_array = onp.asarray(img)\n",
        "        img_as_array = onp.clip(img_as_array, 0, 1)\n",
        "        return 255.0 * img_as_array\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _kf_transition_function(self, state, noise):\n",
        "        nSteps = self.dt / self.sim_dt\n",
        "\n",
        "        if nSteps != np.round(nSteps):\n",
        "            print('Warning from Pendulum: dt does not match up')\n",
        "            nSteps = np.round(nSteps)\n",
        "\n",
        "        c = self.g * self.length * self.mass / self.inertia\n",
        "        for i in range(0, int(nSteps)):\n",
        "            velNew = state[1] + self.sim_dt * \\\n",
        "                (c * np.sin(state[0]) - state[1] * self.friction)\n",
        "            state = onp.array([state[0] + self.sim_dt * velNew, velNew])\n",
        "        state[0] = state[0] % (2 * np.pi)\n",
        "        state[1] = state[1] + noise[1]\n",
        "        return state\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def pendulum_kinematic_single(self, js):\n",
        "        theta, theat_dot = js\n",
        "        x = np.sin(theta)\n",
        "        y = np.cos(theta)\n",
        "        x_dot = theat_dot * y\n",
        "        y_dot = theat_dot * -x\n",
        "        return onp.array([x, y, x_dot, y_dot]) * self.length\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def pendulum_kinematic(self, js_batch):\n",
        "        theta = js_batch[..., :1]\n",
        "        theta_dot = js_batch[..., 1:]\n",
        "        x = np.sin(theta)\n",
        "        y = np.cos(theta)\n",
        "        x_dot = theta_dot * y\n",
        "        y_dot = theta_dot * -x\n",
        "        return onp.concatenate([x, y, x_dot, y_dot], axis=-1)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def inverse_pendulum_kinematics(self, ts_batch):\n",
        "        x = ts_batch[..., :1]\n",
        "        y = ts_batch[..., 1:2]\n",
        "        x_dot = ts_batch[..., 2:3]\n",
        "        y_dot = ts_batch[..., 3:]\n",
        "        val = x / y\n",
        "        theta = np.arctan2(x, y)\n",
        "        theta_dot_outer = 1 / (1 + val**2)\n",
        "        theta_dot_inner = (x_dot * y - y_dot * x) / y**2\n",
        "        return onp.concatenate([theta, theta_dot_outer * theta_dot_inner], axis=-1)\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and modified\n",
        "def generate_pendulums(file_path, task, \n",
        "                       num_train_trials=200, num_test_trials=100,\n",
        "                       impute_rate=0.5, seq_len=100, file_tag=\"\"):\n",
        "    \n",
        "    if task == 'interpolation':\n",
        "        pend_params = Pendulum.pendulum_default_params()\n",
        "        pend_params[Pendulum.FRICTION_KEY] = 0.1\n",
        "        n = seq_len\n",
        "\n",
        "        pendulum = Pendulum(24, observation_mode=Pendulum.OBSERVATION_MODE_LINE,\n",
        "                            transition_noise_std=0.1, observation_noise_std=1e-5,\n",
        "                            seed=42, pendulum_params=pend_params)\n",
        "        rng = pendulum.random\n",
        "\n",
        "        train_obs, _, _, _, train_ts = pendulum.sample_data_set(\n",
        "            num_train_trials, n, full_targets=False)\n",
        "        train_obs = onp.expand_dims(train_obs, -1)\n",
        "        train_targets = train_obs.copy()\n",
        "        train_obs_valid = rng.rand(\n",
        "            train_obs.shape[0], train_obs.shape[1], 1) > impute_rate\n",
        "        train_obs_valid[:, :5] = True\n",
        "        train_obs[onp.logical_not(onp.squeeze(train_obs_valid))] = 0\n",
        "\n",
        "        test_obs, _, _, _, test_ts = pendulum.sample_data_set(\n",
        "            num_test_trials, n, full_targets=False)\n",
        "        test_obs = onp.expand_dims(test_obs, -1)\n",
        "        test_targets = test_obs.copy()\n",
        "        test_obs_valid = rng.rand(\n",
        "            test_obs.shape[0], test_obs.shape[1], 1) > impute_rate\n",
        "        test_obs_valid[:, :5] = True\n",
        "        test_obs[onp.logical_not(onp.squeeze(test_obs_valid))] = 0\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            os.makedirs(file_path)\n",
        "        onp.savez_compressed(os.path.join(file_path, f\"pend_interpolation_ir{impute_rate}\"),\n",
        "                            train_obs=train_obs, train_targets=train_targets, train_obs_valid=train_obs_valid, train_ts=train_ts,\n",
        "                            test_obs=test_obs, test_targets=test_targets, test_obs_valid=test_obs_valid, test_ts=test_ts)\n",
        "    \n",
        "    elif task == 'regression':\n",
        "        pend_params = Pendulum.pendulum_default_params()\n",
        "        pend_params[Pendulum.FRICTION_KEY] = 0.1\n",
        "        pend_params[Pendulum.DT_KEY] = 0.01\n",
        "        n = seq_len\n",
        "\n",
        "        pendulum = Pendulum(24, observation_mode=Pendulum.OBSERVATION_MODE_LINE,\n",
        "                            transition_noise_std=0.1, observation_noise_std=1e-5,\n",
        "                            seed=42, pendulum_params=pend_params)\n",
        "\n",
        "        train_obs, train_targets, _, _, train_ts = pendulum.sample_data_set(\n",
        "            num_train_trials, n, full_targets=False)\n",
        "        # train_obs, _ = pendulum.add_observation_noise(train_obs, first_n_clean=5, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75,\n",
        "        #                                               t_uu=1.0)\n",
        "        train_obs = onp.expand_dims(train_obs, -1)\n",
        "\n",
        "        test_obs, test_targets, _, _, test_ts = pendulum.sample_data_set(\n",
        "            num_test_trials, n, full_targets=False)\n",
        "        # test_obs, _ = pendulum.add_observation_noise(test_obs, first_n_clean=5, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75,\n",
        "        #                                              t_uu=1.0)\n",
        "        test_obs = onp.expand_dims(test_obs, -1)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            os.makedirs(file_path)\n",
        "        onp.savez_compressed(os.path.join(file_path, f\"pend_regression\"+file_tag+\".npz\"),\n",
        "                            train_obs=train_obs, train_targets=train_targets, train_ts=train_ts,\n",
        "                            test_obs=test_obs, test_targets=test_targets, test_ts=test_ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Roo84N4NN9-"
      },
      "source": [
        "The full dataset is (2000, 100, 24, 24, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {
        "id": "RaD7Wl1f9mWx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Create the pendulum dataset (uncomment this block!)\n",
        "# Takes about 2 minutes\n",
        "# generate_pendulums(\"pendulum\", \"regression\", seq_len=200)\n",
        "# generate_pendulums(\"pendulum\", \"regression\", \n",
        "#                    num_train_trials=0, num_test_trials=100, \n",
        "#                    seq_len=400, file_tag=\"_longer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {
        "id": "5Op7ol3UKP0g"
      },
      "outputs": [],
      "source": [
        "# @title Load the pendulum dataset\n",
        "def load_pendulum(run_params, log=False):    \n",
        "    d = run_params[\"dataset_params\"]\n",
        "    train_trials = d[\"train_trials\"]\n",
        "    val_trials = d[\"val_trials\"]\n",
        "    noise_scale = d[\"emission_cov\"] ** 0.5\n",
        "    key_train, key_val, key_pred = jr.split(d[\"seed\"], 3)\n",
        "\n",
        "    data = np.load(\"pendulum/pend_regression.npz\")\n",
        "\n",
        "    def _process_data(data, key):\n",
        "        processed = data[:, ::2] / 255.0\n",
        "        processed += jr.normal(key=key, shape=processed.shape) * noise_scale\n",
        "        # return np.clip(processed, 0, 1)\n",
        "        return processed # We are not cliping the data anymore!\n",
        "\n",
        "    # Take subset, subsample every 2 frames, normalize to [0, 1]\n",
        "    train_data = _process_data(data[\"train_obs\"][:train_trials], key_train)\n",
        "    train_states = data[\"train_targets\"][:train_trials, ::2]\n",
        "    # val_data = _process_data(data[\"test_obs\"][:val_trials], key_val)\n",
        "    data = np.load(\"pendulum/pend_regression_longer.npz\")\n",
        "    val_data = _process_data(data[\"test_obs\"][:val_trials], key_pred)\n",
        "    val_states = data[\"test_targets\"][:val_trials, ::2]\n",
        "\n",
        "    print(\"Full dataset:\", data[\"train_obs\"].shape)\n",
        "    print(\"Subset:\", train_data.shape)\n",
        "    return {\n",
        "        \"train_data\": train_data,\n",
        "        \"val_data\": val_data,\n",
        "        \"train_states\": train_states,\n",
        "        \"val_states\": val_states,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU2AZWhAPjux"
      },
      "source": [
        "# Run experiments!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {
        "id": "_MograqHxqqs"
      },
      "outputs": [],
      "source": [
        "if (\"data_dict\" not in globals()): data_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {
        "cellView": "form",
        "id": "DXDxYDcDrCBG"
      },
      "outputs": [],
      "source": [
        "# @title Define network architecture parameters\n",
        "\n",
        "linear_recnet_architecture = {\n",
        "    \"diagonal_covariance\": False,\n",
        "    \"input_rank\": 1,\n",
        "    \"head_mean_params\": { \"features\": [] },\n",
        "    \"head_var_params\": { \"features\": [] },\n",
        "    \"eps\": 1e-3,\n",
        "    \"cov_init\": 2,\n",
        "}\n",
        "\n",
        "BiRNN_recnet_architecture = {\n",
        "    \"input_rank\": 1,\n",
        "    \"input_type\": \"MLP\",\n",
        "    \"input_params\":{ \"features\": [20,] },\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"head_dyn_params\": { \"features\": [20,] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "planet_posterior_architecture = {\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "linear_decnet_architecture = {\n",
        "    \"diagonal_covariance\": True,\n",
        "    \"input_rank\": 1,\n",
        "    \"head_mean_params\": { \"features\": [] },\n",
        "    \"head_var_params\": { \"features\": [] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "CNN_layers = [\n",
        "            {\"features\": 32, \"kernel_size\": (3, 3), \"strides\": (1, 1) },\n",
        "            {\"features\": 64, \"kernel_size\": (3, 3), \"strides\": (2, 2) },\n",
        "            {\"features\": 32, \"kernel_size\": (3, 3), \"strides\": (2, 2) }\n",
        "]\n",
        "\n",
        "CNN_recnet_architecture = {\n",
        "    \"input_rank\": 3,\n",
        "    \"trunk_type\": \"CNN\",\n",
        "    \"trunk_params\": {\n",
        "        \"layer_params\": CNN_layers\n",
        "    },\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "CNN_BiRNN_recnet_architecture = {\n",
        "    \"input_rank\": 3,\n",
        "    \"input_type\": \"CNN\",\n",
        "    \"input_params\":{\n",
        "        \"layer_params\": CNN_layers\n",
        "    },\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"head_dyn_params\": { \"features\": [20,] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "DCNN_decnet_architecture = {\n",
        "    \"input_shape\": (6, 6, 32),\n",
        "    \"layer_params\": [\n",
        "        { \"features\": 64, \"kernel_size\": (3, 3), \"strides\": (2, 2) },\n",
        "        { \"features\": 32, \"kernel_size\": (3, 3), \"strides\": (2, 2) },\n",
        "        { \"features\": 2, \"kernel_size\": (3, 3) }\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {
        "id": "92Lh9HVoqqZO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run parameter expanders\n",
        "def get_lr(params, max_iters):\n",
        "    base_lr = params[\"base_lr\"]\n",
        "    prior_base_lr = params[\"prior_base_lr\"]\n",
        "    lr = base_lr\n",
        "    prior_lr = prior_base_lr\n",
        "    pprint(params)\n",
        "    if params[\"lr_decay\"]:\n",
        "        print(\"Using learning rate decay!\")\n",
        "        lr = opt.exponential_decay(init_value=base_lr, \n",
        "                                     transition_steps=max_iters,\n",
        "                                     decay_rate=0.99, \n",
        "                                   transition_begin=.8*max_iters, staircase=False)\n",
        "        # This is kind of a different scheme but whatever...\n",
        "        if params[\"prior_lr_warmup\"]:\n",
        "            prior_lr = opt.cosine_onecycle_schedule(max_iters, prior_base_lr, 0.5)\n",
        "    else:\n",
        "        lr = base_lr\n",
        "        if params[\"prior_lr_warmup\"]: \n",
        "            prior_lr = opt.linear_schedule(0, prior_base_lr, .2 * max_iters, 0)\n",
        "    return lr, prior_lr\n",
        "\n",
        "def expand_lds_parameters(params):\n",
        "    num_timesteps = params.get(\"num_timesteps\") or 200\n",
        "    train_trials = { \"small\": 10, \"medium\": 100, \"large\": 1000 }\n",
        "    batch_sizes = {\"small\": 10, \"medium\": 10, \"large\": 20 }\n",
        "    emission_noises = { \"small\": 10., \"medium\": 1., \"large\": .1 }\n",
        "    dynamics_noises = { \"small\": 0.01, \"medium\": .1, \"large\": .1 }\n",
        "    max_iters = 8000\n",
        "\n",
        "    # Modify all the architectures according to the parameters given\n",
        "    D, H, N = params[\"latent_dims\"], params[\"rnn_dims\"], params[\"emission_dims\"]\n",
        "    inf_params = {}\n",
        "    if (params[\"inference_method\"] == \"svae\"):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(linear_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "    elif (params[\"inference_method\"] in [\"dkf\", \"cdkf\"]):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianBiRNN\"\n",
        "        architecture = deepcopy(BiRNN_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "        architecture[\"rnn_dim\"] = H\n",
        "    elif (params[\"inference_method\"] == \"planet\"):\n",
        "        # Here we're considering the filtering setting as default\n",
        "        # Most likely we're not even going to use this so it should be fine\n",
        "        # If we want smoothing then we can probably just replace these with the\n",
        "        # BiRNN version\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(linear_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = H\n",
        "        post_arch = deepcopy(planet_posterior_architecture)\n",
        "        post_arch[\"input_dim\"] = H\n",
        "        post_arch[\"rnn_dim\"] = H\n",
        "        post_arch[\"output_dim\"] = D\n",
        "        inf_params[\"posterior_architecture\"] = post_arch\n",
        "        inf_params[\"sample_kl\"] = True # PlaNet doesn't have built-in suff-stats\n",
        "    else:\n",
        "        print(\"Inference method not found: \" + params[\"inference_method\"])\n",
        "        assert(False)\n",
        "    decnet_architecture = deepcopy(linear_decnet_architecture)\n",
        "    decnet_architecture[\"output_dim\"] = N\n",
        "    inf_params[\"decnet_class\"] = \"GaussianEmission\"\n",
        "    inf_params[\"decnet_architecture\"] = decnet_architecture\n",
        "    inf_params[\"recnet_architecture\"] = architecture\n",
        "\n",
        "    lr, prior_lr = get_lr(params, max_iters)\n",
        "\n",
        "    extended_params = {\n",
        "        \"project_name\": \"SVAE-LDS-Final\",\n",
        "        \"log_to_wandb\": True,\n",
        "        \"dataset\": \"lds\",\n",
        "        \"dataset_params\": {\n",
        "            \"seed\": key_0,\n",
        "            \"num_trials\": train_trials[params[\"dataset_size\"]],\n",
        "            \"num_timesteps\": num_timesteps,\n",
        "            \"emission_cov\": emission_noises[params[\"snr\"]],\n",
        "            \"dynamics_cov\": dynamics_noises[params[\"snr\"]],\n",
        "            \"latent_dims\": D,\n",
        "            \"emission_dims\": N,\n",
        "        },\n",
        "        # Implementation choice\n",
        "        \"use_parallel_kf\": False,\n",
        "        # Training specifics\n",
        "        \"max_iters\": max_iters,\n",
        "        \"elbo_samples\": 1,\n",
        "        \"sample_kl\": False,\n",
        "        \"batch_size\": batch_sizes[params[\"dataset_size\"]],\n",
        "        \"record_params\": lambda i: i % 100 == 0,\n",
        "        \"plot_interval\": 100,\n",
        "        \"learning_rate\": lr, \n",
        "        \"prior_learning_rate\": prior_lr\n",
        "    }\n",
        "    extended_params.update(inf_params)\n",
        "    # This allows us to override ANY of the above...!\n",
        "    extended_params.update(params)\n",
        "    return extended_params\n",
        "\n",
        "def expand_pendulum_parameters(params):\n",
        "    train_trials = { \"small\": 20, \"medium\": 100, \"large\": 2000 }\n",
        "    batch_sizes = {\"small\": 10, \"medium\": 10, \"large\": 40 }\n",
        "    # Not a very good validation split (mostly because we're doing one full batch for val)\n",
        "    val_trials = { \"small\": 4, \"medium\": 20, \"large\": 200 }\n",
        "    noise_scales = { \"small\": 1., \"medium\": .1, \"large\": .01 }\n",
        "    max_iters = 20000\n",
        "\n",
        "    # Modify all the architectures according to the parameters given\n",
        "    D, H = params[\"latent_dims\"], params[\"rnn_dims\"]\n",
        "    inf_params = {}\n",
        "    if (params[\"inference_method\"] == \"svae\"):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(CNN_recnet_architecture)\n",
        "        architecture[\"trunk_params\"][\"output_dim\"] = D\n",
        "        architecture[\"output_dim\"] = D\n",
        "    elif (params[\"inference_method\"] in [\"dkf\", \"cdkf\"]):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianBiRNN\"\n",
        "        architecture = deepcopy(CNN_BiRNN_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "        architecture[\"rnn_dim\"] = H\n",
        "        architecture[\"input_params\"][\"output_dim\"] = H\n",
        "    elif (params[\"inference_method\"] == \"planet\"):\n",
        "        # Here we're considering the filtering setting as default\n",
        "        # Most likely we're not even going to use this so it should be fine\n",
        "        # If we want smoothing then we can probably just replace these with the\n",
        "        # BiRNN version\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(CNN_recnet_architecture)\n",
        "        architecture[\"trunk_params\"][\"output_dim\"] = H\n",
        "        architecture[\"output_dim\"] = H\n",
        "        post_arch = deepcopy(planet_posterior_architecture)\n",
        "        post_arch[\"input_dim\"] = H\n",
        "        post_arch[\"rnn_dim\"] = H\n",
        "        post_arch[\"output_dim\"] = D\n",
        "        inf_params[\"posterior_architecture\"] = post_arch\n",
        "        inf_params[\"sample_kl\"] = True # PlaNet doesn't have built-in suff-stats\n",
        "    else:\n",
        "        print(\"Inference method not found: \" + params[\"inference_method\"])\n",
        "        assert(False)\n",
        "    inf_params[\"recnet_architecture\"] = architecture\n",
        "    \n",
        "    lr, prior_lr = get_lr(params, max_iters)\n",
        "\n",
        "    extended_params = {\n",
        "        \"project_name\": \"SVAE-Pendulum-Final\",\n",
        "        \"log_to_wandb\": True,\n",
        "        \"dataset\": \"pendulum\",\n",
        "        # Must be model learning\n",
        "        \"run_type\": \"model_learning\",\n",
        "        \"decnet_class\": \"GaussianDCNNEmission\",\n",
        "        \"decnet_architecture\": DCNN_decnet_architecture,\n",
        "        \"dataset_params\": {\n",
        "            \"seed\": key_0,\n",
        "            \"train_trials\": train_trials[params[\"dataset_size\"]],\n",
        "            \"val_trials\": val_trials[params[\"dataset_size\"]],\n",
        "            \"emission_cov\": noise_scales[params[\"snr\"]]\n",
        "        },\n",
        "        # Implementation choice\n",
        "        \"use_parallel_kf\": False,\n",
        "        # Training specifics\n",
        "        \"max_iters\": max_iters,\n",
        "        \"elbo_samples\": 1,\n",
        "        \"sample_kl\": False,\n",
        "        \"batch_size\": batch_sizes[params[\"dataset_size\"]],\n",
        "        \"record_params\": lambda i: i % 100 == 0,\n",
        "        \"plot_interval\": 200,\n",
        "        \"mask_type\": \"potential\" if params[\"inference_method\"] == \"svae\" else \"data\",\n",
        "        \"learning_rate\": lr, \n",
        "        \"prior_learning_rate\": prior_lr,\n",
        "        \"use_validation\": True,\n",
        "        \"constrain_dynamics\": True,\n",
        "        \"prediction_horizon\": 5,\n",
        "    }\n",
        "    extended_params.update(inf_params)\n",
        "    # This allows us to override ANY of the above...!\n",
        "    extended_params.update(params)\n",
        "    return extended_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpwzMT9YQSMT"
      },
      "source": [
        "## Run the LDS experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5saocWweLzt"
      },
      "outputs": [],
      "source": [
        "# # @title LDS run parameters\n",
        "# run_params = {\n",
        "#     # Most important: inference method\n",
        "#     \"inference_method\": \"svae\",\n",
        "#     \"use_parallel_kf\": True,\n",
        "#     # Relevant dimensions\n",
        "#     \"latent_dims\": 3,\n",
        "#     \"rnn_dims\": 10,\n",
        "#     \"seed\": jr.PRNGKey(0),\n",
        "#     \"dataset_size\": \"small\", # \"small\", \"medium\", \"large\"\n",
        "#     \"snr\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "#     \"use_natural_grad\": False,\n",
        "#     \"constrain_prior\": True,\n",
        "#     # ---------------------------------------------\n",
        "#     \"constrain_dynamics\": True, # Truncates the singular values of A\n",
        "#     # ---------------------------------------------\n",
        "#     # We set it to true for since it's extra work to compute the sufficient stats \n",
        "#     # from smoothed potentials in the current parallel KF\n",
        "#     \"sample_kl\": False,\n",
        "#     \"base_lr\": 1e-3,\n",
        "#     \"prior_base_lr\": 1e-3, # Set to 0 for debugging\n",
        "#     \"prior_lr_warmup\": True,\n",
        "#     \"lr_decay\": False,\n",
        "#     \"group_tag\": \"\",\n",
        "#     # The only LDS-specific entries\n",
        "#     \"emission_dims\": 5,\n",
        "#     \"num_timesteps\": 10000,\n",
        "#     \"run_type\": \"model_learning\", # \"inference_only\"\n",
        "#     \"log_to_wandb\": False,\n",
        "#     \"visualize_training\": False,\n",
        "#     \"max_iters\": 2000\n",
        "# }\n",
        "\n",
        "# # run_variations = {\n",
        "# #     \"num_timesteps\": [50, 100, 200, 400, 800, 1000]\n",
        "# # }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqhimlvNt39G"
      },
      "outputs": [],
      "source": [
        "# jax.config.update(\"jax_debug_nans\", True)\n",
        "# jax.config.update(\"jax_enable_x64\", False)\n",
        "\n",
        "# results = experiment_scheduler(run_params, \n",
        "#                     #  run_variations=run_variations,\n",
        "#                      dataset_getter=sample_lds_dataset, \n",
        "#                      model_getter=init_model, \n",
        "#                      train_func=start_trainer,\n",
        "#                      params_expander=expand_lds_parameters,\n",
        "#                     #  on_error=on_error,\n",
        "#                      continue_on_error=False,\n",
        "#                      )\n",
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P97PoE4yJsMV"
      },
      "outputs": [],
      "source": [
        "# model = results[1][0][\"model\"]\n",
        "# params = results[1][0][\"trainer\"].params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UoVwAwpQYdx"
      },
      "outputs": [],
      "source": [
        "# mean = np.array([np.mean(np.array(exp[\"trainer\"].time_spent[1:])) for exp in results[1]])\n",
        "# std = np.array([np.std(np.array(exp[\"trainer\"].time_spent[1:])) for exp in results[1]])\n",
        "# plt.plot([50, 100, 200, 400, 800, 1000], mean+std)\n",
        "# plt.plot([50, 100, 200, 400, 800, 1000], mean-std)\n",
        "# assert False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLOSSwKQ9vl3"
      },
      "source": [
        "## Dealing with float32 imprecision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4jST1ldPGfO"
      },
      "outputs": [],
      "source": [
        "# # Code for unpacking and examining results\n",
        "# model = results[1][0][\"model\"]\n",
        "# params = results[1][0][\"trainer\"].params\n",
        "\n",
        "# latent_dim = 20\n",
        "# seq_len = 200\n",
        "\n",
        "# use_x64 = True\n",
        "# # Switch to x64\n",
        "# if (use_x64):\n",
        "#     jax.config.update(\"jax_enable_x64\", True)\n",
        "#     to_64 = lambda x: jax.tree_map(lambda y: np.array(y, dtype=np.float64), x)\n",
        "# else:\n",
        "#     jax.config.update(\"jax_enable_x64\", False)\n",
        "#     to_64 = lambda x: x\n",
        "\n",
        "# data = to_64(data_dict[\"train_data\"][4])\n",
        "# prior_params = to_64(params[\"prior_params\"])\n",
        "# potentials = to_64(model.recognition.apply(params[\"rec_params\"], data))\n",
        "# Sigmas = solve(potentials[\"J\"], np.eye(latent_dim)[None])\n",
        "# mus = vmap(solve)(potentials[\"J\"], potentials[\"h\"])\n",
        "# potentials[\"mu\"] = mus\n",
        "# potentials[\"Sigma\"] = Sigmas\n",
        "\n",
        "# prior_para = ParallelLieParameterizedLinearGaussianChain(latent_dim, seq_len)\n",
        "# posterior_para = ParallelLDSSVAEPosterior_Mean(latent_dim, seq_len)\n",
        "# model.posterior = posterior_para\n",
        "# prior_params_para = prior_para.get_constrained_params(prior_params)\n",
        "# post_params_para = posterior_para.infer(prior_params_para, potentials)\n",
        "\n",
        "# if (use_x64):\n",
        "#     kl_x64 = model.kl_posterior_prior(post_params_para, prior_para.get_constrained_params(prior_params))\n",
        "#     print(\"KL with x64:\", kl_x64)\n",
        "# else:\n",
        "#     kl_x32 = model.kl_posterior_prior(post_params_para, prior_para.get_constrained_params(prior_params))\n",
        "#     print(\"KL with x32:\", kl_x32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li5AH0eRa3in"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_para = posterior_para.distribution(post_params_para)\n",
        "# posterior_dist_para.log_prob(posterior_dist_para.expected_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI-YukjSbqj6"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_para.entropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_caqLPKFcnio"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_old.entropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLvURJK7bDg9"
      },
      "outputs": [],
      "source": [
        "# err = np.mean(posterior_dist_para.expected_states_squared \n",
        "#             - posterior_dist_old.expected_states_squared, axis=(1,2))\n",
        "# plt.plot(err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QOiE80IZ9vY"
      },
      "outputs": [],
      "source": [
        "# posterior_old = ParallelLDSSVAEPosterior(latent_dim, seq_len)\n",
        "# post_params_old = posterior_old.infer(prior_params_para, potentials)\n",
        "# posterior_dist_old = posterior_old.distribution(post_params_old)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mfPPecNavsk"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_old.log_prob(posterior_dist_old.expected_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHtj6ZwJaGKh"
      },
      "outputs": [],
      "source": [
        "# plt.plot(post_params_old[\"mu_filtered\"] - post_params_para[\"mu_filtered\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZP9aMCP3k6l"
      },
      "outputs": [],
      "source": [
        "# Sigmas = solve(potentials[\"J\"], np.eye(latent_dim)[None])\n",
        "# mus = vmap(solve)(potentials[\"J\"], potentials[\"h\"])\n",
        "# p = prior_params_para\n",
        "# params = { \"m1\": p[\"m1\"], \"Q1\": p[\"Q1\"], \n",
        "#           \"A\": p[\"A\"], \"b\": p[\"b\"], \"Q\": p[\"Q\"], \n",
        "#           \"mus\": mus, \"Sigmas\": Sigmas }\n",
        "# np.save(\"faulty_params.npy\", params, allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II5Q019TDHkb"
      },
      "outputs": [],
      "source": [
        "# def log_normalizer(p):\n",
        "#     Q, A, b = p[\"Q\"][None], p[\"A\"][None], p[\"b\"][None]\n",
        "#     AT = (p[\"A\"].T)[None]\n",
        "\n",
        "#     I = np.eye(Q.shape[-1])\n",
        "\n",
        "#     Sigma_filtered, mu_filtered = p[\"Sigma_filtered\"][:-1], p[\"mu_filtered\"][:-1]\n",
        "#     Sigma = Q + A @ Sigma_filtered @ AT\n",
        "#     mu = (A[0] @ mu_filtered.T).T + b\n",
        "#     # Append the first element\n",
        "#     Sigma_pred = np.concatenate([p[\"Q1\"][None], Sigma])\n",
        "#     mu_pred = np.concatenate([p[\"m1\"][None], mu])\n",
        "#     mu_rec, Sigma_rec = p[\"mu\"], p[\"Sigma\"]\n",
        "\n",
        "#     def log_Z_single(mu_pred, Sigma_pred, mu_rec, Sigma_rec):\n",
        "#         return MVN(loc=mu_pred, covariance_matrix=Sigma_pred+Sigma_rec).log_prob(mu_rec)\n",
        "\n",
        "#     log_Z = vmap(log_Z_single)(mu_pred, Sigma_pred, mu_rec, Sigma_rec)\n",
        "#     return np.sum(log_Z)\n",
        "\n",
        "# log_normalizer(post_params_para) - post_params_para[\"log_Z\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbHRN57av7Hk"
      },
      "outputs": [],
      "source": [
        "# def log_prob(self, p, data):\n",
        "\n",
        "#     A = self._dynamics_matrix #params[\"A\"]\n",
        "#     Q = self._dynamics_noise_covariance #params[\"Q\"]\n",
        "#     Q1 = self._initial_covariance #params[\"Q1\"]\n",
        "#     m1 = self._initial_mean #params[\"m1\"]\n",
        "\n",
        "#     num_batch_dims = len(data.shape) - 2\n",
        "\n",
        "#     t1 = np.sum(\n",
        "#         MVN(loc=np.einsum(\"ij,...tj->...ti\", A, data[...,:-1,:]), \n",
        "#             covariance_matrix=Q).log_prob(data[...,1:,:])\n",
        "#         )\n",
        "    \n",
        "#     t2 = MVN(loc=m1, covariance_matrix=Q1).log_prob(data[...,0,:])\n",
        "\n",
        "#     # Add the observation potentials\n",
        "#     # t3_ = - 0.5 * np.einsum(\"...ti,tij,...tj->...t\", data, self._emissions_precisions, data)\n",
        "#     # t3 = np.sum(t3_)\n",
        "#     # t4_ = + np.einsum(\"...ti,ti->...t\", data, self._emissions_linear_potentials)\n",
        "#     # t4 = np.sum(t4_)\n",
        "#     t3 = 0\n",
        "#     t4 = np.sum(MVN(loc=self._emissions_means, \n",
        "#                   covariance_matrix=self._emissions_covariances).log_prob(data))\n",
        "#     # print(t4)\n",
        "#     # Add the log normalizer\n",
        "#     # t5 = -self._log_normalizer\n",
        "#     # t5_ = np.ones(seq_len) * t5 / seq_len\n",
        "#     t5 = -log_normalizer(p)\n",
        "\n",
        "#     s1 = t3 + t4 + t5\n",
        "#     # s1_ = np.sum(t3_ + t4_ + t5_)\n",
        "\n",
        "#     # return s1, (t1, t2, t3, t4, t5, s1, s1_)\n",
        "#     return t1 + t2 + s1, (t1, t2, t3, t4, t5, s1)\n",
        "\n",
        "# if (use_x64):\n",
        "#     dist = posterior_para.distribution(post_params_para)\n",
        "#     lp, x64_trace_ll = log_prob(dist, post_params_para, dist.expected_states)\n",
        "#     print(\"Log prob computed stepwise with x64:\", lp)\n",
        "# else:\n",
        "#     dist = posterior_para.distribution(post_params_para)\n",
        "#     lp, x32_trace_ll = log_prob(dist, post_params_para, dist.expected_states)\n",
        "#     print(\"Log prob computed stepwise with x32:\", lp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liSyGmnRlE9l"
      },
      "outputs": [],
      "source": [
        "# def kl_posterior_prior(posterior, prior, posterior_params, prior_params):\n",
        "#     posterior = posterior.distribution(posterior_params)\n",
        "#     prior = prior.distribution(prior_params)\n",
        "#     Ex = posterior.expected_states\n",
        "#     ExxT = posterior.expected_states_squared\n",
        "#     ExnxT = posterior.expected_states_next_states\n",
        "#     Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "#     Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "#     # J, L = prior_params[\"J\"], prior_params[\"L\"]\n",
        "#     p = dynamics_to_tridiag(prior_params, Ex.shape[0], Ex.shape[1])\n",
        "#     J, L = p[\"J\"], p[\"L\"]\n",
        "\n",
        "#     p = dynamics_to_tridiag(posterior_params, Ex.shape[0], Ex.shape[1])\n",
        "#     J_post, L_post = p[\"J\"] + potentials[\"J\"], p[\"L\"]\n",
        "\n",
        "#     t1 = -prior.log_prob(Ex) \n",
        "#     # t1 = 0#-log_prob(prior, prior_params, Ex)[0]\n",
        "#     t2 = 0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "#     t3 = np.einsum(\"tij,tij->\", L, Sigmatnt)\n",
        "#     # t4 = -posterior.log_prob(Ex) \n",
        "#     t4 = -log_prob(posterior, posterior_params, Ex)[0]\n",
        "#     t5 = 0.5 * np.einsum(\"tij,tij->\", J_post, Sigmatt)\n",
        "#     t6 = np.einsum(\"tij,tij->\", L_post, Sigmatnt)\n",
        "\n",
        "#     # print(t1, t2, t3, t4, t5, t6)\n",
        "\n",
        "#     cross_entropy = t1\n",
        "#     cross_entropy += t2 #0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "#     cross_entropy += t3\n",
        "#     cross_entropy -= t4+t5+t6\n",
        "\n",
        "#     return cross_entropy, (t1, t2, t3, t4, t5, t6, t1 - t4, t2 - t5, t3 - t6)\n",
        "\n",
        "# if (use_x64):\n",
        "#     x64_kl, x64_trace = kl_posterior_prior(posterior_para, prior_para, post_params_para, prior_params_para)\n",
        "#     print(\"KL computed stepwise with x64:\", x64_kl)\n",
        "# else:\n",
        "#     x32_kl, x32_trace = kl_posterior_prior(posterior_para, prior_para, post_params_para, prior_params_para)\n",
        "#     print(\"KL computed stepwise with x32:\", x32_kl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kjE9yc1eNIA"
      },
      "outputs": [],
      "source": [
        "# print(\"KL computed stepwise with x32:\", x32_kl)\n",
        "# print(\"KL computed stepwise with x64:\", x64_kl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2LaiyyRuBRD"
      },
      "outputs": [],
      "source": [
        "# pprint([x64_trace[i] - x32_trace[i] for i in range(len(x64_trace))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ELK6UUjTnNr"
      },
      "source": [
        "## Pendulum experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "id": "JjtwI49pGk7U"
      },
      "outputs": [],
      "source": [
        "# @title Pendulum run params\n",
        "run_params = {\n",
        "    # Most important: inference method\n",
        "    \"inference_method\": \"cdkf\",#\"svae\",\n",
        "    \"use_parallel_kf\": False,\n",
        "    # Relevant dimensions\n",
        "    \"latent_dims\": 5,\n",
        "    \"rnn_dims\": 10,\n",
        "    \"seed\": jr.PRNGKey(0),\n",
        "    \"dataset_size\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "    \"snr\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "    \"use_natural_grad\": False,\n",
        "    \"constrain_prior\": True,\n",
        "    \"base_lr\": 1e-2,\n",
        "    \"prior_base_lr\": 1e-3,\n",
        "    \"prior_lr_warmup\": True,\n",
        "    \"lr_decay\": False,\n",
        "    \"group_tag\": \"re-running pendulum\",\n",
        "    # The only pendulum-specific entry, will be overridden by params expander\n",
        "    \"mask_size\": 40,\n",
        "    # \"plot_interval\": 1,\n",
        "    \"mask_start\": 0,#1000,\n",
        "    \"sample_kl\": False,\n",
        "    \"log_to_wandb\": True,\n",
        "    \"max_iters\": 100,\n",
        "}\n",
        "\n",
        "# methods = {\n",
        "#     # \"inference_method\": [\"svae\", \"cdkf\", \"planet\", \"dkf\"],\n",
        "#     # \"mask_start\": [0, 2000, 2000, 2000]\n",
        "#     \"inference_method\": [\"planet\", \"svae\"],\n",
        "#     \"mask_start\": [2000, 0],\n",
        "#     \"use_natural_grad\": [False, False],\n",
        "#     \"constrain_prior\": [True, True]\n",
        "# }\n",
        "\n",
        "seeds = {\n",
        "    \"seed\": [jr.PRNGKey(i) for i in range(3)]\n",
        "}\n",
        "\n",
        "run_variations = seeds#dict_product(seeds, methods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6ae9d0df46794a5aab0ba296ddc4a8f1",
            "968d2537ea464b8fb662bee994d92f4c",
            "9e1f8e2781b04526bfc80057ff2665c9",
            "3c4b305068034c0aaee3e25ee028f41a",
            "66cb3da4d3ea4a988948b8b8e41f35c4",
            "2c85a01ef2eb4cd1816c958dbd2b7a70",
            "5c45927808e74d5c8f5ca8f87805ec9c",
            "9d9497f48d8a4283aba50b8a39eaf248"
          ]
        },
        "id": "7cd8Vsd5q9Dl",
        "outputId": "68e68ceb-60ba-4625-d386-f9237c1ecab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of runs: 1\n",
            "Base paramerters:\n",
            "{'base_lr': 0.01,\n",
            " 'constrain_prior': True,\n",
            " 'dataset_size': 'medium',\n",
            " 'group_tag': 're-running pendulum',\n",
            " 'inference_method': 'cdkf',\n",
            " 'latent_dims': 5,\n",
            " 'log_to_wandb': True,\n",
            " 'lr_decay': False,\n",
            " 'mask_size': 40,\n",
            " 'mask_start': 0,\n",
            " 'max_iters': 100,\n",
            " 'prior_base_lr': 0.001,\n",
            " 'prior_lr_warmup': True,\n",
            " 'rnn_dims': 10,\n",
            " 'sample_kl': False,\n",
            " 'seed': DeviceArray([0, 0], dtype=uint32),\n",
            " 'snr': 'medium',\n",
            " 'use_natural_grad': False,\n",
            " 'use_parallel_kf': False}\n",
            "##########################################\n",
            "Starting run #0\n",
            "##########################################\n",
            "{'base_lr': 0.01,\n",
            " 'constrain_prior': True,\n",
            " 'dataset_size': 'medium',\n",
            " 'group_tag': 're-running pendulum',\n",
            " 'inference_method': 'cdkf',\n",
            " 'latent_dims': 5,\n",
            " 'log_to_wandb': True,\n",
            " 'lr_decay': False,\n",
            " 'mask_size': 40,\n",
            " 'mask_start': 0,\n",
            " 'max_iters': 100,\n",
            " 'prior_base_lr': 0.001,\n",
            " 'prior_lr_warmup': True,\n",
            " 'rnn_dims': 10,\n",
            " 'sample_kl': False,\n",
            " 'seed': DeviceArray([0, 0], dtype=uint32),\n",
            " 'snr': 'medium',\n",
            " 'use_natural_grad': False,\n",
            " 'use_parallel_kf': False}\n",
            "Loading dataset!\n",
            "Full dataset: (0, 400, 24, 24, 1)\n",
            "Subset: (100, 100, 24, 24, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LP: 40468.477:   0%|          | 0/100 [00:17<?, ?it/s]    "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230120_070928-qhqghywk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/matthew9671/SVAE-Pendulum-Final/runs/qhqghywk\" target=\"_blank\">upbeat-breeze-82</a></strong> to <a href=\"https://wandb.ai/matthew9671/SVAE-Pendulum-Final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href=\"https://wandb.ai/matthew9671/SVAE-Pendulum-Final\" target=\"_blank\">https://wandb.ai/matthew9671/SVAE-Pendulum-Final</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href=\"https://wandb.ai/matthew9671/SVAE-Pendulum-Final/runs/qhqghywk\" target=\"_blank\">https://wandb.ai/matthew9671/SVAE-Pendulum-Final/runs/qhqghywk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP: 40468.477:   1%|          | 1/100 [00:19<31:44, 19.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'base_lr': 0.01,\n",
            " 'batch_size': 10,\n",
            " 'constrain_dynamics': True,\n",
            " 'constrain_prior': True,\n",
            " 'dataset': 'pendulum',\n",
            " 'dataset_params': {'emission_cov': 0.1,\n",
            "                    'seed': DeviceArray([0, 0], dtype=uint32),\n",
            "                    'train_trials': 100,\n",
            "                    'val_trials': 20},\n",
            " 'dataset_size': 'medium',\n",
            " 'decnet_architecture': {'input_shape': (6, 6, 32),\n",
            "                         'layer_params': [{'features': 64,\n",
            "                                           'kernel_size': (3, 3),\n",
            "                                           'strides': (2, 2)},\n",
            "                                          {'features': 32,\n",
            "                                           'kernel_size': (3, 3),\n",
            "                                           'strides': (2, 2)},\n",
            "                                          {'features': 2,\n",
            "                                           'kernel_size': (3, 3)}]},\n",
            " 'decnet_class': 'GaussianDCNNEmission',\n",
            " 'elbo_samples': 1,\n",
            " 'group_tag': 're-running pendulum',\n",
            " 'inference_method': 'cdkf',\n",
            " 'latent_dims': 5,\n",
            " 'learning_rate': 0.01,\n",
            " 'log_to_wandb': True,\n",
            " 'lr_decay': False,\n",
            " 'mask_size': 40,\n",
            " 'mask_start': 0,\n",
            " 'mask_type': 'data',\n",
            " 'max_iters': 100,\n",
            " 'plot_interval': 200,\n",
            " 'prediction_horizon': 5,\n",
            " 'prior_base_lr': 0.001,\n",
            " 'prior_learning_rate': <function polynomial_schedule.<locals>.schedule at 0x7f6a9673dca0>,\n",
            " 'prior_lr_warmup': True,\n",
            " 'project_name': 'SVAE-Pendulum-Final',\n",
            " 'recnet_architecture': {'cov_init': 1,\n",
            "                         'eps': 0.0001,\n",
            "                         'head_dyn_params': {'features': [20]},\n",
            "                         'head_mean_params': {'features': [20, 20]},\n",
            "                         'head_var_params': {'features': [20, 20]},\n",
            "                         'input_params': {'layer_params': [{'features': 32,\n",
            "                                                            'kernel_size': (3,\n",
            "                                                                            3),\n",
            "                                                            'strides': (1, 1)},\n",
            "                                                           {'features': 64,\n",
            "                                                            'kernel_size': (3,\n",
            "                                                                            3),\n",
            "                                                            'strides': (2, 2)},\n",
            "                                                           {'features': 32,\n",
            "                                                            'kernel_size': (3,\n",
            "                                                                            3),\n",
            "                                                            'strides': (2, 2)}],\n",
            "                                          'output_dim': 10},\n",
            "                         'input_rank': 3,\n",
            "                         'input_type': 'CNN',\n",
            "                         'output_dim': 5,\n",
            "                         'rnn_dim': 10},\n",
            " 'recnet_class': 'GaussianBiRNN',\n",
            " 'record_params': <function expand_pendulum_parameters.<locals>.<lambda> at 0x7f6a9673da60>,\n",
            " 'rnn_dims': 10,\n",
            " 'run_type': 'model_learning',\n",
            " 'sample_kl': False,\n",
            " 'seed': DeviceArray([0, 0], dtype=uint32),\n",
            " 'snr': 'medium',\n",
            " 'use_natural_grad': False,\n",
            " 'use_parallel_kf': False,\n",
            " 'use_validation': True}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LP: 19878.936: 100%|██████████| 100/100 [00:49<00:00,  2.01it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='1.899 MB of 1.914 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.992333…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ae9d0df46794a5aab0ba296ddc4a8f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Condition number of A</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▇▇█</td></tr><tr><td>Condition number of Q</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ELBO</td><td>▁▃▂▂▃▄▆▆▇▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>KL</td><td>▄▂▁▁▁▁▁▂▂▃▅▆▇███▇▆▆▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Learning rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Likelihood</td><td>▁▃▂▂▃▄▆▆▇▇▇▇████████████████████████████</td></tr><tr><td>Max singular value of A</td><td>▇▇▇▇▄▃▂▁▃▄▇▇▇▇▇▇▇▇▇███▇█▇▇▇█▇▆▇▇▇▇█▇▇▇▇▇</td></tr><tr><td>Min singular value of A</td><td>██████████████▇▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▂▂▁</td></tr><tr><td>Prior learning rate</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Validation ELBO</td><td>▁▇▇▇██████</td></tr><tr><td>Validation KL</td><td>▁▃██▄▃▂▂▂▂</td></tr><tr><td>Validation likelihood</td><td>▁▇▇███████</td></tr><tr><td>Validation prediction log likelihood</td><td>▁▄▄▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Condition number of A</td><td>1.00223</td></tr><tr><td>Condition number of Q</td><td>1.0</td></tr><tr><td>ELBO</td><td>-19878.93555</td></tr><tr><td>KL</td><td>214.40408</td></tr><tr><td>Learning rate</td><td>0.01</td></tr><tr><td>Likelihood</td><td>-19664.53125</td></tr><tr><td>Max singular value of A</td><td>1.0</td></tr><tr><td>Min singular value of A</td><td>0.99778</td></tr><tr><td>Prior learning rate</td><td>2e-05</td></tr><tr><td>Validation ELBO</td><td>-20063.37891</td></tr><tr><td>Validation KL</td><td>213.35985</td></tr><tr><td>Validation likelihood</td><td>-19850.02148</td></tr><tr><td>Validation prediction log likelihood</td><td>-197.46255</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">upbeat-breeze-82</strong> at: <a href=\"https://wandb.ai/matthew9671/SVAE-Pendulum-Final/runs/qhqghywk\" target=\"_blank\">https://wandb.ai/matthew9671/SVAE-Pendulum-Final/runs/qhqghywk</a><br/>Synced 5 W&B file(s), 20 media file(s), 0 artifact file(s) and 2 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230120_070928-qhqghywk/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Run the pendulum experiments\n",
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "jax.config.update(\"jax_enable_x64\", False)\n",
        "\n",
        "results = experiment_scheduler(run_params, \n",
        "                    #  run_variations=run_variations,\n",
        "                     dataset_getter=load_pendulum, \n",
        "                     model_getter=init_model, \n",
        "                     train_func=start_trainer,\n",
        "                     params_expander=expand_pendulum_parameters,\n",
        "                     on_error=on_error,\n",
        "                     continue_on_error=True)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run path: matthew9671/SVAE-Pendulum-Final/zvbh2nrm"
      ],
      "metadata": {
        "id": "bAd00_V1V_Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzngmpgUw4cb"
      },
      "source": [
        "# Pendulum Diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XbEGFOLU2i4"
      },
      "source": [
        "- [x] Visualize the samples from the predictive posterior\n",
        "- [x] Implement a shorter horizon prediction \n",
        "- [x] See how the svae performs as prediction horizon gets longer\n",
        "- [x] What about the other frameworks? -- best SVAE beats the cDKF...!\n",
        "- [x] Test the linear decoding for the entire dataset\n",
        "- [x] Package the diagnostics code into helper functions\n",
        "- [x] Also include the linear decoding accuracies (MSE) analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5mX32LKg6xXj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Load the run from WandB\n",
        "data_dict = {}\n",
        "\n",
        "def load_run(project_path, run_name):\n",
        "    api = wandb.Api()\n",
        "    \n",
        "    try:\n",
        "        os.remove(\"parameters.pkl\")\n",
        "    except:\n",
        "        pass\n",
        "    with open(wandb.restore(\"parameters.pkl\", project_path+run_name).name, \"rb\") as f:\n",
        "        d = pkl.load(f)\n",
        "    params = d[-1]\n",
        "\n",
        "    # Get the configs for that specific run\n",
        "    run = api.run(project_path+run_name)\n",
        "    run_params = deepcopy(run.config)\n",
        "    # Get the dataset and the model object\n",
        "\n",
        "    # run_params[\"seed\"] = np.array(run_params[\"seed\"], dtype=np.uint32)\n",
        "    # For some unknown reason we're getting an int instead of a PRNGKey object \n",
        "    run_params[\"seed\"] = jr.PRNGKey(run_params[\"seed\"]) if isinstance(run_params[\"seed\"], int) \\\n",
        "        else np.array(run_params[\"seed\"], dtype=np.uint32)\n",
        "    # For some old runs the architecture logged is incorrect\n",
        "    # del(run_params[\"recnet_architecture\"][\"head_mean_params\"][\"features\"][-1])\n",
        "    # del(run_params[\"recnet_architecture\"][\"head_var_params\"][\"features\"][-1])\n",
        "    run_params[\"dataset_params\"][\"seed\"] = np.array(\n",
        "        run_params[\"dataset_params\"][\"seed\"], dtype=np.uint32)\n",
        "    \n",
        "    global data_dict\n",
        "\n",
        "    data_dict = load_pendulum(run_params)\n",
        "    model_dict = init_model(run_params, data_dict)\n",
        "    model = model_dict[\"model\"]\n",
        "    return run_params, params, model\n",
        "\n",
        "def evaluate_run(project_path, run_name):\n",
        "\n",
        "    # Load the run from wandb\n",
        "    run_params, model_params, model = load_run(project_path, run_name)\n",
        "    run_params = deepcopy(run_params)\n",
        "    run_params[\"mask_size\"] = 0\n",
        "\n",
        "    return get_latents_and_predictions(run_params, model_params, model, data_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "JxCueh0pIQK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "314631c1-0590-4004-da7e-ef1eb09518f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full dataset: (0, 400, 24, 24, 1)\n",
            "Subset: (100, 100, 24, 24, 1)\n"
          ]
        }
      ],
      "source": [
        "# project_path = \"matthew9671/SVAE-Pendulum-Final/\"\n",
        "# result = evaluate_run(project_path, \"zvbh2nrm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = np.load(\"results.npy\", allow_pickle=True).item()"
      ],
      "metadata": {
        "id": "io-aJSDp0G_V"
      },
      "execution_count": 383,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a pendulum state prediction plot"
      ],
      "metadata": {
        "id": "xvmCOOvPSKVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_id = 0 #if \"test_id\" not in globals() else test_id + 1\n",
        "# plt.plot(Ex_test[test_id] @ W_theta, label=\"decoded\")\n",
        "# plt.plot(thetas[test_id], label=\"true\")\n",
        "# plt.title(\"Linear decoding of pendulum angle (test sequence #{})\".format(test_id))\n",
        "# plt.legend()\n",
        "# plt.figure()\n",
        "# plt.plot(Ex_test[test_id] @ W_omega, label=\"decoded\")\n",
        "# plt.plot(omegas[test_id], label=\"true\")\n",
        "# plt.title(\"Linear decoding of pendulum velocity (test sequence #{})\".format(test_id))\n",
        "# plt.legend()"
      ],
      "metadata": {
        "id": "GPGf6D5iKiC9"
      },
      "execution_count": 384,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_theta = result[\"w_theta\"]\n",
        "W_omega = result[\"w_omega\"]\n",
        "preds = result[\"predictions\"]\n",
        "targets = data_dict[\"val_states\"][:, :100]\n",
        "thetas = np.arctan2(targets[:,:,0], targets[:,:,1])\n",
        "omegas = thetas[:,1:]-thetas[:,:-1]\n",
        "pred_theta = np.einsum(\"...i,i->...\", preds, W_theta)\n",
        "pred_omega = np.einsum(\"...i,i->...\", preds, W_omega)\n",
        "theta_norm = np.sum(W_theta ** 2) ** .5\n",
        "omega_norm = np.sum(W_omega ** 2) ** .5\n",
        "latent_cov_theta = np.einsum(\"...ij,i,j->...\", result[\"latent_covariance_partial\"], W_theta, W_theta) \n",
        "latent_cov_omega = np.einsum(\"...ij,i,j->...\", result[\"latent_covariance_partial\"], \n",
        "                       W_omega, W_omega) "
      ],
      "metadata": {
        "id": "8eGj7HfTavYx"
      },
      "execution_count": 385,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We probably want to plot the posterior distribution for the first part and \n",
        "i = 0\n",
        "\n",
        "mean = result[\"latent_mean_partial\"][i] @ W_theta\n",
        "plt.plot(mean)\n",
        "std = latent_cov_theta[i] ** .5\n",
        "plt.fill_between(np.arange(mean.shape[0]),mean+std, mean-std, alpha=.2)\n",
        "\n",
        "mean, std = np.mean(pred_theta[i], axis=0), np.std(pred_theta[i], axis=0) \n",
        "plt.plot(np.arange(49, 100), mean)\n",
        "plt.fill_between(np.arange(49, 100),mean+std, mean-std, alpha=.2)\n",
        "plt.plot(thetas[i])\n",
        "# for j in range(pred_theta.shape[1]):\n",
        "#     plt.plot(np.arange(49, 100), pred_theta[i][j], c=\"grey\")\n",
        "plt.figure()\n",
        "\n",
        "mean = result[\"latent_mean_partial\"][i] @ W_omega\n",
        "plt.plot(mean)\n",
        "std = latent_cov_omega[i] ** .5\n",
        "plt.fill_between(np.arange(mean.shape[0]),mean+std, mean-std, alpha=.2)\n",
        "mean, std = np.mean(pred_omega[i], axis=0), np.std(pred_omega[i], axis=0) \n",
        "plt.plot(np.arange(49, 100), mean)\n",
        "plt.fill_between(np.arange(49, 100),mean+std, mean-std, alpha=.2)\n",
        "plt.plot(omegas[i])\n",
        "# for j in range(pred_omega.shape[1]):\n",
        "#     plt.plot(np.arange(49, 100), pred_omega[i][j], c=\"grey\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "fsD81Yytd2Xs",
        "outputId": "16106945-3edf-4bf2-db97-8ece82cafabd"
      },
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f69f9f10d00>]"
            ]
          },
          "metadata": {},
          "execution_count": 386
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a prediction log likelihood falloff plot"
      ],
      "metadata": {
        "id": "-WOdimoQTWR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(-np.mean(data_dict[\"train_data\"][i][50:], axis=(1, 2, 3)))"
      ],
      "metadata": {
        "id": "2kc4IMVSZia_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_ll = result[\"prediction_lls\"]"
      ],
      "metadata": {
        "id": "etz7-P9Rns7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 3\n",
        "mu = np.mean(pred_ll, axis=0)\n",
        "std = np.std(pred_ll, axis=0)\n",
        "plt.plot(mu)\n",
        "plt.fill_between(np.arange(mu.shape[0]), mu+std, mu-std, alpha=.2)"
      ],
      "metadata": {
        "id": "eT_2nda9YGE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Archive"
      ],
      "metadata": {
        "id": "L6fbLo70TQWf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns5x_wrtKc1W"
      },
      "outputs": [],
      "source": [
        "# result = evaluate_run(project_path, \"cdt4gir1\", key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8foTbjOSMiq2"
      },
      "outputs": [],
      "source": [
        "# _ = evaluate_run(project_path, \"dgudjrut\", key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsHe74D5FFVK"
      },
      "outputs": [],
      "source": [
        "# del all_results\n",
        "if (\"all_results\" not in globals()): all_results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uVctTR3JcBN"
      },
      "outputs": [],
      "source": [
        "api = wandb.Api()\n",
        "latent_dims = 5\n",
        "runs = api.runs(\"matthew9671/SVAE-Pendulum-ICML-3\", filters={\"State\": \"finished\", \n",
        "                                                             \"config.inference_method\": \"svae\",\n",
        "                                                             \"config.latent_dims\": latent_dims})\n",
        "svae_runs = []\n",
        "for run in runs:\n",
        "    svae_runs.append(run.id)\n",
        "runs = api.runs(\"matthew9671/SVAE-Pendulum-ICML-3\", filters={\"State\": \"finished\", \n",
        "                                                             \"config.inference_method\": \"cdkf\",\n",
        "                                                             \"config.latent_dims\": latent_dims})\n",
        "cdkf_runs = []\n",
        "for run in runs:\n",
        "    cdkf_runs.append(run.id)\n",
        "runs = api.runs(\"matthew9671/SVAE-Pendulum-ICML-3\", filters={\"State\": \"finished\", \n",
        "                                                             \"config.inference_method\": \"planet\",\n",
        "                                                             \"config.latent_dims\": latent_dims})\n",
        "planet_runs = []\n",
        "for run in runs:\n",
        "    planet_runs.append(run.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKPe7m3kD70V"
      },
      "outputs": [],
      "source": [
        "ax = plt.subplot()\n",
        "project_path = \"matthew9671/SVAE-Pendulum-ICML-3/\"\n",
        "# svae_runs = [\"v6sbb9xh\", \"xpf9s9ie\", \"1jxw27wp\", \"yo3fprzr\"]\n",
        "# cdkf_runs = [\"ik6i6igs\"] #\"cdt4gir1\", \"mpc58ktj\", \"dgudjrut\", \"4nctqeby\", \"cc8s8xxs\"]\n",
        "key = key_0\n",
        "for run_name in svae_runs + cdkf_runs:# + planet_runs:\n",
        "    print(\"Loading run \" + run_name)\n",
        "    key = jr.split(key)[1]\n",
        "    if (run_name not in all_results):\n",
        "        all_results[run_name] = evaluate_run(project_path, run_name, key)\n",
        "    result = all_results[run_name]\n",
        "    mean_pred_lls = result[\"long_horizon_pred_lls\"].mean(axis=0)\n",
        "    if (mean_pred_lls.mean() < -100 and run_name in svae_runs):\n",
        "        print(\"Run \" + run_name + \" gives really bad likelihoods!\")\n",
        "    if (run_name in svae_runs):\n",
        "        ax.plot(mean_pred_lls, color=\"blue\", alpha=.3)\n",
        "    elif (run_name in cdkf_runs):\n",
        "        ax.plot(mean_pred_lls, color=\"red\", alpha=.3)\n",
        "    elif (run_name in planet_runs):\n",
        "        # pass\n",
        "        ax.plot(mean_pred_lls, color=\"green\", alpha=.3)\n",
        "ax.plot(0, label=\"svae\", color=\"blue\")\n",
        "ax.plot(0, label=\"cdkf\", color=\"red\")\n",
        "ax.plot(0, label=\"planet\", color=\"green\")\n",
        "ax.set_title(\"Prediction log likelihood vs. horizon\")\n",
        "# ax.set_ylim(-100, 150)\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjH4oyyKkE-D"
      },
      "outputs": [],
      "source": [
        "svae_thetas = []\n",
        "svae_omegas = []\n",
        "for run_name in svae_runs:\n",
        "    svae_thetas.append(all_results[run_name][\"theta_mse\"])\n",
        "    svae_omegas.append(all_results[run_name][\"omega_mse\"])\n",
        "cdkf_thetas = []\n",
        "cdkf_omegas = []\n",
        "for run_name in cdkf_runs:\n",
        "    cdkf_thetas.append(all_results[run_name][\"theta_mse\"])\n",
        "    cdkf_omegas.append(all_results[run_name][\"omega_mse\"])\n",
        "planet_thetas = []\n",
        "planet_omegas = []\n",
        "for run_name in planet_runs:\n",
        "    planet_thetas.append(all_results[run_name][\"theta_mse\"])\n",
        "    planet_omegas.append(all_results[run_name][\"omega_mse\"])\n",
        "\n",
        "svae_thetas = np.array(svae_thetas)\n",
        "cdkf_thetas = np.array(cdkf_thetas)\n",
        "planet_thetas = np.array(planet_thetas)\n",
        "svae_omegas = np.array(svae_omegas) * 100\n",
        "cdkf_omegas = np.array(cdkf_omegas) * 100\n",
        "planet_omegas = np.array(planet_omegas) * 100\n",
        "bar_width = .2\n",
        "\n",
        "plt.scatter(np.zeros_like(svae_thetas)-bar_width/2, svae_thetas, s=2, color=\"blue\")\n",
        "plt.bar(-bar_width/2, svae_thetas.mean(), width=bar_width, color=\"blue\", alpha=.3, label=\"svae\")\n",
        "plt.scatter(np.zeros_like(cdkf_thetas)+bar_width/2, cdkf_thetas, s=2, color=\"red\")\n",
        "plt.bar(bar_width/2, cdkf_thetas.mean(), width=bar_width, color=\"red\", alpha=.3, label=\"cdkf\")\n",
        "plt.scatter(np.zeros_like(planet_thetas)+bar_width*3/2, planet_thetas, s=2, color=\"green\")\n",
        "plt.bar(bar_width*3/2, planet_thetas.mean(), width=bar_width, color=\"green\", alpha=.3, label=\"planet\")\n",
        "\n",
        "plt.scatter(np.zeros_like(svae_omegas)+1-bar_width/2, svae_omegas, s=2, color=\"blue\")\n",
        "plt.bar(1-bar_width/2, svae_omegas.mean(), width=bar_width, color=\"blue\", alpha=.3)\n",
        "plt.scatter(np.zeros_like(cdkf_omegas)+1+bar_width/2, cdkf_omegas, s=2, color=\"red\")\n",
        "plt.bar(1+bar_width/2, cdkf_omegas.mean(), width=bar_width, color=\"red\", alpha=.3)\n",
        "plt.scatter(np.zeros_like(planet_omegas)+1+bar_width*3/2, planet_omegas, s=2, color=\"green\")\n",
        "plt.bar(1+bar_width*3/2, planet_omegas.mean(), width=bar_width, color=\"green\", alpha=.3)\n",
        "plt.title(\"MSE for linear decoding of true pendulum state\")\n",
        "plt.xticks([0, 1], [\"theta\", \"omega\"])\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iychu_ZEV6Z7"
      },
      "outputs": [],
      "source": [
        "# @title Turns out the correlation between runs comes from the fluctuating averge pixel intensity of the image\n",
        "i = 1 # data id\n",
        "result = all_results[\"v6sbb9xh\"]\n",
        "plt.plot(result[\"long_horizon_pred_lls\"][i])\n",
        "# plt.plot(result[\"predictions\"][i][0])\n",
        "result = all_results[\"xpf9s9ie\"]\n",
        "plt.plot(result[\"long_horizon_pred_lls\"][i])\n",
        "# plt.plot(result[\"predictions\"][i][0])\n",
        "obs_mean = data_dict[\"train_data\"][i,50:100].sum(axis=(1, 2, 3))\n",
        "plt.plot((obs_mean - obs_mean.mean()) * -5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7tG1ohb4wl-"
      },
      "outputs": [],
      "source": [
        "# @title SVAE run names\n",
        "# run_name = \"391tsihg\" # 5d\n",
        "# run_name = \"vs2dkdje\" # 3d\n",
        "# run_name = \"zp5manco\" # 2d\n",
        "# 5d sinusoidal\n",
        "# run_name = \"0q0c0hbw\" \n",
        "# run_name = \"0vogdb8f\"\n",
        "# run_name = \"5b8cefgf\" # ICML-2 hopeful-sweep\n",
        "# run_name = \"7ntuood6\" # ICML-2 dainty-sweep (best prediction I've seen so far)\n",
        "# run_name = \"xpf9s9ie\" # ICML-3 good-sweep-21\n",
        "run_name = \"v6sbb9xh\" # ICML-3 rose-sweep-20 (good prediction)\n",
        "# \"1jxw27wp\" # ICML-3 solar-sweep\n",
        "# \"yo3fprzr\" # ICML-3 dry-sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJi79Ias4IP_"
      },
      "outputs": [],
      "source": [
        "# @title CDKF run names\n",
        "# run_name = \"cy1j7jyg\" # ICML-2 fancy-sweep\n",
        "run_name = \"dgudjrut\" # ICML-3 quiet-sweep\n",
        "run_name = \"4nctqeby\" # ICML-3 honest-sweep\n",
        "# \"cdt4gir1\" tough-sweep\n",
        "# \"mpc58ktj\" young-sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okamp0foh-Yg"
      },
      "outputs": [],
      "source": [
        "# If sampling from the prior becomes problematic, run this to truncate the singular values of A\n",
        "# prior_params[\"A\"] = truncate_singular_values(prior_params[\"A\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlSFRedYCyUR"
      },
      "outputs": [],
      "source": [
        "# @title Sample from the prior and visualize its decoding\n",
        "key = key_0\n",
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "prior_sample = model.prior.sample(prior_params, shape=(1,), key=key)[0]\n",
        "plt.plot(prior_sample)\n",
        "plt.figure()\n",
        "# plot_pcs(prior_sample, 2)\n",
        "out_dist = model.decoder.apply(dec_params, prior_sample)\n",
        "y_decoded = out_dist.mean()\n",
        "plt.figure()\n",
        "plot_img_grid(y_decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPgdXO-fDWue"
      },
      "outputs": [],
      "source": [
        "key = jr.split(key)[0]\n",
        "data_id = 3#jr.choice(key, 100)\n",
        "data = data_dict[\"train_data\"][data_id]\n",
        "# states = targets[data_id]\n",
        "# angles = np.arctan2(states[:,0], states[:,1])\n",
        "plot_img_grid(data)\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sZcQduzMC5m-"
      },
      "outputs": [],
      "source": [
        "# @title Visualize the mean predicted trajectory from the model\n",
        "# This might be the wrong thing to do, because the prior dynamics might not be accurate for\n",
        "# specific observation sequences\n",
        "temp_params = deepcopy(run_params)\n",
        "temp_params[\"mask_size\"] = 0\n",
        "out = model.elbo(jr.PRNGKey(0), data, params, **temp_params)\n",
        "post_params = out[\"posterior_params\"]\n",
        "post_dist = model.posterior.distribution(post_params)\n",
        "Ex = post_dist.mean()\n",
        "A = prior_params[\"A\"]\n",
        "b = prior_params[\"b\"]\n",
        "T = 100\n",
        "Ex_pred = predict_forward(Ex[T//2-1], A, b, T//2)\n",
        "hs = plt.plot(Ex)\n",
        "hs_ = plt.plot(np.arange(T//2-1, T), np.concatenate([Ex[T//2-1][None], Ex_pred]), linestyle=\":\")\n",
        "plt.title(\"Posterior and predictions\")\n",
        "for i in range(len(hs)):\n",
        "    hs_[i].set_color(hs[i].get_color())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDmP8-yldd8U"
      },
      "outputs": [],
      "source": [
        "out_dist = model.decoder.apply(dec_params, np.concatenate([Ex[:T//2], Ex_pred]))\n",
        "y_decoded = out_dist.mean()\n",
        "plt.figure()\n",
        "plot_img_grid(y_decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0aed57nAXHoP"
      },
      "outputs": [],
      "source": [
        "# @title Visualize multiple possible future paths predicted by the model...!\n",
        "\n",
        "\n",
        "jax.config.update(\"jax_debug_nans\", False)\n",
        "train_data = data_dict[\"train_data\"][:20,:100]\n",
        "x_preds, svae_pred_lls = vmap(predict_multiple, in_axes=(0, None, None))\\\n",
        "    (train_data, key_0, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhKaOqzTZHRW"
      },
      "outputs": [],
      "source": [
        "# D = model.prior.latent_dims\n",
        "# offset = 100\n",
        "\n",
        "# plt.figure(figsize=(20, 10))\n",
        "# for i in range(D):\n",
        "#     plt.plot(Ex[:,i] + i * offset, color=colors[i])\n",
        "#     plt.plot(np.arange(50, 100), x_preds[data_id,:,:,i].T + i * offset, color=colors[i], linestyle=\":\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEaCclVd0AZv"
      },
      "source": [
        "## Look at how well the physical state can be decoded from the latent representations linearly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnfLNMgc-eRl"
      },
      "outputs": [],
      "source": [
        "temp_params = deepcopy(run_params)\n",
        "temp_params[\"mask_size\"] = 0\n",
        "\n",
        "targets = np.load(\"pendulum/pend_regression.npz\")[\"train_targets\"][:100]\n",
        "\n",
        "def encode(data):\n",
        "    out = model.elbo(jr.PRNGKey(0), data, params, **temp_params)\n",
        "    post_params = out[\"posterior_params\"]\n",
        "    post_dist = model.posterior.distribution(post_params)\n",
        "    return post_dist.mean()\n",
        "\n",
        "all_latents_train = vmap(encode)(data_dict[\"train_data\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qsufn5KZYUN1"
      },
      "outputs": [],
      "source": [
        "states = targets[:,::2]\n",
        "train_thetas = np.arctan2(states[:,:,0], states[:,:,1])\n",
        "train_omegas = train_thetas[:,1:]-train_thetas[:,:-1]\n",
        "thetas = train_thetas.flatten()\n",
        "omegas = train_omegas.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ofB0o2F_qOS"
      },
      "outputs": [],
      "source": [
        "D = 5\n",
        "xs_theta = all_latents_train.reshape((-1, D))\n",
        "xs_omega = all_latents_train[:,1:].reshape((-1, D))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es8t-Udfasa8"
      },
      "outputs": [],
      "source": [
        "W_theta, _, _, _ = np.linalg.lstsq(xs_theta, thetas)\n",
        "W_omega, _, _, _ = np.linalg.lstsq(xs_omega, omegas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPZckeRTDEbx"
      },
      "outputs": [],
      "source": [
        "test_id = 0 if \"test_id\" not in globals() else test_id + 1\n",
        "plt.plot(all_latents_train[test_id] @ W_theta, label=\"decoded\")\n",
        "plt.plot(train_thetas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum angle (train sequence #{})\".format(test_id))\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(all_latents_train[test_id] @ W_omega, label=\"decoded\")\n",
        "plt.plot(train_omegas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum velocity (train sequence #{})\".format(test_id))\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQAY3_BZA4Px"
      },
      "outputs": [],
      "source": [
        "test_targets = np.load(\"pendulum/pend_regression_longer.npz\")[\"test_targets\"][:20, ::2][:, :100]\n",
        "test_data = data_dict[\"val_data\"][:, :100]#np.load(\"pendulum/pend_regression.npz\")[\"test_obs\"][:, ::2]\n",
        "thetas = np.arctan2(test_targets[:,:,0], test_targets[:,:,1])\n",
        "omegas = thetas[:,1:]-thetas[:,:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmcd1UabBcZg"
      },
      "outputs": [],
      "source": [
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "all_latents_test = vmap(encode)(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QOzPyLqZ78y"
      },
      "outputs": [],
      "source": [
        "test_id = 0 if \"test_id\" not in globals() else test_id + 1\n",
        "plt.plot(all_latents_test[test_id] @ W_theta, label=\"decoded\")\n",
        "plt.plot(thetas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum angle (test sequence #{})\".format(test_id))\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(all_latents_test[test_id] @ W_omega, label=\"decoded\")\n",
        "plt.plot(omegas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum velocity (test sequence #{})\".format(test_id))\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxjbHKWcPSmd"
      },
      "source": [
        "## Evaluate the sliding window prediction log likelihoods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvNpnf-wPXsk"
      },
      "outputs": [],
      "source": [
        "def prediction_lls(post_params):\n",
        "    posterior = model.posterior.distribution(post_params)\n",
        "    # Get the final mean and covariance\n",
        "    mu, Sigma = posterior.mean()[-1], posterior.covariance()[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAuuK-FGRUW2"
      },
      "outputs": [],
      "source": [
        "obj, out_dict = svae_loss(key, model, data_dict[\"train_data\"][:10], params, **temp_params)\n",
        "post_params = out_dict[\"posterior_params\"]\n",
        "posterior = model.posterior.distribution(post_params)\n",
        "J = posterior.filtered_precisions\n",
        "h = posterior.filtered_linear_potentials\n",
        "Sigma_filtered = inv(J)\n",
        "mu_filtered = np.einsum(\"...ij,...j->...i\", Sigma_filtered, h)\n",
        "data_batch = data_dict[\"train_data\"]\n",
        "horizon = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ1y0BMfSNWf"
      },
      "outputs": [],
      "source": [
        "def pred_ll(data_id, key):\n",
        "    num_windows = T-horizon-1\n",
        "    pred_lls = vmap(sliding_window_prediction_lls, in_axes=(None, None, None, 0, 0))(\n",
        "        mu_filtered[data_id], Sigma_filtered[data_id], data_batch[data_id],\n",
        "        np.arange(num_windows), jr.split(key, num_windows))\n",
        "    return pred_lls.mean(axis=0)\n",
        "\n",
        "def sliding_window_prediction_lls(mu, Sigma, data, t, key):\n",
        "    # Build the posterior object on the future latent states \n",
        "    # (\"the posterior predictive distribution\")\n",
        "    # Convert unconstrained params to constrained dynamics parameters\n",
        "    prior_params_constrained = model.prior.get_dynamics_params(prior_params)\n",
        "    dynamics_params = {\n",
        "        \"m1\": mu[t],\n",
        "        \"Q1\": Sigma[t],\n",
        "        \"A\": prior_params_constrained[\"A\"],\n",
        "        \"b\": prior_params_constrained[\"b\"],\n",
        "        \"Q\": prior_params_constrained[\"Q\"]\n",
        "    }\n",
        "    tridiag_params = dynamics_to_tridiag(dynamics_params, horizon+1, D) # Note the +1\n",
        "    J, L, h = tridiag_params[\"J\"], tridiag_params[\"L\"], tridiag_params[\"h\"]\n",
        "    pred_posterior = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "    # Sample from it and evaluate the log likelihood\n",
        "    x_pred = pred_posterior.sample(seed=key)[1:]\n",
        "    likelihood_dist = model.decoder.apply(dec_params, x_pred)\n",
        "    return likelihood_dist.log_prob(\n",
        "        lax.dynamic_slice(data, (t+1, 0, 0, 0), (horizon,) + data.shape[1:])).sum(axis=(1, 2, 3))\n",
        "\n",
        "num_windows = T-horizon-1\n",
        "pred_lls = vmap(pred_ll)(np.arange(10), jr.split(key_0, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoDd5GdpUAqi"
      },
      "outputs": [],
      "source": [
        "plt.plot(pred_lls.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD3BXGWQ6ko_"
      },
      "source": [
        "# What is going on with the dynamics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtChfl-eG5qm"
      },
      "outputs": [],
      "source": [
        "theta = 2 * np.pi / 100\n",
        "lds_params = {\n",
        "    \"m1\": np.zeros(2),\n",
        "    \"Q1\": np.eye(2),\n",
        "    \"A\": np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]]),\n",
        "    \"Q\": np.eye(2) / 100,\n",
        "    \"b\": np.zeros(2)\n",
        "}\n",
        "prior = LinearGaussianChain(2, 100)\n",
        "prior_dist = prior.distribution(prior.get_constrained_params(lds_params))\n",
        "plt.plot(prior_dist.sample(seed=key_0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEvsYfef6kdW"
      },
      "outputs": [],
      "source": [
        "temp_params = deepcopy(run_params)\n",
        "temp_params[\"mask_size\"] = 0\n",
        "_, aux = svae_loss(key_0, model, data_dict[\"train_data\"][:10], params, **temp_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRmFHEyb67Un"
      },
      "outputs": [],
      "source": [
        "pp = deepcopy(params[\"prior_params\"])\n",
        "suff_stats = aux[\"sufficient_statistics\"]\n",
        "pp[\"avg_suff_stats\"] = suff_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YMXhGbU7RmK"
      },
      "outputs": [],
      "source": [
        "fit_prior_params = model.prior.m_step(pp)\n",
        "# fit_prior_params[\"A\"] = scale_singular_values(fit_prior_params[\"A\"])\n",
        "# fit_prior_params[\"A\"] = truncate_singular_values(fit_prior_params[\"A\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qU7e1QiDMvy"
      },
      "outputs": [],
      "source": [
        "key = key_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnFVPlKBBhz0"
      },
      "outputs": [],
      "source": [
        "m1 = Q = fit_prior_params[\"m1\"]\n",
        "Q1 = fit_prior_params[\"Q1\"]\n",
        "Q = fit_prior_params[\"Q\"]\n",
        "A = fit_prior_params[\"A\"]\n",
        "b = fit_prior_params[\"b\"]\n",
        "\n",
        "\n",
        "x = jr.multivariate_normal(key=key, mean=m1, cov=Q1)\n",
        "xs = []\n",
        "for i in range(200):\n",
        "    xs.append(x)\n",
        "    key, _ = jr.split(key)\n",
        "    noise = jr.multivariate_normal(key=key, mean=np.zeros_like(x), cov=Q)\n",
        "    x = A @ x + b + noise\n",
        "\n",
        "plt.plot(np.array(xs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Beq52DGTBr0U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHEwqDfA8Awl"
      },
      "outputs": [],
      "source": [
        "prior = LinearGaussianChain(model.prior.latent_dims, model.prior.seq_len)\n",
        "prior_dist = prior.distribution(prior.get_constrained_params(fit_prior_params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT-OnbBX9SC7"
      },
      "outputs": [],
      "source": [
        "Q = fit_prior_params[\"Q\"]\n",
        "A = fit_prior_params[\"A\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uACoUB3T89M_"
      },
      "outputs": [],
      "source": [
        "sample = prior_dist.sample(seed=key_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISTJrxMD9Mq6"
      },
      "outputs": [],
      "source": [
        "plt.plot(sample)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "CpwzMT9YQSMT",
        "aLOSSwKQ9vl3",
        "L6fbLo70TQWf",
        "uxjbHKWcPSmd",
        "rD3BXGWQ6ko_"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6ae9d0df46794a5aab0ba296ddc4a8f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_968d2537ea464b8fb662bee994d92f4c",
              "IPY_MODEL_9e1f8e2781b04526bfc80057ff2665c9"
            ],
            "layout": "IPY_MODEL_3c4b305068034c0aaee3e25ee028f41a"
          }
        },
        "968d2537ea464b8fb662bee994d92f4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66cb3da4d3ea4a988948b8b8e41f35c4",
            "placeholder": "​",
            "style": "IPY_MODEL_2c85a01ef2eb4cd1816c958dbd2b7a70",
            "value": "1.914 MB of 1.914 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "9e1f8e2781b04526bfc80057ff2665c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c45927808e74d5c8f5ca8f87805ec9c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d9497f48d8a4283aba50b8a39eaf248",
            "value": 1
          }
        },
        "3c4b305068034c0aaee3e25ee028f41a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66cb3da4d3ea4a988948b8b8e41f35c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c85a01ef2eb4cd1816c958dbd2b7a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c45927808e74d5c8f5ca8f87805ec9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d9497f48d8a4283aba50b8a39eaf248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}