{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthew9671/MattLab/blob/main/Revisiting_SVAE_Refactor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFyEe0Xi_84F"
      },
      "source": [
        "- [x] Implement and test Kalman filtering and smoothing with parallel scan\n",
        "- [ ] Make parallel scan KF work with non-zero biases\n",
        "- [x] Write analysis code for pendulum\n",
        "  - [x] Evaluate predictive accuracy\n",
        "  - [x] Do linear regression from latents to angle and velocity "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6u_EWGy0phmX"
      },
      "outputs": [],
      "source": [
        "# This reloads files not modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BJs02uuMM79",
        "outputId": "80a43911-2312-4425-be55-3bbca896b4f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# @title Download stuff \n",
        "import os\n",
        "# Download and install the relevant libraries\n",
        "!pip install -q ml-collections git+https://github.com/google/flax\n",
        "!pip install -q wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UbqX5CKc11Uc"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import dynamax\n",
        "except ModuleNotFoundError:\n",
        "    print('installing dynamax')\n",
        "    !pip install git+https://github.com/probml/dynamax.git#egg=dynamax\n",
        "    import dynamax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njW9wRLEPG7T"
      },
      "source": [
        "# Set everything up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "cellView": "form",
        "id": "b1ikkl1ULTEB"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "# Misc\n",
        "import os\n",
        "from importlib import reload\n",
        "import numpy as onp\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "from tqdm import trange\n",
        "import copy, traceback\n",
        "from pprint import pprint\n",
        "from copy import deepcopy\n",
        "import pickle as pkl\n",
        "\n",
        "# for logging\n",
        "import wandb\n",
        "# Debug\n",
        "import pdb\n",
        "# Jax\n",
        "import jax\n",
        "from jax import vmap, lax, jit, value_and_grad\n",
        "import jax.numpy as np\n",
        "import jax.scipy as scipy\n",
        "import jax.random as jr\n",
        "key_0 = jr.PRNGKey(0) # Convenience\n",
        "from jax.lax import scan, stop_gradient\n",
        "from jax.tree_util import tree_map\n",
        "# optax\n",
        "import optax as opt\n",
        "# Flax\n",
        "import flax.linen as nn\n",
        "from flax.linen import Conv, ConvTranspose\n",
        "from flax.core import frozen_dict as fd\n",
        "\n",
        "# Tensorflow probability\n",
        "import tensorflow_probability.substrates.jax as tfp\n",
        "import tensorflow_probability.substrates.jax.distributions as tfd\n",
        "from tensorflow_probability.python.internal import reparameterization\n",
        "MVN = tfd.MultivariateNormalFullCovariance\n",
        "\n",
        "# Dynamax (central to our implementation)\n",
        "from dynamax.linear_gaussian_ssm.inference import make_lgssm_params, lgssm_smoother\n",
        "# from dynamax.linear_gaussian_ssm.parallel_inference import lgssm_smoother as parallel_lgssm_smoother\n",
        "from dynamax.utils.utils import psd_solve\n",
        "\n",
        "# Common math functions\n",
        "from flax.linen import softplus, sigmoid\n",
        "from jax.scipy.special import logsumexp\n",
        "from jax.scipy.linalg import solve_triangular\n",
        "from jax.numpy.linalg import eigh, cholesky, svd, inv, solve\n",
        "\n",
        "# For typing in neural network utils\n",
        "from typing import (NamedTuple, Any, Callable, Sequence, Iterable, List, Optional, Tuple,\n",
        "                    Set, Type, Union, TypeVar, Generic, Dict)\n",
        "\n",
        "# For making the pendulum dataset\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "\n",
        "# For making nice visualizations\n",
        "from sklearn.decomposition import PCA\n",
        "from IPython.display import clear_output, HTML\n",
        "from matplotlib import animation, rc\n",
        "import seaborn as sns\n",
        "color_names = [\"windows blue\",\n",
        "                \"red\",\n",
        "                \"amber\",\n",
        "                \"faded green\",\n",
        "                \"dusty purple\",\n",
        "                \"orange\",\n",
        "                \"clay\",\n",
        "                \"pink\",\n",
        "                \"greyish\",\n",
        "                \"mint\",\n",
        "                \"light cyan\",\n",
        "                \"steel blue\",\n",
        "                \"forest green\",\n",
        "                \"pastel purple\",\n",
        "                \"salmon\",\n",
        "                \"dark brown\",\n",
        "               \"violet\",\n",
        "               \"mauve\",\n",
        "               \"ocean\",\n",
        "               \"ugly yellow\"]\n",
        "colors = sns.xkcd_palette(color_names)\n",
        "\n",
        "# Get rid of the check types warning\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "class CheckTypesFilter(logging.Filter):\n",
        "    def filter(self, record):\n",
        "        return \"check_types\" not in record.getMessage()\n",
        "logger.addFilter(CheckTypesFilter())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "cellView": "form",
        "id": "2MjuUxHxR5O0"
      },
      "outputs": [],
      "source": [
        "# @title Misc helpers\n",
        "def get_value(x):\n",
        "    try:\n",
        "        return x.val.val.primal\n",
        "    except:\n",
        "        try:\n",
        "            return x.val.val\n",
        "        except:\n",
        "            try:\n",
        "                return x.val\n",
        "            except:\n",
        "                return x  # Oh well.\n",
        "\n",
        "def plot_img_grid(recon):\n",
        "    plt.figure(figsize=(8,8))\n",
        "    # Show the sequence as a block of images\n",
        "    stacked = recon.reshape(10, 24 * 10, 24)\n",
        "    imgrid = stacked.swapaxes(0, 1).reshape(24 * 10, 24 * 10)\n",
        "    plt.imshow(imgrid, vmin=0, vmax=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "cellView": "form",
        "id": "FGleKPUALeEd"
      },
      "outputs": [],
      "source": [
        "# @title Math helpers\n",
        "def softplus(x):\n",
        "    return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0)\n",
        "\n",
        "def inv_softplus(x, eps=1e-4):\n",
        "    return np.log(np.exp(x - eps) - 1)\n",
        "\n",
        "def vectorize_pytree(*args):\n",
        "    \"\"\"\n",
        "    Flatten an arbitrary PyTree into a vector.\n",
        "    :param args:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    flat_tree, _ = jax.tree_util.tree_flatten(args)\n",
        "    flat_vs = [x.flatten() for x in flat_tree]\n",
        "    return np.concatenate(flat_vs, axis=0)\n",
        "\n",
        "# converts an (n(n+1)/2,) vector of Lie parameters\n",
        "# to an (n, n) matrix\n",
        "def lie_params_to_constrained(out_flat, dim, eps=1e-4):\n",
        "    D, A = out_flat[:dim], out_flat[dim:]\n",
        "    # ATTENTION: we changed this!\n",
        "    # D = np.maximum(softplus(D), eps)\n",
        "    D = softplus(D) + eps\n",
        "    # Build a skew-symmetric matrix\n",
        "    S = np.zeros((dim, dim))\n",
        "    i1, i2 = np.tril_indices(dim - 1)\n",
        "    S = S.at[i1+1, i2].set(A)\n",
        "    S = S.T\n",
        "    S = S.at[i1+1, i2].set(-A)\n",
        "\n",
        "    O = scipy.linalg.expm(S)\n",
        "    J = O.T @ np.diag(D) @ O\n",
        "    return J\n",
        "\n",
        "# converts an (n, n) matrix \n",
        "# to an (n, n) matrix with singular values in (0, 1)\n",
        "def get_constrained_dynamics(A):\n",
        "    dim = A.shape[0]\n",
        "    diag = np.diag(A)\n",
        "    diag = sigmoid(diag)\n",
        "    # Build a skew-symmetric matrix\n",
        "    S = np.zeros((dim, dim))\n",
        "    i1, i2 = np.tril_indices(dim - 1)\n",
        "    S = S.at[i1+1, i2].set(A[i1+1, i2])\n",
        "    S = S.T\n",
        "    S = S.at[i1+1, i2].set(-A[i1+1, i2])\n",
        "    U = scipy.linalg.expm(S)\n",
        "\n",
        "    S = np.zeros((dim, dim))\n",
        "    i1, i2 = np.tril_indices(dim - 1)\n",
        "    S = S.at[i1+1, i2].set(A.T[i1+1, i2])\n",
        "    S = S.T\n",
        "    S = S.at[i1+1, i2].set(-A.T[i1+1, i2])\n",
        "    V = scipy.linalg.expm(S)\n",
        "\n",
        "    A = U @ np.diag(diag) @ V\n",
        "    return A, U, V\n",
        "\n",
        "def scale_singular_values(A):\n",
        "    _, s, _ = svd(A)\n",
        "    return A / (np.maximum(1, np.max(s)))\n",
        "\n",
        "def truncate_singular_values(A):\n",
        "    eps = 1e-3\n",
        "    u, s, vt = svd(A)\n",
        "    return u @ np.diag(np.clip(s, eps, 1)) @ vt\n",
        "\n",
        "# Assume that h has a batch shape here\n",
        "def sample_info_gaussian(seed, J, h):\n",
        "    # Avoid inversion.\n",
        "    # see https://github.com/mattjj/pybasicbayes/blob/master/pybasicbayes/util/stats.py#L117-L122\n",
        "    L = np.linalg.cholesky(J)\n",
        "    x = jr.normal(key=seed, shape=h.shape)\n",
        "    return solve_triangular(L,x.T,lower=True,trans='T').T \\\n",
        "        + np.linalg.solve(J,h.T).T\n",
        "\n",
        "def sample_info_gaussian_old(seed, J, h):\n",
        "    cov = np.linalg.inv(J)\n",
        "    loc = np.einsum(\"...ij,...j->...i\", cov, h)\n",
        "    return tfp.distributions.MultivariateNormalFullCovariance(\n",
        "        loc=loc, covariance_matrix=cov).sample(sample_shape=(), seed=seed)\n",
        "\n",
        "def random_rotation(seed, n, theta=None):\n",
        "    key1, key2 = jr.split(seed)\n",
        "\n",
        "    if theta is None:\n",
        "        # Sample a random, slow rotation\n",
        "        theta = 0.5 * np.pi * jr.uniform(key1)\n",
        "\n",
        "    if n == 1:\n",
        "        return jr.uniform(key1) * np.eye(1)\n",
        "\n",
        "    rot = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
        "    out = np.eye(n)\n",
        "    out = out.at[:2, :2].set(rot)\n",
        "    q = np.linalg.qr(jr.uniform(key2, shape=(n, n)))[0]\n",
        "    return q.dot(out).dot(q.T)\n",
        "\n",
        "# Computes ATQ-1A in a way that's guaranteed to be symmetric\n",
        "def inv_quad_form(Q, A):\n",
        "    sqrt_Q = np.linalg.cholesky(Q)\n",
        "    trm = solve_triangular(sqrt_Q, A, lower=True, check_finite=False)\n",
        "    return trm.T @ trm\n",
        "\n",
        "def inv_symmetric(Q):\n",
        "    sqrt_Q = np.linalg.cholesky(Q)\n",
        "    sqrt_Q_inv = np.linalg.inv(sqrt_Q)\n",
        "    return sqrt_Q_inv.T @ sqrt_Q_inv\n",
        "\n",
        "# Converts from (A, b, Q) to (J, L, h)\n",
        "def dynamics_to_tridiag(dynamics_params, T, D):\n",
        "    Q1, m1, A, Q, b = dynamics_params[\"Q1\"], \\\n",
        "        dynamics_params[\"m1\"], dynamics_params[\"A\"], \\\n",
        "        dynamics_params[\"Q\"], dynamics_params[\"b\"]\n",
        "    # diagonal blocks of precision matrix\n",
        "    J = np.zeros((T, D, D))\n",
        "    J = J.at[0].add(inv_symmetric(Q1))\n",
        "\n",
        "    J = J.at[:-1].add(inv_quad_form(Q, A))\n",
        "    J = J.at[1:].add(inv_symmetric(Q))\n",
        "    # lower diagonal blocks of precision matrix\n",
        "    L = -np.linalg.solve(Q, A)\n",
        "    L = np.tile(L[None, :, :], (T - 1, 1, 1))\n",
        "    # linear potential\n",
        "    h = np.zeros((T, D)) \n",
        "    h = h.at[0].add(np.linalg.solve(Q1, m1))\n",
        "    h = h.at[:-1].add(-np.dot(A.T, np.linalg.solve(Q, b)))\n",
        "    h = h.at[1:].add(np.linalg.solve(Q, b))\n",
        "    return { \"J\": J, \"L\": L, \"h\": h }\n",
        "\n",
        "# Helper function: solve a linear regression given expected sufficient statistics\n",
        "def fit_linear_regression(Ex, Ey, ExxT, EyxT, EyyT, En):\n",
        "    big_ExxT = np.row_stack([np.column_stack([ExxT, Ex]),\n",
        "                            np.concatenate( [Ex.T, np.array([En])])])\n",
        "    big_EyxT = np.column_stack([EyxT, Ey])\n",
        "    Cd = np.linalg.solve(big_ExxT, big_EyxT.T).T\n",
        "    C, d = Cd[:, :-1], Cd[:, -1]\n",
        "    R = (EyyT - 2 * Cd @ big_EyxT.T + Cd @ big_ExxT @ Cd.T) / En\n",
        "\n",
        "    # Manually symmetrize R\n",
        "    R = (R + R.T) / 2\n",
        "    return C, d, R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "cellView": "form",
        "id": "NeeYgySA0hPT"
      },
      "outputs": [],
      "source": [
        "# @title Experiment scheduler\n",
        "LINE_SEP = \"#\" * 42\n",
        "\n",
        "def dict_len(d):\n",
        "    if (type(d) == list):\n",
        "        return len(d)\n",
        "    else:\n",
        "        return dict_len(d[list(d.keys())[0]])\n",
        "\n",
        "def dict_map(d, func):\n",
        "    if type(d) == list:\n",
        "        return func(d)\n",
        "    elif type(d) == dict:\n",
        "        r = copy.deepcopy(d)\n",
        "        for key in d.keys():\n",
        "            r[key] = dict_map(r[key], func)\n",
        "            # Ignore all the Nones\n",
        "            if r[key] is None:\n",
        "                r.pop(key)\n",
        "        if len(r.keys()) == 0:\n",
        "            # There's no content\n",
        "            return None\n",
        "        else:\n",
        "            return r\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def dict_product(d1, d2):\n",
        "    l1, l2 = dict_len(d1), dict_len(d2)\n",
        "    def expand_list(d):\n",
        "        result = []\n",
        "        for item in d:\n",
        "            result.append(item)\n",
        "            result.extend([None] * (l2-1))\n",
        "        return result\n",
        "    def multiply_list(d):\n",
        "        return d * l1\n",
        "    result = dict_map(d1, expand_list)\n",
        "    additions = dict_map(d2, multiply_list)\n",
        "    return dict_update(result, additions)\n",
        "\n",
        "def dict_get(d, id):\n",
        "    return dict_map(d, lambda l: l[id])\n",
        "\n",
        "def dict_update(d, u):\n",
        "    if d is None:\n",
        "        d = dict()\n",
        "    for key in u.keys():\n",
        "        if type(u[key]) == dict:\n",
        "            d.update({\n",
        "                key: dict_update(d.get(key), u[key])\n",
        "            })\n",
        "        else:\n",
        "            d.update({key: u[key]})\n",
        "    return d\n",
        "\n",
        "# A standardized function that structures and schedules experiments\n",
        "# Can chain multiple variations of experiment parameters together\n",
        "def experiment_scheduler(run_params, dataset_getter, model_getter, train_func, \n",
        "                         logger_func=None, err_logger_func=None, \n",
        "                         run_variations=None, params_expander=None,\n",
        "                         on_error=None, continue_on_error=True, use_wandb=True):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        run_params: dict{\"dataset_params\"} \n",
        "            A large dictionary containing all relevant parameters to the run \n",
        "        dataset_getter: run_params -> dict{\"train_data\", [\"generative_model\"]}\n",
        "            A function that loads/samples a dataset\n",
        "        model_getter: run_params, data_dict -> model\n",
        "            A function that creates a model given parameters. Note that the model\n",
        "            could depend on the specifics of the dataset/generative model as well\n",
        "        train_func: model, data, run_params -> results\n",
        "            A function that contains the training loop. \n",
        "            TODO: later we might wanna open up this pipeline and customize further!\n",
        "        (optional) logger_func: results, run_params -> ()\n",
        "            A function that logs the current run.\n",
        "        (optional) err_logger_func: message, run_params -> ()\n",
        "            A function that is called when the run fails.\n",
        "        (optional) run_variations: dict{}\n",
        "            A nested dictionary where the leaves are lists of different parameters.\n",
        "            None means no change from parameters of the last run.\n",
        "        (optional) params_expander: dict{} -> dict{}\n",
        "            Turns high level parameters into specific low level parameters.\n",
        "    returns:\n",
        "        all_results: List<result>\n",
        "            A list containing results from all runs. Failed runs are indicated\n",
        "            with a None value.\n",
        "    \"\"\"\n",
        "    params_expander = params_expander or (lambda d: d)\n",
        "\n",
        "    num_runs = dict_len(run_variations) if run_variations else 1\n",
        "    params = copy.deepcopy(run_params)\n",
        "    print(\"Total number of runs: {}\".format(num_runs))\n",
        "    print(\"Base paramerters:\")\n",
        "    pprint(params)\n",
        "\n",
        "    global data_dict\n",
        "    all_results = []\n",
        "    all_models = []\n",
        "\n",
        "    def _single_run(data_out, model_out):\n",
        "        print(\"Loading dataset!\")\n",
        "        data_dict = dataset_getter(curr_params)\n",
        "        data_out.append(data_dict)\n",
        "        # Make a new model\n",
        "        model_dict = model_getter(curr_params, data_dict)\n",
        "        model_out.append(model_dict)\n",
        "        all_models.append(model_dict)\n",
        "        results = train_func(model_dict, data_dict, curr_params)\n",
        "        all_results.append(results)\n",
        "        if logger_func:\n",
        "            logger_func(results, curr_params, data_dict)\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        print(LINE_SEP)\n",
        "        print(\"Starting run #{}\".format(run))\n",
        "        print(LINE_SEP)\n",
        "        curr_variation = dict_get(run_variations, run)\n",
        "        if curr_variation is None:\n",
        "            if (run != 0):\n",
        "                print(\"Variation #{} is a duplicate, skipping run.\".format(run))\n",
        "                continue\n",
        "            curr_params = params_expander(params)\n",
        "        else:\n",
        "            print(\"Current parameter variation:\")\n",
        "            pprint(curr_variation)\n",
        "            curr_params = dict_update(params, curr_variation)\n",
        "            curr_params = params_expander(curr_params)\n",
        "            print(\"Current full parameters:\")\n",
        "            pprint(curr_params)\n",
        "            if curr_variation.get(\"dataset_params\"):\n",
        "                reload_data = True\n",
        "        # Hack to get the values even when they err out\n",
        "        data_out = []\n",
        "        model_out = []\n",
        "        if not continue_on_error:\n",
        "            _single_run(data_out, model_out)\n",
        "        else:\n",
        "            try:\n",
        "                _single_run(data_out, model_out)\n",
        "                if use_wandb: wandb.finish()\n",
        "            except:\n",
        "                all_results.append(None)\n",
        "                if (on_error): \n",
        "                    try:\n",
        "                        on_error(data_out[0], model_out[0])\n",
        "                    except:\n",
        "                        pass # Oh well...\n",
        "                print(\"Run errored out due to some the following reason:\")\n",
        "                traceback.print_exc()\n",
        "                if use_wandb: wandb.finish(exit_code=1)\n",
        "    return all_results, all_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OPHhcbZObuu"
      },
      "source": [
        "## Define the base SVAE object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "9Pj8Xn9jNBz1"
      },
      "outputs": [],
      "source": [
        "class SVAE:\n",
        "    def __init__(self,\n",
        "                 recognition=None, decoder=None, prior=None, posterior=None,\n",
        "                 input_dummy=None, latent_dummy=None):\n",
        "        \"\"\"\n",
        "        rec_net, dec_net, prior are all objects that take in parameters\n",
        "        rec_net.apply(params, data) returns Gaussian potentials (parameters)\n",
        "        dec_net.apply(params, latents) returns probability distributions\n",
        "        prior : SVAEPrior\n",
        "        \"\"\"\n",
        "        self.recognition = recognition\n",
        "        self.decoder = decoder\n",
        "        self.prior = prior\n",
        "        self.posterior = posterior\n",
        "        self.input_dummy = input_dummy\n",
        "        self.latent_dummy = latent_dummy\n",
        "\n",
        "    def init(self, key=None):\n",
        "        if key is None:\n",
        "            key = jr.PRNGKey(0)\n",
        "\n",
        "        rec_key, dec_key, prior_key, post_key = jr.split(key, 4)\n",
        "\n",
        "        return {\n",
        "            \"rec_params\": self.recognition.init(rec_key, self.input_dummy),\n",
        "            \"dec_params\": self.decoder.init(dec_key, self.latent_dummy),\n",
        "            \"prior_params\": self.prior.init(prior_key),\n",
        "            \"post_params\": self.posterior.init(post_key)\n",
        "        }\n",
        "\n",
        "    def kl_posterior_prior(self, posterior_params, prior_params, \n",
        "                           samples=None):\n",
        "        posterior = self.posterior.distribution(posterior_params)\n",
        "        prior = self.prior.distribution(prior_params)\n",
        "        if samples is None:\n",
        "            return posterior.kl_divergence(prior)\n",
        "        else:\n",
        "            return np.mean(posterior.log_prob(samples) - prior.log_prob(samples))\n",
        "\n",
        "    def elbo(self, key, data, model_params, sample_kl=False, **params):\n",
        "        rec_params = model_params[\"rec_params\"]\n",
        "        dec_params = model_params[\"dec_params\"]\n",
        "        prior_params = self.prior.get_constrained_params(model_params[\"prior_params\"])\n",
        "\n",
        "        # Mask out a large window of states\n",
        "        mask_size = params.get(\"mask_size\")\n",
        "        T = data.shape[0]\n",
        "        D = self.prior.latent_dims\n",
        "\n",
        "        mask = onp.ones((T,))\n",
        "        key, dropout_key = jr.split(key)\n",
        "        if mask_size:\n",
        "            # Potential dropout...!\n",
        "            # Use a trick to generate the mask without indexing with a tracer\n",
        "            start_id = jr.choice(dropout_key, T - mask_size + 1)\n",
        "            mask = np.array(np.arange(T) >= start_id) \\\n",
        "                 * np.array(np.arange(T) < start_id + mask_size)\n",
        "            mask = 1 - mask\n",
        "            if params.get(\"mask_type\") == \"potential\":\n",
        "                # This only works with svaes\n",
        "                potential = self.recognition.apply(rec_params, data)\n",
        "                # Uninformative potential\n",
        "                infinity = 1e5\n",
        "                uninf_potential = {\"mu\": np.zeros((T, D)), \n",
        "                                   \"Sigma\": np.tile(np.eye(D) * infinity, (T, 1, 1))}\n",
        "                # Replace masked parts with uninformative potentials\n",
        "                potential = tree_map(\n",
        "                    lambda t1, t2: np.einsum(\"i,i...->i...\", mask[:t1.shape[0]], t1) \n",
        "                                 + np.einsum(\"i,i...->i...\", 1-mask[:t2.shape[0]], t2), \n",
        "                    potential, \n",
        "                    uninf_potential)\n",
        "            else:\n",
        "                potential = self.recognition.apply(rec_params, \n",
        "                                                   np.einsum(\"t...,t->t...\", data, mask))\n",
        "        else:\n",
        "            # Don't do any masking\n",
        "            potential = self.recognition.apply(rec_params, data)\n",
        "\n",
        "        # Update: it makes more sense that inference is done in the posterior object\n",
        "        posterior_params = self.posterior.infer(prior_params, potential)\n",
        "        \n",
        "        # Take samples under the posterior\n",
        "        num_samples = params.get(\"obj_samples\") or 1\n",
        "        samples = self.posterior.sample(posterior_params, (num_samples,), key)\n",
        "        # and compute average ll\n",
        "\n",
        "        def likelihood_outputs(latent):\n",
        "            likelihood_dist = self.decoder.apply(dec_params, latent)\n",
        "            return likelihood_dist.mean(), likelihood_dist.log_prob(data)\n",
        "\n",
        "        mean, ells = vmap(likelihood_outputs)(samples)\n",
        "        # Take average over samples then sum the rest\n",
        "        ell = np.sum(np.mean(ells, axis=0))\n",
        "        # Compute kl from posterior to prior\n",
        "        if sample_kl:\n",
        "            kl = self.kl_posterior_prior(posterior_params, prior_params, \n",
        "                                         samples=samples)\n",
        "        else:\n",
        "            kl = self.kl_posterior_prior(posterior_params, prior_params)\n",
        "\n",
        "        elbo = ell - kl\n",
        "\n",
        "        return {\n",
        "            \"elbo\": elbo,\n",
        "            \"ell\": ell,\n",
        "            \"kl\": kl,\n",
        "            \"posterior_params\": posterior_params,\n",
        "            \"posterior_samples\": samples,\n",
        "            \"reconstruction\": mean,\n",
        "            \"mask\": mask\n",
        "        }\n",
        "\n",
        "    def compute_objective(self, key, data, model_params, **params):\n",
        "        results = self.elbo(key, data, model_params, **params)\n",
        "        results[\"objective\"] = results[\"elbo\"]\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "cellView": "form",
        "id": "7ckaLRUL1QVb"
      },
      "outputs": [],
      "source": [
        "# @title The DeepLDS object (implements custom kl function)\n",
        "class DeepLDS(SVAE):\n",
        "    def kl_posterior_prior(self, posterior_params, prior_params, \n",
        "                           samples=None):\n",
        "        posterior = self.posterior.distribution(posterior_params)\n",
        "        prior = self.prior.distribution(prior_params)\n",
        "        if samples is None:\n",
        "            Ex = posterior.expected_states\n",
        "            ExxT = posterior.expected_states_squared\n",
        "            ExnxT = posterior.expected_states_next_states\n",
        "            Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "            Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "            J, L = prior_params[\"J\"], prior_params[\"L\"]\n",
        "\n",
        "            cross_entropy = -prior.log_prob(Ex)\n",
        "            cross_entropy += 0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "            cross_entropy += np.einsum(\"tij,tij->\", L, Sigmatnt)\n",
        "            return cross_entropy - posterior.entropy()\n",
        "            \n",
        "        else:\n",
        "            return np.mean(posterior.log_prob(samples) - prior.log_prob(samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "Nre5PK2MPt2N"
      },
      "outputs": [],
      "source": [
        "# @title SVAE Prior object\n",
        "class SVAEPrior:\n",
        "    def init(self, key):\n",
        "        \"\"\"\n",
        "        Returns the initial prior parameters.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def distribution(self, prior_params):\n",
        "        \"\"\"\n",
        "        Returns a tfp distribution object\n",
        "        Takes constrained params\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def m_step(self, prior_params, posterior, post_params):\n",
        "        \"\"\"\n",
        "        Returns updated prior parameters.\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def sample(self, params, shape, key):\n",
        "        return self.distribution(\n",
        "            self.get_constrained_params(params)).sample(sample_shape=shape, seed=key)\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        return deepcopy(params)\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOPpeMzjj42H"
      },
      "source": [
        "## Information form (deprecated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "cellView": "form",
        "id": "NHkdKpcxOi1S"
      },
      "outputs": [],
      "source": [
        "# @title MVN tridiag object (taken from ssm)\n",
        "from tensorflow_probability.python.internal import reparameterization\n",
        "\n",
        "def block_tridiag_mvn_log_normalizer(J_diag, J_lower_diag, h):\n",
        "    \"\"\" TODO\n",
        "    \"\"\"\n",
        "    # extract dimensions\n",
        "    num_timesteps, dim = J_diag.shape[:2]\n",
        "\n",
        "    # Pad the L's with one extra set of zeros for the last predict step\n",
        "    J_lower_diag_pad = np.concatenate((J_lower_diag, np.zeros((1, dim, dim))), axis=0)\n",
        "\n",
        "    def marginalize(carry, t):\n",
        "        Jp, hp, lp = carry\n",
        "\n",
        "        # Condition\n",
        "        Jc = J_diag[t] + Jp\n",
        "        hc = h[t] + hp\n",
        "\n",
        "        # Predict -- Cholesky approach seems unstable!\n",
        "        # sqrt_Jc = np.linalg.cholesky(Jc)\n",
        "        # trm1 = solve_triangular(sqrt_Jc, hc, lower=True)\n",
        "        # trm2 = solve_triangular(sqrt_Jc, J_lower_diag_pad[t].T, lower=True)\n",
        "        # log_Z = 0.5 * dim * np.log(2 * np.pi)\n",
        "        # log_Z += -np.sum(np.log(np.diag(sqrt_Jc)))  # sum these terms only to get approx log|J|\n",
        "        # log_Z += 0.5 * np.dot(trm1.T, trm1)\n",
        "        # Jp = -np.dot(trm2.T, trm2)\n",
        "        # hp = -np.dot(trm2.T, trm1)\n",
        "\n",
        "        # Alternative predict step:\n",
        "        log_Z = 0.5 * dim * np.log(2 * np.pi)\n",
        "        log_Z += -0.5 * np.linalg.slogdet(Jc)[1]\n",
        "        log_Z += 0.5 * np.dot(hc, np.linalg.solve(Jc, hc))\n",
        "        Jp = -np.dot(J_lower_diag_pad[t], np.linalg.solve(Jc, J_lower_diag_pad[t].T))\n",
        "        # Jp = (Jp + Jp.T) * .5   # Manual symmetrization\n",
        "        hp = -np.dot(J_lower_diag_pad[t], np.linalg.solve(Jc, hc))\n",
        "\n",
        "        new_carry = Jp, hp, lp + log_Z\n",
        "        return new_carry, (Jc, hc)\n",
        "\n",
        "    # Initialize\n",
        "    Jp0 = np.zeros((dim, dim))\n",
        "    hp0 = np.zeros((dim,))\n",
        "    (_, _, log_Z), (filtered_Js, filtered_hs) = lax.scan(marginalize, (Jp0, hp0, 0), np.arange(num_timesteps))\n",
        "    return log_Z, (filtered_Js, filtered_hs)\n",
        "\n",
        "class MultivariateNormalBlockTridiag(tfd.Distribution):\n",
        "    \"\"\"\n",
        "    The Gaussian linear dynamical system's posterior distribution over latent states\n",
        "    is a multivariate normal distribution whose _precision_ matrix is\n",
        "    block tridiagonal.\n",
        "\n",
        "        x | y ~ N(\\mu, \\Sigma)\n",
        "\n",
        "    where\n",
        "\n",
        "        \\Sigma^{-1} = J = [[J_{0,0},   J_{0,1},   0,       0,      0],\n",
        "                           [J_{1,0},   J_{1,1},   J_{1,2}, 0,      0],\n",
        "                           [0,         J_{2,1},   J_{2,2}, \\ddots, 0],\n",
        "                           [0,         0,         \\ddots,  \\ddots,  ],\n",
        "\n",
        "    is block tridiagonal, and J_{t, t+1} = J_{t+1, t}^T.\n",
        "\n",
        "    The pdf is\n",
        "\n",
        "        p(x) = exp \\{-1/2 x^T J x + x^T h - \\log Z(J, h) \\}\n",
        "             = exp \\{- 1/2 \\sum_{t=1}^T x_t^T J_{t,t} x_t\n",
        "                     - \\sum_{t=1}^{T-1} x_{t+1}^T J_{t+1,t} x_t\n",
        "                     + \\sum_{t=1}^T x_t^T h_t\n",
        "                     -\\log Z(J, h)\\}\n",
        "\n",
        "    where J = \\Sigma^{-1} and h = \\Sigma^{-1} \\mu = J \\mu.\n",
        "\n",
        "    Using exponential family tricks we know that\n",
        "\n",
        "        E[x_t] = \\grad_{h_t} \\log Z(J, h)\n",
        "        E[x_t x_t^T] = -2 \\grad_{J_{t,t}} \\log Z(J, h)\n",
        "        E[x_{t+1} x_t^T] = -\\grad_{J_{t+1,t}} \\log Z(J, h)\n",
        "\n",
        "    These are the expectations we need for EM.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 precision_diag_blocks,\n",
        "                 precision_lower_diag_blocks,\n",
        "                 linear_potential,\n",
        "                 log_normalizer,\n",
        "                 filtered_precisions,\n",
        "                 filtered_linear_potentials,\n",
        "                 expected_states,\n",
        "                 expected_states_squared,\n",
        "                 expected_states_next_states,\n",
        "                 validate_args=False,\n",
        "                 allow_nan_stats=True,\n",
        "                 name=\"MultivariateNormalBlockTridiag\",\n",
        "             ) -> None:\n",
        "\n",
        "        self._precision_diag_blocks = precision_diag_blocks\n",
        "        self._precision_lower_diag_blocks = precision_lower_diag_blocks\n",
        "        self._linear_potential = linear_potential\n",
        "        self._log_normalizer = log_normalizer\n",
        "        self._filtered_precisions = filtered_precisions\n",
        "        self._filtered_linear_potentials = filtered_linear_potentials\n",
        "        self._expected_states = expected_states\n",
        "        self._expected_states_squared = expected_states_squared\n",
        "        self._expected_states_next_states = expected_states_next_states\n",
        "\n",
        "        # We would detect the dtype dynamically but that would break vmap\n",
        "        # see https://github.com/tensorflow/probability/issues/1271\n",
        "        dtype = np.float32\n",
        "        super(MultivariateNormalBlockTridiag, self).__init__(\n",
        "            dtype=dtype,\n",
        "            validate_args=validate_args,\n",
        "            allow_nan_stats=allow_nan_stats,\n",
        "            reparameterization_type=reparameterization.NOT_REPARAMETERIZED,\n",
        "            parameters=dict(precision_diag_blocks=self._precision_diag_blocks,\n",
        "                            precision_lower_diag_blocks=self._precision_lower_diag_blocks,\n",
        "                            linear_potential=self._linear_potential,\n",
        "                            log_normalizer=self._log_normalizer,\n",
        "                            filtered_precisions=self._filtered_precisions,\n",
        "                            filtered_linear_potentials=self._filtered_linear_potentials,\n",
        "                            expected_states=self._expected_states,\n",
        "                            expected_states_squared=self._expected_states_squared,\n",
        "                            expected_states_next_states=self._expected_states_next_states),\n",
        "            name=name,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _parameter_properties(cls, dtype, num_classes=None):\n",
        "        # pylint: disable=g-long-lambda\n",
        "        return dict(\n",
        "            precision_diag_blocks=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            precision_lower_diag_blocks=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            linear_potential=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "            log_normalizer=tfp.internal.parameter_properties.ParameterProperties(event_ndims=0),\n",
        "            filtered_precisions=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            filtered_linear_potentials=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "            expected_states=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "            expected_states_squared=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            expected_states_next_states=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def infer(cls,\n",
        "              precision_diag_blocks,\n",
        "              precision_lower_diag_blocks,\n",
        "              linear_potential):\n",
        "        assert precision_diag_blocks.ndim == 3\n",
        "        num_timesteps, dim = precision_diag_blocks.shape[:2]\n",
        "        assert precision_diag_blocks.shape[2] == dim\n",
        "        assert precision_lower_diag_blocks.shape == (num_timesteps - 1, dim, dim)\n",
        "        assert linear_potential.shape == (num_timesteps, dim)\n",
        "\n",
        "        # Run message passing code to get the log normalizer, the filtering potentials,\n",
        "        # and the expected values of x. Technically, the natural parameters are -1/2 J\n",
        "        # so we need to do a little correction of the gradients to get the expectations.\n",
        "        f = value_and_grad(block_tridiag_mvn_log_normalizer, argnums=(0, 1, 2), has_aux=True)\n",
        "        (log_normalizer, (filtered_precisions, filtered_linear_potentials)), grads = \\\n",
        "            f(precision_diag_blocks, precision_lower_diag_blocks, linear_potential)\n",
        "\n",
        "        # Manually symmetrize ExxT due to numerical issues...!!!\n",
        "        # Correct for the -1/2 J -> J implementation\n",
        "        expected_states_squared = - grads[0] - np.swapaxes(grads[0], -2, -1)\n",
        "        expected_states_next_states = -grads[1]\n",
        "        expected_states = grads[2]\n",
        "\n",
        "        return cls(precision_diag_blocks,\n",
        "                   precision_lower_diag_blocks,\n",
        "                   linear_potential,\n",
        "                   log_normalizer,\n",
        "                   filtered_precisions,\n",
        "                   filtered_linear_potentials,\n",
        "                   expected_states,\n",
        "                   expected_states_squared,\n",
        "                   expected_states_next_states)\n",
        "\n",
        "    @classmethod\n",
        "    def infer_from_precision_and_mean(cls,\n",
        "                                      precision_diag_blocks,\n",
        "                                      precision_lower_diag_blocks,\n",
        "                                      mean):\n",
        "        assert precision_diag_blocks.ndim == 3\n",
        "        num_timesteps, dim = precision_diag_blocks.shape[:2]\n",
        "        assert precision_diag_blocks.shape[2] == dim\n",
        "        assert precision_lower_diag_blocks.shape == (num_timesteps - 1, dim, dim)\n",
        "        assert mean.shape == (num_timesteps, dim)\n",
        "\n",
        "        # Convert the mean to the linear potential\n",
        "        linear_potential = np.einsum('tij,tj->ti', precision_diag_blocks, mean)\n",
        "        linear_potential = linear_potential.at[:-1].add(\n",
        "            np.einsum('tji,tj->ti', precision_lower_diag_blocks, mean[1:]))\n",
        "        linear_potential = linear_potential.at[1:].add(\n",
        "            np.einsum('tij,tj->ti', precision_lower_diag_blocks, mean[:-1]))\n",
        "\n",
        "        # Call the constructor above\n",
        "        return cls.infer(precision_diag_blocks,\n",
        "                         precision_lower_diag_blocks,\n",
        "                         linear_potential)\n",
        "\n",
        "    # Properties to get private class variables\n",
        "    @property\n",
        "    def precision_diag_blocks(self):\n",
        "        return self._precision_diag_blocks\n",
        "\n",
        "    @property\n",
        "    def precision_lower_diag_blocks(self):\n",
        "        return self._precision_lower_diag_blocks\n",
        "\n",
        "    @property\n",
        "    def linear_potential(self):\n",
        "        return self._linear_potential\n",
        "\n",
        "    @property\n",
        "    def log_normalizer(self):\n",
        "        return self._log_normalizer\n",
        "\n",
        "    @property\n",
        "    def filtered_precisions(self):\n",
        "        return self._filtered_precisions\n",
        "\n",
        "    @property\n",
        "    def filtered_linear_potentials(self):\n",
        "        return self._filtered_linear_potentials\n",
        "\n",
        "    @property\n",
        "    def filtered_covariances(self):\n",
        "        return inv(self._filtered_precisions)\n",
        "\n",
        "    @property\n",
        "    def filtered_means(self):\n",
        "        # TODO: this is bad numerically\n",
        "        return np.einsum(\"...ij,...j->...i\", self.filtered_covariances, \n",
        "                         self.filtered_linear_potentials)\n",
        "\n",
        "    @property\n",
        "    def expected_states(self):\n",
        "        return self._expected_states\n",
        "\n",
        "    @property\n",
        "    def expected_states_squared(self):\n",
        "        return self._expected_states_squared\n",
        "\n",
        "    @property\n",
        "    def expected_states_next_states(self):\n",
        "        return self._expected_states_next_states\n",
        "\n",
        "    def _log_prob(self, data, **kwargs):\n",
        "        lp = -0.5 * np.einsum('...ti,...tij,...tj->...', data, self._precision_diag_blocks, data)\n",
        "        lp += -np.einsum('...ti,...tij,...tj->...', data[...,1:,:], self._precision_lower_diag_blocks, data[...,:-1,:])\n",
        "        lp += np.einsum('...ti,...ti->...', data, self._linear_potential)\n",
        "        lp -= self.log_normalizer\n",
        "        return lp\n",
        "\n",
        "    def _mean(self):\n",
        "        return self.expected_states\n",
        "\n",
        "    def _covariance(self):\n",
        "        \"\"\"\n",
        "        NOTE: This computes the _marginal_ covariance Cov[x_t] for each t\n",
        "        \"\"\"\n",
        "        Ex = self._expected_states\n",
        "        ExxT = self._expected_states_squared\n",
        "        return ExxT - np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "\n",
        "    def _sample_n(self, n, seed=None):\n",
        "        filtered_Js = self._filtered_precisions\n",
        "        filtered_hs = self._filtered_linear_potentials\n",
        "        J_lower_diag = self._precision_lower_diag_blocks\n",
        "\n",
        "        def sample_single(seed, filtered_Js, filtered_hs, J_lower_diag):\n",
        "\n",
        "            def _sample_info_gaussian(seed, J, h, sample_shape=()):\n",
        "                # TODO: avoid inversion.\n",
        "                # see https://github.com/mattjj/pybasicbayes/blob/master/pybasicbayes/util/stats.py#L117-L122\n",
        "                # L = np.linalg.cholesky(J)\n",
        "                # x = np.random.randn(h.shape[0])\n",
        "                # return scipy.linalg.solve_triangular(L,x,lower=True,trans='T') \\\n",
        "                #     + dpotrs(L,h,lower=True)[0]\n",
        "                cov = np.linalg.inv(J)\n",
        "                loc = np.einsum(\"...ij,...j->...i\", cov, h)\n",
        "                return tfp.distributions.MultivariateNormalFullCovariance(\n",
        "                    loc=loc, covariance_matrix=cov).sample(sample_shape=sample_shape, seed=seed)\n",
        "\n",
        "            def _step(carry, inpt):\n",
        "                x_next, seed = carry\n",
        "                Jf, hf, L = inpt\n",
        "\n",
        "                # Condition on the next observation\n",
        "                Jc = Jf\n",
        "                hc = hf - np.einsum('ni,ij->nj', x_next, L)\n",
        "\n",
        "                # Split the seed\n",
        "                seed, this_seed = jr.split(seed)\n",
        "                x = _sample_info_gaussian(this_seed, Jc, hc)\n",
        "                return (x, seed), x\n",
        "\n",
        "            # Initialize with sample of last timestep and sample in reverse\n",
        "            seed_T, seed = jr.split(seed)\n",
        "            x_T = _sample_info_gaussian(seed_T, filtered_Js[-1], filtered_hs[-1], sample_shape=(n,))\n",
        "            inputs = (filtered_Js[:-1][::-1], filtered_hs[:-1][::-1], J_lower_diag[::-1])\n",
        "            _, x_rev = lax.scan(_step, (x_T, seed), inputs)\n",
        "\n",
        "            # Reverse and concatenate the last time-step's sample\n",
        "            x = np.concatenate((x_rev[::-1], x_T[None, ...]), axis=0)\n",
        "\n",
        "            # Transpose to be (num_samples, num_timesteps, dim)\n",
        "            return np.transpose(x, (1, 0, 2))\n",
        "\n",
        "        # TODO: Handle arbitrary batch shapes\n",
        "        if filtered_Js.ndim == 4:\n",
        "            # batch mode\n",
        "            samples = vmap(sample_single)(seed, filtered_Js, filtered_hs, J_lower_diag)\n",
        "            # Transpose to be (num_samples, num_batches, num_timesteps, dim)\n",
        "            samples = np.transpose(samples, (1, 0, 2, 3))\n",
        "        else:\n",
        "            # non-batch mode\n",
        "            samples = sample_single(seed, filtered_Js, filtered_hs, J_lower_diag)\n",
        "        return samples\n",
        "\n",
        "    def _entropy(self):\n",
        "        \"\"\"\n",
        "        Compute the entropy\n",
        "\n",
        "            H[X] = -E[\\log p(x)]\n",
        "                 = -E[-1/2 x^T J x + x^T h - log Z(J, h)]\n",
        "                 = 1/2 <J, E[x x^T] - <h, E[x]> + log Z(J, h)\n",
        "        \"\"\"\n",
        "        Ex = self._expected_states\n",
        "        ExxT = self._expected_states_squared\n",
        "        ExnxT = self._expected_states_next_states\n",
        "        J_diag = self._precision_diag_blocks\n",
        "        J_lower_diag = self._precision_lower_diag_blocks\n",
        "        h = self._linear_potential\n",
        "\n",
        "        entropy = 0.5 * np.sum(J_diag * ExxT)\n",
        "        entropy += np.sum(J_lower_diag * ExnxT)\n",
        "        entropy -= np.sum(h * Ex)\n",
        "        entropy += self.log_normalizer\n",
        "        return entropy\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        children = (self._precision_diag_blocks,\n",
        "                    self._precision_lower_diag_blocks,\n",
        "                    self._linear_potential,\n",
        "                    self._log_normalizer,\n",
        "                    self._filtered_precisions,\n",
        "                    self._filtered_linear_potentials,\n",
        "                    self._expected_states,\n",
        "                    self._expected_states_squared,\n",
        "                    self._expected_states_next_states)\n",
        "        aux_data = None\n",
        "        return children, aux_data\n",
        "\n",
        "    @classmethod\n",
        "    def tree_unflatten(cls, aux_data, children):\n",
        "        return cls(*children)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bss7PlIgQRDp"
      },
      "source": [
        "## Important distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "cellView": "form",
        "id": "KZ0Avt4Ad8G-"
      },
      "outputs": [],
      "source": [
        "# @title Linear Gaussian chain distribution object\n",
        "# As a prior distribution, we only need to be able to 1) Evaluate log prob 2) sample\n",
        "# As a posterior distribution, we also need to figure out the sufficient stats (Ex, ExxT, ExnxT)\n",
        "class LinearGaussianChain:\n",
        "    def __init__(self, dynamics_matrix, dynamics_bias, noise_covariance,\n",
        "                 expected_states, expected_states_squared, expected_states_next_states):\n",
        "        \"\"\"\n",
        "        params: dictionary containing the following keys:\n",
        "            A:  (seq_len, dim, dim)\n",
        "            Q:  (seq_len, dim, dim)\n",
        "            b:  (seq_len, dim)\n",
        "        \"\"\"\n",
        "        self._dynamics_matrix = dynamics_matrix\n",
        "        self._dynamics_bias = dynamics_bias\n",
        "        self._noise_covariance = noise_covariance\n",
        "        self._expected_states = expected_states\n",
        "        self._expected_states_squared = expected_states_squared\n",
        "        self._expected_states_next_states = expected_states_next_states\n",
        "\n",
        "    @classmethod\n",
        "    def from_stationary_dynamics(cls, m1, Q1, A, b, Q, T):\n",
        "        dynamics_matrix = np.tile(A[None], (T, 1, 1))\n",
        "        dynamics_bias = np.concatenate([m1[None], \n",
        "                                        np.tile(b[None], (T-1, 1))])\n",
        "        noise_covariance = np.concatenate([Q1[None], \n",
        "                                           np.tile(Q[None], (T-1, 1, 1))])\n",
        "        return cls.from_nonstationary_dynamics(dynamics_matrix, dynamics_bias, noise_covariance)\n",
        "    \n",
        "    @classmethod\n",
        "    def from_nonstationary_dynamics(cls, dynamics_matrix, dynamics_bias, noise_covariance):\n",
        "        # Compute the means and covariances via parallel scan\n",
        "        init_elems = (dynamics_matrix, dynamics_bias, noise_covariance)\n",
        "\n",
        "        @vmap\n",
        "        def assoc_op(elem1, elem2):\n",
        "            A1, b1, Q1 = elem1\n",
        "            A2, b2, Q2 = elem2\n",
        "            return A2 @ A1, A2 @ b1 + b2, A2 @ Q1 @ A2.T + Q2\n",
        "\n",
        "        _, Ex, covariances = lax.associative_scan(assoc_op, init_elems)\n",
        "        expected_states = Ex\n",
        "        expected_states_squared = covariances + np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "        expected_states_next_states = np.einsum(\"...ij,...jk->...ik\", \n",
        "            covariances[:-1], dynamics_matrix[1:]) + np.einsum(\"...i,...j->...ji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "        return cls(dynamics_matrix, dynamics_bias, noise_covariance,\n",
        "                   expected_states, expected_states_squared, expected_states_next_states)\n",
        "\n",
        "    @property\n",
        "    def mean(self):\n",
        "        return self._expected_states\n",
        "\n",
        "    @property\n",
        "    def covariance(self):\n",
        "        Ex = self._expected_states\n",
        "        ExxT = self._expected_states_squared\n",
        "        return ExxT - np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "        \n",
        "    @property\n",
        "    def expected_states(self):\n",
        "        return self._expected_states\n",
        "\n",
        "    @property\n",
        "    def expected_states_squared(self):\n",
        "        return self._expected_states_squared\n",
        "\n",
        "    @property\n",
        "    def expected_states_next_states(self):\n",
        "        return self._expected_states_next_states\n",
        "\n",
        "    # Works with batched distributions and arguments...!\n",
        "    def log_prob(self, xs):\n",
        "\n",
        "        @partial(np.vectorize, signature=\"(t,d,d),(t,d),(t,d,d),(t,d)->()\")\n",
        "        def log_prob_single(A, b, Q, x):\n",
        "            ll = MVN(loc=b[0], covariance_matrix=Q[0]).log_prob(x[0])\n",
        "            ll += MVN(loc=np.einsum(\"tij,tj->ti\", A[1:], x[:-1]) + b[1:], \n",
        "                      covariance_matrix=Q[1:]).log_prob(x[1:]).sum()\n",
        "            return ll\n",
        "\n",
        "        return log_prob_single(self._dynamics_matrix,\n",
        "                               self._dynamics_bias, \n",
        "                               self._noise_covariance, xs)\n",
        "        \n",
        "    # Only supports 0d and 1d sample shapes\n",
        "    # Does not support sampling with batched object\n",
        "    def sample(self, seed, sample_shape=()):\n",
        "\n",
        "        @partial(np.vectorize, signature=\"(n),(t,d,d),(t,d),(t,d,d)->(t,d)\")\n",
        "        def sample_single(key, A, b, Q):\n",
        "\n",
        "            biases = MVN(loc=b, covariance_matrix=Q).sample(seed=key)\n",
        "            init_elems = (A, biases)\n",
        "\n",
        "            @vmap\n",
        "            def assoc_op(elem1, elem2):\n",
        "                A1, b1 = elem1\n",
        "                A2, b2 = elem2\n",
        "                return A2 @ A1, A2 @ b1 + b2\n",
        "\n",
        "            _, sample = lax.associative_scan(assoc_op, init_elems)\n",
        "            return sample\n",
        "        \n",
        "        if (len(sample_shape) == 0):\n",
        "            return sample_single(seed, self._dynamics_matrix,\n",
        "                                 self._dynamics_bias, \n",
        "                                 self._noise_covariance)\n",
        "        elif (len(sample_shape) == 1):\n",
        "            return sample_single(jr.split(seed, sample_shape[0]),\n",
        "                                 self._dynamics_matrix[None],\n",
        "                                 self._dynamics_bias[None],\n",
        "                                 self._noise_covariance[None]) \n",
        "        else:\n",
        "            raise Exception(\"More than one sample dimensions are not supported!\")\n",
        "\n",
        "    def entropy(self):\n",
        "        \"\"\"\n",
        "        Compute the entropy\n",
        "\n",
        "            H[X] = -E[\\log p(x)]\n",
        "                 = -E[-1/2 x^T J x + x^T h - log Z(J, h)]\n",
        "                 = 1/2 <J, E[x x^T] - <h, E[x]> + log Z(J, h)\n",
        "        \"\"\"\n",
        "        Ex = self.expected_states\n",
        "        ExxT = self.expected_states_squared\n",
        "        ExnxT = self.expected_states_next_states\n",
        "        \n",
        "        dim = Ex.shape[-1]        \n",
        "        Q_inv = solve(self._noise_covariance, np.eye(dim)[None])\n",
        "        A = self._dynamics_matrix\n",
        "\n",
        "        J_lower_diag = np.einsum(\"til,tlj->tij\", -Q_inv[1:], A[1:])\n",
        "        ATQinvA = np.einsum(\"tji,tjl,tlk->tik\", A[1:], Q_inv[1:], A[1:])\n",
        "        J_diag = Q_inv.at[:-1].add(ATQinvA)\n",
        "\n",
        "        Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "        Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "        trm1 = 0.5 * np.sum(J_diag * Sigmatt)\n",
        "        trm2 = np.sum(J_lower_diag * Sigmatnt)\n",
        "\n",
        "        return trm1 + trm2 - self.log_prob(Ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "cellView": "form",
        "id": "lBVe4-YTOQLp"
      },
      "outputs": [],
      "source": [
        "# @title Testing the correctness of the linear Gaussian chain\n",
        "# jax.config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "# D = 3\n",
        "# T = 200\n",
        "\n",
        "# mu1 = np.zeros(D)\n",
        "# Q1 = np.eye(D)\n",
        "# # TODO: A could be a sensitive parameter\n",
        "# # A = jnp.linspace(0.001, 1.0, D)\n",
        "# A = random_rotation(key_0, D, np.pi / 20)\n",
        "# b = jr.normal(key_0, shape=(D,))#np.zeros(D)\n",
        "# Q = np.eye(D)\n",
        "\n",
        "# C = np.eye(D)\n",
        "# d = np.zeros(D)\n",
        "\n",
        "# dist = LinearGaussianChain.from_stationary_dynamics(mu1, Q1, A, b, Q, T)\n",
        "# dist.entropy()\n",
        "# # Sigmas = np.tile(0.01 * np.eye(D), (T, 1, 1))\n",
        "\n",
        "# p = dynamics_to_tridiag({\"m1\": mu1, \"Q1\": Q1, \"A\": A, \"b\": b, \"Q\": Q}, T, D)\n",
        "# dist_ = MultivariateNormalBlockTridiag.infer(p[\"J\"], p[\"L\"], p[\"h\"])\n",
        "\n",
        "# ExnxT_ = dist_.expected_states_next_states\n",
        "# ExnxT = dist.expected_states_next_states\n",
        "# print(ExnxT - ExnxT_)\n",
        "# print(dist_.entropy() - dist.entropy())\n",
        "# print(dist.expected_states_squared - dist_.expected_states_squared)\n",
        "# posterior = LinearGaussianChain.from_stationary_dynamics(mu1, Q1, A, b, Q, T)\n",
        "# key_1 = jr.split(key_0)[0]\n",
        "# A1 = random_rotation(key_1, D, np.pi / 20)\n",
        "# b1 = jr.normal(key_1, shape=(D,))#np.zeros(D)\n",
        "# prior_params = dynamics_to_tridiag({\"m1\": mu1, \"Q1\": Q1, \"A\": A1, \"b\": b1, \"Q\": Q}, T, D)\n",
        "# prior = LinearGaussianChain.from_stationary_dynamics(mu1, Q1, A1, b1, Q, T)\n",
        "# Ex = posterior.expected_states\n",
        "# ExxT = posterior.expected_states_squared\n",
        "# ExnxT = posterior.expected_states_next_states\n",
        "# Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "# Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "# J, L = prior_params[\"J\"], prior_params[\"L\"]\n",
        "# cross_entropy = -prior.log_prob(Ex)\n",
        "# cross_entropy += 0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "# cross_entropy += np.einsum(\"tij,tij->\", L, Sigmatnt)\n",
        "# print(\"Closed form KL:\", cross_entropy - posterior.entropy())\n",
        "# samples = posterior.sample(key_1, (100,))\n",
        "# print(\"Sampled KL:\", np.mean(posterior.log_prob(samples) - prior.log_prob(samples)))\n",
        "# # return np.mean(posterior.log_prob(samples) - prior.log_prob(samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "6D7qIBFpRyH8"
      },
      "outputs": [],
      "source": [
        "# @title Linear Gaussian chain prior\n",
        "# This is a linear Gaussian chain\n",
        "class LinearGaussianChainPrior(SVAEPrior):\n",
        "\n",
        "    def __init__(self, latent_dims, seq_len):\n",
        "        self.latent_dims = latent_dims\n",
        "        # The only annoying thing is that we have to specify the sequence length\n",
        "        # ahead of time\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return (self.seq_len, self.latent_dims)\n",
        "\n",
        "    # Must be the full set of constrained parameters!\n",
        "    def distribution(self, params):\n",
        "        As, bs, Qs = params[\"As\"], params[\"bs\"], params[\"Qs\"]\n",
        "        Ex, ExxT, ExnxT = params[\"Ex\"], params[\"ExxT\"], params[\"ExnxT\"]\n",
        "        return LinearGaussianChain(As, bs, Qs, Ex, ExxT, ExnxT)\n",
        "\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        params = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"Q1\": np.eye(D),\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": np.eye(D)\n",
        "        }\n",
        "        constrained = self.get_constrained_params(params)\n",
        "        return params\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        return params\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        tridiag = dynamics_to_tridiag(params, self.seq_len, self.latent_dims)\n",
        "        p.update(tridiag)\n",
        "        dist = LinearGaussianChain.from_stationary_dynamics(p[\"m1\"], p[\"Q1\"], \n",
        "                                         p[\"A\"], p[\"b\"], p[\"Q\"], self.seq_len)\n",
        "        p.update({\n",
        "            \"As\": dist._dynamics_matrix,\n",
        "            \"bs\": dist._dynamics_bias,\n",
        "            \"Qs\": dist._noise_covariance,\n",
        "            \"Ex\": dist.expected_states,\n",
        "            \"ExxT\": dist.expected_states_squared,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    # This is pretty much deprecated since we're using sgd\n",
        "    def m_step(self, prior_params):\n",
        "        suff_stats = prior_params[\"avg_suff_stats\"]\n",
        "        ExxT = suff_stats[\"ExxT\"]\n",
        "        ExnxT = suff_stats[\"ExnxT\"]\n",
        "        Ex = suff_stats[\"Ex\"]\n",
        "        seq_len = Ex.shape[0]\n",
        "        # Update the initials\n",
        "        m1 = Ex[0]\n",
        "        Q1 = ExxT[0] - np.outer(m1, m1)\n",
        "        D = self.latent_dims\n",
        "        A, b, Q = fit_linear_regression(Ex[:-1].sum(axis=0), \n",
        "                                        Ex[1:].sum(axis=0), \n",
        "                                        ExxT[:-1].sum(axis=0), \n",
        "                                        ExnxT.sum(axis=0), \n",
        "                                        ExxT[1:].sum(axis=0), \n",
        "                                        seq_len - 1)\n",
        "        out = { \"m1\": m1, \"Q1\": Q1, \"A\": A, \"b\": b, \"Q\": Q }\n",
        "        out[\"avg_suff_stats\"] = deepcopy(suff_stats)\n",
        "        return out\n",
        "\n",
        "# This is a bit clumsy but it's the best we can do without using some sophisticated way\n",
        "# Of marking the constrained/optimized parameters vs. unconstrained parameters\n",
        "class LieParameterizedLinearGaussianChainPrior(LinearGaussianChainPrior):\n",
        "    def init(self, key):\n",
        "        D = self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        # Equivalent to the unit matrix\n",
        "        Q_flat = np.concatenate([np.ones(D) * inv_softplus(1), np.zeros((D*(D-1)//2))])\n",
        "        params = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"Q1\": Q_flat,\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": Q_flat\n",
        "        }\n",
        "        return params\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        return {\n",
        "            \"m1\": params[\"m1\"],\n",
        "            \"Q1\": lie_params_to_constrained(params[\"Q1\"], self.latent_dims),\n",
        "            \"A\": params[\"A\"],\n",
        "            \"b\": params[\"b\"],\n",
        "            \"Q\": lie_params_to_constrained(params[\"Q\"], self.latent_dims)   \n",
        "        }\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        D = self.latent_dims\n",
        "        p = self.get_dynamics_params(params)\n",
        "        return super().get_constrained_params(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "6c7K9dJWzyG8"
      },
      "outputs": [],
      "source": [
        "# @title Linear Gaussian chain posteriors\n",
        "\n",
        "# The infer function for the DKF version just uses the posterior params \n",
        "class CDKFPosterior(LinearGaussianChainPrior):\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        params = {\n",
        "            \"As\": np.zeros((T, D, D)), \n",
        "            \"bs\": np.zeros((T, D)),\n",
        "            \"Qs\": np.tile(np.eye(D)[None], (T, 1, 1))\n",
        "        }\n",
        "        return self.get_constrained_params(params)\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        dist = LinearGaussianChain.from_nonstationary_dynamics(p[\"As\"], p[\"bs\"], p[\"Qs\"])\n",
        "        p.update({\n",
        "            \"Ex\": dist.expected_states,\n",
        "            \"ExxT\": dist.expected_states_squared,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    def sufficient_statistics(self, params):\n",
        "        return {\n",
        "            \"Ex\": params[\"Ex\"],\n",
        "            \"ExxT\": params[\"ExxT\"],\n",
        "            \"ExnxT\": params[\"ExnxT\"]\n",
        "        }\n",
        "\n",
        "    def infer(self, prior_params, posterior_params):\n",
        "        return self.get_constrained_params(posterior_params)\n",
        "\n",
        "class DKFPosterior(CDKFPosterior):\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        # The DKF produces a factored posterior\n",
        "        # So the dynamics matrix is zeroed out\n",
        "        p[\"As\"] *= 0\n",
        "        dist = LinearGaussianChain.from_nonstationary_dynamics(p[\"As\"], p[\"bs\"], p[\"Qs\"])\n",
        "        p.update({\n",
        "            \"Ex\": dist.expected_states,\n",
        "            \"ExxT\": dist.expected_states_squared,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "cellView": "form",
        "id": "Rbe3PsAIHliY"
      },
      "outputs": [],
      "source": [
        "# @title PlaNet type posterior\n",
        "# The infer function for the DKF version just uses the posterior params \n",
        "# TODO: Put the dummies in the params dictionary as well\n",
        "class PlaNetPosterior(DKFPosterior):\n",
        "    def __init__(self, network_params, latent_dims, seq_len):\n",
        "        super().__init__(latent_dims, seq_len)\n",
        "        self.network = StochasticRNNCell.from_params(**network_params)\n",
        "        self.input_dim = network_params[\"input_dim\"]      # u\n",
        "        self.latent_dim = network_params[\"rnn_dim\"]       # h\n",
        "        self.output_dim = network_params[\"output_dim\"]    # x\n",
        "\n",
        "    def init(self, key):\n",
        "        input_dummy = np.zeros((self.input_dim,))\n",
        "        latent_dummy = np.zeros((self.latent_dim,))\n",
        "        output_dummy = np.zeros((self.output_dim,))\n",
        "        rnn_params = self.network.init(key, input_dummy, latent_dummy, output_dummy)\n",
        "        return {\n",
        "            \"rnn_params\": rnn_params,\n",
        "            \"input_dummy\": input_dummy,\n",
        "            \"latent_dummy\": latent_dummy,\n",
        "            \"output_dummy\": output_dummy,\n",
        "        }\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        # All of the information is stored in the second argument already\n",
        "        return params\n",
        "\n",
        "    def distribution(self, params):\n",
        "        return DeepAutoregressiveDynamics(self.network, params)\n",
        "        \n",
        "    # These are just dummies\n",
        "    def sufficient_statistics(self, params):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        return {\n",
        "            \"Ex\": np.zeros((T, D)),\n",
        "            \"ExxT\": np.zeros((T, D, D)),\n",
        "            \"ExnxT\": np.zeros((T-1, D, D))\n",
        "        }\n",
        "\n",
        "# We only need to be able to 1) Evaluate log prob 2) sample\n",
        "# The tricky thing here is evaluating the \n",
        "class DeepAutoregressiveDynamics:\n",
        "\n",
        "    def __init__(self, network, params):\n",
        "        self.cell = network\n",
        "        self.params = params[\"network_params\"]\n",
        "        self.inputs = params[\"network_input\"]\n",
        "        # self.input_dummy = params[\"network_params\"][\"input_dummy\"]\n",
        "        # self.latent_dummy = params[\"network_params\"][\"latent_dummy\"]\n",
        "        # self.output_dummy = params[\"network_params\"][\"output_dummy\"]\n",
        "        self._mean = None\n",
        "        self._covariance = None\n",
        "\n",
        "    def mean(self):\n",
        "        if (self._mean is None):\n",
        "            self.compute_mean_and_cov()\n",
        "        return self._mean\n",
        "\n",
        "    def covariance(self):\n",
        "        if (self._covariance is None):\n",
        "            self.compute_mean_and_cov()\n",
        "        return self._covariance\n",
        "\n",
        "    @property\n",
        "    def filtered_covariances(self):\n",
        "        return self.covariance()\n",
        "\n",
        "    @property\n",
        "    def filtered_means(self):\n",
        "        return self.mean()\n",
        "        \n",
        "    def compute_mean_and_cov(self):\n",
        "        num_samples = 25\n",
        "        samples = self.sample((num_samples,), key_0)\n",
        "        Ex = np.mean(samples, axis=0)\n",
        "        self._mean = Ex\n",
        "        ExxT = np.einsum(\"s...ti,s...tj->s...tij\", samples, samples).mean(axis=0)\n",
        "        self._covariance = ExxT - np.einsum(\"...ti,...tj->...tij\", Ex, Ex)\n",
        "\n",
        "    # TODO: make this work properly with a batched distribution object\n",
        "    def log_prob(self, xs):\n",
        "        params = self.params\n",
        "        def log_prob_single(x_):\n",
        "            def _log_prob_step(carry, i):\n",
        "                h, prev_x = carry\n",
        "                x, u = x_[i], self.inputs[i]\n",
        "                h, (cov, mean) = self.cell.apply(params[\"rnn_params\"], h, prev_x, u)\n",
        "                pred_dist = tfd.MultivariateNormalFullCovariance(loc=mean, \n",
        "                                                            covariance_matrix=cov)\n",
        "                log_prob = pred_dist.log_prob(x)\n",
        "                carry = h, x\n",
        "                return carry, log_prob\n",
        "            # Assuming these are zero arrays already\n",
        "            init = (params[\"latent_dummy\"], params[\"output_dummy\"])\n",
        "            _, log_probs = scan(_log_prob_step, init, np.arange(x_.shape[0]))\n",
        "            return np.sum(log_probs, axis=0)\n",
        "        return vmap(log_prob_single)(xs)\n",
        "\n",
        "    # TODO: make this work with a batched distribution object\n",
        "    # Only supports rank 0 and 1 sample shapes\n",
        "    # Output: ([num_samples,] [batch_size,] seq_len, event_dim)\n",
        "    def sample(self, sample_shape, seed):\n",
        "        def _sample_single(key, params, inputs):\n",
        "            def _sample_step(carry, u):\n",
        "                key, h, x = carry\n",
        "                key, new_key = jr.split(key)\n",
        "                h, (cov, mean) = self.cell.apply(params[\"rnn_params\"], h, x, u)\n",
        "                sample = jr.multivariate_normal(key, mean, cov)\n",
        "                carry = new_key, h, sample\n",
        "                output = sample\n",
        "                return carry, output\n",
        "\n",
        "            init = (key, params[\"latent_dummy\"], params[\"output_dummy\"])\n",
        "            _, sample = scan(_sample_step, init, inputs)\n",
        "            return sample\n",
        "\n",
        "        if (len(self.inputs.shape) == 2):\n",
        "            if (len(sample_shape) == 0):\n",
        "                return _sample_single(seed, self.params, self.inputs)\n",
        "            else:\n",
        "                assert (len(sample_shape) == 1)\n",
        "                return vmap(_sample_single, in_axes=(0, None, None))(jr.split(seed, sample_shape[0]),\n",
        "                                            self.params, self.inputs)\n",
        "        else:\n",
        "            # This is a batched distribution object\n",
        "            assert(len(self.inputs.shape) == 3)\n",
        "            batch_size = self.inputs.shape[0]\n",
        "            if (len(sample_shape) == 0):\n",
        "                return vmap(_sample_single)(\n",
        "                            jr.split(seed, batch_size), \n",
        "                            self.params,\n",
        "                            self.inputs\n",
        "                        )\n",
        "            else:\n",
        "                assert (len(sample_shape) == 1)\n",
        "                return vmap(\n",
        "                        lambda s:vmap(_sample_single)(\n",
        "                            jr.split(s, batch_size), \n",
        "                            self.params,\n",
        "                            self.inputs\n",
        "                        )\n",
        "                    )(jr.split(seed, sample_shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "cellView": "form",
        "id": "qDsz18BcyDcf"
      },
      "outputs": [],
      "source": [
        "# @title LDS object (might wanna refactor this)\n",
        "\n",
        "# Takes a linear Gaussian chain as its base\n",
        "class LDS(LinearGaussianChainPrior):\n",
        "    def __init__(self, latent_dims, seq_len, base=None, posterior=None):\n",
        "        super().__init__(latent_dims, seq_len)\n",
        "        self.posterior = posterior or LDSSVAEPosterior(latent_dims, seq_len)\n",
        "        self.base = base or LinearGaussianChainPrior(latent_dims, seq_len) # Slightly redundant...\n",
        "\n",
        "    # Takes unconstrained params\n",
        "    def sample(self, params, shape, key):\n",
        "        latents = self.base.sample(params, shape, key)\n",
        "        sample_shape = latents.shape[:-1]\n",
        "        key, _ = jr.split(key)\n",
        "        C, d, R = params[\"C\"], params[\"d\"], params[\"R\"]\n",
        "        obs_noise = tfd.MultivariateNormalFullCovariance(loc=d, covariance_matrix=R)\\\n",
        "            .sample(sample_shape=sample_shape, seed=key)\n",
        "        obs = np.einsum(\"ij,...tj->...ti\", C, latents) + obs_noise\n",
        "        return latents, obs\n",
        "\n",
        "    # Should work with any batch dimension\n",
        "    def log_prob(self, params, states, data):\n",
        "        latent_dist = self.base.distribution(self.base.get_constrained_params(params))\n",
        "        latent_ll = latent_dist.log_prob(states)\n",
        "        C, d, R = params[\"C\"], params[\"d\"], params[\"R\"]\n",
        "        # Gets around batch dimensions\n",
        "        noise = tfd.MultivariateNormalFullCovariance(loc=d, covariance_matrix=R)\n",
        "        obs_ll = noise.log_prob(data - np.einsum(\"ij,...tj->...ti\", C, states))\n",
        "        return latent_ll + obs_ll.sum(axis=-1)\n",
        "\n",
        "    # Assumes single data points\n",
        "    def e_step(self, params, data):\n",
        "        # Shorthand names for parameters\n",
        "        C, d, R = params[\"C\"], params[\"d\"], params[\"R\"]\n",
        "\n",
        "        J = np.dot(C.T, np.linalg.solve(R, C))\n",
        "        J = np.tile(J[None, :, :], (self.seq_len, 1, 1))\n",
        "        # linear potential\n",
        "        h = np.dot(data - d, np.linalg.solve(R, C))\n",
        "\n",
        "        Sigma = solve(J, np.eye(self.latent_dims)[None])\n",
        "        mu = vmap(solve)(J, h)\n",
        "\n",
        "        return self.posterior.infer(self.base.get_constrained_params(params), {\"J\": J, \"h\": h, \n",
        "                                                                    \"mu\": mu, \"Sigma\": Sigma})\n",
        "        \n",
        "    # Also assumes single data points\n",
        "    def marginal_log_likelihood(self, params, data):\n",
        "        posterior = self.posterior.distribution(self.e_step(params, data))\n",
        "        states = posterior.mean()\n",
        "        prior_ll = self.log_prob(params, states, data)\n",
        "        posterior_ll = posterior.log_prob(states)\n",
        "        # This is numerically unstable!\n",
        "        lps = prior_ll - posterior_ll\n",
        "        return lps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGISYI1CN6cv"
      },
      "source": [
        "## Making a mean parameter posterior object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "cellView": "form",
        "id": "il0xUSa3nn0K"
      },
      "outputs": [],
      "source": [
        "# @title Parallel Kalman filtering and smoothing\n",
        "\n",
        "def _make_associative_sampling_elements(params, key, filtered_means, filtered_covariances):\n",
        "    \"\"\"Preprocess filtering output to construct input for smoothing assocative scan.\"\"\"\n",
        "\n",
        "    F = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "\n",
        "    def _last_sampling_element(key, m, P):\n",
        "        return np.zeros_like(P), MVN(m, P).sample(seed=key)\n",
        "\n",
        "    def _generic_sampling_element(params, key, m, P):\n",
        "\n",
        "        eps = 1e-3\n",
        "        P += np.eye(dims) * eps\n",
        "        Pp = F @ P @ F.T + Q\n",
        "\n",
        "        FP = F @ P\n",
        "        E  = psd_solve(Pp, FP).T\n",
        "        g  = m - E @ F @ m\n",
        "        L  = P - E @ Pp @ E.T\n",
        "\n",
        "        L = (L + L.T) * .5 + np.eye(dims) * eps # Add eps to the crucial covariance matrix\n",
        "\n",
        "        h = MVN(g, L).sample(seed=key)\n",
        "        return E, h\n",
        "\n",
        "    num_timesteps = len(filtered_means)\n",
        "    dims = filtered_means.shape[-1]\n",
        "    keys = jr.split(key, num_timesteps)\n",
        "    last_elems = _last_sampling_element(keys[-1], filtered_means[-1], \n",
        "                                        filtered_covariances[-1])\n",
        "    generic_elems = vmap(_generic_sampling_element, (None, 0, 0, 0))(\n",
        "        params, keys[:-1], filtered_means[:-1], filtered_covariances[:-1]\n",
        "        )\n",
        "    combined_elems = tuple(np.append(gen_elm, last_elm[None,:], axis=0)\n",
        "                           for gen_elm, last_elm in zip(generic_elems, last_elems))\n",
        "    return combined_elems\n",
        "\n",
        "def _make_associative_filtering_elements(params, potentials):\n",
        "    \"\"\"Preprocess observations to construct input for filtering assocative scan.\"\"\"\n",
        "\n",
        "    F = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "    Q1 = params[\"Q1\"]\n",
        "    P0 = Q1\n",
        "    P1 = Q1\n",
        "    m1 = params[\"m1\"]\n",
        "    dim = Q.shape[0]\n",
        "    H = np.eye(dim)\n",
        "\n",
        "    def _first_filtering_element(params, mu, Sigma):\n",
        "\n",
        "        y, R = mu, Sigma\n",
        "\n",
        "        S = H @ Q @ H.T + R\n",
        "        CF, low = scipy.linalg.cho_factor(S)\n",
        "\n",
        "        S1 = H @ P1 @ H.T + R\n",
        "        K1 = psd_solve(S1, H @ P1).T\n",
        "\n",
        "        A = np.zeros_like(F)\n",
        "        b = m1 + K1 @ (y - H @ m1)\n",
        "        C = P1 - K1 @ S1 @ K1.T\n",
        "        eta = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), y)\n",
        "        J = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), H @ F)\n",
        "\n",
        "        logZ = -MVN(loc=np.zeros_like(y), covariance_matrix=H @ P0 @ H.T + R).log_prob(y)\n",
        "\n",
        "        return A, b, C, J, eta, logZ\n",
        "\n",
        "\n",
        "    def _generic_filtering_element(params, mu, Sigma):\n",
        "\n",
        "        y, R = mu, Sigma\n",
        "\n",
        "        S = H @ Q @ H.T + R\n",
        "        CF, low = scipy.linalg.cho_factor(S)\n",
        "        K = scipy.linalg.cho_solve((CF, low), H @ Q).T\n",
        "        A = F - K @ H @ F\n",
        "        b = K @ y\n",
        "        C = Q - K @ H @ Q\n",
        "\n",
        "        eta = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), y)\n",
        "        J = F.T @ H.T @ scipy.linalg.cho_solve((CF, low), H @ F)\n",
        "\n",
        "        logZ = -MVN(loc=np.zeros_like(y), covariance_matrix=S).log_prob(y)\n",
        "\n",
        "        return A, b, C, J, eta, logZ\n",
        "\n",
        "    mus, Sigmas = potentials[\"mu\"], potentials[\"Sigma\"]\n",
        "\n",
        "    first_elems = _first_filtering_element(params, mus[0], Sigmas[0])\n",
        "    generic_elems = vmap(_generic_filtering_element, (None, 0, 0))(params, mus[1:], Sigmas[1:])\n",
        "    combined_elems = tuple(np.concatenate((first_elm[None,...], gen_elm))\n",
        "                           for first_elm, gen_elm in zip(first_elems, generic_elems))\n",
        "    return combined_elems\n",
        "\n",
        "def lgssm_filter(params, emissions):\n",
        "    \"\"\"A parallel version of the lgssm filtering algorithm.\n",
        "    See S. Srkk and . F. Garca-Fernndez (2021) - https://arxiv.org/abs/1905.13002.\n",
        "    Note: This function does not yet handle `inputs` to the system.\n",
        "    \"\"\"\n",
        "\n",
        "    initial_elements = _make_associative_filtering_elements(params, emissions)\n",
        "\n",
        "    @vmap\n",
        "    def filtering_operator(elem1, elem2):\n",
        "        A1, b1, C1, J1, eta1, logZ1 = elem1\n",
        "        A2, b2, C2, J2, eta2, logZ2 = elem2\n",
        "        dim = A1.shape[0]\n",
        "        I = np.eye(dim)\n",
        "\n",
        "        I_C1J2 = I + C1 @ J2\n",
        "        temp = scipy.linalg.solve(I_C1J2.T, A2.T).T\n",
        "        A = temp @ A1\n",
        "        b = temp @ (b1 + C1 @ eta2) + b2\n",
        "        C = temp @ C1 @ A2.T + C2\n",
        "\n",
        "        I_J2C1 = I + J2 @ C1\n",
        "        temp = scipy.linalg.solve(I_J2C1.T, A1).T\n",
        "\n",
        "        eta = temp @ (eta2 - J2 @ b1) + eta1\n",
        "        J = temp @ J2 @ A1 + J1\n",
        "\n",
        "        # mu = scipy.linalg.solve(J2, eta2)\n",
        "        # t2 = - eta2 @ mu + (b1 - mu) @ scipy.linalg.solve(I_J2C1, (J2 @ b1 - eta2))\n",
        "\n",
        "        mu = np.linalg.solve(C1, b1)\n",
        "        t1 = (b1 @ mu - (eta2 + mu) @ np.linalg.solve(I_C1J2, C1 @ eta2 + b1))\n",
        "\n",
        "        logZ = (logZ1 + logZ2 + 0.5 * np.linalg.slogdet(I_C1J2)[1] + 0.5 * t1)\n",
        "\n",
        "        return A, b, C, J, eta, logZ\n",
        "\n",
        "    _, filtered_means, filtered_covs, _, _, logZ = lax.associative_scan(\n",
        "                                                filtering_operator, initial_elements\n",
        "                                                )\n",
        "\n",
        "    return {\n",
        "        \"marginal_logliks\": -logZ,\n",
        "        \"marginal_loglik\": -logZ[-1],\n",
        "        \"filtered_means\": filtered_means, \n",
        "        \"filtered_covariances\": filtered_covs\n",
        "    }\n",
        "\n",
        "def _make_associative_smoothing_elements(params, filtered_means, filtered_covariances):\n",
        "    \"\"\"Preprocess filtering output to construct input for smoothing assocative scan.\"\"\"\n",
        "\n",
        "    F = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "\n",
        "    def _last_smoothing_element(m, P):\n",
        "        return np.zeros_like(P), m, P\n",
        "\n",
        "    def _generic_smoothing_element(params, m, P):\n",
        "\n",
        "        Pp = F @ P @ F.T + Q\n",
        "\n",
        "        E  = psd_solve(Pp, F @ P).T\n",
        "        g  = m - E @ F @ m\n",
        "        L  = P - E @ Pp @ E.T\n",
        "        return E, g, L\n",
        "\n",
        "    last_elems = _last_smoothing_element(filtered_means[-1], filtered_covariances[-1])\n",
        "    generic_elems = vmap(_generic_smoothing_element, (None, 0, 0))(\n",
        "        params, filtered_means[:-1], filtered_covariances[:-1]\n",
        "        )\n",
        "    combined_elems = tuple(np.append(gen_elm, last_elm[None,:], axis=0)\n",
        "                           for gen_elm, last_elm in zip(generic_elems, last_elems))\n",
        "    return combined_elems\n",
        "\n",
        "\n",
        "def parallel_lgssm_smoother(params, emissions):\n",
        "    \"\"\"A parallel version of the lgssm smoothing algorithm.\n",
        "    See S. Srkk and . F. Garca-Fernndez (2021) - https://arxiv.org/abs/1905.13002.\n",
        "    Note: This function does not yet handle `inputs` to the system.\n",
        "    \"\"\"\n",
        "    filtered_posterior = lgssm_filter(params, emissions)\n",
        "    filtered_means = filtered_posterior[\"filtered_means\"]\n",
        "    filtered_covs = filtered_posterior[\"filtered_covariances\"]\n",
        "    initial_elements = _make_associative_smoothing_elements(params, filtered_means, filtered_covs)\n",
        "\n",
        "    @vmap\n",
        "    def smoothing_operator(elem1, elem2):\n",
        "        E1, g1, L1 = elem1\n",
        "        E2, g2, L2 = elem2\n",
        "\n",
        "        E = E2 @ E1\n",
        "        g = E2 @ g1 + g2\n",
        "        L = E2 @ L1 @ E2.T + L2\n",
        "\n",
        "        return E, g, L\n",
        "\n",
        "    _, smoothed_means, smoothed_covs, *_ = lax.associative_scan(\n",
        "                                                smoothing_operator, initial_elements, reverse=True\n",
        "                                                )\n",
        "    return {\n",
        "        \"marginal_loglik\": filtered_posterior[\"marginal_loglik\"],\n",
        "        \"filtered_means\": filtered_means,\n",
        "        \"filtered_covariances\": filtered_covs,\n",
        "        \"smoothed_means\": smoothed_means,\n",
        "        \"smoothed_covariances\": smoothed_covs\n",
        "    }\n",
        "\n",
        "def lgssm_log_normalizer(dynamics_params, mu_filtered, Sigma_filtered, potentials):\n",
        "    p = dynamics_params\n",
        "    Q, A, b = p[\"Q\"][None], p[\"A\"][None], p[\"b\"][None]\n",
        "    AT = (p[\"A\"].T)[None]\n",
        "\n",
        "    I = np.eye(Q.shape[-1])\n",
        "\n",
        "    Sigma_filtered, mu_filtered = Sigma_filtered[:-1], mu_filtered[:-1]\n",
        "    Sigma = Q + A @ Sigma_filtered @ AT\n",
        "    mu = (A[0] @ mu_filtered.T).T + b\n",
        "    # Append the first element\n",
        "    Sigma_pred = np.concatenate([p[\"Q1\"][None], Sigma])\n",
        "    mu_pred = np.concatenate([p[\"m1\"][None], mu])\n",
        "    mu_rec, Sigma_rec = potentials[\"mu\"], potentials[\"Sigma\"]\n",
        "\n",
        "    def log_Z_single(mu_pred, Sigma_pred, mu_rec, Sigma_rec):\n",
        "        return MVN(loc=mu_pred, covariance_matrix=Sigma_pred+Sigma_rec).log_prob(mu_rec)\n",
        "\n",
        "    log_Z = vmap(log_Z_single)(mu_pred, Sigma_pred, mu_rec, Sigma_rec)\n",
        "    return np.sum(log_Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "cellView": "form",
        "id": "_zQepN9hRCdG"
      },
      "outputs": [],
      "source": [
        "# @title Parallel linear Gaussian state space model object\n",
        "def lgssm_log_normalizer(dynamics_params, mu_filtered, Sigma_filtered, potentials):\n",
        "    p = dynamics_params\n",
        "    Q, A, b = p[\"Q\"][None], p[\"A\"][None], p[\"b\"][None]\n",
        "    AT = (p[\"A\"].T)[None]\n",
        "\n",
        "    I = np.eye(Q.shape[-1])\n",
        "\n",
        "    Sigma_filtered, mu_filtered = Sigma_filtered[:-1], mu_filtered[:-1]\n",
        "    Sigma = Q + A @ Sigma_filtered @ AT\n",
        "    mu = (A[0] @ mu_filtered.T).T + b\n",
        "    # Append the first element\n",
        "    Sigma_pred = np.concatenate([p[\"Q1\"][None], Sigma])\n",
        "    mu_pred = np.concatenate([p[\"m1\"][None], mu])\n",
        "    mu_rec, Sigma_rec = potentials[\"mu\"], potentials[\"Sigma\"]\n",
        "\n",
        "    def log_Z_single(mu_pred, Sigma_pred, mu_rec, Sigma_rec):\n",
        "        return MVN(loc=mu_pred, covariance_matrix=Sigma_pred+Sigma_rec).log_prob(mu_rec)\n",
        "\n",
        "    log_Z = vmap(log_Z_single)(mu_pred, Sigma_pred, mu_rec, Sigma_rec)\n",
        "    return np.sum(log_Z)\n",
        "\n",
        "class LinearGaussianSSM(tfd.Distribution):\n",
        "    def __init__(self,\n",
        "                 initial_mean,\n",
        "                 initial_covariance,\n",
        "                 dynamics_matrix,\n",
        "                 dynamics_bias,\n",
        "                 dynamics_noise_covariance,\n",
        "                 emissions_means,\n",
        "                 emissions_covariances,\n",
        "                 log_normalizer,\n",
        "                 filtered_means,\n",
        "                 filtered_covariances,\n",
        "                 smoothed_means,\n",
        "                 smoothed_covariances,\n",
        "                 smoothed_cross,\n",
        "                 validate_args=False,\n",
        "                 allow_nan_stats=True,\n",
        "                 name=\"LinearGaussianSSM\",\n",
        "             ) -> None:\n",
        "        # Dynamics\n",
        "        self._initial_mean = initial_mean\n",
        "        self._initial_covariance = initial_covariance\n",
        "        self._dynamics_matrix = dynamics_matrix\n",
        "        self._dynamics_bias = dynamics_bias\n",
        "        self._dynamics_noise_covariance = dynamics_noise_covariance\n",
        "        # Emissions\n",
        "        self._emissions_means = emissions_means\n",
        "        self._emissions_covariances = emissions_covariances\n",
        "        # Filtered\n",
        "        self._log_normalizer = log_normalizer\n",
        "        self._filtered_means = filtered_means\n",
        "        self._filtered_covariances = filtered_covariances\n",
        "        # Smoothed\n",
        "        self._smoothed_means = smoothed_means\n",
        "        self._smoothed_covariances = smoothed_covariances\n",
        "        self._smoothed_cross = smoothed_cross\n",
        "\n",
        "        # We would detect the dtype dynamically but that would break vmap\n",
        "        # see https://github.com/tensorflow/probability/issues/1271\n",
        "        dtype = np.float32\n",
        "        super(LinearGaussianSSM, self).__init__(\n",
        "            dtype=dtype,\n",
        "            validate_args=validate_args,\n",
        "            allow_nan_stats=allow_nan_stats,\n",
        "            reparameterization_type=reparameterization.NOT_REPARAMETERIZED,\n",
        "            parameters=dict(initial_mean=self._initial_mean,\n",
        "                            initial_covariance=self._initial_covariance,\n",
        "                            dynamics_matrix=self._dynamics_matrix,\n",
        "                            dynamics_bias=self._dynamics_bias,\n",
        "                            dynamics_noise_covariance=self._dynamics_noise_covariance,\n",
        "                            emissions_means=self._emissions_means,\n",
        "                            emissions_covariances=self._emissions_covariances,\n",
        "                            log_normalizer=self._log_normalizer,\n",
        "                            filtered_means=self._filtered_means,\n",
        "                            filtered_covariances=self._filtered_covariances,\n",
        "                            smoothed_means=self._smoothed_means,\n",
        "                            smoothed_covariances=self._smoothed_covariances,\n",
        "                            smoothed_cross=self._smoothed_cross),\n",
        "            name=name,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _parameter_properties(cls, dtype, num_classes=None):\n",
        "        # pylint: disable=g-long-lambda\n",
        "        return dict(initial_mean=tfp.internal.parameter_properties.ParameterProperties(event_ndims=1),\n",
        "                    initial_covariance=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    dynamics_matrix=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    dynamics_bias=tfp.internal.parameter_properties.ParameterProperties(event_ndims=1),\n",
        "                    dynamics_noise_covariance=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    emissions_means=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    emissions_covariances=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "                    log_normalizer=tfp.internal.parameter_properties.ParameterProperties(event_ndims=0),\n",
        "                    filtered_means=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    filtered_covariances=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "                    smoothed_means=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    smoothed_covariances=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "                    smoothed_cross=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3)\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def infer_from_dynamics_and_potential(cls, dynamics_params, emissions_potentials):\n",
        "        p = dynamics_params\n",
        "        mus, Sigmas = emissions_potentials[\"mu\"], emissions_potentials[\"Sigma\"]\n",
        "\n",
        "        dim = mus.shape[-1]\n",
        "        C = np.eye(dim)\n",
        "        d = np.zeros(dim)\n",
        "\n",
        "        params = make_lgssm_params(p[\"m1\"], p[\"Q1\"], p[\"A\"], p[\"Q\"], C, Sigmas, \n",
        "                                   dynamics_bias=p[\"b\"], emissions_bias=d)\n",
        "\n",
        "        smoothed = lgssm_smoother(params, mus)._asdict()\n",
        "        \n",
        "        # Compute ExxT\n",
        "        A, Q = dynamics_params[\"A\"], dynamics_params[\"Q\"]\n",
        "        filtered_cov = smoothed[\"filtered_covariances\"]\n",
        "        filtered_mean = smoothed[\"smoothed_means\"]\n",
        "        smoothed_cov = smoothed[\"smoothed_covariances\"]\n",
        "        smoothed_mean = smoothed[\"smoothed_means\"]\n",
        "        G = vmap(lambda C: psd_solve(Q + A @ C @ A.T, A @ C).T)(filtered_cov)\n",
        "\n",
        "        # Compute the smoothed expectation of z_t z_{t+1}^T\n",
        "        smoothed_cross = vmap(\n",
        "            lambda Gt, mean, next_mean, next_cov: Gt @ next_cov + np.outer(mean, next_mean))\\\n",
        "            (G[:-1], smoothed_mean[:-1], smoothed_mean[1:], smoothed_cov[1:])\n",
        "\n",
        "        log_Z = lgssm_log_normalizer(dynamics_params, \n",
        "                                     smoothed[\"filtered_means\"], \n",
        "                                     smoothed[\"filtered_covariances\"], \n",
        "                                     emissions_potentials)\n",
        "\n",
        "        return cls(dynamics_params[\"m1\"],\n",
        "                   dynamics_params[\"Q1\"],\n",
        "                   dynamics_params[\"A\"],\n",
        "                   dynamics_params[\"b\"],\n",
        "                   dynamics_params[\"Q\"],\n",
        "                   emissions_potentials[\"mu\"],\n",
        "                   emissions_potentials[\"Sigma\"],\n",
        "                   log_Z, # smoothed[\"marginal_loglik\"],\n",
        "                   smoothed[\"filtered_means\"],\n",
        "                   smoothed[\"filtered_covariances\"],\n",
        "                   smoothed[\"smoothed_means\"],\n",
        "                   smoothed[\"smoothed_covariances\"],\n",
        "                   smoothed_cross)\n",
        "        \n",
        "    # Properties to get private class variables\n",
        "    @property\n",
        "    def log_normalizer(self):\n",
        "        return self._log_normalizer\n",
        "\n",
        "    @property\n",
        "    def filtered_means(self):\n",
        "        return self._filtered_means\n",
        "\n",
        "    @property\n",
        "    def filtered_covariances(self):\n",
        "        return self._filtered_covariances\n",
        "\n",
        "    @property\n",
        "    def smoothed_means(self):\n",
        "        return self._smoothed_means\n",
        "\n",
        "    @property\n",
        "    def smoothed_covariances(self):\n",
        "        return self._smoothed_covariances\n",
        "\n",
        "    @property\n",
        "    def expected_states(self):\n",
        "        return self._smoothed_means\n",
        "\n",
        "    @property\n",
        "    def expected_states_squared(self):\n",
        "        Ex = self._smoothed_means\n",
        "        return self._smoothed_covariances + np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "\n",
        "    @property\n",
        "    def expected_states_next_states(self):\n",
        "        return self._smoothed_cross\n",
        "\n",
        "    def _mean(self):\n",
        "        return self.smoothed_means\n",
        "\n",
        "    def _covariance(self):\n",
        "        return self.smoothed_covariances\n",
        "    \n",
        "    # TODO: currently this function does not depend on the dynamics bias\n",
        "    def _log_prob(self, data, **kwargs):\n",
        "        A = self._dynamics_matrix #params[\"A\"]\n",
        "        Q = self._dynamics_noise_covariance #params[\"Q\"]\n",
        "        Q1 = self._initial_covariance #params[\"Q1\"]\n",
        "        m1 = self._initial_mean #params[\"m1\"]\n",
        "\n",
        "        num_batch_dims = len(data.shape) - 2\n",
        "\n",
        "        ll = np.sum(\n",
        "            MVN(loc=np.einsum(\"ij,...tj->...ti\", A, data[...,:-1,:]), \n",
        "                covariance_matrix=Q).log_prob(data[...,1:,:])\n",
        "            )\n",
        "        ll += MVN(loc=m1, covariance_matrix=Q1).log_prob(data[...,0,:])\n",
        "\n",
        "        # Add the observation potentials\n",
        "        # ll += - 0.5 * np.einsum(\"...ti,tij,...tj->...\", data, self._emissions_precisions, data) \\\n",
        "        #       + np.einsum(\"...ti,ti->...\", data, self._emissions_linear_potentials)\n",
        "        ll += np.sum(MVN(loc=self._emissions_means, \n",
        "                  covariance_matrix=self._emissions_covariances).log_prob(data), axis=-1)\n",
        "        # Add the log normalizer\n",
        "        ll -= self._log_normalizer\n",
        "\n",
        "        return ll\n",
        "\n",
        "    def _sample_n(self, n, seed=None):\n",
        "\n",
        "        F = self._dynamics_matrix\n",
        "        b = self._dynamics_bias\n",
        "        Q = self._dynamics_noise_covariance\n",
        "        \n",
        "        def sample_single(\n",
        "            key,\n",
        "            filtered_means,\n",
        "            filtered_covariances\n",
        "        ):\n",
        "\n",
        "            initial_elements = _make_associative_sampling_elements(\n",
        "                { \"A\": F, \"b\": b, \"Q\": Q }, key, filtered_means, filtered_covariances)\n",
        "\n",
        "            @vmap\n",
        "            def sampling_operator(elem1, elem2):\n",
        "                E1, h1 = elem1\n",
        "                E2, h2 = elem2\n",
        "\n",
        "                E = E2 @ E1\n",
        "                h = E2 @ h1 + h2\n",
        "                return E, h\n",
        "\n",
        "            _, sample = \\\n",
        "                lax.associative_scan(sampling_operator, initial_elements, reverse=True)\n",
        "                \n",
        "            return sample\n",
        "\n",
        "        # TODO: Handle arbitrary batch shapes\n",
        "        if self._filtered_covariances.ndim == 4:\n",
        "            # batch mode\n",
        "            samples = vmap(vmap(sample_single, in_axes=(None, 0, 0)), in_axes=(0, None, None))\\\n",
        "                (jr.split(seed, n), self._filtered_means, self._filtered_covariances)\n",
        "            # Transpose to be (num_samples, num_batches, num_timesteps, dim)\n",
        "            # samples = np.transpose(samples, (1, 0, 2, 3))\n",
        "        else:\n",
        "            # non-batch mode\n",
        "            samples = vmap(sample_single, in_axes=(0, None, None))\\\n",
        "                (jr.split(seed, n), self._filtered_means, self._filtered_covariances)\n",
        "        return samples\n",
        "\n",
        "    def _entropy(self):\n",
        "        \"\"\"\n",
        "        Compute the entropy\n",
        "\n",
        "            H[X] = -E[\\log p(x)]\n",
        "                 = -E[-1/2 x^T J x + x^T h - log Z(J, h)]\n",
        "                 = 1/2 <J, E[x x^T] - <h, E[x]> + log Z(J, h)\n",
        "        \"\"\"\n",
        "        Ex = self.expected_states\n",
        "        ExxT = self.expected_states_squared\n",
        "        ExnxT = self.expected_states_next_states\n",
        "        p = dynamics_to_tridiag(\n",
        "            {\n",
        "                \"m1\": self._initial_mean,\n",
        "                \"Q1\": self._initial_covariance,\n",
        "                \"A\": self._dynamics_matrix,\n",
        "                \"b\": self._dynamics_bias,\n",
        "                \"Q\": self._dynamics_noise_covariance,\n",
        "            }, Ex.shape[0], Ex.shape[1]\n",
        "        )\n",
        "        J_diag = p[\"J\"] + solve(self._emissions_covariances, np.eye(Ex.shape[-1])[None])\n",
        "        J_lower_diag = p[\"L\"]\n",
        "\n",
        "        Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "        Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "        entropy = 0.5 * np.sum(J_diag * Sigmatt)\n",
        "        entropy += np.sum(J_lower_diag * Sigmatnt)\n",
        "        return entropy - self.log_prob(Ex)\n",
        "\n",
        "class ParallelLinearGaussianSSM(LinearGaussianSSM):\n",
        "    def __init__(self, *args, **kwargs) -> None:\n",
        "        kwargs[\"name\"] = \"ParallelLinearGaussianSSM\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def infer_from_dynamics_and_potential(cls, dynamics_params, emissions_potentials):\n",
        "        # p = dynamics_params\n",
        "        # mus, Sigmas = emissions_potentials[\"mu\"], emissions_potentials[\"Sigma\"]\n",
        "\n",
        "        # dim = mus.shape[-1]\n",
        "        # C = np.eye(dim)\n",
        "        # d = np.zeros(dim)\n",
        "\n",
        "        smoothed = parallel_lgssm_smoother(dynamics_params, emissions_potentials)\n",
        "        \n",
        "        # Compute ExxT\n",
        "        A, Q = dynamics_params[\"A\"], dynamics_params[\"Q\"]\n",
        "        filtered_cov = smoothed[\"filtered_covariances\"]\n",
        "        filtered_mean = smoothed[\"smoothed_means\"]\n",
        "        smoothed_cov = smoothed[\"smoothed_covariances\"]\n",
        "        smoothed_mean = smoothed[\"smoothed_means\"]\n",
        "        G = vmap(lambda C: psd_solve(Q + A @ C @ A.T, A @ C).T)(filtered_cov)\n",
        "\n",
        "        # Compute the smoothed expectation of z_t z_{t+1}^T\n",
        "        smoothed_cross = vmap(\n",
        "            lambda Gt, mean, next_mean, next_cov: Gt @ next_cov + np.outer(mean, next_mean))\\\n",
        "            (G[:-1], smoothed_mean[:-1], smoothed_mean[1:], smoothed_cov[1:])\n",
        "\n",
        "        log_Z = lgssm_log_normalizer(dynamics_params, \n",
        "                                     smoothed[\"filtered_means\"], \n",
        "                                     smoothed[\"filtered_covariances\"], \n",
        "                                     emissions_potentials)\n",
        "\n",
        "        return cls(dynamics_params[\"m1\"],\n",
        "                   dynamics_params[\"Q1\"],\n",
        "                   dynamics_params[\"A\"],\n",
        "                   dynamics_params[\"b\"],\n",
        "                   dynamics_params[\"Q\"],\n",
        "                   emissions_potentials[\"mu\"],\n",
        "                   emissions_potentials[\"Sigma\"],\n",
        "                   log_Z, # smoothed[\"marginal_loglik\"],\n",
        "                   smoothed[\"filtered_means\"],\n",
        "                   smoothed[\"filtered_covariances\"],\n",
        "                   smoothed[\"smoothed_means\"],\n",
        "                   smoothed[\"smoothed_covariances\"],\n",
        "                   smoothed_cross)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "HOdprEnnTpSN"
      },
      "outputs": [],
      "source": [
        "# @title Parallel versions of the same priors and posteriors\n",
        "\n",
        "# Super simple because all the machinary is already taken care of\n",
        "class LDSSVAEPosterior(SVAEPrior):\n",
        "    def __init__(self, latent_dims, seq_len, use_parallel=False):\n",
        "        self.latent_dims = latent_dims\n",
        "        # The only annoying thing is that we have to specify the sequence length\n",
        "        # ahead of time\n",
        "        self.seq_len = seq_len\n",
        "        self.dist = ParallelLinearGaussianSSM if use_parallel else LinearGaussianSSM\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return (self.seq_len, self.latent_dims)\n",
        "\n",
        "    # Must be the full set of constrained parameters!\n",
        "    def distribution(self, p):\n",
        "        m1, Q1, A, b, Q, mus, Sigmas = p[\"m1\"], p[\"Q1\"], p[\"A\"], p[\"b\"], p[\"Q\"], p[\"mu\"], p[\"Sigma\"]\n",
        "        log_Z, mu_filtered, Sigma_filtered = p[\"log_Z\"], p[\"mu_filtered\"], p[\"Sigma_filtered\"]\n",
        "        mu_smoothed, Sigma_smoothed, ExnxT = p[\"mu_smoothed\"], p[\"Sigma_smoothed\"], p[\"ExnxT\"]\n",
        "        return self.dist(m1, Q1, A, b, Q, mus, Sigmas, \n",
        "                             log_Z, mu_filtered, Sigma_filtered, \n",
        "                             mu_smoothed, Sigma_smoothed, ExnxT)\n",
        "\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        p = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"Q1\": np.eye(D),\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": np.eye(D),\n",
        "            \"Sigma\": np.tile(np.eye(D)[None], (T, 1, 1)),\n",
        "            \"mu\": np.zeros((T, D))\n",
        "        }\n",
        "\n",
        "        dist = self.dist.infer_from_dynamics_and_potential(p, \n",
        "                                    {\"mu\": p[\"mu\"], \"Sigma\": p[\"Sigma\"]})\n",
        "        \n",
        "        p.update({\n",
        "            \"log_Z\": dist.log_normalizer,\n",
        "            \"mu_filtered\": dist.filtered_means,\n",
        "            \"Sigma_filtered\": dist.filtered_covariances,\n",
        "            \"mu_smoothed\": dist.smoothed_means,\n",
        "            \"Sigma_smoothed\": dist.smoothed_covariances,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        return params\n",
        "\n",
        "    def infer(self, prior_params, potential_params):\n",
        "        p = {\n",
        "            \"m1\": prior_params[\"m1\"],\n",
        "            \"Q1\": prior_params[\"Q1\"],\n",
        "            \"A\": prior_params[\"A\"],\n",
        "            \"b\": prior_params[\"b\"],\n",
        "            \"Q\": prior_params[\"Q\"],\n",
        "            \"Sigma\": potential_params[\"Sigma\"],\n",
        "            \"mu\": potential_params[\"mu\"]\n",
        "        }\n",
        "\n",
        "        dist = self.dist.infer_from_dynamics_and_potential(prior_params, \n",
        "                                    {\"mu\": p[\"mu\"], \"Sigma\": p[\"Sigma\"]})\n",
        "        p.update({\n",
        "            \"log_Z\": dist.log_normalizer,\n",
        "            \"mu_filtered\": dist.filtered_means,\n",
        "            \"Sigma_filtered\": dist.filtered_covariances,\n",
        "            \"mu_smoothed\": dist.smoothed_means,\n",
        "            \"Sigma_smoothed\": dist.smoothed_covariances,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtvHU_dlOklC"
      },
      "source": [
        "## Define neural network architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "40wvAwfoOn_E"
      },
      "outputs": [],
      "source": [
        "# @title Neural network utils\n",
        "\n",
        "PRNGKey = Any\n",
        "Shape = Iterable[int]\n",
        "Dtype = Any  # this could be a real type?\n",
        "Array = Any\n",
        "\n",
        "# Note: the last layer output does not have a relu activation!\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Define a simple fully connected MLP with ReLU activations.\n",
        "    \"\"\"\n",
        "    features: Sequence[int]\n",
        "    kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.he_normal()\n",
        "    bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.zeros\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for feat in self.features[:-1]:\n",
        "            x = nn.relu(nn.Dense(feat, \n",
        "                kernel_init=self.kernel_init,\n",
        "                bias_init=self.bias_init,)(x))\n",
        "        x = nn.Dense(self.features[-1], \n",
        "            kernel_init=self.kernel_init, \n",
        "            bias_init=self.bias_init)(x)\n",
        "        return x\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    \"\"\"\n",
        "    A layer which passes the input through unchanged.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "class Static(nn.Module):\n",
        "    \"\"\"\n",
        "    A layer which just returns some static parameters.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "    kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.lecun_normal()\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        kernel = self.param('kernel',\n",
        "                            self.kernel_init,\n",
        "                            (self.features, ))\n",
        "        return kernel\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"A simple CNN model.\"\"\"\n",
        "    input_rank : int = None   \n",
        "    output_dim : int = None\n",
        "    layer_params : Sequence[dict] = None\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for params in self.layer_params:\n",
        "            x = nn.relu(Conv(**params)(x))\n",
        "        # No activations at the output\n",
        "        x = nn.Dense(features=self.output_dim)(x.flatten())\n",
        "        return x\n",
        "\n",
        "class DCNN(nn.Module):\n",
        "    \"\"\"A simple DCNN model.\"\"\"   \n",
        "\n",
        "    input_shape: Sequence[int] = None\n",
        "    layer_params: Sequence[dict] = None\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        input_features = onp.prod(onp.array(self.input_shape))\n",
        "        x = nn.Dense(features=input_features)(x)\n",
        "        x = x.reshape(self.input_shape)\n",
        "        # Note that the last layer doesn't have an activation\n",
        "        for params in self.layer_params:\n",
        "            x = ConvTranspose(**params)(nn.relu(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Ja-LJ88rybCV"
      },
      "outputs": [],
      "source": [
        "# @title Potential networks (outputs potentials on single observations)\n",
        "class PotentialNetwork(nn.Module):\n",
        "    def __call__(self, inputs):\n",
        "        Sigma, mu = self._generate_distribution_parameters(inputs)\n",
        "        # J, h = solve(Sigma, np.eye(mu.shape[-1])[None]), solve(Sigma, mu)\n",
        "        # if (len(J.shape) == 3):\n",
        "        #     seq_len, latent_dims, _ = J.shape\n",
        "        #     # lower diagonal blocks of precision matrix\n",
        "        #     L = np.zeros((seq_len-1, latent_dims, latent_dims))\n",
        "        # elif (len(J.shape) == 4):\n",
        "        #     batch_size, seq_len, latent_dims, _ = J.shape\n",
        "        #     # lower diagonal blocks of precision matrix\n",
        "        #     L = np.zeros((batch_size, seq_len-1, latent_dims, latent_dims))\n",
        "        # else:\n",
        "        #     L = np.zeros(tuple())\n",
        "        return {#\"J\": J, \"L\": L, \"h\": h, \n",
        "                \"Sigma\": Sigma, \"mu\": mu}\n",
        "\n",
        "    def _generate_distribution_parameters(self, inputs):\n",
        "        if (len(inputs.shape) == self.input_rank + 2):\n",
        "            # We have both a batch dimension and a time dimension\n",
        "            # and we have to vmap over both...!\n",
        "            return vmap(vmap(self._call_single, 0), 0)(inputs)\n",
        "        elif (len(inputs.shape) == self.input_rank + 1):\n",
        "            return vmap(self._call_single)(inputs)\n",
        "        elif (len(inputs.shape) == self.input_rank):\n",
        "            return self._call_single(inputs)\n",
        "        else:\n",
        "            # error\n",
        "            return None\n",
        "\n",
        "    def _call_single(self, inputs):\n",
        "        pass\n",
        "\n",
        "# A new, more general implementation of the Gaussian recognition network\n",
        "# Uses mean parameterization which works better empirically\n",
        "class GaussianRecognition(PotentialNetwork):\n",
        "\n",
        "    use_diag : int = None\n",
        "    input_rank : int = None\n",
        "    latent_dims : int = None\n",
        "    trunk_fn : nn.Module = None\n",
        "    head_mean_fn : nn.Module = None\n",
        "    head_log_var_fn : nn.Module = None\n",
        "    eps : float = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_params(cls, input_rank=1, input_dim=None, output_dim=None, \n",
        "                    trunk_type=\"Identity\", trunk_params=None, \n",
        "                    head_mean_type=\"MLP\", head_mean_params=None,\n",
        "                    head_var_type=\"MLP\", head_var_params=None, diagonal_covariance=False,\n",
        "                    cov_init=1, eps=1e-3): \n",
        "\n",
        "        if trunk_type == \"Identity\":\n",
        "            trunk_params = { \"features\": input_dim }\n",
        "        if head_mean_type == \"MLP\":\n",
        "            head_mean_params[\"features\"] += [output_dim]\n",
        "        if head_var_type == \"MLP\":\n",
        "            if (diagonal_covariance):\n",
        "                head_var_params[\"features\"] += [output_dim]\n",
        "            else:\n",
        "                head_var_params[\"features\"] += [output_dim * (output_dim + 1) // 2]\n",
        "            head_var_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_var_params[\"bias_init\"] = nn.initializers.constant(cov_init)\n",
        "\n",
        "        trunk_fn = globals()[trunk_type](**trunk_params)\n",
        "        head_mean_fn = globals()[head_mean_type](**head_mean_params)\n",
        "        head_log_var_fn = globals()[head_var_type](**head_var_params)\n",
        "\n",
        "        return cls(diagonal_covariance, input_rank, output_dim, trunk_fn, \n",
        "                   head_mean_fn, head_log_var_fn, eps)\n",
        "\n",
        "    def _call_single(self, inputs):\n",
        "        # Apply the trunk.\n",
        "        trunk_output = self.trunk_fn(inputs)\n",
        "        # Get the mean.\n",
        "        mu = self.head_mean_fn(trunk_output)\n",
        "        # Get the covariance parameters and build a full matrix from it.\n",
        "        var_output_flat = self.head_log_var_fn(trunk_output)\n",
        "        if self.use_diag:\n",
        "            Sigma = np.diag(softplus(var_output_flat) + self.eps)\n",
        "        else:\n",
        "            Sigma = lie_params_to_constrained(var_output_flat, self.latent_dims, self.eps)\n",
        "        # h = np.linalg.solve(Sigma, mu)\n",
        "        # J = np.linalg.inv(Sigma)\n",
        "        # lower diagonal blocks of precision matrix\n",
        "        return (Sigma, mu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "gmeCrsbCy96H"
      },
      "outputs": [],
      "source": [
        "# @title Posterior networks (outputs full posterior for entire sequence)\n",
        "# Outputs Gaussian distributions for the entire sequence at once\n",
        "class PosteriorNetwork(PotentialNetwork):\n",
        "    def __call__(self, inputs):\n",
        "        As, bs, Qs = self._generate_distribution_parameters(inputs)\n",
        "        return {\"As\": As, \"bs\": bs, \"Qs\": Qs}\n",
        "\n",
        "    def _generate_distribution_parameters(self, inputs):\n",
        "        is_batched = (len(inputs.shape) == self.input_rank+2)\n",
        "        if is_batched:\n",
        "            return vmap(self._call_single, in_axes=0)(inputs)\n",
        "        else:\n",
        "            assert(len(inputs.shape) == self.input_rank+1)\n",
        "            return self._call_single(inputs)\n",
        "\n",
        "class GaussianBiRNN(PosteriorNetwork):\n",
        "    \n",
        "    input_rank : int = None\n",
        "    rnn_dim : int = None\n",
        "    output_dim : int = None\n",
        "    forward_RNN : nn.Module = None\n",
        "    backward_RNN : nn.Module = None\n",
        "    input_fn : nn.Module = None\n",
        "    trunk_fn: nn.Module = None\n",
        "    head_mean_fn : nn.Module = None\n",
        "    head_log_var_fn : nn.Module = None\n",
        "    head_dyn_fn : nn.Module = None\n",
        "    eps : float = None\n",
        "        \n",
        "    @classmethod\n",
        "    def from_params(cls, input_rank=1, cell_type=nn.GRUCell,\n",
        "                    input_dim=None, rnn_dim=None, output_dim=None, \n",
        "                    input_type=\"MLP\", input_params=None,\n",
        "                    trunk_type=\"Identity\", trunk_params=None, \n",
        "                    head_mean_type=\"MLP\", head_mean_params=None,\n",
        "                    head_var_type=\"MLP\", head_var_params=None,\n",
        "                    head_dyn_type=\"MLP\", head_dyn_params=None,\n",
        "                    cov_init=1, eps=1e-4): \n",
        "\n",
        "        forward_RNN = nn.scan(cell_type, variable_broadcast=\"params\", \n",
        "                                             split_rngs={\"params\": False})()\n",
        "        backward_RNN = nn.scan(cell_type, variable_broadcast=\"params\", \n",
        "                                               split_rngs={\"params\": False}, reverse=True)()\n",
        "        if trunk_type == \"Identity\":\n",
        "            trunk_params = { \"features\": rnn_dim }\n",
        "        if input_type == \"MLP\":\n",
        "            input_params[\"features\"] += [rnn_dim]\n",
        "        if head_mean_type == \"MLP\":\n",
        "            head_mean_params[\"features\"] += [output_dim]\n",
        "        if head_var_type == \"MLP\":\n",
        "            head_var_params[\"features\"] += [output_dim * (output_dim + 1) // 2]\n",
        "            head_var_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_var_params[\"bias_init\"] = nn.initializers.constant(cov_init)\n",
        "        if head_dyn_type == \"MLP\":\n",
        "            head_dyn_params[\"features\"] += [output_dim ** 2,]\n",
        "            head_dyn_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_dyn_params[\"bias_init\"] = nn.initializers.zeros\n",
        "\n",
        "        trunk_fn = globals()[trunk_type](**trunk_params)\n",
        "        input_fn = globals()[input_type](**input_params)\n",
        "        head_mean_fn = globals()[head_mean_type](**head_mean_params)\n",
        "        head_log_var_fn = globals()[head_var_type](**head_var_params)\n",
        "        head_dyn_fn = globals()[head_dyn_type](**head_dyn_params)\n",
        "\n",
        "        return cls(input_rank, rnn_dim, output_dim, \n",
        "                   forward_RNN, backward_RNN, \n",
        "                   input_fn, trunk_fn, \n",
        "                   head_mean_fn, head_log_var_fn, head_dyn_fn, eps)\n",
        "\n",
        "    # Applied the BiRNN to a single sequence of inputs\n",
        "    def _call_single(self, inputs):\n",
        "        output_dim = self.output_dim\n",
        "        \n",
        "        inputs = vmap(self.input_fn)(inputs)\n",
        "        init_carry_forward = np.zeros((self.rnn_dim,))\n",
        "        _, out_forward = self.forward_RNN(init_carry_forward, inputs)\n",
        "        init_carry_backward = np.zeros((self.rnn_dim,))\n",
        "        _, out_backward = self.backward_RNN(init_carry_backward, inputs)\n",
        "        # Concatenate the forward and backward outputs\n",
        "        out_combined = np.concatenate([out_forward, out_backward], axis=-1)\n",
        "        \n",
        "        # Get the mean.\n",
        "        # vmap over the time dimension\n",
        "        b = vmap(self.head_mean_fn)(out_combined)\n",
        "\n",
        "        # Get the variance output and reshape it.\n",
        "        # vmap over the time dimension\n",
        "        var_output_flat = vmap(self.head_log_var_fn)(out_combined)\n",
        "        Q = vmap(lie_params_to_constrained, in_axes=(0, None, None))\\\n",
        "            (var_output_flat, output_dim, self.eps)\n",
        "        dynamics_flat = vmap(self.head_dyn_fn)(out_combined)\n",
        "        A = dynamics_flat.reshape((-1, output_dim, output_dim))\n",
        "\n",
        "        return (A, b, Q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "cellView": "form",
        "id": "caU8H-hMEB_f"
      },
      "outputs": [],
      "source": [
        "# @title Special architectures for PlaNet\n",
        "class PlaNetRecognitionWrapper:\n",
        "    def __init__(self, rec_net):\n",
        "        self.rec_net = rec_net\n",
        "\n",
        "    def init(self, key, *inputs):\n",
        "        return self.rec_net.init(key, *inputs)\n",
        "    \n",
        "    def apply(self, params, x):\n",
        "        return {\n",
        "            \"network_input\": self.rec_net.apply(params[\"rec_params\"], x)[\"h\"],\n",
        "            \"network_params\": params[\"post_params\"],\n",
        "        }\n",
        "\n",
        "class StochasticRNNCell(nn.Module):\n",
        "\n",
        "    output_dim : int = None\n",
        "    rnn_cell : nn.Module = None\n",
        "    trunk_fn: nn.Module = None\n",
        "    head_mean_fn : nn.Module = None\n",
        "    head_log_var_fn : nn.Module = None\n",
        "    eps : float = None\n",
        "        \n",
        "    @classmethod\n",
        "    def from_params(cls, cell_type=nn.GRUCell,\n",
        "                    rnn_dim=None, output_dim=None, \n",
        "                    trunk_type=\"Identity\", trunk_params=None, \n",
        "                    head_mean_type=\"MLP\", head_mean_params=None,\n",
        "                    head_var_type=\"MLP\", head_var_params=None,\n",
        "                    cov_init=1, eps=1e-4, **kwargs): \n",
        "\n",
        "        rnn_cell = cell_type()\n",
        "\n",
        "        if trunk_type == \"Identity\":\n",
        "            trunk_params = { \"features\": rnn_dim }\n",
        "        if head_mean_type == \"MLP\":\n",
        "            head_mean_params[\"features\"] += [output_dim]\n",
        "        if head_var_type == \"MLP\":\n",
        "            head_var_params[\"features\"] += [output_dim * (output_dim + 1) // 2]\n",
        "            head_var_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_var_params[\"bias_init\"] = nn.initializers.constant(cov_init)\n",
        "\n",
        "        trunk_fn = globals()[trunk_type](**trunk_params)\n",
        "        head_mean_fn = globals()[head_mean_type](**head_mean_params)\n",
        "        head_log_var_fn = globals()[head_var_type](**head_var_params)\n",
        "\n",
        "        return cls(output_dim, rnn_cell, trunk_fn, head_mean_fn, head_log_var_fn, eps)\n",
        "\n",
        "    # h: latent state that's carried to the next\n",
        "    # x: last sample\n",
        "    # u: input at this timestep\n",
        "    def __call__(self, h, x, u):\n",
        "        h, out = self.rnn_cell(h, np.concatenate([x, u]))\n",
        "        out = self.trunk_fn(out)\n",
        "        mean, cov_flat = self.head_mean_fn(out), self.head_log_var_fn(out)\n",
        "        cov = lie_params_to_constrained(cov_flat, self.output_dim, self.eps)\n",
        "        return h, (cov, mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "_DU0-NEWx_Il"
      },
      "outputs": [],
      "source": [
        "# @title Emission network (outputs distribution instead of parameters)\n",
        "\n",
        "# This is largely for convenience\n",
        "class GaussianEmission(GaussianRecognition):\n",
        "    def __call__(self, inputs):\n",
        "        J, h = self._generate_distribution_parameters(inputs)\n",
        "        # TODO: inverting J is pretty bad numerically, perhaps save Cholesky instead?\n",
        "        if (len(J.shape) == 3):\n",
        "            Sigma = vmap(inv)(J)\n",
        "            mu = np.einsum(\"tij,tj->ti\", Sigma, h)\n",
        "        elif (len(J.shape) == 2):\n",
        "            Sigma = inv(J)\n",
        "            mu = np.linalg.solve(J, h)\n",
        "        else:\n",
        "            # Error\n",
        "            return None\n",
        "        return tfd.MultivariateNormalFullCovariance(\n",
        "            loc=mu, covariance_matrix=Sigma)\n",
        "        \n",
        "class GaussianDCNNEmission(PotentialNetwork):\n",
        "\n",
        "    input_rank : int = None\n",
        "    network : nn.Module = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_params(cls, **params):\n",
        "        network = DCNN(**params)\n",
        "        return cls(1, network)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        out = self._generate_distribution_parameters(inputs)\n",
        "        mu = out[\"mu\"]\n",
        "        # Adding a constant to prevent the model from getting too crazy\n",
        "        sigma = out[\"sigma\"] + 1e-4\n",
        "        return tfd.Normal(loc=mu, scale=sigma)\n",
        "\n",
        "    def _call_single(self, x):\n",
        "        out_raw = self.network(x)\n",
        "        mu_raw, sigma_raw = np.split(out_raw, 2, axis=-1)\n",
        "        # Get rid of the Sigmoid\n",
        "        # mu = sigmoid(mu_raw)\n",
        "        mu = mu_raw\n",
        "        sigma = softplus(sigma_raw)\n",
        "        # sigma = np.ones_like(mu) * 0.1\n",
        "        return { \"mu\": mu, \"sigma\": sigma }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xevru2BSSSZ"
      },
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "cellView": "form",
        "id": "28XHvF41SVWK"
      },
      "outputs": [],
      "source": [
        "# @title Visualization/animation helpers\n",
        "\n",
        "# Returns a random projection matrix from ND to 2D\n",
        "def random_projection(seed, N):\n",
        "    key1, key2 = jr.split(seed, 2)\n",
        "    v1 = jr.normal(key1, (N,))\n",
        "    v2 = jr.normal(key2, (N,))\n",
        "\n",
        "    v1 /= np.linalg.norm(v1)\n",
        "    v2 -= v1 * np.dot(v1, v2)\n",
        "    v2 /= np.linalg.norm(v2)\n",
        "\n",
        "    return np.stack([v1, v2])\n",
        "\n",
        "def get_gaussian_draw_params(mu, Sigma, proj_seed=None):\n",
        "\n",
        "    Sigma = (Sigma + Sigma.T) * .5\n",
        "\n",
        "    if (mu.shape[0] > 2):\n",
        "        P = random_projection(proj_seed, mu.shape[0])\n",
        "        mu = P @ mu\n",
        "    angles = np.hstack([np.arange(0, 2*np.pi, 0.01), 0])\n",
        "    circle = np.vstack([np.sin(angles), np.cos(angles)])\n",
        "    min_eig = np.min(eigh(Sigma)[0])\n",
        "    eps = 1e-6\n",
        "    if (min_eig <= eps): Sigma += np.eye(Sigma.shape[0]) * eps\n",
        "    L = np.linalg.cholesky(Sigma)\n",
        "    u, svs, vt = svd(P @ L)\n",
        "    ellipse = np.dot(u * svs, circle) * 2\n",
        "    return (mu[0], mu[1]), (ellipse[0, :] + mu[0], ellipse[1, :] + mu[1])\n",
        "\n",
        "def plot_gaussian_2D(mu, Sigma, proj_seed=None, ax=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Helper function to plot 2D Gaussian contours\n",
        "    \"\"\"\n",
        "    (px, py), (exs, eys) = get_gaussian_draw_params(mu, Sigma, proj_seed)\n",
        "\n",
        "    ax = plt.gca() if ax is None else ax\n",
        "    point = ax.plot(px, py, marker='D', **kwargs)\n",
        "    line, = ax.plot(exs, eys, **kwargs)\n",
        "    return (point, line)\n",
        "\n",
        "def get_artists(ax, mus, Sigmas, proj_seed, num_pts, **draw_params):\n",
        "    point_artists = []\n",
        "    line_artists = []\n",
        "\n",
        "    for j in range(num_pts):\n",
        "        mean_params, cov_params = get_gaussian_draw_params(mus[j], \n",
        "                                                           Sigmas[j], \n",
        "                                                           proj_seed)\n",
        "        point = ax.plot(mean_params[0], mean_params[1], marker='D', \n",
        "                        color=colors[j], **draw_params)[0]\n",
        "        line = ax.plot(cov_params[0], cov_params[1], \n",
        "                       color=colors[j], **draw_params)[0]\n",
        "        point_artists.append(point)\n",
        "        line_artists.append(line)\n",
        "    return point_artists, line_artists\n",
        "\n",
        "def update_draw_params(point_artists, line_artists, mus, Sigmas, proj_seed, num_pts):\n",
        "    for i in range(num_pts):\n",
        "        mean_params, cov_params = get_gaussian_draw_params(mus[i], Sigmas[i], proj_seed)\n",
        "        point_artists[i].set_data(mean_params[0], mean_params[1])\n",
        "        line_artists[i].set_data(cov_params[0], cov_params[1])\n",
        "\n",
        "# Some animation helpers\n",
        "def animate_gaussians(inf_mus, inf_Sigmas, \n",
        "                      tgt_mus, tgt_Sigmas, \n",
        "                      true_mus, true_Sigmas,\n",
        "                      num_pts,\n",
        "                      proj_seed=None, x_lim=None, y_lim=None, **kwargs):\n",
        "    proj_seed = jr.PRNGKey(0) if proj_seed is None else proj_seed\n",
        "    print(\"Animating Gaussian blobs...!\")\n",
        "    # First set up the figure, the axis, and the plot element we want to animate\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlim(-10, 10)\n",
        "    ax.set_ylim(-10, 10)\n",
        "    plt.close()\n",
        "    \n",
        "\n",
        "    tgt_points, tgt_lines = get_artists(ax, tgt_mus[0], tgt_Sigmas[0], proj_seed, num_pts,\n",
        "                                        alpha=0.3, linestyle=\"dotted\")\n",
        "    inf_points, inf_lines = get_artists(ax, inf_mus[0], inf_Sigmas[0], proj_seed, num_pts)\n",
        "    true_points, true_lines = get_artists(ax, true_mus, true_Sigmas, \n",
        "                                          proj_seed, num_pts, alpha=0.2)\n",
        "\n",
        "    artists = tgt_points + tgt_lines + inf_points + inf_lines + true_points + true_lines\n",
        "\n",
        "    T = len(inf_mus)\n",
        "\n",
        "    # animation function. This is called sequentially  \n",
        "    def animate(i):\n",
        "        update_draw_params(tgt_points, tgt_lines, tgt_mus[i], tgt_Sigmas[i], proj_seed, num_pts)\n",
        "        update_draw_params(inf_points, inf_lines, inf_mus[i], inf_Sigmas[i], proj_seed, num_pts)\n",
        "        clear_output(wait=True)\n",
        "        print(\"Processing frame #{}/{}\".format(i+1, T))\n",
        "        return artists\n",
        "    \n",
        "    if x_lim is not None:\n",
        "        ax.set_xlim(x_lim)\n",
        "    if y_lim is not None:\n",
        "        ax.set_ylim(y_lim)\n",
        "\n",
        "    anim = animation.FuncAnimation(fig, animate, \n",
        "                                frames=T, interval=50, blit=True)\n",
        "    print(\"Frames created! Displaying animation in output cell...\")\n",
        "    # Note: below is the part which makes it work on Colab\n",
        "    rc('animation', html='jshtml')\n",
        "    return anim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "cellView": "form",
        "id": "EUG5JgVpTkC3"
      },
      "outputs": [],
      "source": [
        "# @title Helper for computing posterior marginals\n",
        "def get_emission_matrices(dec_params):\n",
        "    eps = 1e-4\n",
        "    dec_mean_params = dec_params[\"params\"][\"head_mean_fn\"][\"Dense_0\"]\n",
        "    dec_cov_params = dec_params[\"params\"][\"head_log_var_fn\"][\"Dense_0\"]\n",
        "    C_, d_ = dec_mean_params[\"kernel\"].T, dec_mean_params[\"bias\"]\n",
        "    R_ = np.diag(softplus(dec_cov_params[\"bias\"]) + eps)\n",
        "    return { \"C\": C_, \"d\": d_, \"R\": R_ }\n",
        "\n",
        "def get_marginals_and_targets(seed, data, num_points, model, \n",
        "                              past_params, true_model_params):\n",
        "    N, T = data.shape[:2]\n",
        "    rand_sample = jr.permutation(seed, onp.arange(N * T))[:num_points]\n",
        "    trials = rand_sample // T\n",
        "    times = rand_sample % T\n",
        "\n",
        "    # Compute a linear transformation in the latent space that will attempt to \n",
        "    # align the learned posterior to the true posterior\n",
        "    C, d = true_model_params[\"C\"], true_model_params[\"d\"]\n",
        "    CTC = C.T @ C\n",
        "    C_pinv = np.linalg.solve(CTC, C.T)\n",
        "\n",
        "    def align_latents(mus, Sigmas, p):\n",
        "        emissions_matrices = get_emission_matrices(p)\n",
        "        C_, d_ = emissions_matrices[\"C\"], emissions_matrices[\"d\"]\n",
        "        P = C_pinv @ C_\n",
        "        mus = np.einsum(\"ij,nj->ni\", P, mus) + (C_pinv @ (d_ - d))[None,:]\n",
        "        Sigmas = np.einsum(\"ij,njk,kl->nil\", P, Sigmas, P.T)\n",
        "        return mus, Sigmas\n",
        "\n",
        "    def posterior_mean_and_cov(post_params, t):\n",
        "        dist = model.posterior.distribution(post_params)\n",
        "        return dist.mean()[t], dist.covariance()[t]\n",
        "\n",
        "    def gaussian_posterior_mean_and_cov(post_params, t):\n",
        "        dist = true_lds.posterior.distribution(post_params)\n",
        "        return dist.mean()[t], dist.covariance()[t]\n",
        "\n",
        "    index_into_leaves = lambda l: l[trials]\n",
        "    \n",
        "    inf_mus = []\n",
        "    inf_Sigmas = []\n",
        "    tgt_mus = []\n",
        "    tgt_Sigmas = []\n",
        "    \n",
        "    true_lds = LDS(model.prior.latent_dims, T)\n",
        "    true_post_params = vmap(true_lds.e_step, in_axes=(None, 0))\\\n",
        "        (true_model_params, data[trials])\n",
        "    true_mus, true_Sigmas = vmap(gaussian_posterior_mean_and_cov)(true_post_params, times)\n",
        "\n",
        "    # TODO: this is temporary! Only for testing parallel KF!\n",
        "    base = LieParameterizedLinearGaussianChainPrior(model.prior.latent_dims, T)\n",
        "    model_lds = LDS(model.prior.latent_dims, T, base=base)\n",
        "\n",
        "    for i in range(len(past_params)):\n",
        "        model_params = past_params[i]\n",
        "        post_params = jax.tree_util.tree_map(index_into_leaves, \n",
        "                                             model_params[\"post_params\"])\n",
        "        dec_params = model_params[\"dec_params\"]\n",
        "        prior_params = deepcopy(model_params[\"prior_params\"])\n",
        "        # Compute posterior marginals\n",
        "        mus, Sigmas = vmap(posterior_mean_and_cov)(post_params, times)\n",
        "        mus, Sigmas = align_latents(mus, Sigmas, dec_params)\n",
        "        inf_mus.append(mus)\n",
        "        inf_Sigmas.append(Sigmas)\n",
        "\n",
        "        # Infer true posterior under current model params\n",
        "        prior_params.update(get_emission_matrices(dec_params))\n",
        "        tgt_post_params = vmap(model_lds.e_step, in_axes=(None, 0))(prior_params, data[trials])\n",
        "        mus, Sigmas = vmap(gaussian_posterior_mean_and_cov)(tgt_post_params, times)\n",
        "        mus, Sigmas = align_latents(mus, Sigmas, dec_params)\n",
        "\n",
        "        tgt_mus.append(mus)\n",
        "        tgt_Sigmas.append(Sigmas)\n",
        "    return inf_mus, inf_Sigmas, tgt_mus, tgt_Sigmas, true_mus, true_Sigmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "aTYdrgHMdJct"
      },
      "outputs": [],
      "source": [
        "# Trying to figure out the proper way of plotting the projection\n",
        "# Of high dimensional Gaussians\n",
        "# key = jr.split(key)[1]\n",
        "# dim = 5\n",
        "# Q = jr.uniform(key, shape=(dim, dim))\n",
        "# A = Q @ Q.T\n",
        "# L = cholesky(A)\n",
        "# P = random_projection(key_0, dim)\n",
        "# plot_gaussian_2D(np.zeros(dim), A, key_0)\n",
        "# points = jr.normal(key_0, (400, dim))\n",
        "# points /= np.sum(points ** 2, axis=1, keepdims=True) ** .5\n",
        "# points = P @ L @ points.T\n",
        "# plt.scatter(points[0, :], points[1, :], s=1)\n",
        "# u, svs, vt = svd(P @ L)\n",
        "# plt.scatter(u[0][0] * svs[0], u[1][0] * svs[0])\n",
        "# plt.scatter(u[0][1] * svs[1], u[1][1] * svs[1])\n",
        "# # plt.scatter(u[0][0] * np.sqrt(svs[0]), u[1][0] * np.sqrt(svs[0]))\n",
        "# # plt.scatter(u[0][1] * svs[1], u[1][1] * svs[1])\n",
        "# # P = random_projection(key_0, 3)\n",
        "# angles = np.hstack([np.arange(0, 2*np.pi, 0.01), 0])\n",
        "# circle = np.vstack([np.sin(angles), np.cos(angles)])\n",
        "# ellipse = np.dot(u * svs, circle) * 2\n",
        "# plt.plot(ellipse[0,:], ellipse[1,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ImmaouPD-G"
      },
      "source": [
        "## Define training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "cellView": "form",
        "id": "cPqoyXb6PgpV"
      },
      "outputs": [],
      "source": [
        "# @title Trainer object \n",
        "from time import time\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    model: a pytree node\n",
        "    loss (key, params, model, data, **train_params) -> (loss, aux)\n",
        "        Returns a loss (a single float) and an auxillary output (e.g. posterior)\n",
        "    init (key, model, data, **train_params) -> (params, opts)\n",
        "        Returns the initial parameters and optimizers to go with those parameters\n",
        "    update (params, grads, opts, model, aux, **train_params) -> (params, opts)\n",
        "        Returns updated parameters, optimizers\n",
        "    \"\"\"\n",
        "    def __init__(self, model, \n",
        "                 train_params=None, \n",
        "                 init=None, \n",
        "                 loss=None, \n",
        "                 val_loss=None,\n",
        "                 update=None,\n",
        "                 initial_params=None):\n",
        "        # Trainer state\n",
        "        self.params = initial_params\n",
        "        self.model = model\n",
        "        self.past_params = []\n",
        "        self.time_spent = []\n",
        "\n",
        "        if train_params is None:\n",
        "            train_params = dict()\n",
        "\n",
        "        self.train_params = train_params\n",
        "\n",
        "        if init is not None:\n",
        "            self.init = init\n",
        "        if loss is not None:\n",
        "            self.loss = loss\n",
        "\n",
        "        self.val_loss = val_loss or self.loss\n",
        "        if update is not None: \n",
        "            self.update = update\n",
        "\n",
        "    # @partial(jit, static_argnums=(0,))\n",
        "    def train_step(self, key, params, data, opt_states):\n",
        "        model = self.model\n",
        "        results = \\\n",
        "            jax.value_and_grad(\n",
        "                lambda params: partial(self.loss, **self.train_params)(key, model, data, params), has_aux=True)(params)\n",
        "        (loss, aux), grads = results\n",
        "        params, opts = self.update(params, grads, self.opts, opt_states, model, aux, **self.train_params)\n",
        "        return params, opts, (loss, aux), grads\n",
        "\n",
        "    # @partial(jit, static_argnums=(0,))\n",
        "    def val_step(self, key, params, data):\n",
        "        return self.val_loss(key, self.model, data, params, **self.train_params)\n",
        "\n",
        "    # def test_step(self, key, params, model, data):\n",
        "    #     loss_out = self.loss(key, params, model, data)\n",
        "    #     return loss_out\n",
        "\n",
        "    \"\"\"\n",
        "    Callback: a function that takes training iterations and relevant parameter\n",
        "        And logs to WandB\n",
        "    \"\"\"\n",
        "    def train(self, data_dict, max_iters, \n",
        "              callback=None, val_callback=None, \n",
        "              summary=None, key=None,\n",
        "              early_stop_start=5000, \n",
        "              max_lose_streak=1000):\n",
        "\n",
        "        if key is None:\n",
        "            key = jr.PRNGKey(0)\n",
        "\n",
        "        model = self.model\n",
        "        train_data = data_dict[\"train_data\"]\n",
        "        batch_size = self.train_params.get(\"batch_size\") or train_data.shape[0]\n",
        "        num_batches = train_data.shape[0] // batch_size\n",
        "\n",
        "        init_key, key = jr.split(key, 2)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.params, self.opts, self.opt_states = self.init(init_key, model, \n",
        "                                                       train_data[:batch_size], \n",
        "                                                       self.params,\n",
        "                                                       **self.train_params)\n",
        "        self.train_losses = []\n",
        "        self.test_losses = []\n",
        "        self.val_losses = []\n",
        "        self.past_params = []\n",
        "\n",
        "        pbar = trange(max_iters)\n",
        "        pbar.set_description(\"[jit compling...]\")\n",
        "        \n",
        "        mask_start = self.train_params.get(\"mask_start\")\n",
        "        if (mask_start):\n",
        "            mask_size = self.train_params[\"mask_size\"]\n",
        "            self.train_params[\"mask_size\"] = 0\n",
        "\n",
        "        train_step = jit(self.train_step)\n",
        "        val_step = jit(self.val_step)\n",
        "\n",
        "        best_loss = None\n",
        "        best_itr = 0\n",
        "        val_loss = None\n",
        "\n",
        "        for itr in pbar:\n",
        "            train_key, val_key, key = jr.split(key, 3)\n",
        "\n",
        "            batch_id = itr % num_batches\n",
        "            batch_start = batch_id * batch_size\n",
        "\n",
        "            t = time()\n",
        "            # Training step\n",
        "            # ----------------------------------------\n",
        "            step_results = train_step(train_key, self.params, \n",
        "                           train_data[batch_start:batch_start+batch_size], self.opt_states)\n",
        "            self.params, self.opt_states, loss_out, grads = \\\n",
        "                jax.tree_map(lambda x: x.block_until_ready(), step_results)\n",
        "            # ----------------------------------------\n",
        "            dt = time() - t\n",
        "            self.time_spent.append(dt)\n",
        "\n",
        "            loss, aux = loss_out\n",
        "            self.train_losses.append(loss)\n",
        "            pbar.set_description(\"LP: {:.3f}\".format(loss))\n",
        "\n",
        "            if batch_id == num_batches - 1:\n",
        "                # We're at the end of an epoch\n",
        "                # We could randomly shuffle the data\n",
        "                # train_data = jr.permutation(key, train_data)\n",
        "                if (self.train_params.get(\"use_validation\")):\n",
        "                    val_loss_out = val_step(val_key, self.params, data_dict[\"val_data\"])\n",
        "                    if (val_callback): val_callback(self, val_loss_out, data_dict)\n",
        "                    val_loss, _ = val_loss_out\n",
        "                    \n",
        "            if not self.train_params.get(\"use_validation\") or val_loss is None:\n",
        "                curr_loss = loss\n",
        "            else:\n",
        "                curr_loss = val_loss\n",
        "\n",
        "            if itr >= early_stop_start:\n",
        "                if best_loss is None or curr_loss < best_loss:\n",
        "                    best_itr = itr\n",
        "                    best_loss = curr_loss\n",
        "                if curr_loss > best_loss and itr - best_itr > max_lose_streak:\n",
        "                    print(\"Early stopping!\")\n",
        "                    break\n",
        "\n",
        "            if (callback): callback(self, loss_out, data_dict, grads)\n",
        "\n",
        "            # Record parameters\n",
        "            record_params = self.train_params.get(\"record_params\")\n",
        "            if record_params and record_params(itr):\n",
        "                curr_params = deepcopy(self.params)\n",
        "                curr_params[\"iteration\"] = itr\n",
        "                self.past_params.append(curr_params)\n",
        "\n",
        "            if (mask_start and itr == mask_start):\n",
        "                self.train_params[\"mask_size\"] = mask_size\n",
        "                train_step = jit(self.train_step)\n",
        "                val_step = jit(self.val_step)\n",
        "\n",
        "        if summary:\n",
        "            summary(self)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "cellView": "form",
        "id": "KmTBoMwgArBw"
      },
      "outputs": [],
      "source": [
        "# @title Logging to WandB\n",
        "\n",
        "def visualize_lds(trainer, data_dict, aux):\n",
        "    data = data_dict[\"train_data\"]\n",
        "    true_model_params = data_dict[\"lds_params\"]\n",
        "    model = trainer.model\n",
        "    params = [trainer.params]\n",
        "    num_pts = 10\n",
        "    # We want to visualize the posterior marginals\n",
        "    inf_mus, inf_Sigmas, tgt_mus, tgt_Sigmas, true_mus, true_Sigmas = \\\n",
        "        get_marginals_and_targets(key_0, data, num_pts, model, params, true_model_params)\n",
        "    # Create the axis\n",
        "    fig, ax = plt.subplots()\n",
        "    # Plot each of the groups\n",
        "    tgt_points, tgt_lines = get_artists(ax, tgt_mus[0], tgt_Sigmas[0], key_0, num_pts,\n",
        "                                        alpha=0.3, linestyle=\"dotted\", label=\"current target\")\n",
        "    inf_points, inf_lines = get_artists(ax, inf_mus[0], inf_Sigmas[0], key_0, num_pts, label=\"inferred\")\n",
        "    true_points, true_lines = get_artists(ax, true_mus, true_Sigmas, \n",
        "                                          key_0, num_pts, alpha=0.2, label=\"true target\")\n",
        "    # The legend is too large and blocks most of the plot\n",
        "    # plt.legend()\n",
        "    # Relimit the axes\n",
        "    ax.relim()\n",
        "    # update ax.viewLim using the new dataLim\n",
        "    ax.autoscale_view()\n",
        "    fig.canvas.draw()\n",
        "    img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
        "    post_img = wandb.Image(img, caption=\"Posterior marginals versus targets\")\n",
        "    plt.close()\n",
        "    return {\n",
        "        \"Posterior marginals\": post_img\n",
        "    }\n",
        "\n",
        "def visualize_pendulum(trainer, aux):\n",
        "    # This assumes single sequence has shape (100, 24, 24, 1)\n",
        "    recon = aux[\"reconstruction\"][0][0]\n",
        "    # Show the sequence as a block of images\n",
        "    stacked = recon.reshape(10, 24 * 10, 24)\n",
        "    imgrid = stacked.swapaxes(0, 1).reshape(24 * 10, 24 * 10)\n",
        "    recon_img = wandb.Image(onp.array(imgrid), caption=\"Sample Reconstruction\")\n",
        "\n",
        "    fig = plt.figure()\n",
        "    mask = aux[\"mask\"][0]\n",
        "    post_sample = aux[\"posterior_samples\"][0][0]\n",
        "    top, bot = np.max(post_sample) + 5, np.min(post_sample) - 5\n",
        "    left, right = 0, post_sample.shape[0]\n",
        "    plt.imshow(mask[None], cmap=\"gray\", alpha=.4, vmin=0, vmax=1,\n",
        "               extent=(left, right, top, bot))\n",
        "    plt.plot(post_sample)\n",
        "    fig.canvas.draw()\n",
        "    img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
        "    post_img = wandb.Image(img, caption=\"Posterior Sample\")\n",
        "    plt.close()\n",
        "    return {\n",
        "        \"Reconstruction\": recon_img, \n",
        "        \"Posterior Sample\": post_img\n",
        "    }\n",
        "\n",
        "def get_group_name(run_params):\n",
        "    p = run_params\n",
        "    run_type = \"\" if p[\"inference_method\"] in [\"EM\", \"GT\", \"SMC\"] else \"_\" + p[\"run_type\"]\n",
        "    if p[\"dataset\"] == \"pendulum\":\n",
        "        dataset_summary = \"pendulum\"\n",
        "    elif p[\"dataset\"] == \"lds\":\n",
        "        d = p[\"dataset_params\"]\n",
        "        dataset_summary = \"lds_dims_{}_{}_noises_{}_{}\".format(\n",
        "            d[\"latent_dims\"], d[\"emission_dims\"], \n",
        "            d[\"dynamics_cov\"], d[\"emission_cov\"])\n",
        "    else:\n",
        "        dataset_summary = \"???\"\n",
        "\n",
        "    model_summary = \"_{}d_latent_\".format(p[\"latent_dims\"]) + p[\"inference_method\"]\n",
        "\n",
        "    group_tag = p.get(\"group_tag\") or \"\"\n",
        "    if group_tag != \"\": group_tag += \"_\"\n",
        "\n",
        "    group_name = (group_tag +\n",
        "        dataset_summary\n",
        "        + model_summary\n",
        "        + run_type\n",
        "    )\n",
        "    return group_name\n",
        "\n",
        "def validation_log_to_wandb(trainer, loss_out, data_dict):\n",
        "    p = trainer.train_params\n",
        "    if not p.get(\"log_to_wandb\"): return\n",
        "    \n",
        "    project_name = p[\"project_name\"]\n",
        "    group_name = get_group_name(p)\n",
        "\n",
        "    obj, aux = loss_out\n",
        "    elbo = -obj\n",
        "    kl = np.mean(aux[\"kl\"])\n",
        "    ell = np.mean(aux[\"ell\"])\n",
        "\n",
        "    model = trainer.model\n",
        "    prior = model.prior\n",
        "    D = prior.latent_dims\n",
        "    prior_params = trainer.params[\"prior_params\"]\n",
        "\n",
        "    visualizations = {}\n",
        "    if p[\"dataset\"] == \"pendulum\":\n",
        "        visualizations = visualize_pendulum(trainer, aux)\n",
        "        pred_ll = np.mean(aux[\"prediction_ll\"])\n",
        "        visualizations = {\n",
        "            \"Validation reconstruction\": visualizations[\"Reconstruction\"], \n",
        "            \"Validation posterior sample\": visualizations[\"Posterior Sample\"],\n",
        "            \"Validation prediction log likelihood\": pred_ll\n",
        "        }\n",
        "        \n",
        "    to_log = {\"Validation ELBO\": elbo, \"Validation KL\": kl, \"Validation likelihood\": ell,}\n",
        "    to_log.update(visualizations)\n",
        "    wandb.log(to_log)\n",
        "\n",
        "def log_to_wandb(trainer, loss_out, data_dict, grads):\n",
        "    p = trainer.train_params\n",
        "    if not p.get(\"log_to_wandb\"): return\n",
        "    \n",
        "    project_name = p[\"project_name\"]\n",
        "    group_name = get_group_name(p)\n",
        "\n",
        "    itr = len(trainer.train_losses) - 1\n",
        "    if len(trainer.train_losses) == 1:\n",
        "        wandb.init(project=project_name, group=group_name, config=p)\n",
        "        pprint(p)\n",
        "\n",
        "    obj, aux = loss_out\n",
        "    elbo = -obj\n",
        "    kl = np.mean(aux[\"kl\"])\n",
        "    ell = np.mean(aux[\"ell\"])\n",
        "\n",
        "    model = trainer.model\n",
        "    prior = model.prior\n",
        "    D = prior.latent_dims\n",
        "    prior_params = trainer.params[\"prior_params\"]\n",
        "    if (p.get(\"use_natural_grad\")):\n",
        "        Q = prior_params[\"Q\"]\n",
        "        A = prior_params[\"A\"]\n",
        "    else:\n",
        "        Q = lie_params_to_constrained(prior_params[\"Q\"], D)\n",
        "        A = prior_params[\"A\"]\n",
        "\n",
        "    eigs = eigh(Q)[0]\n",
        "    Q_cond_num = np.max(eigs) / np.min(eigs)\n",
        "    svs = svd(A)[1]\n",
        "    max_sv, min_sv = np.max(svs), np.min(svs)\n",
        "    A_cond_num = max_sv / min_sv\n",
        "\n",
        "    # Also log the prior params gradients\n",
        "    # prior_grads = grads[\"prior_params\"][\"sgd_params\"]\n",
        "    # prior_grads_norm = np.linalg.norm(\n",
        "    #     jax.tree_util.tree_leaves(tree_map(np.linalg.norm, prior_grads)))\n",
        "\n",
        "    visualizations = {}\n",
        "    if (itr % p[\"plot_interval\"] == 0):\n",
        "        if p[\"dataset\"] == \"lds\" and p.get(\"visualize_training\"):\n",
        "            visualizations = visualize_lds(trainer, data_dict, aux)\n",
        "        elif p[\"dataset\"] == \"pendulum\" and p.get(\"visualize_training\"):\n",
        "            visualizations = visualize_pendulum(trainer, aux)\n",
        "\n",
        "        # fig = plt.figure()\n",
        "        # prior_sample = prior.sample(prior_params, shape=(1,), key=jr.PRNGKey(0))[0]\n",
        "        # plt.plot(prior_sample)\n",
        "        # fig.canvas.draw()\n",
        "        # img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
        "        # prior_img = wandb.Image(img, caption=\"Prior Sample\")\n",
        "        # plt.close()\n",
        "        # visualizations[\"Prior sample\"] = prior_img\n",
        "    # Also log the learning rates\n",
        "    lr = p[\"learning_rate\"] \n",
        "    lr = lr if isinstance(lr, float) else lr(itr)\n",
        "    prior_lr = p[\"prior_learning_rate\"] \n",
        "    prior_lr = prior_lr if isinstance(prior_lr, float) else prior_lr(itr)\n",
        "\n",
        "    to_log = { \"ELBO\": elbo, \"KL\": kl, \"Likelihood\": ell, # \"Prior graident norm\": prior_grads_norm,\n",
        "               \"Max singular value of A\": max_sv, \"Min singular value of A\": min_sv,\n",
        "               \"Condition number of A\": A_cond_num, \"Condition number of Q\": Q_cond_num,\n",
        "               \"Learning rate\": lr, \"Prior learning rate\": prior_lr }\n",
        "    to_log.update(visualizations)\n",
        "    wandb.log(to_log)\n",
        "\n",
        "def save_params_to_wandb(trainer):\n",
        "    file_name = \"parameters.pkl\"\n",
        "    with open(file_name, \"wb\") as f:\n",
        "        pkl.dump(trainer.past_params, f)\n",
        "        wandb.save(file_name, policy=\"now\")\n",
        "\n",
        "def on_error(data_dict, model_dict):\n",
        "    save_params_to_wandb(model_dict[\"trainer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "cellView": "form",
        "id": "ve7zlI0-P8tP"
      },
      "outputs": [],
      "source": [
        "# @title Specifics of SVAE training\n",
        "def svae_init(key, model, data, initial_params=None, **train_params):\n",
        "    init_params = model.init(key)\n",
        "    if (initial_params): init_params.update(initial_params)\n",
        "    \n",
        "    if (train_params[\"inference_method\"] == \"planet\"):\n",
        "        init_params[\"rec_params\"] = {\n",
        "            \"rec_params\": init_params[\"rec_params\"],\n",
        "            \"post_params\": init_params[\"post_params\"]\n",
        "        }\n",
        "    # Expand the posterior parameters by batch size\n",
        "    init_params[\"post_params\"] = vmap(lambda _: init_params[\"post_params\"])(data)\n",
        "    init_params[\"post_samples\"] = np.zeros((data.shape[0], \n",
        "                                            train_params.get(\"obj_samples\") or 1) \n",
        "                                             + model.posterior.shape)\n",
        "\n",
        "    learning_rate = train_params[\"learning_rate\"]\n",
        "    rec_opt = opt.adam(learning_rate=learning_rate)\n",
        "    rec_opt_state = rec_opt.init(init_params[\"rec_params\"])\n",
        "    dec_opt = opt.adam(learning_rate=learning_rate)\n",
        "    dec_opt_state = dec_opt.init(init_params[\"dec_params\"])\n",
        "\n",
        "    if (train_params.get(\"use_natural_grad\")):\n",
        "        prior_lr = None\n",
        "        prior_opt = None\n",
        "        prior_opt_state = None\n",
        "    else:\n",
        "        # Add the option of using an gradient optimizer for prior parameters\n",
        "        prior_lr = train_params.get(\"prior_learning_rate\") or learning_rate\n",
        "        prior_opt = opt.adam(learning_rate=prior_lr)\n",
        "        prior_opt_state = prior_opt.init(init_params[\"prior_params\"])\n",
        "\n",
        "    return (init_params, \n",
        "            (rec_opt, dec_opt, prior_opt), \n",
        "            (rec_opt_state, dec_opt_state, prior_opt_state))\n",
        "    \n",
        "def svae_loss(key, model, data_batch, model_params, **train_params):\n",
        "    batch_size = data_batch.shape[0]\n",
        "    # Axes specification for vmap\n",
        "    # We're just going to ignore this for now\n",
        "    params_in_axes = None\n",
        "    # params_in_axes = dict.fromkeys(model_params.keys(), None)\n",
        "    # params_in_axes[\"post_samples\"] = 0\n",
        "    result = vmap(partial(model.compute_objective, **train_params), \n",
        "                  in_axes=(0, 0, params_in_axes))(jr.split(key, batch_size), data_batch, model_params)\n",
        "    objs = result[\"objective\"]\n",
        "    post_params = result[\"posterior_params\"]\n",
        "    post_samples = result[\"posterior_samples\"]\n",
        "    # Need to compute sufficient stats if we want the natural gradient update\n",
        "    if (train_params.get(\"use_natural_grad\")):\n",
        "        post_suff_stats = vmap(model.posterior.sufficient_statistics)(post_params)\n",
        "        expected_post_suff_stats = tree_map(\n",
        "            lambda l: np.mean(l,axis=0), post_suff_stats)\n",
        "        result[\"sufficient_statistics\"] = expected_post_suff_stats\n",
        "    return -np.mean(objs), result\n",
        "\n",
        "def predict_forward(x, A, b, T):\n",
        "    def _step(carry, t):\n",
        "        carry = A @ carry + b\n",
        "        return carry, carry\n",
        "    return scan(_step, x, np.arange(T))[1]\n",
        "\n",
        "# Note: this is for pendulum data only\n",
        "def svae_val_loss(key, model, data_batch, model_params, **train_params):  \n",
        "    N, T = data_batch.shape[:2]\n",
        "    # We only care about the first 100 timesteps\n",
        "    T = T // 2\n",
        "    D = model.prior.latent_dims\n",
        "\n",
        "    # obs_data, pred_data = data_batch[:,:T//2], data_batch[:,T//2:]\n",
        "    obs_data = data_batch[:,:T]\n",
        "    obj, out_dict = svae_loss(key, model, obs_data, model_params, **train_params)\n",
        "    # Compute the prediction accuracy\n",
        "    prior_params = model_params[\"prior_params\"] \n",
        "    # Instead of this, we want to evaluate the expected log likelihood of the future observations\n",
        "    # under the posterior given the current set of observations\n",
        "    # So E_{q(x'|y)}[p(y'|x')] where the primes represent the future\n",
        "    post_params = out_dict[\"posterior_params\"]\n",
        "\n",
        "    posterior = model.posterior.distribution(post_params)\n",
        "    # J = posterior.filtered_precisions\n",
        "    # h = posterior.filtered_linear_potentials\n",
        "    Sigma_filtered = posterior.filtered_covariances # inv(J)\n",
        "    mu_filtered = posterior.filtered_means # np.einsum(\"...ij,...j->...i\", Sigma_filtered, h)\n",
        "    horizon = train_params[\"prediction_horizon\"] or 5\n",
        "\n",
        "    def _prediction_lls(data_id, key):\n",
        "        num_windows = T-horizon-1\n",
        "        pred_lls = vmap(_sliding_window_prediction_lls, in_axes=(None, None, None, 0, 0))(\n",
        "            mu_filtered[data_id], Sigma_filtered[data_id], obs_data[data_id],\n",
        "            np.arange(num_windows), jr.split(key, num_windows))\n",
        "        return pred_lls.mean()\n",
        "\n",
        "    # TODO: change this...!\n",
        "    def _sliding_window_prediction_lls(mu, Sigma, data, t, key):\n",
        "        # Build the posterior object on the future latent states \n",
        "        # (\"the posterior predictive distribution\")\n",
        "        # Convert unconstrained params to constrained dynamics parameters\n",
        "        prior_params_constrained = model.prior.get_dynamics_params(prior_params)\n",
        "        dynamics_params = {\n",
        "            \"m1\": mu[t],\n",
        "            \"Q1\": Sigma[t],\n",
        "            \"A\": prior_params_constrained[\"A\"],\n",
        "            \"b\": prior_params_constrained[\"b\"],\n",
        "            \"Q\": prior_params_constrained[\"Q\"]\n",
        "        }\n",
        "        tridiag_params = dynamics_to_tridiag(dynamics_params, horizon+1, D) # Note the +1\n",
        "        J, L, h = tridiag_params[\"J\"], tridiag_params[\"L\"], tridiag_params[\"h\"]\n",
        "        pred_posterior = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "        # Sample from it and evaluate the log likelihood\n",
        "        x_pred = pred_posterior.sample(seed=key)[1:]\n",
        "        likelihood_dist = model.decoder.apply(model_params[\"dec_params\"], x_pred)\n",
        "        return likelihood_dist.log_prob(\n",
        "            lax.dynamic_slice(data, (t+1, 0, 0, 0), (horizon,) + data.shape[1:])).sum(axis=(1, 2, 3))\n",
        "\n",
        "    # pred_lls = vmap(_prediction_lls)(\n",
        "    #     out_dict[\"posterior_params\"], data_batch, jr.split(key, N))\n",
        "    pred_lls = vmap(_prediction_lls)(np.arange(N), jr.split(key, N))\n",
        "    out_dict[\"prediction_ll\"] = pred_lls\n",
        "    return obj, out_dict\n",
        "\n",
        "def svae_update(params, grads, opts, opt_states, model, aux, **train_params):\n",
        "    rec_opt, dec_opt, prior_opt = opts\n",
        "    rec_opt_state, dec_opt_state, prior_opt_state = opt_states\n",
        "    rec_grad, dec_grad = grads[\"rec_params\"], grads[\"dec_params\"]\n",
        "    updates, rec_opt_state = rec_opt.update(rec_grad, rec_opt_state)\n",
        "    params[\"rec_params\"] = opt.apply_updates(params[\"rec_params\"], updates)\n",
        "    params[\"post_params\"] = aux[\"posterior_params\"]\n",
        "    params[\"post_samples\"] = aux[\"posterior_samples\"]\n",
        "    if train_params[\"run_type\"] == \"model_learning\":\n",
        "        # Update decoder\n",
        "        updates, dec_opt_state = dec_opt.update(dec_grad, dec_opt_state)\n",
        "        params[\"dec_params\"] = opt.apply_updates(params[\"dec_params\"], updates)\n",
        "\n",
        "        old_Q = deepcopy(params[\"prior_params\"][\"Q\"])\n",
        "        old_b = deepcopy(params[\"prior_params\"][\"b\"])\n",
        "\n",
        "        # Update prior parameters\n",
        "        if (train_params.get(\"use_natural_grad\")):\n",
        "            # Here we interpolate the sufficient statistics instead of the parameters\n",
        "            suff_stats = aux[\"sufficient_statistics\"]\n",
        "            lr = params.get(\"prior_learning_rate\") or 1\n",
        "            avg_suff_stats = params[\"prior_params\"][\"avg_suff_stats\"]\n",
        "            # Interpolate the sufficient statistics\n",
        "            params[\"prior_params\"][\"avg_suff_stats\"] = tree_map(lambda x,y : (1 - lr) * x + lr * y, \n",
        "                avg_suff_stats, suff_stats)\n",
        "            params[\"prior_params\"] = model.prior.m_step(params[\"prior_params\"])\n",
        "        else:\n",
        "            updates, prior_opt_state = prior_opt.update(grads[\"prior_params\"], prior_opt_state)\n",
        "            params[\"prior_params\"] = opt.apply_updates(params[\"prior_params\"], updates)\n",
        "        \n",
        "        if (train_params.get(\"constrain_prior\")):\n",
        "            # Revert Q and b to their previous values\n",
        "            params[\"prior_params\"][\"Q\"] = old_Q\n",
        "            params[\"prior_params\"][\"b\"] = old_b\n",
        "\n",
        "        if (train_params.get(\"constrain_dynamics\")):\n",
        "            # Scale A so that its maximum singular value does not exceed 1\n",
        "            params[\"prior_params\"][\"A\"] = truncate_singular_values(params[\"prior_params\"][\"A\"])\n",
        "            # params[\"prior_params\"][\"A\"] = scale_singular_values(params[\"prior_params\"][\"A\"])\n",
        "\n",
        "    return params, (rec_opt_state, dec_opt_state, prior_opt_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "8AdW1WXA4n1U"
      },
      "outputs": [],
      "source": [
        "# @title Model initialization and trainer\n",
        "def init_model(run_params, data_dict):\n",
        "    p = deepcopy(run_params)\n",
        "    d = p[\"dataset_params\"]\n",
        "    latent_dims = p[\"latent_dims\"]\n",
        "    input_shape = data_dict[\"train_data\"].shape[1:]\n",
        "    num_timesteps = input_shape[0]\n",
        "    data = data_dict[\"train_data\"]\n",
        "    seed = p[\"seed\"]\n",
        "    seed_model, seed_elbo, seed_ems, seed_rec = jr.split(seed, 4)\n",
        "\n",
        "    run_type = p[\"run_type\"]\n",
        "    recnet_class = globals()[p[\"recnet_class\"]]\n",
        "    decnet_class = globals()[p[\"decnet_class\"]]\n",
        "\n",
        "    if p[\"inference_method\"] == \"dkf\":\n",
        "        posterior = DKFPosterior(latent_dims, num_timesteps)\n",
        "    elif p[\"inference_method\"] == \"cdkf\":\n",
        "        posterior = CDKFPosterior(latent_dims, num_timesteps)\n",
        "    elif p[\"inference_method\"] == \"planet\":\n",
        "        posterior = PlaNetPosterior(p[\"posterior_architecture\"],\n",
        "                                    latent_dims, num_timesteps)\n",
        "    elif p[\"inference_method\"] == \"svae\":\n",
        "        # The parallel Kalman stuff only applies to SVAE\n",
        "        # Since RNN based methods are inherently sequential\n",
        "        posterior = LDSSVAEPosterior(latent_dims, num_timesteps, \n",
        "                                     use_parallel=p.get(\"use_parallel_kf\"))\n",
        "        \n",
        "    rec_net = recnet_class.from_params(**p[\"recnet_architecture\"])\n",
        "    dec_net = decnet_class.from_params(**p[\"decnet_architecture\"])\n",
        "    if p[\"inference_method\"] == \"planet\":\n",
        "        # Wrap the recognition network\n",
        "        rec_net = PlaNetRecognitionWrapper(rec_net)\n",
        "\n",
        "    if (p.get(\"use_natural_grad\")):\n",
        "        prior = LinearGaussianChainPrior(latent_dims, num_timesteps)\n",
        "    else:\n",
        "        prior = LieParameterizedLinearGaussianChainPrior(latent_dims, num_timesteps)\n",
        "\n",
        "    model = DeepLDS(\n",
        "        recognition=rec_net,\n",
        "        decoder=dec_net,\n",
        "        prior=prior,\n",
        "        posterior=posterior,\n",
        "        input_dummy=np.zeros(input_shape),\n",
        "        latent_dummy=np.zeros((num_timesteps, latent_dims))\n",
        "    )\n",
        "    \n",
        "    # TODO: Let's get the full linear version working first before moving on\n",
        "    # assert(run_params[\"run_type\"] == \"full_linear\")\n",
        "    if (run_type == \"inference_only\"):\n",
        "        p = data_dict[\"lds_params\"]\n",
        "        prior_params = { \"A\": p[\"A\"], \"b\": p[\"b\"], \n",
        "                        \"Q\": p[\"Q\"], \"m1\": p[\"m1\"], \"Q1\": p[\"Q1\"],\n",
        "                        \"avg_suff_stats\": p[\"avg_suff_stats\"]}\n",
        "        dec_params = fd.FrozenDict(\n",
        "            {\n",
        "                \"params\": {\n",
        "                    \"head_log_var_fn\":{\n",
        "                        \"Dense_0\":{\n",
        "                            \"bias\": inv_softplus(np.diag(p[\"R\"])),\n",
        "                            \"kernel\": np.zeros_like(p[\"C\"]).T\n",
        "                        }\n",
        "                    },\n",
        "                    \"head_mean_fn\":{\n",
        "                        \"Dense_0\":{\n",
        "                            \"bias\": p[\"d\"],\n",
        "                            \"kernel\": p[\"C\"].T\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            })\n",
        "        initial_params = { \"prior_params\": prior_params, \"dec_params\": dec_params }\n",
        "    else:\n",
        "        initial_params = None\n",
        "\n",
        "    # emission_params = emission.init(seed_ems, np.ones((num_latent_dims,)))\n",
        "    # Define the trainer object here\n",
        "    trainer = Trainer(model, train_params=run_params, init=svae_init, \n",
        "                      loss=svae_loss, \n",
        "                      val_loss=svae_val_loss, \n",
        "                      update=svae_update, initial_params=initial_params)\n",
        "\n",
        "    return {\n",
        "        # We don't actually need to include model here\n",
        "        # 'cause it's included in the trainer object\n",
        "        \"model\": model,\n",
        "        # \"emission_params\": emission_params\n",
        "        \"trainer\": trainer\n",
        "    }\n",
        "\n",
        "def start_trainer(model_dict, data_dict, run_params):\n",
        "    trainer = model_dict[\"trainer\"]\n",
        "    summary = save_params_to_wandb if run_params.get(\"log_to_wandb\") else None\n",
        "    trainer.train(data_dict,\n",
        "                  max_iters=run_params[\"max_iters\"],\n",
        "                  key=run_params[\"seed\"],\n",
        "                  callback=log_to_wandb, val_callback=validation_log_to_wandb,\n",
        "                  summary=summary)\n",
        "    return (trainer.model, trainer.params, trainer.train_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NSj6XUpPZRG"
      },
      "source": [
        "## Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "cellView": "form",
        "id": "ZeG1j7HgqWya"
      },
      "outputs": [],
      "source": [
        "# @title Sample from LDS\n",
        "def sample_lds_dataset(run_params):    \n",
        "    d = run_params[\"dataset_params\"]\n",
        "    \n",
        "    global data_dict\n",
        "    if data_dict is not None \\\n",
        "        and \"dataset_params\" in data_dict \\\n",
        "        and str(data_dict[\"dataset_params\"]) == str(fd.freeze(d)):\n",
        "        print(\"Using existing data.\")\n",
        "        print(\"Data MLL: \", data_dict[\"marginal_log_likelihood\"])\n",
        "        \n",
        "        # return jax.device_put(data_dict, jax.devices(\"gpu\")[0])\n",
        "        return data_dict\n",
        "\n",
        "    data_dict = {}\n",
        "\n",
        "    seed = d[\"seed\"]\n",
        "    emission_dims = d[\"emission_dims\"]\n",
        "    latent_dims = d[\"latent_dims\"]\n",
        "    emission_cov = d[\"emission_cov\"]\n",
        "    dynamics_cov = d[\"dynamics_cov\"]\n",
        "    num_timesteps = d[\"num_timesteps\"]\n",
        "    num_trials = d[\"num_trials\"]\n",
        "    seed_m1, seed_C, seed_d, seed_A, seed_sample = jr.split(seed, 5)\n",
        "\n",
        "    R = emission_cov * np.eye(emission_dims)\n",
        "    Q = dynamics_cov * np.eye(latent_dims)\n",
        "    C = jr.normal(seed_C, shape=(emission_dims, latent_dims))\n",
        "    d = jr.normal(seed_d, shape=(emission_dims,))\n",
        "\n",
        "    # Here we let Q1 = Q\n",
        "    lds = LDS(latent_dims, num_timesteps)\n",
        "    \n",
        "    params = {\n",
        "            \"m1\": jr.normal(key=seed_m1, shape=(latent_dims,)),\n",
        "            \"Q1\": Q,\n",
        "            \"Q\": Q,\n",
        "            \"A\": random_rotation(seed_A, latent_dims, theta=np.pi/20),\n",
        "            \"b\": np.zeros(latent_dims),\n",
        "            \"R\": R,\n",
        "            \"C\": C,\n",
        "            \"d\": d,\n",
        "        }\n",
        "    constrained = lds.get_constrained_params(params)\n",
        "    params[\"avg_suff_stats\"] = { \"Ex\": constrained[\"Ex\"], \n",
        "                                \"ExxT\": constrained[\"ExxT\"], \n",
        "                                \"ExnxT\": constrained[\"ExnxT\"] }\n",
        "\n",
        "    states, data = lds.sample(params, \n",
        "                              shape=(num_trials,), \n",
        "                              key=seed_sample)\n",
        "    \n",
        "    mll = vmap(lds.marginal_log_likelihood, in_axes=(None, 0))(params, data)\n",
        "    mll = np.mean(mll, axis=0)\n",
        "    print(\"Data MLL: \", mll)\n",
        "    \n",
        "    seed_val, _ = jr.split(seed_sample)\n",
        "    val_states, val_data = lds.sample(params, \n",
        "                              shape=(num_trials,), \n",
        "                              key=seed_val)\n",
        "\n",
        "    # data_dict[\"generative_model\"] = lds\n",
        "    data_dict[\"marginal_log_likelihood\"] = mll\n",
        "    data_dict[\"train_data\"] = data\n",
        "    data_dict[\"train_states\"] = states\n",
        "    data_dict[\"val_data\"] = val_data\n",
        "    data_dict[\"val_states\"] = val_states\n",
        "    data_dict[\"dataset_params\"] = fd.freeze(run_params[\"dataset_params\"])\n",
        "    data_dict[\"lds_params\"] = params\n",
        "    return data_dict\n",
        "    # return jax.device_put(data_dict, jax.devices(\"gpu\")[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "cellView": "form",
        "id": "MKj_UmfS6JdM"
      },
      "outputs": [],
      "source": [
        "# @title Testing the correctness of LDS m-step\n",
        "# seed = jr.PRNGKey(0)\n",
        "# emission_dims = 5\n",
        "# latent_dims = 3\n",
        "# emission_cov = 10.\n",
        "# dynamics_cov = .1\n",
        "# num_timesteps = 100\n",
        "# seed, seed_C, seed_d, seed_sample = jr.split(seed, 4)\n",
        "# R = emission_cov * np.eye(emission_dims)\n",
        "# Q = dynamics_cov * np.eye(latent_dims)\n",
        "# C = jr.normal(seed_C, shape=(emission_dims, latent_dims))\n",
        "# d = jr.normal(seed_d, shape=(emission_dims,))\n",
        "# # Here we let Q1 = Q\n",
        "# lds = LDS(latent_dims, num_timesteps)\n",
        "# params = lds.init(seed)\n",
        "# params.update(\n",
        "#     {\n",
        "#         \"Q1\": Q,\n",
        "#         \"Q\": Q,\n",
        "#         \"R\": R,\n",
        "#         \"C\": C,\n",
        "#         \"d\": d,\n",
        "#     }\n",
        "# )\n",
        "# states, data = lds.sample(params, \n",
        "#                           shape=(50,), \n",
        "#                           key=seed_sample)\n",
        "# post_params = vmap(lds.e_step, in_axes=(None, 0))(params, data)\n",
        "# posterior = LinearGaussianChainPosterior(latent_dims, num_timesteps)\n",
        "# suff_stats = vmap(posterior.sufficient_statistics)(post_params)\n",
        "# expected_suff_stats = jax.tree_util.tree_map(\n",
        "#         lambda l: np.mean(l,axis=0), suff_stats)\n",
        "# inferred_params = lds.m_step(params, expected_suff_stats)\n",
        "# for key in [\"m1\", \"Q1\", \"A\", \"Q\", \"b\"]:\n",
        "#     print(inferred_params[key] - params[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "cellView": "form",
        "id": "Qzp5Fb0CGtqk"
      },
      "outputs": [],
      "source": [
        "# @title Code for the pendulum dataset (~128 mb)\n",
        "\n",
        "\n",
        "# Modeling Irregular Time Series with Continuous Recurrent Units (CRUs)\n",
        "# Copyright (c) 2022 Robert Bosch GmbH\n",
        "# This program is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Affero General Public License as published\n",
        "# by the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "#\n",
        "# This program is distributed in the hope that it will be useful,\n",
        "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "# GNU Affero General Public License for more details.\n",
        "#\n",
        "# You should have received a copy of the GNU Affero General Public License\n",
        "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
        "#\n",
        "# This source code is derived from Pytorch RKN Implementation (https://github.com/ALRhub/rkn_share)\n",
        "# Copyright (c) 2021 Philipp Becker (Autonomous Learning Robots Lab @ KIT)\n",
        "# licensed under MIT License\n",
        "# cf. 3rd-party-licenses.txt file in the root directory of this source tree.\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "def add_img_noise(imgs, first_n_clean, random, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75, t_uu=1.0):\n",
        "    \"\"\"\n",
        "    :param imgs: Images to add noise to\n",
        "    :param first_n_clean: Keep first_n_images clean to allow the filter to burn in\n",
        "    :param random: np.random.RandomState used for sampling\n",
        "    :param r: \"correlation (over time) factor\" the smaller the more the noise is correlated\n",
        "    :param t_ll: lower bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_lu: upper bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_ul: lower bound of the interval the upper bound for each sequence is sampled from\n",
        "    :param t_uu: upper bound of the interval the upper bound for each sequence is sampled from\n",
        "    :return: noisy images, factors used to create them\n",
        "    \"\"\"\n",
        "\n",
        "    assert t_ll <= t_lu <= t_ul <= t_uu, \"Invalid bounds for noise generation\"\n",
        "    if len(imgs.shape) < 5:\n",
        "        imgs = onp.expand_dims(imgs, -1)\n",
        "    batch_size, seq_len = imgs.shape[:2]\n",
        "    factors = onp.zeros([batch_size, seq_len])\n",
        "    factors[:, 0] = random.uniform(low=0.0, high=1.0, size=batch_size)\n",
        "    for i in range(seq_len - 1):\n",
        "        factors[:, i + 1] = onp.clip(factors[:, i] + random.uniform(\n",
        "            low=-r, high=r, size=batch_size), a_min=0.0, a_max=1.0)\n",
        "\n",
        "    t1 = random.uniform(low=t_ll, high=t_lu, size=(batch_size, 1))\n",
        "    t2 = random.uniform(low=t_ul, high=t_uu, size=(batch_size, 1))\n",
        "\n",
        "    factors = (factors - t1) / (t2 - t1)\n",
        "    factors = onp.clip(factors, a_min=0.0, a_max=1.0)\n",
        "    factors = onp.reshape(factors, list(factors.shape) + [1, 1, 1])\n",
        "    factors[:, :first_n_clean] = 1.0\n",
        "    noisy_imgs = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        if imgs.dtype == onp.uint8:\n",
        "            noise = random.uniform(low=0.0, high=255, size=imgs.shape[1:])\n",
        "            noisy_imgs.append(\n",
        "                (factors[i] * imgs[i] + (1 - factors[i]) * noise).astype(onp.uint8))\n",
        "        else:\n",
        "            noise = random.uniform(low=0.0, high=1.1, size=imgs.shape[1:])\n",
        "            noisy_imgs.append(factors[i] * imgs[i] + (1 - factors[i]) * noise)\n",
        "\n",
        "    return onp.squeeze(onp.concatenate([onp.expand_dims(n, 0) for n in noisy_imgs], 0)), factors\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "def add_img_noise4(imgs, first_n_clean, random, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75, t_uu=1.0):\n",
        "    \"\"\"\n",
        "    :param imgs: Images to add noise to\n",
        "    :param first_n_clean: Keep first_n_images clean to allow the filter to burn in\n",
        "    :param random: np.random.RandomState used for sampling\n",
        "    :param r: \"correlation (over time) factor\" the smaller the more the noise is correlated\n",
        "    :param t_ll: lower bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_lu: upper bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_ul: lower bound of the interval the upper bound for each sequence is sampled from\n",
        "    :param t_uu: upper bound of the interval the upper bound for each sequence is sampled from\n",
        "    :return: noisy images, factors used to create them\n",
        "    \"\"\"\n",
        "\n",
        "    half_x = int(imgs.shape[2] / 2)\n",
        "    half_y = int(imgs.shape[3] / 2)\n",
        "    assert t_ll <= t_lu <= t_ul <= t_uu, \"Invalid bounds for noise generation\"\n",
        "    if len(imgs.shape) < 5:\n",
        "        imgs = np.expand_dims(imgs, -1)\n",
        "    batch_size, seq_len = imgs.shape[:2]\n",
        "    factors = np.zeros([batch_size, seq_len, 4])\n",
        "    factors[:, 0] = random.uniform(low=0.0, high=1.0, size=(batch_size, 4))\n",
        "    for i in range(seq_len - 1):\n",
        "        factors[:, i + 1] = np.clip(factors[:, i] + random.uniform(\n",
        "            low=-r, high=r, size=(batch_size, 4)), a_min=0.0, a_max=1.0)\n",
        "\n",
        "    t1 = random.uniform(low=t_ll, high=t_lu, size=(batch_size, 1, 4))\n",
        "    t2 = random.uniform(low=t_ul, high=t_uu, size=(batch_size, 1, 4))\n",
        "\n",
        "    factors = (factors - t1) / (t2 - t1)\n",
        "    factors = np.clip(factors, a_min=0.0, a_max=1.0)\n",
        "    factors = np.reshape(factors, list(factors.shape) + [1, 1, 1])\n",
        "    factors[:, :first_n_clean] = 1.0\n",
        "    noisy_imgs = []\n",
        "    qs = []\n",
        "    for i in range(batch_size):\n",
        "        if imgs.dtype == np.uint8:\n",
        "            qs.append(detect_pendulums(imgs[i], half_x, half_y))\n",
        "            noise = random.uniform(low=0.0, high=255, size=[\n",
        "                                   4, seq_len, half_x, half_y, imgs.shape[-1]]).astype(np.uint8)\n",
        "            curr = np.zeros(imgs.shape[1:], dtype=np.uint8)\n",
        "            curr[:, :half_x, :half_y] = (factors[i, :, 0] * imgs[i, :, :half_x, :half_y] + (\n",
        "                1 - factors[i, :, 0]) * noise[0]).astype(np.uint8)\n",
        "            curr[:, :half_x, half_y:] = (factors[i, :, 1] * imgs[i, :, :half_x, half_y:] + (\n",
        "                1 - factors[i, :, 1]) * noise[1]).astype(np.uint8)\n",
        "            curr[:, half_x:, :half_y] = (factors[i, :, 2] * imgs[i, :, half_x:, :half_y] + (\n",
        "                1 - factors[i, :, 2]) * noise[2]).astype(np.uint8)\n",
        "            curr[:, half_x:, half_y:] = (factors[i, :, 3] * imgs[i, :, half_x:, half_y:] + (\n",
        "                1 - factors[i, :, 3]) * noise[3]).astype(np.uint8)\n",
        "        else:\n",
        "            noise = random.uniform(low=0.0, high=1.0, size=[\n",
        "                                   4, seq_len, half_x, half_y, imgs.shape[-1]])\n",
        "            curr = np.zeros(imgs.shape[1:])\n",
        "            curr[:, :half_x, :half_y] = factors[i, :, 0] * imgs[i, :,\n",
        "                                                                :half_x, :half_y] + (1 - factors[i, :, 0]) * noise[0]\n",
        "            curr[:, :half_x, half_y:] = factors[i, :, 1] * imgs[i, :,\n",
        "                                                                :half_x, half_y:] + (1 - factors[i, :, 1]) * noise[1]\n",
        "            curr[:, half_x:, :half_y] = factors[i, :, 2] * imgs[i, :,\n",
        "                                                                half_x:, :half_y] + (1 - factors[i, :, 2]) * noise[2]\n",
        "            curr[:, half_x:, half_y:] = factors[i, :, 3] * imgs[i, :,\n",
        "                                                                half_x:, half_y:] + (1 - factors[i, :, 3]) * noise[3]\n",
        "        noisy_imgs.append(curr)\n",
        "\n",
        "    factors_ext = np.concatenate([np.squeeze(factors), np.zeros(\n",
        "        [factors.shape[0], factors.shape[1], 1])], -1)\n",
        "    q = np.concatenate([np.expand_dims(q, 0) for q in qs], 0)\n",
        "    f = np.zeros(q.shape)\n",
        "    for i in range(f.shape[0]):\n",
        "        for j in range(f.shape[1]):\n",
        "            for k in range(3):\n",
        "                f[i, j, k] = factors_ext[i, j, q[i, j, k]]\n",
        "\n",
        "    return np.squeeze(np.concatenate([np.expand_dims(n, 0) for n in noisy_imgs], 0)), f\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "def detect_pendulums(imgs, half_x, half_y):\n",
        "    qs = [imgs[:, :half_x, :half_y], imgs[:, :half_x, half_y:],\n",
        "          imgs[:, half_x:, :half_y], imgs[:, half_x:, half_y:]]\n",
        "\n",
        "    r_cts = np.array(\n",
        "        [np.count_nonzero(q[:, :, :, 0] > 5, axis=(-1, -2)) for q in qs]).T\n",
        "    g_cts = np.array(\n",
        "        [np.count_nonzero(q[:, :, :, 1] > 5, axis=(-1, -2)) for q in qs]).T\n",
        "    b_cts = np.array(\n",
        "        [np.count_nonzero(q[:, :, :, 2] > 5, axis=(-1, -2)) for q in qs]).T\n",
        "\n",
        "    cts = np.concatenate([np.expand_dims(c, 1)\n",
        "                         for c in [r_cts, g_cts, b_cts]], 1)\n",
        "\n",
        "    q_max = np.max(cts, -1)\n",
        "    q = np.argmax(cts, -1)\n",
        "    q[q_max < 10] = 4\n",
        "    return q\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "class Pendulum:\n",
        "\n",
        "    MAX_VELO_KEY = 'max_velo'\n",
        "    MAX_TORQUE_KEY = 'max_torque'\n",
        "    MASS_KEY = 'mass'\n",
        "    LENGTH_KEY = 'length'\n",
        "    GRAVITY_KEY = 'g'\n",
        "    FRICTION_KEY = 'friction'\n",
        "    DT_KEY = 'dt'\n",
        "    SIM_DT_KEY = 'sim_dt'\n",
        "    TRANSITION_NOISE_TRAIN_KEY = 'transition_noise_train'\n",
        "    TRANSITION_NOISE_TEST_KEY = 'transition_noise_test'\n",
        "\n",
        "    OBSERVATION_MODE_LINE = \"line\"\n",
        "    OBSERVATION_MODE_BALL = \"ball\"\n",
        "\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def __init__(self,\n",
        "                 img_size,\n",
        "                 observation_mode,\n",
        "                 generate_actions=False,\n",
        "                 transition_noise_std=0.0,\n",
        "                 observation_noise_std=0.0,\n",
        "                 pendulum_params=None,\n",
        "                 seed=0):\n",
        "\n",
        "        assert observation_mode == Pendulum.OBSERVATION_MODE_BALL or observation_mode == Pendulum.OBSERVATION_MODE_LINE\n",
        "        # Global Parameters\n",
        "        self.state_dim = 2\n",
        "        self.action_dim = 1\n",
        "        self.img_size = img_size\n",
        "        self.observation_dim = img_size ** 2\n",
        "        self.observation_mode = observation_mode\n",
        "\n",
        "        self.random = onp.random.RandomState(seed)\n",
        "\n",
        "        # image parameters\n",
        "        self.img_size_internal = 128\n",
        "        self.x0 = self.y0 = 64\n",
        "        self.plt_length = 55 if self.observation_mode == Pendulum.OBSERVATION_MODE_LINE else 50\n",
        "        self.plt_width = 8\n",
        "\n",
        "        self.generate_actions = generate_actions\n",
        "\n",
        "        # simulation parameters\n",
        "        if pendulum_params is None:\n",
        "            pendulum_params = self.pendulum_default_params()\n",
        "        self.max_velo = pendulum_params[Pendulum.MAX_VELO_KEY]\n",
        "        self.max_torque = pendulum_params[Pendulum.MAX_TORQUE_KEY]\n",
        "        self.dt = pendulum_params[Pendulum.DT_KEY]\n",
        "        self.mass = pendulum_params[Pendulum.MASS_KEY]\n",
        "        self.length = pendulum_params[Pendulum.LENGTH_KEY]\n",
        "        self.inertia = self.mass * self.length**2 / 3\n",
        "        self.g = pendulum_params[Pendulum.GRAVITY_KEY]\n",
        "        self.friction = pendulum_params[Pendulum.FRICTION_KEY]\n",
        "        self.sim_dt = pendulum_params[Pendulum.SIM_DT_KEY]\n",
        "\n",
        "        self.observation_noise_std = observation_noise_std\n",
        "        self.transition_noise_std = transition_noise_std\n",
        "\n",
        "        self.tranisition_covar_mat = onp.diag(\n",
        "            np.array([1e-8, self.transition_noise_std**2, 1e-8, 1e-8]))\n",
        "        self.observation_covar_mat = onp.diag(\n",
        "            [self.observation_noise_std**2, self.observation_noise_std**2])\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def sample_data_set(self, num_episodes, episode_length, full_targets):\n",
        "        states = onp.zeros((num_episodes, episode_length, self.state_dim))\n",
        "        actions = self._sample_action(\n",
        "            (num_episodes, episode_length, self.action_dim))\n",
        "        states[:, 0, :] = self._sample_init_state(num_episodes)\n",
        "        t = onp.zeros((num_episodes, episode_length))\n",
        "\n",
        "        for i in range(1, episode_length):\n",
        "            states[:, i, :], dt = self._get_next_states(\n",
        "                states[:, i - 1, :], actions[:, i - 1, :])\n",
        "            t[:, i:] += dt\n",
        "        states[..., 0] -= onp.pi\n",
        "\n",
        "        if self.observation_noise_std > 0.0:\n",
        "            observation_noise = self.random.normal(loc=0.0,\n",
        "                                                   scale=self.observation_noise_std,\n",
        "                                                   size=states.shape)\n",
        "        else:\n",
        "            observation_noise = onp.zeros(states.shape)\n",
        "\n",
        "        targets = self.pendulum_kinematic(states)\n",
        "        if self.observation_mode == Pendulum.OBSERVATION_MODE_LINE:\n",
        "            noisy_states = states + observation_noise\n",
        "            noisy_targets = self.pendulum_kinematic(noisy_states)\n",
        "        elif self.observation_mode == Pendulum.OBSERVATION_MODE_BALL:\n",
        "            noisy_targets = targets + observation_noise\n",
        "        imgs = self._generate_images(noisy_targets[..., :2])\n",
        "\n",
        "        return imgs, targets[..., :(4 if full_targets else 2)], states, noisy_targets[..., :(4 if full_targets else 2)], t/self.dt\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    @staticmethod\n",
        "    def pendulum_default_params():\n",
        "        return {\n",
        "            Pendulum.MAX_VELO_KEY: 8,\n",
        "            Pendulum.MAX_TORQUE_KEY: 10,\n",
        "            Pendulum.MASS_KEY: 1,\n",
        "            Pendulum.LENGTH_KEY: 1,\n",
        "            Pendulum.GRAVITY_KEY: 9.81,\n",
        "            Pendulum.FRICTION_KEY: 0,\n",
        "            Pendulum.DT_KEY: 0.05,\n",
        "            Pendulum.SIM_DT_KEY: 1e-4\n",
        "        }\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _sample_action(self, shape):\n",
        "        if self.generate_actions:\n",
        "            return self.random.uniform(-self.max_torque, self.max_torque, shape)\n",
        "        else:\n",
        "            return np.zeros(shape=shape)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _transition_function(self, states, actions):\n",
        "        dt = self.dt\n",
        "        n_steps = dt / self.sim_dt\n",
        "\n",
        "        if n_steps != np.round(n_steps):\n",
        "            #print(n_steps, 'Warning from Pendulum: dt does not match up')\n",
        "            n_steps = np.round(n_steps)\n",
        "\n",
        "        c = self.g * self.length * self.mass / self.inertia\n",
        "        for i in range(0, int(n_steps)):\n",
        "            velNew = states[..., 1:2] + self.sim_dt * (c * np.sin(states[..., 0:1])\n",
        "                                                       + actions / self.inertia\n",
        "                                                       - states[..., 1:2] * self.friction)\n",
        "            states = onp.concatenate(\n",
        "                (states[..., 0:1] + self.sim_dt * velNew, velNew), axis=1)\n",
        "        return states, dt\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _get_next_states(self, states, actions):\n",
        "        actions = np.maximum(-self.max_torque,\n",
        "                             np.minimum(actions, self.max_torque))\n",
        "\n",
        "        states, dt = self._transition_function(states, actions)\n",
        "        if self.transition_noise_std > 0.0:\n",
        "            states[:, 1] += self.random.normal(loc=0.0,\n",
        "                                               scale=self.transition_noise_std,\n",
        "                                               size=[len(states)])\n",
        "\n",
        "        states[:, 0] = ((states[:, 0]) % (2 * np.pi))\n",
        "        return states, dt\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def get_ukf_smothing(self, obs):\n",
        "        batch_size, seq_length = obs.shape[:2]\n",
        "        succ = np.zeros(batch_size, dtype=np.bool)\n",
        "        means = np.zeros([batch_size, seq_length, 4])\n",
        "        covars = np.zeros([batch_size, seq_length, 4, 4])\n",
        "        fail_ct = 0\n",
        "        for i in range(batch_size):\n",
        "            if i % 10 == 0:\n",
        "                print(i)\n",
        "            try:\n",
        "                means[i], covars[i] = self.ukf.filter(obs[i])\n",
        "                succ[i] = True\n",
        "            except:\n",
        "                fail_ct += 1\n",
        "        print(fail_ct / batch_size, \"failed\")\n",
        "\n",
        "        return means[succ], covars[succ], succ\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _sample_init_state(self, nr_epochs):\n",
        "        return onp.concatenate((self.random.uniform(0, 2 * np.pi, (nr_epochs, 1)), np.zeros((nr_epochs, 1))), 1)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def add_observation_noise(self, imgs, first_n_clean, r=0.2, t_ll=0.1, t_lu=0.4, t_ul=0.6, t_uu=0.9):\n",
        "        return add_img_noise(imgs, first_n_clean, self.random, r, t_ll, t_lu, t_ul, t_uu)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _get_task_space_pos(self, joint_states):\n",
        "        task_space_pos = onp.zeros(list(joint_states.shape[:-1]) + [2])\n",
        "        task_space_pos[..., 0] = np.sin(joint_states[..., 0]) * self.length\n",
        "        task_space_pos[..., 1] = np.cos(joint_states[..., 0]) * self.length\n",
        "        return task_space_pos\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _generate_images(self, ts_pos):\n",
        "        imgs = onp.zeros(shape=list(ts_pos.shape)[\n",
        "                        :-1] + [self.img_size, self.img_size], dtype=np.uint8)\n",
        "        for seq_idx in range(ts_pos.shape[0]):\n",
        "            for idx in range(ts_pos.shape[1]):\n",
        "                imgs[seq_idx, idx] = self._generate_single_image(\n",
        "                    ts_pos[seq_idx, idx])\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _generate_single_image(self, pos):\n",
        "        x1 = pos[0] * (self.plt_length / self.length) + self.x0\n",
        "        y1 = pos[1] * (self.plt_length / self.length) + self.y0\n",
        "        img = Image.new('F', (self.img_size_internal,\n",
        "                        self.img_size_internal), 0.0)\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        if self.observation_mode == Pendulum.OBSERVATION_MODE_LINE:\n",
        "            draw.line([(self.x0, self.y0), (x1, y1)],\n",
        "                      fill=1.0, width=self.plt_width)\n",
        "        elif self.observation_mode == Pendulum.OBSERVATION_MODE_BALL:\n",
        "            x_l = x1 - self.plt_width\n",
        "            x_u = x1 + self.plt_width\n",
        "            y_l = y1 - self.plt_width\n",
        "            y_u = y1 + self.plt_width\n",
        "            draw.ellipse((x_l, y_l, x_u, y_u), fill=1.0)\n",
        "\n",
        "        img = img.resize((self.img_size, self.img_size),\n",
        "                         resample=Image.ANTIALIAS)\n",
        "        img_as_array = onp.asarray(img)\n",
        "        img_as_array = onp.clip(img_as_array, 0, 1)\n",
        "        return 255.0 * img_as_array\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _kf_transition_function(self, state, noise):\n",
        "        nSteps = self.dt / self.sim_dt\n",
        "\n",
        "        if nSteps != np.round(nSteps):\n",
        "            print('Warning from Pendulum: dt does not match up')\n",
        "            nSteps = np.round(nSteps)\n",
        "\n",
        "        c = self.g * self.length * self.mass / self.inertia\n",
        "        for i in range(0, int(nSteps)):\n",
        "            velNew = state[1] + self.sim_dt * \\\n",
        "                (c * np.sin(state[0]) - state[1] * self.friction)\n",
        "            state = onp.array([state[0] + self.sim_dt * velNew, velNew])\n",
        "        state[0] = state[0] % (2 * np.pi)\n",
        "        state[1] = state[1] + noise[1]\n",
        "        return state\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def pendulum_kinematic_single(self, js):\n",
        "        theta, theat_dot = js\n",
        "        x = np.sin(theta)\n",
        "        y = np.cos(theta)\n",
        "        x_dot = theat_dot * y\n",
        "        y_dot = theat_dot * -x\n",
        "        return onp.array([x, y, x_dot, y_dot]) * self.length\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def pendulum_kinematic(self, js_batch):\n",
        "        theta = js_batch[..., :1]\n",
        "        theta_dot = js_batch[..., 1:]\n",
        "        x = np.sin(theta)\n",
        "        y = np.cos(theta)\n",
        "        x_dot = theta_dot * y\n",
        "        y_dot = theta_dot * -x\n",
        "        return onp.concatenate([x, y, x_dot, y_dot], axis=-1)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def inverse_pendulum_kinematics(self, ts_batch):\n",
        "        x = ts_batch[..., :1]\n",
        "        y = ts_batch[..., 1:2]\n",
        "        x_dot = ts_batch[..., 2:3]\n",
        "        y_dot = ts_batch[..., 3:]\n",
        "        val = x / y\n",
        "        theta = np.arctan2(x, y)\n",
        "        theta_dot_outer = 1 / (1 + val**2)\n",
        "        theta_dot_inner = (x_dot * y - y_dot * x) / y**2\n",
        "        return onp.concatenate([theta, theta_dot_outer * theta_dot_inner], axis=-1)\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and modified\n",
        "def generate_pendulums(file_path, task, \n",
        "                       num_train_trials=200, num_test_trials=100,\n",
        "                       impute_rate=0.5, seq_len=100, file_tag=\"\"):\n",
        "    \n",
        "    if task == 'interpolation':\n",
        "        pend_params = Pendulum.pendulum_default_params()\n",
        "        pend_params[Pendulum.FRICTION_KEY] = 0.1\n",
        "        n = seq_len\n",
        "\n",
        "        pendulum = Pendulum(24, observation_mode=Pendulum.OBSERVATION_MODE_LINE,\n",
        "                            transition_noise_std=0.1, observation_noise_std=1e-5,\n",
        "                            seed=42, pendulum_params=pend_params)\n",
        "        rng = pendulum.random\n",
        "\n",
        "        train_obs, _, _, _, train_ts = pendulum.sample_data_set(\n",
        "            num_train_trials, n, full_targets=False)\n",
        "        train_obs = onp.expand_dims(train_obs, -1)\n",
        "        train_targets = train_obs.copy()\n",
        "        train_obs_valid = rng.rand(\n",
        "            train_obs.shape[0], train_obs.shape[1], 1) > impute_rate\n",
        "        train_obs_valid[:, :5] = True\n",
        "        train_obs[onp.logical_not(onp.squeeze(train_obs_valid))] = 0\n",
        "\n",
        "        test_obs, _, _, _, test_ts = pendulum.sample_data_set(\n",
        "            num_test_trials, n, full_targets=False)\n",
        "        test_obs = onp.expand_dims(test_obs, -1)\n",
        "        test_targets = test_obs.copy()\n",
        "        test_obs_valid = rng.rand(\n",
        "            test_obs.shape[0], test_obs.shape[1], 1) > impute_rate\n",
        "        test_obs_valid[:, :5] = True\n",
        "        test_obs[onp.logical_not(onp.squeeze(test_obs_valid))] = 0\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            os.makedirs(file_path)\n",
        "        onp.savez_compressed(os.path.join(file_path, f\"pend_interpolation_ir{impute_rate}\"),\n",
        "                            train_obs=train_obs, train_targets=train_targets, train_obs_valid=train_obs_valid, train_ts=train_ts,\n",
        "                            test_obs=test_obs, test_targets=test_targets, test_obs_valid=test_obs_valid, test_ts=test_ts)\n",
        "    \n",
        "    elif task == 'regression':\n",
        "        pend_params = Pendulum.pendulum_default_params()\n",
        "        pend_params[Pendulum.FRICTION_KEY] = 0.1\n",
        "        pend_params[Pendulum.DT_KEY] = 0.01\n",
        "        n = seq_len\n",
        "\n",
        "        pendulum = Pendulum(24, observation_mode=Pendulum.OBSERVATION_MODE_LINE,\n",
        "                            transition_noise_std=0.1, observation_noise_std=1e-5,\n",
        "                            seed=42, pendulum_params=pend_params)\n",
        "\n",
        "        train_obs, train_targets, _, _, train_ts = pendulum.sample_data_set(\n",
        "            num_train_trials, n, full_targets=False)\n",
        "        # train_obs, _ = pendulum.add_observation_noise(train_obs, first_n_clean=5, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75,\n",
        "        #                                               t_uu=1.0)\n",
        "        train_obs = onp.expand_dims(train_obs, -1)\n",
        "\n",
        "        test_obs, test_targets, _, _, test_ts = pendulum.sample_data_set(\n",
        "            num_test_trials, n, full_targets=False)\n",
        "        # test_obs, _ = pendulum.add_observation_noise(test_obs, first_n_clean=5, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75,\n",
        "        #                                              t_uu=1.0)\n",
        "        test_obs = onp.expand_dims(test_obs, -1)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            os.makedirs(file_path)\n",
        "        onp.savez_compressed(os.path.join(file_path, f\"pend_regression\"+file_tag+\".npz\"),\n",
        "                            train_obs=train_obs, train_targets=train_targets, train_ts=train_ts,\n",
        "                            test_obs=test_obs, test_targets=test_targets, test_ts=test_ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Roo84N4NN9-"
      },
      "source": [
        "The full dataset is (2000, 100, 24, 24, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "RaD7Wl1f9mWx"
      },
      "outputs": [],
      "source": [
        "# @title Create the pendulum dataset (uncomment this block!)\n",
        "# Takes about 2 minutes\n",
        "# generate_pendulums(\"pendulum\", \"regression\", seq_len=200)\n",
        "# generate_pendulums(\"pendulum\", \"regression\", \n",
        "#                    num_train_trials=0, num_test_trials=100, \n",
        "#                    seq_len=400, file_tag=\"_longer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "5Op7ol3UKP0g"
      },
      "outputs": [],
      "source": [
        "# @title Load the pendulum dataset\n",
        "def load_pendulum(run_params, log=False):    \n",
        "    d = run_params[\"dataset_params\"]\n",
        "    train_trials = d[\"train_trials\"]\n",
        "    val_trials = d[\"val_trials\"]\n",
        "    noise_scale = d[\"emission_cov\"] ** 0.5\n",
        "    key_train, key_val, key_pred = jr.split(d[\"seed\"], 3)\n",
        "\n",
        "    data = np.load(\"pendulum/pend_regression.npz\")\n",
        "\n",
        "    def _process_data(data, key):\n",
        "        processed = data[:, ::2] / 255.0\n",
        "        processed += jr.normal(key=key, shape=processed.shape) * noise_scale\n",
        "        # return np.clip(processed, 0, 1)\n",
        "        return processed # We are not cliping the data anymore!\n",
        "\n",
        "    # Take subset, subsample every 2 frames, normalize to [0, 1]\n",
        "    train_data = _process_data(data[\"train_obs\"][:train_trials], key_train)\n",
        "    # val_data = _process_data(data[\"test_obs\"][:val_trials], key_val)\n",
        "    val_data = _process_data(\n",
        "        np.load(\"pendulum/pend_regression_longer.npz\")[\"test_obs\"][:val_trials], key_pred)\n",
        "\n",
        "    print(\"Full dataset:\", data[\"train_obs\"].shape)\n",
        "    print(\"Subset:\", train_data.shape)\n",
        "    return {\n",
        "        \"train_data\": train_data,\n",
        "        \"val_data\": val_data,\n",
        "        # \"val_data_pred\": val_data_pred\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU2AZWhAPjux"
      },
      "source": [
        "# Run experiments!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "_MograqHxqqs"
      },
      "outputs": [],
      "source": [
        "if (\"data_dict\" not in globals()): data_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "cellView": "form",
        "id": "DXDxYDcDrCBG"
      },
      "outputs": [],
      "source": [
        "# @title Define network architecture parameters\n",
        "\n",
        "linear_recnet_architecture = {\n",
        "    \"diagonal_covariance\": False,\n",
        "    \"input_rank\": 1,\n",
        "    \"head_mean_params\": { \"features\": [] },\n",
        "    \"head_var_params\": { \"features\": [] },\n",
        "    \"eps\": 1e-3,\n",
        "    \"cov_init\": 2,\n",
        "}\n",
        "\n",
        "BiRNN_recnet_architecture = {\n",
        "    \"input_rank\": 1,\n",
        "    \"input_type\": \"MLP\",\n",
        "    \"input_params\":{ \"features\": [20,] },\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"head_dyn_params\": { \"features\": [20,] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "planet_posterior_architecture = {\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "linear_decnet_architecture = {\n",
        "    \"diagonal_covariance\": True,\n",
        "    \"input_rank\": 1,\n",
        "    \"head_mean_params\": { \"features\": [] },\n",
        "    \"head_var_params\": { \"features\": [] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "CNN_layers = [\n",
        "            {\"features\": 32, \"kernel_size\": (3, 3), \"strides\": (1, 1) },\n",
        "            {\"features\": 64, \"kernel_size\": (3, 3), \"strides\": (2, 2) },\n",
        "            {\"features\": 32, \"kernel_size\": (3, 3), \"strides\": (2, 2) }\n",
        "]\n",
        "\n",
        "CNN_recnet_architecture = {\n",
        "    \"input_rank\": 3,\n",
        "    \"trunk_type\": \"CNN\",\n",
        "    \"trunk_params\": {\n",
        "        \"layer_params\": CNN_layers\n",
        "    },\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "CNN_BiRNN_recnet_architecture = {\n",
        "    \"input_rank\": 3,\n",
        "    \"input_type\": \"CNN\",\n",
        "    \"input_params\":{\n",
        "        \"layer_params\": CNN_layers\n",
        "    },\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"head_dyn_params\": { \"features\": [20,] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "DCNN_decnet_architecture = {\n",
        "    \"input_shape\": (6, 6, 32),\n",
        "    \"layer_params\": [\n",
        "        { \"features\": 64, \"kernel_size\": (3, 3), \"strides\": (2, 2) },\n",
        "        { \"features\": 32, \"kernel_size\": (3, 3), \"strides\": (2, 2) },\n",
        "        { \"features\": 2, \"kernel_size\": (3, 3) }\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "92Lh9HVoqqZO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run parameter expanders\n",
        "def get_lr(params, max_iters):\n",
        "    base_lr = params[\"base_lr\"]\n",
        "    prior_base_lr = params[\"prior_base_lr\"]\n",
        "    lr = base_lr\n",
        "    prior_lr = prior_base_lr\n",
        "    pprint(params)\n",
        "    if params[\"lr_decay\"]:\n",
        "        print(\"Using learning rate decay!\")\n",
        "        lr = opt.exponential_decay(init_value=base_lr, \n",
        "                                     transition_steps=max_iters,\n",
        "                                     decay_rate=0.99, \n",
        "                                   transition_begin=.8*max_iters, staircase=False)\n",
        "        # This is kind of a different scheme but whatever...\n",
        "        if params[\"prior_lr_warmup\"]:\n",
        "            prior_lr = opt.cosine_onecycle_schedule(max_iters, prior_base_lr, 0.5)\n",
        "    else:\n",
        "        lr = base_lr\n",
        "        if params[\"prior_lr_warmup\"]: \n",
        "            prior_lr = opt.linear_schedule(0, prior_base_lr, .2 * max_iters, 0)\n",
        "    return lr, prior_lr\n",
        "\n",
        "def expand_lds_parameters(params):\n",
        "    num_timesteps = params.get(\"num_timesteps\") or 200\n",
        "    train_trials = { \"small\": 10, \"medium\": 100, \"large\": 1000 }\n",
        "    batch_sizes = {\"small\": 10, \"medium\": 10, \"large\": 20 }\n",
        "    emission_noises = { \"small\": 10., \"medium\": 1., \"large\": .1 }\n",
        "    dynamics_noises = { \"small\": 0.01, \"medium\": .1, \"large\": .1 }\n",
        "    max_iters = 8000\n",
        "\n",
        "    # Modify all the architectures according to the parameters given\n",
        "    D, H, N = params[\"latent_dims\"], params[\"rnn_dims\"], params[\"emission_dims\"]\n",
        "    inf_params = {}\n",
        "    if (params[\"inference_method\"] == \"svae\"):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(linear_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "    elif (params[\"inference_method\"] in [\"dkf\", \"cdkf\"]):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianBiRNN\"\n",
        "        architecture = deepcopy(BiRNN_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "        architecture[\"rnn_dim\"] = H\n",
        "    elif (params[\"inference_method\"] == \"planet\"):\n",
        "        # Here we're considering the filtering setting as default\n",
        "        # Most likely we're not even going to use this so it should be fine\n",
        "        # If we want smoothing then we can probably just replace these with the\n",
        "        # BiRNN version\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(linear_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = H\n",
        "        post_arch = deepcopy(planet_posterior_architecture)\n",
        "        post_arch[\"input_dim\"] = H\n",
        "        post_arch[\"rnn_dim\"] = H\n",
        "        post_arch[\"output_dim\"] = D\n",
        "        inf_params[\"posterior_architecture\"] = post_arch\n",
        "        inf_params[\"sample_kl\"] = True # PlaNet doesn't have built-in suff-stats\n",
        "    else:\n",
        "        print(\"Inference method not found: \" + params[\"inference_method\"])\n",
        "        assert(False)\n",
        "    decnet_architecture = deepcopy(linear_decnet_architecture)\n",
        "    decnet_architecture[\"output_dim\"] = N\n",
        "    inf_params[\"decnet_class\"] = \"GaussianEmission\"\n",
        "    inf_params[\"decnet_architecture\"] = decnet_architecture\n",
        "    inf_params[\"recnet_architecture\"] = architecture\n",
        "\n",
        "    lr, prior_lr = get_lr(params, max_iters)\n",
        "\n",
        "    extended_params = {\n",
        "        \"project_name\": \"SVAE-LDS-Final\",\n",
        "        \"log_to_wandb\": True,\n",
        "        \"dataset\": \"lds\",\n",
        "        \"dataset_params\": {\n",
        "            \"seed\": key_0,\n",
        "            \"num_trials\": train_trials[params[\"dataset_size\"]],\n",
        "            \"num_timesteps\": num_timesteps,\n",
        "            \"emission_cov\": emission_noises[params[\"snr\"]],\n",
        "            \"dynamics_cov\": dynamics_noises[params[\"snr\"]],\n",
        "            \"latent_dims\": D,\n",
        "            \"emission_dims\": N,\n",
        "        },\n",
        "        # Implementation choice\n",
        "        \"use_parallel_kf\": False,\n",
        "        # Training specifics\n",
        "        \"max_iters\": max_iters,\n",
        "        \"elbo_samples\": 1,\n",
        "        \"sample_kl\": False,\n",
        "        \"batch_size\": batch_sizes[params[\"dataset_size\"]],\n",
        "        \"record_params\": lambda i: i % 100 == 0,\n",
        "        \"plot_interval\": 100,\n",
        "        \"learning_rate\": lr, \n",
        "        \"prior_learning_rate\": prior_lr\n",
        "    }\n",
        "    extended_params.update(inf_params)\n",
        "    # This allows us to override ANY of the above...!\n",
        "    extended_params.update(params)\n",
        "    return extended_params\n",
        "\n",
        "def expand_pendulum_parameters(params):\n",
        "    train_trials = { \"small\": 20, \"medium\": 100, \"large\": 2000 }\n",
        "    batch_sizes = {\"small\": 10, \"medium\": 10, \"large\": 40 }\n",
        "    # Not a very good validation split (mostly because we're doing one full batch for val)\n",
        "    val_trials = { \"small\": 4, \"medium\": 20, \"large\": 200 }\n",
        "    noise_scales = { \"small\": 1., \"medium\": .1, \"large\": .01 }\n",
        "    max_iters = 20000\n",
        "\n",
        "    # Modify all the architectures according to the parameters given\n",
        "    D, H = params[\"latent_dims\"], params[\"rnn_dims\"]\n",
        "    inf_params = {}\n",
        "    if (params[\"inference_method\"] == \"svae\"):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(CNN_recnet_architecture)\n",
        "        architecture[\"trunk_params\"][\"output_dim\"] = D\n",
        "        architecture[\"output_dim\"] = D\n",
        "    elif (params[\"inference_method\"] in [\"dkf\", \"cdkf\"]):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianBiRNN\"\n",
        "        architecture = deepcopy(CNN_BiRNN_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "        architecture[\"rnn_dim\"] = H\n",
        "        architecture[\"input_params\"][\"output_dim\"] = H\n",
        "    elif (params[\"inference_method\"] == \"planet\"):\n",
        "        # Here we're considering the filtering setting as default\n",
        "        # Most likely we're not even going to use this so it should be fine\n",
        "        # If we want smoothing then we can probably just replace these with the\n",
        "        # BiRNN version\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(CNN_recnet_architecture)\n",
        "        architecture[\"trunk_params\"][\"output_dim\"] = H\n",
        "        architecture[\"output_dim\"] = H\n",
        "        post_arch = deepcopy(planet_posterior_architecture)\n",
        "        post_arch[\"input_dim\"] = H\n",
        "        post_arch[\"rnn_dim\"] = H\n",
        "        post_arch[\"output_dim\"] = D\n",
        "        inf_params[\"posterior_architecture\"] = post_arch\n",
        "        inf_params[\"sample_kl\"] = True # PlaNet doesn't have built-in suff-stats\n",
        "    else:\n",
        "        print(\"Inference method not found: \" + params[\"inference_method\"])\n",
        "        assert(False)\n",
        "    inf_params[\"recnet_architecture\"] = architecture\n",
        "    \n",
        "    lr, prior_lr = get_lr(params, max_iters)\n",
        "\n",
        "    extended_params = {\n",
        "        \"project_name\": \"SVAE-Pendulum-Final\",\n",
        "        \"log_to_wandb\": True,\n",
        "        \"dataset\": \"pendulum\",\n",
        "        # Must be model learning\n",
        "        \"run_type\": \"model_learning\",\n",
        "        \"decnet_class\": \"GaussianDCNNEmission\",\n",
        "        \"decnet_architecture\": DCNN_decnet_architecture,\n",
        "        \"dataset_params\": {\n",
        "            \"seed\": key_0,\n",
        "            \"train_trials\": train_trials[params[\"dataset_size\"]],\n",
        "            \"val_trials\": val_trials[params[\"dataset_size\"]],\n",
        "            \"emission_cov\": noise_scales[params[\"snr\"]]\n",
        "        },\n",
        "        # Implementation choice\n",
        "        \"use_parallel_kf\": False,\n",
        "        # Training specifics\n",
        "        \"max_iters\": max_iters,\n",
        "        \"elbo_samples\": 1,\n",
        "        \"sample_kl\": False,\n",
        "        \"batch_size\": batch_sizes[params[\"dataset_size\"]],\n",
        "        \"record_params\": lambda i: i % 100 == 0,\n",
        "        \"plot_interval\": 200,\n",
        "        \"mask_type\": \"potential\" if params[\"inference_method\"] == \"svae\" else \"data\",\n",
        "        \"learning_rate\": lr, \n",
        "        \"prior_learning_rate\": prior_lr,\n",
        "        \"use_validation\": True,\n",
        "        \"constrain_dynamics\": True,\n",
        "        \"prediction_horizon\": 5,\n",
        "    }\n",
        "    extended_params.update(inf_params)\n",
        "    # This allows us to override ANY of the above...!\n",
        "    extended_params.update(params)\n",
        "    return extended_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpwzMT9YQSMT"
      },
      "source": [
        "## Run the LDS experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "Q5saocWweLzt"
      },
      "outputs": [],
      "source": [
        "# # @title LDS run parameters\n",
        "# run_params = {\n",
        "#     # Most important: inference method\n",
        "#     \"inference_method\": \"svae\",\n",
        "#     \"use_parallel_kf\": True,\n",
        "#     # Relevant dimensions\n",
        "#     \"latent_dims\": 3,\n",
        "#     \"rnn_dims\": 10,\n",
        "#     \"seed\": jr.PRNGKey(0),\n",
        "#     \"dataset_size\": \"small\", # \"small\", \"medium\", \"large\"\n",
        "#     \"snr\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "#     \"use_natural_grad\": False,\n",
        "#     \"constrain_prior\": True,\n",
        "#     # ---------------------------------------------\n",
        "#     \"constrain_dynamics\": True, # Truncates the singular values of A\n",
        "#     # ---------------------------------------------\n",
        "#     # We set it to true for since it's extra work to compute the sufficient stats \n",
        "#     # from smoothed potentials in the current parallel KF\n",
        "#     \"sample_kl\": False,\n",
        "#     \"base_lr\": 1e-3,\n",
        "#     \"prior_base_lr\": 1e-3, # Set to 0 for debugging\n",
        "#     \"prior_lr_warmup\": True,\n",
        "#     \"lr_decay\": False,\n",
        "#     \"group_tag\": \"\",\n",
        "#     # The only LDS-specific entries\n",
        "#     \"emission_dims\": 5,\n",
        "#     \"num_timesteps\": 10000,\n",
        "#     \"run_type\": \"model_learning\", # \"inference_only\"\n",
        "#     \"log_to_wandb\": False,\n",
        "#     \"visualize_training\": False,\n",
        "#     \"max_iters\": 2000\n",
        "# }\n",
        "\n",
        "# # run_variations = {\n",
        "# #     \"num_timesteps\": [50, 100, 200, 400, 800, 1000]\n",
        "# # }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "cqhimlvNt39G"
      },
      "outputs": [],
      "source": [
        "# jax.config.update(\"jax_debug_nans\", True)\n",
        "# jax.config.update(\"jax_enable_x64\", False)\n",
        "\n",
        "# results = experiment_scheduler(run_params, \n",
        "#                     #  run_variations=run_variations,\n",
        "#                      dataset_getter=sample_lds_dataset, \n",
        "#                      model_getter=init_model, \n",
        "#                      train_func=start_trainer,\n",
        "#                      params_expander=expand_lds_parameters,\n",
        "#                     #  on_error=on_error,\n",
        "#                      continue_on_error=False,\n",
        "#                      )\n",
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "P97PoE4yJsMV"
      },
      "outputs": [],
      "source": [
        "# model = results[1][0][\"model\"]\n",
        "# params = results[1][0][\"trainer\"].params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "3UoVwAwpQYdx"
      },
      "outputs": [],
      "source": [
        "# mean = np.array([np.mean(np.array(exp[\"trainer\"].time_spent[1:])) for exp in results[1]])\n",
        "# std = np.array([np.std(np.array(exp[\"trainer\"].time_spent[1:])) for exp in results[1]])\n",
        "# plt.plot([50, 100, 200, 400, 800, 1000], mean+std)\n",
        "# plt.plot([50, 100, 200, 400, 800, 1000], mean-std)\n",
        "# assert False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLOSSwKQ9vl3"
      },
      "source": [
        "## Dealing with float32 imprecision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "Z4jST1ldPGfO"
      },
      "outputs": [],
      "source": [
        "# # Code for unpacking and examining results\n",
        "# model = results[1][0][\"model\"]\n",
        "# params = results[1][0][\"trainer\"].params\n",
        "\n",
        "# latent_dim = 20\n",
        "# seq_len = 200\n",
        "\n",
        "# use_x64 = True\n",
        "# # Switch to x64\n",
        "# if (use_x64):\n",
        "#     jax.config.update(\"jax_enable_x64\", True)\n",
        "#     to_64 = lambda x: jax.tree_map(lambda y: np.array(y, dtype=np.float64), x)\n",
        "# else:\n",
        "#     jax.config.update(\"jax_enable_x64\", False)\n",
        "#     to_64 = lambda x: x\n",
        "\n",
        "# data = to_64(data_dict[\"train_data\"][4])\n",
        "# prior_params = to_64(params[\"prior_params\"])\n",
        "# potentials = to_64(model.recognition.apply(params[\"rec_params\"], data))\n",
        "# Sigmas = solve(potentials[\"J\"], np.eye(latent_dim)[None])\n",
        "# mus = vmap(solve)(potentials[\"J\"], potentials[\"h\"])\n",
        "# potentials[\"mu\"] = mus\n",
        "# potentials[\"Sigma\"] = Sigmas\n",
        "\n",
        "# prior_para = ParallelLieParameterizedLinearGaussianChain(latent_dim, seq_len)\n",
        "# posterior_para = ParallelLDSSVAEPosterior_Mean(latent_dim, seq_len)\n",
        "# model.posterior = posterior_para\n",
        "# prior_params_para = prior_para.get_constrained_params(prior_params)\n",
        "# post_params_para = posterior_para.infer(prior_params_para, potentials)\n",
        "\n",
        "# if (use_x64):\n",
        "#     kl_x64 = model.kl_posterior_prior(post_params_para, prior_para.get_constrained_params(prior_params))\n",
        "#     print(\"KL with x64:\", kl_x64)\n",
        "# else:\n",
        "#     kl_x32 = model.kl_posterior_prior(post_params_para, prior_para.get_constrained_params(prior_params))\n",
        "#     print(\"KL with x32:\", kl_x32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "li5AH0eRa3in"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_para = posterior_para.distribution(post_params_para)\n",
        "# posterior_dist_para.log_prob(posterior_dist_para.expected_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "XI-YukjSbqj6"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_para.entropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "_caqLPKFcnio"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_old.entropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "kLvURJK7bDg9"
      },
      "outputs": [],
      "source": [
        "# err = np.mean(posterior_dist_para.expected_states_squared \n",
        "#             - posterior_dist_old.expected_states_squared, axis=(1,2))\n",
        "# plt.plot(err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "-QOiE80IZ9vY"
      },
      "outputs": [],
      "source": [
        "# posterior_old = ParallelLDSSVAEPosterior(latent_dim, seq_len)\n",
        "# post_params_old = posterior_old.infer(prior_params_para, potentials)\n",
        "# posterior_dist_old = posterior_old.distribution(post_params_old)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "8mfPPecNavsk"
      },
      "outputs": [],
      "source": [
        "# posterior_dist_old.log_prob(posterior_dist_old.expected_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "SHtj6ZwJaGKh"
      },
      "outputs": [],
      "source": [
        "# plt.plot(post_params_old[\"mu_filtered\"] - post_params_para[\"mu_filtered\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "3ZP9aMCP3k6l"
      },
      "outputs": [],
      "source": [
        "# Sigmas = solve(potentials[\"J\"], np.eye(latent_dim)[None])\n",
        "# mus = vmap(solve)(potentials[\"J\"], potentials[\"h\"])\n",
        "# p = prior_params_para\n",
        "# params = { \"m1\": p[\"m1\"], \"Q1\": p[\"Q1\"], \n",
        "#           \"A\": p[\"A\"], \"b\": p[\"b\"], \"Q\": p[\"Q\"], \n",
        "#           \"mus\": mus, \"Sigmas\": Sigmas }\n",
        "# np.save(\"faulty_params.npy\", params, allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "II5Q019TDHkb"
      },
      "outputs": [],
      "source": [
        "# def log_normalizer(p):\n",
        "#     Q, A, b = p[\"Q\"][None], p[\"A\"][None], p[\"b\"][None]\n",
        "#     AT = (p[\"A\"].T)[None]\n",
        "\n",
        "#     I = np.eye(Q.shape[-1])\n",
        "\n",
        "#     Sigma_filtered, mu_filtered = p[\"Sigma_filtered\"][:-1], p[\"mu_filtered\"][:-1]\n",
        "#     Sigma = Q + A @ Sigma_filtered @ AT\n",
        "#     mu = (A[0] @ mu_filtered.T).T + b\n",
        "#     # Append the first element\n",
        "#     Sigma_pred = np.concatenate([p[\"Q1\"][None], Sigma])\n",
        "#     mu_pred = np.concatenate([p[\"m1\"][None], mu])\n",
        "#     mu_rec, Sigma_rec = p[\"mu\"], p[\"Sigma\"]\n",
        "\n",
        "#     def log_Z_single(mu_pred, Sigma_pred, mu_rec, Sigma_rec):\n",
        "#         return MVN(loc=mu_pred, covariance_matrix=Sigma_pred+Sigma_rec).log_prob(mu_rec)\n",
        "\n",
        "#     log_Z = vmap(log_Z_single)(mu_pred, Sigma_pred, mu_rec, Sigma_rec)\n",
        "#     return np.sum(log_Z)\n",
        "\n",
        "# log_normalizer(post_params_para) - post_params_para[\"log_Z\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "HbHRN57av7Hk"
      },
      "outputs": [],
      "source": [
        "# def log_prob(self, p, data):\n",
        "\n",
        "#     A = self._dynamics_matrix #params[\"A\"]\n",
        "#     Q = self._dynamics_noise_covariance #params[\"Q\"]\n",
        "#     Q1 = self._initial_covariance #params[\"Q1\"]\n",
        "#     m1 = self._initial_mean #params[\"m1\"]\n",
        "\n",
        "#     num_batch_dims = len(data.shape) - 2\n",
        "\n",
        "#     t1 = np.sum(\n",
        "#         MVN(loc=np.einsum(\"ij,...tj->...ti\", A, data[...,:-1,:]), \n",
        "#             covariance_matrix=Q).log_prob(data[...,1:,:])\n",
        "#         )\n",
        "    \n",
        "#     t2 = MVN(loc=m1, covariance_matrix=Q1).log_prob(data[...,0,:])\n",
        "\n",
        "#     # Add the observation potentials\n",
        "#     # t3_ = - 0.5 * np.einsum(\"...ti,tij,...tj->...t\", data, self._emissions_precisions, data)\n",
        "#     # t3 = np.sum(t3_)\n",
        "#     # t4_ = + np.einsum(\"...ti,ti->...t\", data, self._emissions_linear_potentials)\n",
        "#     # t4 = np.sum(t4_)\n",
        "#     t3 = 0\n",
        "#     t4 = np.sum(MVN(loc=self._emissions_means, \n",
        "#                   covariance_matrix=self._emissions_covariances).log_prob(data))\n",
        "#     # print(t4)\n",
        "#     # Add the log normalizer\n",
        "#     # t5 = -self._log_normalizer\n",
        "#     # t5_ = np.ones(seq_len) * t5 / seq_len\n",
        "#     t5 = -log_normalizer(p)\n",
        "\n",
        "#     s1 = t3 + t4 + t5\n",
        "#     # s1_ = np.sum(t3_ + t4_ + t5_)\n",
        "\n",
        "#     # return s1, (t1, t2, t3, t4, t5, s1, s1_)\n",
        "#     return t1 + t2 + s1, (t1, t2, t3, t4, t5, s1)\n",
        "\n",
        "# if (use_x64):\n",
        "#     dist = posterior_para.distribution(post_params_para)\n",
        "#     lp, x64_trace_ll = log_prob(dist, post_params_para, dist.expected_states)\n",
        "#     print(\"Log prob computed stepwise with x64:\", lp)\n",
        "# else:\n",
        "#     dist = posterior_para.distribution(post_params_para)\n",
        "#     lp, x32_trace_ll = log_prob(dist, post_params_para, dist.expected_states)\n",
        "#     print(\"Log prob computed stepwise with x32:\", lp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "liSyGmnRlE9l"
      },
      "outputs": [],
      "source": [
        "# def kl_posterior_prior(posterior, prior, posterior_params, prior_params):\n",
        "#     posterior = posterior.distribution(posterior_params)\n",
        "#     prior = prior.distribution(prior_params)\n",
        "#     Ex = posterior.expected_states\n",
        "#     ExxT = posterior.expected_states_squared\n",
        "#     ExnxT = posterior.expected_states_next_states\n",
        "#     Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "#     Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "#     # J, L = prior_params[\"J\"], prior_params[\"L\"]\n",
        "#     p = dynamics_to_tridiag(prior_params, Ex.shape[0], Ex.shape[1])\n",
        "#     J, L = p[\"J\"], p[\"L\"]\n",
        "\n",
        "#     p = dynamics_to_tridiag(posterior_params, Ex.shape[0], Ex.shape[1])\n",
        "#     J_post, L_post = p[\"J\"] + potentials[\"J\"], p[\"L\"]\n",
        "\n",
        "#     t1 = -prior.log_prob(Ex) \n",
        "#     # t1 = 0#-log_prob(prior, prior_params, Ex)[0]\n",
        "#     t2 = 0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "#     t3 = np.einsum(\"tij,tij->\", L, Sigmatnt)\n",
        "#     # t4 = -posterior.log_prob(Ex) \n",
        "#     t4 = -log_prob(posterior, posterior_params, Ex)[0]\n",
        "#     t5 = 0.5 * np.einsum(\"tij,tij->\", J_post, Sigmatt)\n",
        "#     t6 = np.einsum(\"tij,tij->\", L_post, Sigmatnt)\n",
        "\n",
        "#     # print(t1, t2, t3, t4, t5, t6)\n",
        "\n",
        "#     cross_entropy = t1\n",
        "#     cross_entropy += t2 #0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "#     cross_entropy += t3\n",
        "#     cross_entropy -= t4+t5+t6\n",
        "\n",
        "#     return cross_entropy, (t1, t2, t3, t4, t5, t6, t1 - t4, t2 - t5, t3 - t6)\n",
        "\n",
        "# if (use_x64):\n",
        "#     x64_kl, x64_trace = kl_posterior_prior(posterior_para, prior_para, post_params_para, prior_params_para)\n",
        "#     print(\"KL computed stepwise with x64:\", x64_kl)\n",
        "# else:\n",
        "#     x32_kl, x32_trace = kl_posterior_prior(posterior_para, prior_para, post_params_para, prior_params_para)\n",
        "#     print(\"KL computed stepwise with x32:\", x32_kl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "0kjE9yc1eNIA"
      },
      "outputs": [],
      "source": [
        "# print(\"KL computed stepwise with x32:\", x32_kl)\n",
        "# print(\"KL computed stepwise with x64:\", x64_kl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "X2LaiyyRuBRD"
      },
      "outputs": [],
      "source": [
        "# pprint([x64_trace[i] - x32_trace[i] for i in range(len(x64_trace))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ELK6UUjTnNr"
      },
      "source": [
        "## Pendulum experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "JjtwI49pGk7U"
      },
      "outputs": [],
      "source": [
        "# @title Pendulum run params\n",
        "run_params = {\n",
        "    # Most important: inference method\n",
        "    \"inference_method\": \"svae\",#\"svae\",\n",
        "    \"use_parallel_kf\": True,\n",
        "    # Relevant dimensions\n",
        "    \"latent_dims\": 5,\n",
        "    \"rnn_dims\": 10,\n",
        "    \"seed\": jr.PRNGKey(0),\n",
        "    \"dataset_size\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "    \"snr\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "    \"use_natural_grad\": False,\n",
        "    \"constrain_prior\": True,\n",
        "    \"base_lr\": 1e-2,\n",
        "    \"prior_base_lr\": 1e-3,\n",
        "    \"prior_lr_warmup\": True,\n",
        "    \"lr_decay\": False,\n",
        "    \"group_tag\": \"re-running pendulum\",\n",
        "    # The only pendulum-specific entry, will be overridden by params expander\n",
        "    \"mask_size\": 40,\n",
        "    # \"plot_interval\": 1,\n",
        "    \"mask_start\": 0,#1000,\n",
        "    \"sample_kl\": False,\n",
        "    \"log_to_wandb\": True,\n",
        "    \"max_iters\": 10000,\n",
        "}\n",
        "\n",
        "# methods = {\n",
        "#     # \"inference_method\": [\"svae\", \"cdkf\", \"planet\", \"dkf\"],\n",
        "#     # \"mask_start\": [0, 2000, 2000, 2000]\n",
        "#     \"inference_method\": [\"planet\", \"svae\"],\n",
        "#     \"mask_start\": [2000, 0],\n",
        "#     \"use_natural_grad\": [False, False],\n",
        "#     \"constrain_prior\": [True, True]\n",
        "# }\n",
        "\n",
        "seeds = {\n",
        "    \"seed\": [jr.PRNGKey(i) for i in range(3)]\n",
        "}\n",
        "\n",
        "run_variations = seeds#dict_product(seeds, methods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "25e33708c4e5462489f709c95d2fd5a8",
            "ab3a497daf684923b6a22232e28241f1",
            "d561a477705d4ec9a88f2ff0a0c82dc6",
            "09b0411fde1249de91a5b66b4ae4737d",
            "009f400bbb8f4d9da1cc6af1cc1d80c4",
            "649ee6df87e54b008f4b2c20ab0838ce",
            "b15f455258bb442f8ecc3293049e6707",
            "3890db897a2c4212b0327f183d74ed66"
          ]
        },
        "id": "7cd8Vsd5q9Dl",
        "outputId": "a6b80fb3-1305-47b1-b3e0-07a4980d396c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of runs: 1\n",
            "Base paramerters:\n",
            "{'base_lr': 0.01,\n",
            " 'constrain_prior': True,\n",
            " 'dataset_size': 'medium',\n",
            " 'group_tag': 're-running pendulum',\n",
            " 'inference_method': 'svae',\n",
            " 'latent_dims': 5,\n",
            " 'log_to_wandb': True,\n",
            " 'lr_decay': False,\n",
            " 'mask_size': 40,\n",
            " 'mask_start': 0,\n",
            " 'max_iters': 10000,\n",
            " 'prior_base_lr': 0.001,\n",
            " 'prior_lr_warmup': True,\n",
            " 'rnn_dims': 10,\n",
            " 'sample_kl': False,\n",
            " 'seed': DeviceArray([0, 0], dtype=uint32),\n",
            " 'snr': 'medium',\n",
            " 'use_natural_grad': False,\n",
            " 'use_parallel_kf': True}\n",
            "##########################################\n",
            "Starting run #0\n",
            "##########################################\n",
            "{'base_lr': 0.01,\n",
            " 'constrain_prior': True,\n",
            " 'dataset_size': 'medium',\n",
            " 'group_tag': 're-running pendulum',\n",
            " 'inference_method': 'svae',\n",
            " 'latent_dims': 5,\n",
            " 'log_to_wandb': True,\n",
            " 'lr_decay': False,\n",
            " 'mask_size': 40,\n",
            " 'mask_start': 0,\n",
            " 'max_iters': 10000,\n",
            " 'prior_base_lr': 0.001,\n",
            " 'prior_lr_warmup': True,\n",
            " 'rnn_dims': 10,\n",
            " 'sample_kl': False,\n",
            " 'seed': DeviceArray([0, 0], dtype=uint32),\n",
            " 'snr': 'medium',\n",
            " 'use_natural_grad': False,\n",
            " 'use_parallel_kf': True}\n",
            "Loading dataset!\n",
            "Full dataset: (200, 200, 24, 24, 1)\n",
            "Subset: (100, 100, 24, 24, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LP: 41821.133:   0%|          | 0/10000 [00:32<?, ?it/s]    "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230119_085518-zvbh2nrm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/matthew9671/SVAE-Pendulum-Final/runs/zvbh2nrm\" target=\"_blank\">kind-plant-74</a></strong> to <a href=\"https://wandb.ai/matthew9671/SVAE-Pendulum-Final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href=\"https://wandb.ai/matthew9671/SVAE-Pendulum-Final\" target=\"_blank\">https://wandb.ai/matthew9671/SVAE-Pendulum-Final</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href=\"https://wandb.ai/matthew9671/SVAE-Pendulum-Final/runs/zvbh2nrm\" target=\"_blank\">https://wandb.ai/matthew9671/SVAE-Pendulum-Final/runs/zvbh2nrm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP: 41821.133:   0%|          | 1/10000 [00:35<98:44:51, 35.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'base_lr': 0.01,\n",
            " 'batch_size': 10,\n",
            " 'constrain_dynamics': True,\n",
            " 'constrain_prior': True,\n",
            " 'dataset': 'pendulum',\n",
            " 'dataset_params': {'emission_cov': 0.1,\n",
            "                    'seed': DeviceArray([0, 0], dtype=uint32),\n",
            "                    'train_trials': 100,\n",
            "                    'val_trials': 20},\n",
            " 'dataset_size': 'medium',\n",
            " 'decnet_architecture': {'input_shape': (6, 6, 32),\n",
            "                         'layer_params': [{'features': 64,\n",
            "                                           'kernel_size': (3, 3),\n",
            "                                           'strides': (2, 2)},\n",
            "                                          {'features': 32,\n",
            "                                           'kernel_size': (3, 3),\n",
            "                                           'strides': (2, 2)},\n",
            "                                          {'features': 2,\n",
            "                                           'kernel_size': (3, 3)}]},\n",
            " 'decnet_class': 'GaussianDCNNEmission',\n",
            " 'elbo_samples': 1,\n",
            " 'group_tag': 're-running pendulum',\n",
            " 'inference_method': 'svae',\n",
            " 'latent_dims': 5,\n",
            " 'learning_rate': 0.01,\n",
            " 'log_to_wandb': True,\n",
            " 'lr_decay': False,\n",
            " 'mask_size': 40,\n",
            " 'mask_start': 0,\n",
            " 'mask_type': 'potential',\n",
            " 'max_iters': 10000,\n",
            " 'plot_interval': 200,\n",
            " 'prediction_horizon': 5,\n",
            " 'prior_base_lr': 0.001,\n",
            " 'prior_learning_rate': <function polynomial_schedule.<locals>.schedule at 0x7f5bded3c8b0>,\n",
            " 'prior_lr_warmup': True,\n",
            " 'project_name': 'SVAE-Pendulum-Final',\n",
            " 'recnet_architecture': {'cov_init': 1,\n",
            "                         'eps': 0.0001,\n",
            "                         'head_mean_params': {'features': [20, 20]},\n",
            "                         'head_var_params': {'features': [20, 20]},\n",
            "                         'input_rank': 3,\n",
            "                         'output_dim': 5,\n",
            "                         'trunk_params': {'layer_params': [{'features': 32,\n",
            "                                                            'kernel_size': (3,\n",
            "                                                                            3),\n",
            "                                                            'strides': (1, 1)},\n",
            "                                                           {'features': 64,\n",
            "                                                            'kernel_size': (3,\n",
            "                                                                            3),\n",
            "                                                            'strides': (2, 2)},\n",
            "                                                           {'features': 32,\n",
            "                                                            'kernel_size': (3,\n",
            "                                                                            3),\n",
            "                                                            'strides': (2, 2)}],\n",
            "                                          'output_dim': 5},\n",
            "                         'trunk_type': 'CNN'},\n",
            " 'recnet_class': 'GaussianRecognition',\n",
            " 'record_params': <function expand_pendulum_parameters.<locals>.<lambda> at 0x7f5bded3c940>,\n",
            " 'rnn_dims': 10,\n",
            " 'run_type': 'model_learning',\n",
            " 'sample_kl': False,\n",
            " 'seed': DeviceArray([0, 0], dtype=uint32),\n",
            " 'snr': 'medium',\n",
            " 'use_natural_grad': False,\n",
            " 'use_parallel_kf': True,\n",
            " 'use_validation': True}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LP: 16834.826: 100%|| 10000/10000 [28:43<00:00,  5.80it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='45.351 MB of 123.114 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.368"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25e33708c4e5462489f709c95d2fd5a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Condition number of A</td><td></td></tr><tr><td>Condition number of Q</td><td></td></tr><tr><td>ELBO</td><td></td></tr><tr><td>KL</td><td></td></tr><tr><td>Learning rate</td><td></td></tr><tr><td>Likelihood</td><td></td></tr><tr><td>Max singular value of A</td><td></td></tr><tr><td>Min singular value of A</td><td></td></tr><tr><td>Prior learning rate</td><td></td></tr><tr><td>Validation ELBO</td><td></td></tr><tr><td>Validation KL</td><td></td></tr><tr><td>Validation likelihood</td><td></td></tr><tr><td>Validation prediction log likelihood</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Condition number of A</td><td>1.11198</td></tr><tr><td>Condition number of Q</td><td>1.0</td></tr><tr><td>ELBO</td><td>-16834.82617</td></tr><tr><td>KL</td><td>220.7368</td></tr><tr><td>Learning rate</td><td>0.01</td></tr><tr><td>Likelihood</td><td>-16614.0918</td></tr><tr><td>Max singular value of A</td><td>1.0</td></tr><tr><td>Min singular value of A</td><td>0.8993</td></tr><tr><td>Prior learning rate</td><td>0.001</td></tr><tr><td>Validation ELBO</td><td>-17070.84766</td></tr><tr><td>Validation KL</td><td>247.92986</td></tr><tr><td>Validation likelihood</td><td>-16822.91797</td></tr><tr><td>Validation prediction log likelihood</td><td>-182.18813</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">kind-plant-74</strong> at: <a href=\"https://wandb.ai/matthew9671/SVAE-Pendulum-Final/runs/zvbh2nrm\" target=\"_blank\">https://wandb.ai/matthew9671/SVAE-Pendulum-Final/runs/zvbh2nrm</a><br/>Synced 5 W&B file(s), 2000 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230119_085518-zvbh2nrm/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Run the pendulum experiments\n",
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "jax.config.update(\"jax_enable_x64\", False)\n",
        "\n",
        "results = experiment_scheduler(run_params, \n",
        "                    #  run_variations=run_variations,\n",
        "                     dataset_getter=load_pendulum, \n",
        "                     model_getter=init_model, \n",
        "                     train_func=start_trainer,\n",
        "                     params_expander=expand_pendulum_parameters,\n",
        "                     on_error=on_error,\n",
        "                     continue_on_error=True)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run path: matthew9671/SVAE-Pendulum-Final/zvbh2nrm"
      ],
      "metadata": {
        "id": "bAd00_V1V_Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzngmpgUw4cb"
      },
      "source": [
        "# Pendulum Diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XbEGFOLU2i4"
      },
      "source": [
        "- [x] Visualize the samples from the predictive posterior\n",
        "- [x] Implement a shorter horizon prediction \n",
        "- [x] See how the svae performs as prediction horizon gets longer\n",
        "- [x] What about the other frameworks? -- best SVAE beats the cDKF...!\n",
        "- [x] Test the linear decoding for the entire dataset\n",
        "- [x] Package the diagnostics code into helper functions\n",
        "- [x] Also include the linear decoding accuracies (MSE) analysis\n",
        "\n",
        "TODOS:\n",
        "- [ ] Investigate why the sliding window prediction is still not giving us what we what\n",
        "- [ ] Pick one best run for all methods and plot the uncertainty estimates learned by them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "5mX32LKg6xXj"
      },
      "outputs": [],
      "source": [
        "# @title Load the run from WandB\n",
        "data_dict = {}\n",
        "\n",
        "def load_run(project_path, run_name):\n",
        "    api = wandb.Api()\n",
        "    \n",
        "    try:\n",
        "        os.remove(\"parameters.pkl\")\n",
        "    except:\n",
        "        pass\n",
        "    with open(wandb.restore(\"parameters.pkl\", project_path+run_name).name, \"rb\") as f:\n",
        "        d = pkl.load(f)\n",
        "    params = d[-1]\n",
        "\n",
        "    # Get the configs for that specific run\n",
        "    run = api.run(project_path+run_name)\n",
        "    run_params = deepcopy(run.config)\n",
        "    # Get the dataset and the model object\n",
        "\n",
        "    # run_params[\"seed\"] = np.array(run_params[\"seed\"], dtype=np.uint32)\n",
        "    # For some unknown reason we're getting an int instead of a PRNGKey object \n",
        "    run_params[\"seed\"] = jr.PRNGKey(run_params[\"seed\"])\n",
        "    # For some old runs the architecture logged is incorrect\n",
        "    # del(run_params[\"recnet_architecture\"][\"head_mean_params\"][\"features\"][-1])\n",
        "    # del(run_params[\"recnet_architecture\"][\"head_var_params\"][\"features\"][-1])\n",
        "    run_params[\"dataset_params\"][\"seed\"] = np.array(\n",
        "        run_params[\"dataset_params\"][\"seed\"], dtype=np.uint32)\n",
        "    \n",
        "    global data_dict\n",
        "\n",
        "    data_dict = load_pendulum(run_params)\n",
        "    model_dict = init_model(run_params, data_dict)\n",
        "    model = model_dict[\"model\"]\n",
        "    return run_params, params, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "LA1iJuvv4o39"
      },
      "outputs": [],
      "source": [
        "def predict_multiple(run_params, model_params, model, data, key, num_samples=6):\n",
        "    out = model.elbo(key, data, model_params, **run_params)\n",
        "    post_params = out[\"posterior_params\"]\n",
        "    posterior = model.posterior.distribution(post_params)\n",
        "    T, D = model.prior.seq_len // 2, model.prior.latent_dims\n",
        "    # Get the final mean and covariance\n",
        "    mu, Sigma = posterior.mean()[T-1], posterior.covariance()[T-1]\n",
        "    # Build the posterior object on the future latent states \n",
        "    # (\"the posterior predictive distribution\")\n",
        "    # Convert unconstrained params to constrained dynamics parameters\n",
        "    prior_params_constrained = model.prior.get_dynamics_params(model_params[\"prior_params\"])\n",
        "    dynamics_params = {\n",
        "        \"m1\": mu,\n",
        "        \"Q1\": Sigma,\n",
        "        \"A\": prior_params_constrained[\"A\"],\n",
        "        \"b\": prior_params_constrained[\"b\"],\n",
        "        \"Q\": prior_params_constrained[\"Q\"]\n",
        "    }\n",
        "\n",
        "    tridiag_params = dynamics_to_tridiag(dynamics_params, T+1, D) # Note the +1\n",
        "    J, L, h = tridiag_params[\"J\"], tridiag_params[\"L\"], tridiag_params[\"h\"]\n",
        "    pred_posterior = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "    # Sample from it and evaluate the log likelihood\n",
        "    x_preds = pred_posterior.sample(seed=key, sample_shape=(num_samples,))[:,1:]\n",
        "\n",
        "    def pred_ll(x_pred):\n",
        "        likelihood_dist = model.decoder.apply(model_params[\"dec_params\"], x_pred)\n",
        "        return likelihood_dist.log_prob(data[T:])\n",
        "\n",
        "    pred_lls = vmap(pred_ll)(x_preds)\n",
        "    # This assumes the pendulum dataset\n",
        "    pred_lls = pred_lls.sum(axis=(2, 3, 4))\n",
        "    pred_lls = pred_lls.mean(axis=0)\n",
        "    return posterior.mean(), x_preds, pred_lls\n",
        "\n",
        "def evaluate_run(project_path, run_name, key=None):\n",
        "\n",
        "    key = key_0 if key is None else key\n",
        "\n",
        "    run_params, model_params, model = load_run(project_path, run_name)\n",
        "    run_params = deepcopy(run_params)\n",
        "    run_params[\"mask_size\"] = 0\n",
        "\n",
        "    train_data = data_dict[\"train_data\"][:20,:100]\n",
        "    Ex, x_preds, pred_lls = vmap(predict_multiple, in_axes=(None, None, None, 0, None, None))\\\n",
        "        (run_params, model_params, model, train_data, key, 10)\n",
        "\n",
        "    # Also visualize a sample prediction\n",
        "    # out_dist = model.decoder.apply(model_params[\"dec_params\"], np.concatenate([Ex[0, :50], x_preds[0, 0]]))\n",
        "    # y_decoded = out_dist.mean()\n",
        "    # plt.figure()\n",
        "    # plot_img_grid(y_decoded)\n",
        "\n",
        "    targets = np.load(\"pendulum/pend_regression.npz\")[\"train_targets\"][:20]\n",
        "    states = targets[:,::2]\n",
        "    train_thetas = np.arctan2(states[:,:,0], states[:,:,1])\n",
        "    train_omegas = train_thetas[:,1:]-train_thetas[:,:-1]\n",
        "    thetas = train_thetas.flatten()\n",
        "    omegas = train_omegas.flatten()\n",
        "    D = model.prior.latent_dims\n",
        "    xs_theta = Ex.reshape((-1, D))\n",
        "    xs_omega = Ex[:,1:].reshape((-1, D))\n",
        "    W_theta, _, _, _ = np.linalg.lstsq(xs_theta, thetas)\n",
        "    W_omega, _, _, _ = np.linalg.lstsq(xs_omega, omegas)\n",
        "\n",
        "    test_targets = np.load(\"pendulum/pend_regression_longer.npz\")[\"test_targets\"][:20, ::2][:, :100]\n",
        "    test_data = data_dict[\"val_data\"][:, :100]#np.load(\"pendulum/pend_regression.npz\")[\"test_obs\"][:, ::2]\n",
        "    thetas = np.arctan2(test_targets[:,:,0], test_targets[:,:,1])\n",
        "    omegas = thetas[:,1:]-thetas[:,:-1]\n",
        "\n",
        "    def encode(data):\n",
        "        out = model.elbo(jr.PRNGKey(0), data, model_params, **run_params)\n",
        "        post_params = out[\"posterior_params\"]\n",
        "        post_dist = model.posterior.distribution(post_params)\n",
        "        return post_dist.mean()\n",
        "\n",
        "    Ex_test = vmap(encode)(test_data)\n",
        "    pred_thetas = np.einsum(\"i,...i->...\", W_theta, Ex_test)\n",
        "    theta_mse = np.mean((pred_thetas - thetas) ** 2)\n",
        "    pred_omegas = np.einsum(\"i,...i->...\", W_omega, Ex_test[:,1:])\n",
        "    omega_mse = np.mean((pred_omegas - omegas) ** 2)\n",
        "    # test_id = 0 #if \"test_id\" not in globals() else test_id + 1\n",
        "    # plt.plot(Ex_test[test_id] @ W_theta, label=\"decoded\")\n",
        "    # plt.plot(thetas[test_id], label=\"true\")\n",
        "    # plt.title(\"Linear decoding of pendulum angle (test sequence #{})\".format(test_id))\n",
        "    # plt.legend()\n",
        "    # plt.figure()\n",
        "    # plt.plot(Ex_test[test_id] @ W_omega, label=\"decoded\")\n",
        "    # plt.plot(omegas[test_id], label=\"true\")\n",
        "    # plt.title(\"Linear decoding of pendulum velocity (test sequence #{})\".format(test_id))\n",
        "    # plt.legend()\n",
        "\n",
        "    return {\n",
        "        # \"prior_sample\":,\n",
        "        \"long_horizon_pred_lls\": pred_lls,\n",
        "        \"predictions\": x_preds,\n",
        "        # \"sliding_window_pred_lls\":,\n",
        "        \"w_theta\": W_theta,\n",
        "        \"w_omega\": W_omega,\n",
        "        \"theta_mse\": theta_mse,\n",
        "        \"omega_mse\": omega_mse,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "JxCueh0pIQK_"
      },
      "outputs": [],
      "source": [
        "# result = evaluate_run(project_path, \"v6sbb9xh\", key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "ns5x_wrtKc1W"
      },
      "outputs": [],
      "source": [
        "# result = evaluate_run(project_path, \"cdt4gir1\", key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "8foTbjOSMiq2"
      },
      "outputs": [],
      "source": [
        "# _ = evaluate_run(project_path, \"dgudjrut\", key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "rsHe74D5FFVK"
      },
      "outputs": [],
      "source": [
        "# del all_results\n",
        "if (\"all_results\" not in globals()): all_results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "_uVctTR3JcBN"
      },
      "outputs": [],
      "source": [
        "api = wandb.Api()\n",
        "latent_dims = 5\n",
        "runs = api.runs(\"matthew9671/SVAE-Pendulum-ICML-3\", filters={\"State\": \"finished\", \n",
        "                                                             \"config.inference_method\": \"svae\",\n",
        "                                                             \"config.latent_dims\": latent_dims})\n",
        "svae_runs = []\n",
        "for run in runs:\n",
        "    svae_runs.append(run.id)\n",
        "runs = api.runs(\"matthew9671/SVAE-Pendulum-ICML-3\", filters={\"State\": \"finished\", \n",
        "                                                             \"config.inference_method\": \"cdkf\",\n",
        "                                                             \"config.latent_dims\": latent_dims})\n",
        "cdkf_runs = []\n",
        "for run in runs:\n",
        "    cdkf_runs.append(run.id)\n",
        "runs = api.runs(\"matthew9671/SVAE-Pendulum-ICML-3\", filters={\"State\": \"finished\", \n",
        "                                                             \"config.inference_method\": \"planet\",\n",
        "                                                             \"config.latent_dims\": latent_dims})\n",
        "planet_runs = []\n",
        "for run in runs:\n",
        "    planet_runs.append(run.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "PKPe7m3kD70V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "outputId": "5d72a03d-7213-4298-d2ef-f97d2d6fda53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading run omfjadyt\n",
            "Full dataset: (200, 200, 24, 24, 1)\n",
            "Subset: (100, 100, 24, 24, 1)\n",
            "Loading run yo3fprzr\n",
            "Full dataset: (200, 200, 24, 24, 1)\n",
            "Subset: (100, 100, 24, 24, 1)\n",
            "Loading run 1jxw27wp\n",
            "Full dataset: (200, 200, 24, 24, 1)\n",
            "Subset: (100, 100, 24, 24, 1)\n",
            "Loading run xpf9s9ie\n",
            "Full dataset: (200, 200, 24, 24, 1)\n",
            "Subset: (100, 100, 24, 24, 1)\n",
            "Loading run v6sbb9xh\n",
            "Full dataset: (200, 200, 24, 24, 1)\n",
            "Subset: (100, 100, 24, 24, 1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-2a118b8413c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mall_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmean_pred_lls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"long_horizon_pred_lls\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-fc5812563137>\u001b[0m in \u001b[0;36mevaluate_run\u001b[0;34m(project_path, run_name, key)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mEx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_lls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_multiple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mrun_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltering_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mvmap_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1680\u001b[0m     axis_size_ = (axis_size if axis_size is not None else\n\u001b[1;32m   1681\u001b[0m                   _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, \"vmap\"))\n\u001b[0;32m-> 1682\u001b[0;31m     out_flat = batching.batch(\n\u001b[0m\u001b[1;32m   1683\u001b[0m         \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_size_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_axes_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mflatten_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vmap out_axes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-fc5812563137>\u001b[0m in \u001b[0;36mpredict_multiple\u001b[0;34m(run_params, model_params, model, data, key, num_samples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_multiple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrun_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpost_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"posterior_params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mposterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9ad0c81eeaf5>\u001b[0m in \u001b[0;36melbo\u001b[0;34m(self, key, data, model_params, sample_kl, **params)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# Don't do any masking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mpotential\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Update: it makes more sense that inference is done in the posterior object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltering_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, variables, rngs, method, mutable, capture_intermediates, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1424\u001b[0m       \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m     \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_unbound_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m     return apply(\n\u001b[0m\u001b[1;32m   1427\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mmutable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmutable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/core/scope.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(variables, rngs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m     with bind(variables, rngs=rngs, mutable=mutable,\n\u001b[1;32m    907\u001b[0m               flags=flags).temporary() as root:\n\u001b[0;32m--> 908\u001b[0;31m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmutable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmutable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mscope_fn\u001b[0;34m(scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1941\u001b[0m     \u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapture_intermediates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1943\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1944\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m       \u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mwrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_wrapped_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36m_call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_use_named_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_derive_profiling_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m           \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-7a8d31dcbeb6>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPotentialNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mSigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_distribution_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m# J, h = solve(Sigma, np.eye(mu.shape[-1])[None]), solve(Sigma, mu)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# if (len(J.shape) == 3):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mwrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_wrapped_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36m_call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_use_named_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_derive_profiling_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m           \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-7a8d31dcbeb6>\u001b[0m in \u001b[0;36m_generate_distribution_parameters\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_rank\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_single\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltering_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mvmap_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1680\u001b[0m     axis_size_ = (axis_size if axis_size is not None else\n\u001b[1;32m   1681\u001b[0m                   _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, \"vmap\"))\n\u001b[0;32m-> 1682\u001b[0;31m     out_flat = batching.batch(\n\u001b[0m\u001b[1;32m   1683\u001b[0m         \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_size_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_axes_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mflatten_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vmap out_axes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mwrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_wrapped_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36m_call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_use_named_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_derive_profiling_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m           \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-7a8d31dcbeb6>\u001b[0m in \u001b[0;36m_call_single\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Apply the trunk.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mtrunk_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrunk_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Get the mean.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_mean_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunk_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mwrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_wrapped_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36m_call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_use_named_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_derive_profiling_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m           \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-a8151b98a3cf>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;31m# No activations at the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mwrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_wrapped_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36m_call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_use_named_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_derive_profiling_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m           \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpromote_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m       y = lax.conv_general_dilated(\n\u001b[0m\u001b[1;32m    437\u001b[0m           \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m           \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/lax/convolution.py\u001b[0m in \u001b[0;36mconv_general_dilated\u001b[0;34m(lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation, dimension_numbers, feature_group_count, batch_group_count, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreferred_element_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       dtypes.canonicalize_dtype(np.dtype(preferred_element_type)))\n\u001b[0;32m--> 156\u001b[0;31m   return conv_general_dilated_p.bind(\n\u001b[0m\u001b[1;32m    157\u001b[0m       \u001b[0mlhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_strides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_strides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m       \u001b[0mlhs_dilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs_dilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs_dilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs_dilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    327\u001b[0m     assert (not config.jax_enable_checks or\n\u001b[1;32m    328\u001b[0m             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/interpreters/batching.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mbatched_primitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_primitive_batcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mval_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatched_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_info_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/lax/convolution.py\u001b[0m in \u001b[0;36m_conv_general_dilated_batch_rule\u001b[0;34m(batched_args, batch_dims, window_strides, padding, lhs_dilation, rhs_dilation, dimension_numbers, feature_group_count, batch_group_count, precision, preferred_element_type, **unused_kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch_group_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m       \u001b[0mnew_lhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reshape_axis_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs_bdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlhs_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m       out = conv_general_dilated(new_lhs, rhs, window_strides, padding,\n\u001b[0m\u001b[1;32m    580\u001b[0m                                  \u001b[0mlhs_dilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs_dilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension_numbers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                                  \u001b[0mfeature_group_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/lax/convolution.py\u001b[0m in \u001b[0;36mconv_general_dilated\u001b[0;34m(lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation, dimension_numbers, feature_group_count, batch_group_count, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreferred_element_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       dtypes.canonicalize_dtype(np.dtype(preferred_element_type)))\n\u001b[0;32m--> 156\u001b[0;31m   return conv_general_dilated_p.bind(\n\u001b[0m\u001b[1;32m    157\u001b[0m       \u001b[0mlhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_strides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_strides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m       \u001b[0mlhs_dilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs_dilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs_dilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs_dilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    327\u001b[0m     assert (not config.jax_enable_checks or\n\u001b[1;32m    328\u001b[0m             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/interpreters/batching.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mbatched_primitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_primitive_batcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mval_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatched_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_info_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/lax/convolution.py\u001b[0m in \u001b[0;36m_conv_general_dilated_batch_rule\u001b[0;34m(batched_args, batch_dims, window_strides, padding, lhs_dilation, rhs_dilation, dimension_numbers, feature_group_count, batch_group_count, precision, preferred_element_type, **unused_kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m                                  \u001b[0mfeature_group_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                                  preferred_element_type=preferred_element_type)\n\u001b[0;32m--> 583\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reshape_axis_out_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlhs_bdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/lax/convolution.py\u001b[0m in \u001b[0;36m_reshape_axis_out_of\u001b[0;34m(src, size1, x)\u001b[0m\n\u001b[1;32m    749\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mragged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m   \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msize1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(operand, new_sizes, dimensions)\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0mdyn_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_new_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_tracers_dyn_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m     return reshape_p.bind(\n\u001b[0m\u001b[1;32m    884\u001b[0m       \u001b[0moperand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdyn_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic_new_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m       dimensions=None if dims is None or same_dims else dims)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    327\u001b[0m     assert (not config.jax_enable_checks or\n\u001b[1;32m    328\u001b[0m             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    113\u001b[0m   compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),\n\u001b[1;32m    114\u001b[0m                                         **params)\n\u001b[0;32m--> 115\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;31m# TODO(phawkins,frostig,mattjj): update code referring to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    198\u001b[0m                                     prim.name, donated_invars, False, *arg_specs)\n\u001b[1;32m    199\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36m_execute_compiled\u001b[0;34m(name, compiled, input_handler, output_buffer_counts, result_handler, has_unordered_effects, ordered_effects, kept_var_idx, has_host_callbacks, *args)\u001b[0m\n\u001b[1;32m    894\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     \u001b[0mout_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m   \u001b[0mcheck_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_buffer_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mordered_effects\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhas_unordered_effects\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhas_host_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mcheck_special\u001b[0;34m(name, bufs)\u001b[0m\n\u001b[1;32m    842\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mneeds_check_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbufs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m       \u001b[0m_check_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36m_check_special\u001b[0;34m(name, dtype, buf)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minexact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_debug_nans\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mFloatingPointError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"invalid value (nan) encountered in {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_debug_infs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "ax = plt.subplot()\n",
        "project_path = \"matthew9671/SVAE-Pendulum-ICML-3/\"\n",
        "# svae_runs = [\"v6sbb9xh\", \"xpf9s9ie\", \"1jxw27wp\", \"yo3fprzr\"]\n",
        "# cdkf_runs = [\"ik6i6igs\"] #\"cdt4gir1\", \"mpc58ktj\", \"dgudjrut\", \"4nctqeby\", \"cc8s8xxs\"]\n",
        "key = key_0\n",
        "for run_name in svae_runs + cdkf_runs:# + planet_runs:\n",
        "    print(\"Loading run \" + run_name)\n",
        "    key = jr.split(key)[1]\n",
        "    if (run_name not in all_results):\n",
        "        all_results[run_name] = evaluate_run(project_path, run_name, key)\n",
        "    result = all_results[run_name]\n",
        "    mean_pred_lls = result[\"long_horizon_pred_lls\"].mean(axis=0)\n",
        "    if (mean_pred_lls.mean() < -100 and run_name in svae_runs):\n",
        "        print(\"Run \" + run_name + \" gives really bad likelihoods!\")\n",
        "    if (run_name in svae_runs):\n",
        "        ax.plot(mean_pred_lls, color=\"blue\", alpha=.3)\n",
        "    elif (run_name in cdkf_runs):\n",
        "        ax.plot(mean_pred_lls, color=\"red\", alpha=.3)\n",
        "    elif (run_name in planet_runs):\n",
        "        # pass\n",
        "        ax.plot(mean_pred_lls, color=\"green\", alpha=.3)\n",
        "ax.plot(0, label=\"svae\", color=\"blue\")\n",
        "ax.plot(0, label=\"cdkf\", color=\"red\")\n",
        "ax.plot(0, label=\"planet\", color=\"green\")\n",
        "ax.set_title(\"Prediction log likelihood vs. horizon\")\n",
        "# ax.set_ylim(-100, 150)\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjH4oyyKkE-D"
      },
      "outputs": [],
      "source": [
        "svae_thetas = []\n",
        "svae_omegas = []\n",
        "for run_name in svae_runs:\n",
        "    svae_thetas.append(all_results[run_name][\"theta_mse\"])\n",
        "    svae_omegas.append(all_results[run_name][\"omega_mse\"])\n",
        "cdkf_thetas = []\n",
        "cdkf_omegas = []\n",
        "for run_name in cdkf_runs:\n",
        "    cdkf_thetas.append(all_results[run_name][\"theta_mse\"])\n",
        "    cdkf_omegas.append(all_results[run_name][\"omega_mse\"])\n",
        "planet_thetas = []\n",
        "planet_omegas = []\n",
        "for run_name in planet_runs:\n",
        "    planet_thetas.append(all_results[run_name][\"theta_mse\"])\n",
        "    planet_omegas.append(all_results[run_name][\"omega_mse\"])\n",
        "\n",
        "svae_thetas = np.array(svae_thetas)\n",
        "cdkf_thetas = np.array(cdkf_thetas)\n",
        "planet_thetas = np.array(planet_thetas)\n",
        "svae_omegas = np.array(svae_omegas) * 100\n",
        "cdkf_omegas = np.array(cdkf_omegas) * 100\n",
        "planet_omegas = np.array(planet_omegas) * 100\n",
        "bar_width = .2\n",
        "\n",
        "plt.scatter(np.zeros_like(svae_thetas)-bar_width/2, svae_thetas, s=2, color=\"blue\")\n",
        "plt.bar(-bar_width/2, svae_thetas.mean(), width=bar_width, color=\"blue\", alpha=.3, label=\"svae\")\n",
        "plt.scatter(np.zeros_like(cdkf_thetas)+bar_width/2, cdkf_thetas, s=2, color=\"red\")\n",
        "plt.bar(bar_width/2, cdkf_thetas.mean(), width=bar_width, color=\"red\", alpha=.3, label=\"cdkf\")\n",
        "plt.scatter(np.zeros_like(planet_thetas)+bar_width*3/2, planet_thetas, s=2, color=\"green\")\n",
        "plt.bar(bar_width*3/2, planet_thetas.mean(), width=bar_width, color=\"green\", alpha=.3, label=\"planet\")\n",
        "\n",
        "plt.scatter(np.zeros_like(svae_omegas)+1-bar_width/2, svae_omegas, s=2, color=\"blue\")\n",
        "plt.bar(1-bar_width/2, svae_omegas.mean(), width=bar_width, color=\"blue\", alpha=.3)\n",
        "plt.scatter(np.zeros_like(cdkf_omegas)+1+bar_width/2, cdkf_omegas, s=2, color=\"red\")\n",
        "plt.bar(1+bar_width/2, cdkf_omegas.mean(), width=bar_width, color=\"red\", alpha=.3)\n",
        "plt.scatter(np.zeros_like(planet_omegas)+1+bar_width*3/2, planet_omegas, s=2, color=\"green\")\n",
        "plt.bar(1+bar_width*3/2, planet_omegas.mean(), width=bar_width, color=\"green\", alpha=.3)\n",
        "plt.title(\"MSE for linear decoding of true pendulum state\")\n",
        "plt.xticks([0, 1], [\"theta\", \"omega\"])\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iychu_ZEV6Z7"
      },
      "outputs": [],
      "source": [
        "# @title Turns out the correlation between runs comes from the fluctuating averge pixel intensity of the image\n",
        "i = 1 # data id\n",
        "result = all_results[\"v6sbb9xh\"]\n",
        "plt.plot(result[\"long_horizon_pred_lls\"][i])\n",
        "# plt.plot(result[\"predictions\"][i][0])\n",
        "result = all_results[\"xpf9s9ie\"]\n",
        "plt.plot(result[\"long_horizon_pred_lls\"][i])\n",
        "# plt.plot(result[\"predictions\"][i][0])\n",
        "obs_mean = data_dict[\"train_data\"][i,50:100].sum(axis=(1, 2, 3))\n",
        "plt.plot((obs_mean - obs_mean.mean()) * -5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7tG1ohb4wl-"
      },
      "outputs": [],
      "source": [
        "# @title SVAE run names\n",
        "# run_name = \"391tsihg\" # 5d\n",
        "# run_name = \"vs2dkdje\" # 3d\n",
        "# run_name = \"zp5manco\" # 2d\n",
        "# 5d sinusoidal\n",
        "# run_name = \"0q0c0hbw\" \n",
        "# run_name = \"0vogdb8f\"\n",
        "# run_name = \"5b8cefgf\" # ICML-2 hopeful-sweep\n",
        "# run_name = \"7ntuood6\" # ICML-2 dainty-sweep (best prediction I've seen so far)\n",
        "# run_name = \"xpf9s9ie\" # ICML-3 good-sweep-21\n",
        "run_name = \"v6sbb9xh\" # ICML-3 rose-sweep-20 (good prediction)\n",
        "# \"1jxw27wp\" # ICML-3 solar-sweep\n",
        "# \"yo3fprzr\" # ICML-3 dry-sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJi79Ias4IP_"
      },
      "outputs": [],
      "source": [
        "# @title CDKF run names\n",
        "# run_name = \"cy1j7jyg\" # ICML-2 fancy-sweep\n",
        "run_name = \"dgudjrut\" # ICML-3 quiet-sweep\n",
        "run_name = \"4nctqeby\" # ICML-3 honest-sweep\n",
        "# \"cdt4gir1\" tough-sweep\n",
        "# \"mpc58ktj\" young-sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okamp0foh-Yg"
      },
      "outputs": [],
      "source": [
        "# If sampling from the prior becomes problematic, run this to truncate the singular values of A\n",
        "# prior_params[\"A\"] = truncate_singular_values(prior_params[\"A\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlSFRedYCyUR"
      },
      "outputs": [],
      "source": [
        "# @title Sample from the prior and visualize its decoding\n",
        "key = key_0\n",
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "prior_sample = model.prior.sample(prior_params, shape=(1,), key=key)[0]\n",
        "plt.plot(prior_sample)\n",
        "plt.figure()\n",
        "# plot_pcs(prior_sample, 2)\n",
        "out_dist = model.decoder.apply(dec_params, prior_sample)\n",
        "y_decoded = out_dist.mean()\n",
        "plt.figure()\n",
        "plot_img_grid(y_decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPgdXO-fDWue"
      },
      "outputs": [],
      "source": [
        "key = jr.split(key)[0]\n",
        "data_id = 3#jr.choice(key, 100)\n",
        "data = data_dict[\"train_data\"][data_id]\n",
        "# states = targets[data_id]\n",
        "# angles = np.arctan2(states[:,0], states[:,1])\n",
        "plot_img_grid(data)\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sZcQduzMC5m-"
      },
      "outputs": [],
      "source": [
        "# @title Visualize the mean predicted trajectory from the model\n",
        "# This might be the wrong thing to do, because the prior dynamics might not be accurate for\n",
        "# specific observation sequences\n",
        "temp_params = deepcopy(run_params)\n",
        "temp_params[\"mask_size\"] = 0\n",
        "out = model.elbo(jr.PRNGKey(0), data, params, **temp_params)\n",
        "post_params = out[\"posterior_params\"]\n",
        "post_dist = model.posterior.distribution(post_params)\n",
        "Ex = post_dist.mean()\n",
        "A = prior_params[\"A\"]\n",
        "b = prior_params[\"b\"]\n",
        "T = 100\n",
        "Ex_pred = predict_forward(Ex[T//2-1], A, b, T//2)\n",
        "hs = plt.plot(Ex)\n",
        "hs_ = plt.plot(np.arange(T//2-1, T), np.concatenate([Ex[T//2-1][None], Ex_pred]), linestyle=\":\")\n",
        "plt.title(\"Posterior and predictions\")\n",
        "for i in range(len(hs)):\n",
        "    hs_[i].set_color(hs[i].get_color())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDmP8-yldd8U"
      },
      "outputs": [],
      "source": [
        "out_dist = model.decoder.apply(dec_params, np.concatenate([Ex[:T//2], Ex_pred]))\n",
        "y_decoded = out_dist.mean()\n",
        "plt.figure()\n",
        "plot_img_grid(y_decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0aed57nAXHoP"
      },
      "outputs": [],
      "source": [
        "# @title Visualize multiple possible future paths predicted by the model...!\n",
        "\n",
        "\n",
        "jax.config.update(\"jax_debug_nans\", False)\n",
        "train_data = data_dict[\"train_data\"][:20,:100]\n",
        "x_preds, svae_pred_lls = vmap(predict_multiple, in_axes=(0, None, None))\\\n",
        "    (train_data, key_0, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhKaOqzTZHRW"
      },
      "outputs": [],
      "source": [
        "# D = model.prior.latent_dims\n",
        "# offset = 100\n",
        "\n",
        "# plt.figure(figsize=(20, 10))\n",
        "# for i in range(D):\n",
        "#     plt.plot(Ex[:,i] + i * offset, color=colors[i])\n",
        "#     plt.plot(np.arange(50, 100), x_preds[data_id,:,:,i].T + i * offset, color=colors[i], linestyle=\":\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEaCclVd0AZv"
      },
      "source": [
        "## Look at how well the physical state can be decoded from the latent representations linearly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnfLNMgc-eRl"
      },
      "outputs": [],
      "source": [
        "temp_params = deepcopy(run_params)\n",
        "temp_params[\"mask_size\"] = 0\n",
        "\n",
        "targets = np.load(\"pendulum/pend_regression.npz\")[\"train_targets\"][:100]\n",
        "\n",
        "def encode(data):\n",
        "    out = model.elbo(jr.PRNGKey(0), data, params, **temp_params)\n",
        "    post_params = out[\"posterior_params\"]\n",
        "    post_dist = model.posterior.distribution(post_params)\n",
        "    return post_dist.mean()\n",
        "\n",
        "all_latents_train = vmap(encode)(data_dict[\"train_data\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qsufn5KZYUN1"
      },
      "outputs": [],
      "source": [
        "states = targets[:,::2]\n",
        "train_thetas = np.arctan2(states[:,:,0], states[:,:,1])\n",
        "train_omegas = train_thetas[:,1:]-train_thetas[:,:-1]\n",
        "thetas = train_thetas.flatten()\n",
        "omegas = train_omegas.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ofB0o2F_qOS"
      },
      "outputs": [],
      "source": [
        "D = 5\n",
        "xs_theta = all_latents_train.reshape((-1, D))\n",
        "xs_omega = all_latents_train[:,1:].reshape((-1, D))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es8t-Udfasa8"
      },
      "outputs": [],
      "source": [
        "W_theta, _, _, _ = np.linalg.lstsq(xs_theta, thetas)\n",
        "W_omega, _, _, _ = np.linalg.lstsq(xs_omega, omegas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPZckeRTDEbx"
      },
      "outputs": [],
      "source": [
        "test_id = 0 if \"test_id\" not in globals() else test_id + 1\n",
        "plt.plot(all_latents_train[test_id] @ W_theta, label=\"decoded\")\n",
        "plt.plot(train_thetas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum angle (train sequence #{})\".format(test_id))\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(all_latents_train[test_id] @ W_omega, label=\"decoded\")\n",
        "plt.plot(train_omegas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum velocity (train sequence #{})\".format(test_id))\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQAY3_BZA4Px"
      },
      "outputs": [],
      "source": [
        "test_targets = np.load(\"pendulum/pend_regression_longer.npz\")[\"test_targets\"][:20, ::2][:, :100]\n",
        "test_data = data_dict[\"val_data\"][:, :100]#np.load(\"pendulum/pend_regression.npz\")[\"test_obs\"][:, ::2]\n",
        "thetas = np.arctan2(test_targets[:,:,0], test_targets[:,:,1])\n",
        "omegas = thetas[:,1:]-thetas[:,:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmcd1UabBcZg"
      },
      "outputs": [],
      "source": [
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "all_latents_test = vmap(encode)(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QOzPyLqZ78y"
      },
      "outputs": [],
      "source": [
        "test_id = 0 if \"test_id\" not in globals() else test_id + 1\n",
        "plt.plot(all_latents_test[test_id] @ W_theta, label=\"decoded\")\n",
        "plt.plot(thetas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum angle (test sequence #{})\".format(test_id))\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(all_latents_test[test_id] @ W_omega, label=\"decoded\")\n",
        "plt.plot(omegas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum velocity (test sequence #{})\".format(test_id))\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxjbHKWcPSmd"
      },
      "source": [
        "## Evaluate the sliding window prediction log likelihoods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvNpnf-wPXsk"
      },
      "outputs": [],
      "source": [
        "def prediction_lls(post_params):\n",
        "    posterior = model.posterior.distribution(post_params)\n",
        "    # Get the final mean and covariance\n",
        "    mu, Sigma = posterior.mean()[-1], posterior.covariance()[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAuuK-FGRUW2"
      },
      "outputs": [],
      "source": [
        "obj, out_dict = svae_loss(key, model, data_dict[\"train_data\"][:10], params, **temp_params)\n",
        "post_params = out_dict[\"posterior_params\"]\n",
        "posterior = model.posterior.distribution(post_params)\n",
        "J = posterior.filtered_precisions\n",
        "h = posterior.filtered_linear_potentials\n",
        "Sigma_filtered = inv(J)\n",
        "mu_filtered = np.einsum(\"...ij,...j->...i\", Sigma_filtered, h)\n",
        "data_batch = data_dict[\"train_data\"]\n",
        "horizon = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ1y0BMfSNWf"
      },
      "outputs": [],
      "source": [
        "def pred_ll(data_id, key):\n",
        "    num_windows = T-horizon-1\n",
        "    pred_lls = vmap(sliding_window_prediction_lls, in_axes=(None, None, None, 0, 0))(\n",
        "        mu_filtered[data_id], Sigma_filtered[data_id], data_batch[data_id],\n",
        "        np.arange(num_windows), jr.split(key, num_windows))\n",
        "    return pred_lls.mean(axis=0)\n",
        "\n",
        "def sliding_window_prediction_lls(mu, Sigma, data, t, key):\n",
        "    # Build the posterior object on the future latent states \n",
        "    # (\"the posterior predictive distribution\")\n",
        "    # Convert unconstrained params to constrained dynamics parameters\n",
        "    prior_params_constrained = model.prior.get_dynamics_params(prior_params)\n",
        "    dynamics_params = {\n",
        "        \"m1\": mu[t],\n",
        "        \"Q1\": Sigma[t],\n",
        "        \"A\": prior_params_constrained[\"A\"],\n",
        "        \"b\": prior_params_constrained[\"b\"],\n",
        "        \"Q\": prior_params_constrained[\"Q\"]\n",
        "    }\n",
        "    tridiag_params = dynamics_to_tridiag(dynamics_params, horizon+1, D) # Note the +1\n",
        "    J, L, h = tridiag_params[\"J\"], tridiag_params[\"L\"], tridiag_params[\"h\"]\n",
        "    pred_posterior = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "    # Sample from it and evaluate the log likelihood\n",
        "    x_pred = pred_posterior.sample(seed=key)[1:]\n",
        "    likelihood_dist = model.decoder.apply(dec_params, x_pred)\n",
        "    return likelihood_dist.log_prob(\n",
        "        lax.dynamic_slice(data, (t+1, 0, 0, 0), (horizon,) + data.shape[1:])).sum(axis=(1, 2, 3))\n",
        "\n",
        "num_windows = T-horizon-1\n",
        "pred_lls = vmap(pred_ll)(np.arange(10), jr.split(key_0, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoDd5GdpUAqi"
      },
      "outputs": [],
      "source": [
        "plt.plot(pred_lls.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD3BXGWQ6ko_"
      },
      "source": [
        "# What is going on with the dynamics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtChfl-eG5qm"
      },
      "outputs": [],
      "source": [
        "theta = 2 * np.pi / 100\n",
        "lds_params = {\n",
        "    \"m1\": np.zeros(2),\n",
        "    \"Q1\": np.eye(2),\n",
        "    \"A\": np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]]),\n",
        "    \"Q\": np.eye(2) / 100,\n",
        "    \"b\": np.zeros(2)\n",
        "}\n",
        "prior = LinearGaussianChain(2, 100)\n",
        "prior_dist = prior.distribution(prior.get_constrained_params(lds_params))\n",
        "plt.plot(prior_dist.sample(seed=key_0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEvsYfef6kdW"
      },
      "outputs": [],
      "source": [
        "temp_params = deepcopy(run_params)\n",
        "temp_params[\"mask_size\"] = 0\n",
        "_, aux = svae_loss(key_0, model, data_dict[\"train_data\"][:10], params, **temp_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRmFHEyb67Un"
      },
      "outputs": [],
      "source": [
        "pp = deepcopy(params[\"prior_params\"])\n",
        "suff_stats = aux[\"sufficient_statistics\"]\n",
        "pp[\"avg_suff_stats\"] = suff_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YMXhGbU7RmK"
      },
      "outputs": [],
      "source": [
        "fit_prior_params = model.prior.m_step(pp)\n",
        "# fit_prior_params[\"A\"] = scale_singular_values(fit_prior_params[\"A\"])\n",
        "# fit_prior_params[\"A\"] = truncate_singular_values(fit_prior_params[\"A\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qU7e1QiDMvy"
      },
      "outputs": [],
      "source": [
        "key = key_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnFVPlKBBhz0"
      },
      "outputs": [],
      "source": [
        "m1 = Q = fit_prior_params[\"m1\"]\n",
        "Q1 = fit_prior_params[\"Q1\"]\n",
        "Q = fit_prior_params[\"Q\"]\n",
        "A = fit_prior_params[\"A\"]\n",
        "b = fit_prior_params[\"b\"]\n",
        "\n",
        "\n",
        "x = jr.multivariate_normal(key=key, mean=m1, cov=Q1)\n",
        "xs = []\n",
        "for i in range(200):\n",
        "    xs.append(x)\n",
        "    key, _ = jr.split(key)\n",
        "    noise = jr.multivariate_normal(key=key, mean=np.zeros_like(x), cov=Q)\n",
        "    x = A @ x + b + noise\n",
        "\n",
        "plt.plot(np.array(xs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Beq52DGTBr0U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHEwqDfA8Awl"
      },
      "outputs": [],
      "source": [
        "prior = LinearGaussianChain(model.prior.latent_dims, model.prior.seq_len)\n",
        "prior_dist = prior.distribution(prior.get_constrained_params(fit_prior_params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT-OnbBX9SC7"
      },
      "outputs": [],
      "source": [
        "Q = fit_prior_params[\"Q\"]\n",
        "A = fit_prior_params[\"A\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uACoUB3T89M_"
      },
      "outputs": [],
      "source": [
        "sample = prior_dist.sample(seed=key_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISTJrxMD9Mq6"
      },
      "outputs": [],
      "source": [
        "plt.plot(sample)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "njW9wRLEPG7T",
        "8Xevru2BSSSZ",
        "U6ImmaouPD-G",
        "CpwzMT9YQSMT",
        "aLOSSwKQ9vl3",
        "KEaCclVd0AZv",
        "uxjbHKWcPSmd",
        "rD3BXGWQ6ko_"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "25e33708c4e5462489f709c95d2fd5a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab3a497daf684923b6a22232e28241f1",
              "IPY_MODEL_d561a477705d4ec9a88f2ff0a0c82dc6"
            ],
            "layout": "IPY_MODEL_09b0411fde1249de91a5b66b4ae4737d"
          }
        },
        "ab3a497daf684923b6a22232e28241f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_009f400bbb8f4d9da1cc6af1cc1d80c4",
            "placeholder": "",
            "style": "IPY_MODEL_649ee6df87e54b008f4b2c20ab0838ce",
            "value": "123.128 MB of 123.128 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "d561a477705d4ec9a88f2ff0a0c82dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b15f455258bb442f8ecc3293049e6707",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3890db897a2c4212b0327f183d74ed66",
            "value": 1
          }
        },
        "09b0411fde1249de91a5b66b4ae4737d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "009f400bbb8f4d9da1cc6af1cc1d80c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "649ee6df87e54b008f4b2c20ab0838ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b15f455258bb442f8ecc3293049e6707": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3890db897a2c4212b0327f183d74ed66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}