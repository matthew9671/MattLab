{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthew9671/MattLab/blob/main/Revisiting_SVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFyEe0Xi_84F"
      },
      "source": [
        "- [x] Implement and test Kalman filtering and smoothing with parallel scan\n",
        "- [ ] Make parallel scan KF work with non-zero biases\n",
        "- [x] Write analysis code for pendulum\n",
        "  - [x] Evaluate predictive accuracy\n",
        "  - [x] Do linear regression from latents to angle and velocity "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u_EWGy0phmX"
      },
      "outputs": [],
      "source": [
        "# This reloads files not modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BJs02uuMM79",
        "outputId": "3c9b5dea-7fdb-4f2d-ff32-0d6b22bdf0f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.1/238.1 KB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 KB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.3/174.3 KB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# @title Download stuff \n",
        "import os\n",
        "# Download and install the relevant libraries\n",
        "!pip install -q ml-collections git+https://github.com/google/flax\n",
        "!pip install -q wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njW9wRLEPG7T"
      },
      "source": [
        "# Set everything up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "cellView": "form",
        "id": "b1ikkl1ULTEB"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "# Misc\n",
        "import os\n",
        "from importlib import reload\n",
        "import numpy as onp\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "from tqdm import trange\n",
        "import copy, traceback\n",
        "from pprint import pprint\n",
        "from copy import deepcopy\n",
        "import pickle as pkl\n",
        "\n",
        "# for logging\n",
        "import wandb\n",
        "# Debug\n",
        "import pdb\n",
        "# Jax\n",
        "import jax\n",
        "from jax import vmap, lax, jit, value_and_grad\n",
        "import jax.numpy as np\n",
        "import jax.scipy as scipy\n",
        "import jax.random as jr\n",
        "key_0 = jr.PRNGKey(0) # Convenience\n",
        "from jax.lax import scan, stop_gradient\n",
        "from jax.tree_util import tree_map\n",
        "# optax\n",
        "import optax as opt\n",
        "# Flax\n",
        "import flax.linen as nn\n",
        "from flax.linen import Conv, ConvTranspose\n",
        "from flax.core import frozen_dict as fd\n",
        "\n",
        "# Tensorflow probability\n",
        "import tensorflow_probability.substrates.jax as tfp\n",
        "import tensorflow_probability.substrates.jax.distributions as tfd\n",
        "# Common math functions\n",
        "from flax.linen import softplus, sigmoid\n",
        "from jax.scipy.special import logsumexp\n",
        "from jax.scipy.linalg import solve_triangular\n",
        "from jax.numpy.linalg import eigh, cholesky, svd, inv\n",
        "\n",
        "# For typing in neural network utils\n",
        "from typing import (NamedTuple, Any, Callable, Sequence, Iterable, List, Optional, Tuple,\n",
        "                    Set, Type, Union, TypeVar, Generic, Dict)\n",
        "\n",
        "# For making the pendulum dataset\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "\n",
        "# For making nice visualizations\n",
        "from sklearn.decomposition import PCA\n",
        "from IPython.display import clear_output, HTML\n",
        "from matplotlib import animation, rc\n",
        "import seaborn as sns\n",
        "color_names = [\"windows blue\",\n",
        "                \"red\",\n",
        "                \"amber\",\n",
        "                \"faded green\",\n",
        "                \"dusty purple\",\n",
        "                \"orange\",\n",
        "                \"clay\",\n",
        "                \"pink\",\n",
        "                \"greyish\",\n",
        "                \"mint\",\n",
        "                \"light cyan\",\n",
        "                \"steel blue\",\n",
        "                \"forest green\",\n",
        "                \"pastel purple\",\n",
        "                \"salmon\",\n",
        "                \"dark brown\",\n",
        "               \"violet\",\n",
        "               \"mauve\",\n",
        "               \"ocean\",\n",
        "               \"ugly yellow\"]\n",
        "colors = sns.xkcd_palette(color_names)\n",
        "\n",
        "# Get rid of the check types warning\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "class CheckTypesFilter(logging.Filter):\n",
        "    def filter(self, record):\n",
        "        return \"check_types\" not in record.getMessage()\n",
        "logger.addFilter(CheckTypesFilter())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "2MjuUxHxR5O0"
      },
      "outputs": [],
      "source": [
        "# @title Misc helpers\n",
        "def get_value(x):\n",
        "    try:\n",
        "        return x.val.val.primal\n",
        "    except:\n",
        "        try:\n",
        "            return x.val.val\n",
        "        except:\n",
        "            try:\n",
        "                return x.val\n",
        "            except:\n",
        "                return x  # Oh well.\n",
        "\n",
        "def plot_img_grid(recon):\n",
        "    plt.figure(figsize=(8,8))\n",
        "    # Show the sequence as a block of images\n",
        "    stacked = recon.reshape(10, 24 * 10, 24)\n",
        "    imgrid = stacked.swapaxes(0, 1).reshape(24 * 10, 24 * 10)\n",
        "    plt.imshow(imgrid, vmin=0, vmax=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "id": "FGleKPUALeEd"
      },
      "outputs": [],
      "source": [
        "# @title Math helpers\n",
        "def softplus(x):\n",
        "    return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0)\n",
        "\n",
        "def inv_softplus(x, eps=1e-4):\n",
        "    return np.log(np.exp(x - eps) - 1)\n",
        "\n",
        "def vectorize_pytree(*args):\n",
        "    \"\"\"\n",
        "    Flatten an arbitrary PyTree into a vector.\n",
        "    :param args:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    flat_tree, _ = jax.tree_util.tree_flatten(args)\n",
        "    flat_vs = [x.flatten() for x in flat_tree]\n",
        "    return np.concatenate(flat_vs, axis=0)\n",
        "\n",
        "# converts an (n(n+1)/2,) vector of Lie parameters\n",
        "# to an (n, n) matrix\n",
        "def lie_params_to_constrained(out_flat, dim, eps=1e-4):\n",
        "    D, A = out_flat[:dim], out_flat[dim:]\n",
        "    # ATTENTION: we changed this!\n",
        "    # D = np.maximum(softplus(D), eps)\n",
        "    D = softplus(D) + eps\n",
        "    # Build a skew-symmetric matrix\n",
        "    S = np.zeros((dim, dim))\n",
        "    i1, i2 = np.tril_indices(dim - 1)\n",
        "    S = S.at[i1+1, i2].set(A)\n",
        "    S = S.T\n",
        "    S = S.at[i1+1, i2].set(-A)\n",
        "\n",
        "    O = scipy.linalg.expm(S)\n",
        "    J = O.T @ np.diag(D) @ O\n",
        "    return J\n",
        "\n",
        "# converts an (n, n) matrix \n",
        "# to an (n, n) matrix with singular values in (0, 1)\n",
        "def get_constrained_dynamics(A):\n",
        "    dim = A.shape[0]\n",
        "    diag = np.diag(A)\n",
        "    diag = sigmoid(diag)\n",
        "    # Build a skew-symmetric matrix\n",
        "    S = np.zeros((dim, dim))\n",
        "    i1, i2 = np.tril_indices(dim - 1)\n",
        "    S = S.at[i1+1, i2].set(A[i1+1, i2])\n",
        "    S = S.T\n",
        "    S = S.at[i1+1, i2].set(-A[i1+1, i2])\n",
        "    U = scipy.linalg.expm(S)\n",
        "\n",
        "    S = np.zeros((dim, dim))\n",
        "    i1, i2 = np.tril_indices(dim - 1)\n",
        "    S = S.at[i1+1, i2].set(A.T[i1+1, i2])\n",
        "    S = S.T\n",
        "    S = S.at[i1+1, i2].set(-A.T[i1+1, i2])\n",
        "    V = scipy.linalg.expm(S)\n",
        "\n",
        "    A = U @ np.diag(diag) @ V\n",
        "    return A, U, V\n",
        "\n",
        "def scale_singular_values(A):\n",
        "    _, s, _ = svd(A)\n",
        "    return A / (np.maximum(1, np.max(s)))\n",
        "\n",
        "def truncate_singular_values(A):\n",
        "    u, s, vt = svd(A)\n",
        "    return u @ np.diag(np.clip(s, 0.1, 1)) @ vt\n",
        "\n",
        "# Assume that h has a batch shape here\n",
        "def sample_info_gaussian(seed, J, h):\n",
        "    # Avoid inversion.\n",
        "    # see https://github.com/mattjj/pybasicbayes/blob/master/pybasicbayes/util/stats.py#L117-L122\n",
        "    L = np.linalg.cholesky(J)\n",
        "    x = jr.normal(key=seed, shape=h.shape)\n",
        "    return solve_triangular(L,x.T,lower=True,trans='T').T \\\n",
        "        + np.linalg.solve(J,h.T).T\n",
        "\n",
        "def sample_info_gaussian_old(seed, J, h):\n",
        "    cov = np.linalg.inv(J)\n",
        "    loc = np.einsum(\"...ij,...j->...i\", cov, h)\n",
        "    return tfp.distributions.MultivariateNormalFullCovariance(\n",
        "        loc=loc, covariance_matrix=cov).sample(sample_shape=(), seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "8YJraFzqBAbL"
      },
      "outputs": [],
      "source": [
        "# @title Kalman filtering and smoothing\n",
        "# def block_tridiag_mvn_log_normalizer(J_diag, J_lower_diag, h):\n",
        "#     # extract dimensions\n",
        "#     num_timesteps, dim = J_diag.shape[:2]\n",
        "\n",
        "#     # Pad the L's with one extra set of zeros for the last predict step\n",
        "#     J_lower_diag_pad = np.concatenate((J_lower_diag, \n",
        "#                                        np.zeros((1, dim, dim))), axis=0)\n",
        "\n",
        "#     def marginalize(carry, t):\n",
        "#         Jp, hp, lp = carry\n",
        "\n",
        "#         # Condition\n",
        "#         Jc = J_diag[t] + Jp\n",
        "#         hc = h[t] + hp\n",
        "\n",
        "#         # Predict -- Cholesky approach seems unstable!\n",
        "#         # sqrt_Jc = np.linalg.cholesky(Jc)\n",
        "#         # trm1 = solve_triangular(sqrt_Jc, hc, lower=True)\n",
        "#         # trm2 = solve_triangular(sqrt_Jc, J_lower_diag_pad[t].T, lower=True)\n",
        "#         # log_Z = 0.5 * dim * np.log(2 * np.pi)\n",
        "#         # log_Z += -np.sum(np.log(np.diag(sqrt_Jc)))  # sum these terms only to get approx log|J|\n",
        "#         # log_Z += 0.5 * np.dot(trm1.T, trm1)\n",
        "#         # Jp = -np.dot(trm2.T, trm2)\n",
        "#         # hp = -np.dot(trm2.T, trm1)\n",
        "\n",
        "#         # Alternative predict step:\n",
        "#         log_Z = 0.5 * dim * np.log(2 * np.pi)\n",
        "#         log_Z += -0.5 * np.linalg.slogdet(Jc)[1]\n",
        "#         log_Z += 0.5 * np.dot(hc, np.linalg.solve(Jc, hc))\n",
        "#         Jp = -np.dot(J_lower_diag_pad[t], np.linalg.solve(Jc, J_lower_diag_pad[t].T))\n",
        "#         Jp = (Jp + Jp.T) * .5   # Manual symmetrization\n",
        "#         hp = -np.dot(J_lower_diag_pad[t], np.linalg.solve(Jc, hc))\n",
        "\n",
        "#         new_carry = Jp, hp, lp + log_Z\n",
        "#         return new_carry, (Jc, hc)\n",
        "\n",
        "#     # Initialize\n",
        "#     Jp0 = np.zeros((dim, dim))\n",
        "#     hp0 = np.zeros((dim,))\n",
        "\n",
        "#     (_, _, log_Z), (filtered_Js, filtered_hs) = lax.scan(marginalize, \n",
        "#                             (Jp0, hp0, 0), np.arange(num_timesteps))\n",
        "#     return log_Z, (filtered_Js, filtered_hs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "cellView": "form",
        "id": "NeeYgySA0hPT"
      },
      "outputs": [],
      "source": [
        "# @title Experiment scheduler\n",
        "LINE_SEP = \"#\" * 42\n",
        "\n",
        "def dict_len(d):\n",
        "    if (type(d) == list):\n",
        "        return len(d)\n",
        "    else:\n",
        "        return dict_len(d[list(d.keys())[0]])\n",
        "\n",
        "def dict_map(d, func):\n",
        "    if type(d) == list:\n",
        "        return func(d)\n",
        "    elif type(d) == dict:\n",
        "        r = copy.deepcopy(d)\n",
        "        for key in d.keys():\n",
        "            r[key] = dict_map(r[key], func)\n",
        "            # Ignore all the Nones\n",
        "            if r[key] is None:\n",
        "                r.pop(key)\n",
        "        if len(r.keys()) == 0:\n",
        "            # There's no content\n",
        "            return None\n",
        "        else:\n",
        "            return r\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def dict_product(d1, d2):\n",
        "    l1, l2 = dict_len(d1), dict_len(d2)\n",
        "    def expand_list(d):\n",
        "        result = []\n",
        "        for item in d:\n",
        "            result.append(item)\n",
        "            result.extend([None] * (l2-1))\n",
        "        return result\n",
        "    def multiply_list(d):\n",
        "        return d * l1\n",
        "    result = dict_map(d1, expand_list)\n",
        "    additions = dict_map(d2, multiply_list)\n",
        "    return dict_update(result, additions)\n",
        "\n",
        "def dict_get(d, id):\n",
        "    return dict_map(d, lambda l: l[id])\n",
        "\n",
        "def dict_update(d, u):\n",
        "    if d is None:\n",
        "        d = dict()\n",
        "    for key in u.keys():\n",
        "        if type(u[key]) == dict:\n",
        "            d.update({\n",
        "                key: dict_update(d.get(key), u[key])\n",
        "            })\n",
        "        else:\n",
        "            d.update({key: u[key]})\n",
        "    return d\n",
        "\n",
        "# A standardized function that structures and schedules experiments\n",
        "# Can chain multiple variations of experiment parameters together\n",
        "def experiment_scheduler(run_params, dataset_getter, model_getter, train_func, \n",
        "                         logger_func=None, err_logger_func=None, \n",
        "                         run_variations=None, params_expander=None,\n",
        "                         on_error=None, continue_on_error=True, use_wandb=True):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        run_params: dict{\"dataset_params\"} \n",
        "            A large dictionary containing all relevant parameters to the run \n",
        "        dataset_getter: run_params -> dict{\"train_data\", [\"generative_model\"]}\n",
        "            A function that loads/samples a dataset\n",
        "        model_getter: run_params, data_dict -> model\n",
        "            A function that creates a model given parameters. Note that the model\n",
        "            could depend on the specifics of the dataset/generative model as well\n",
        "        train_func: model, data, run_params -> results\n",
        "            A function that contains the training loop. \n",
        "            TODO: later we might wanna open up this pipeline and customize further!\n",
        "        (optional) logger_func: results, run_params -> ()\n",
        "            A function that logs the current run.\n",
        "        (optional) err_logger_func: message, run_params -> ()\n",
        "            A function that is called when the run fails.\n",
        "        (optional) run_variations: dict{}\n",
        "            A nested dictionary where the leaves are lists of different parameters.\n",
        "            None means no change from parameters of the last run.\n",
        "        (optional) params_expander: dict{} -> dict{}\n",
        "            Turns high level parameters into specific low level parameters.\n",
        "    returns:\n",
        "        all_results: List<result>\n",
        "            A list containing results from all runs. Failed runs are indicated\n",
        "            with a None value.\n",
        "    \"\"\"\n",
        "    params_expander = params_expander or (lambda d: d)\n",
        "\n",
        "    num_runs = dict_len(run_variations) if run_variations else 1\n",
        "    params = copy.deepcopy(run_params)\n",
        "    print(\"Total number of runs: {}\".format(num_runs))\n",
        "    print(\"Base paramerters:\")\n",
        "    pprint(params)\n",
        "\n",
        "    global data_dict\n",
        "    all_results = []\n",
        "    all_models = []\n",
        "\n",
        "    def _single_run(data_out, model_out):\n",
        "        print(\"Loading dataset!\")\n",
        "        data_dict = dataset_getter(curr_params)\n",
        "        data_out.append(data_dict)\n",
        "        # Make a new model\n",
        "        model_dict = model_getter(curr_params, data_dict)\n",
        "        model_out.append(model_dict)\n",
        "        all_models.append(model_dict)\n",
        "        results = train_func(model_dict, data_dict, curr_params)\n",
        "        all_results.append(results)\n",
        "        if logger_func:\n",
        "            logger_func(results, curr_params, data_dict)\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        print(LINE_SEP)\n",
        "        print(\"Starting run #{}\".format(run))\n",
        "        print(LINE_SEP)\n",
        "        curr_variation = dict_get(run_variations, run)\n",
        "        if curr_variation is None:\n",
        "            if (run != 0):\n",
        "                print(\"Variation #{} is a duplicate, skipping run.\".format(run))\n",
        "                continue\n",
        "            curr_params = params_expander(params)\n",
        "        else:\n",
        "            print(\"Current parameter variation:\")\n",
        "            pprint(curr_variation)\n",
        "            curr_params = dict_update(params, curr_variation)\n",
        "            curr_params = params_expander(curr_params)\n",
        "            print(\"Current full parameters:\")\n",
        "            pprint(curr_params)\n",
        "            if curr_variation.get(\"dataset_params\"):\n",
        "                reload_data = True\n",
        "        # Hack to get the values even when they err out\n",
        "        data_out = []\n",
        "        model_out = []\n",
        "        if not continue_on_error:\n",
        "            _single_run(data_out, model_out)\n",
        "        else:\n",
        "            try:\n",
        "                _single_run(data_out, model_out)\n",
        "                if use_wandb: wandb.finish()\n",
        "            except:\n",
        "                all_results.append(None)\n",
        "                if (on_error): \n",
        "                    try:\n",
        "                        on_error(data_out[0], model_out[0])\n",
        "                    except:\n",
        "                        pass # Oh well...\n",
        "                print(\"Run errored out due to some the following reason:\")\n",
        "                traceback.print_exc()\n",
        "                if use_wandb: wandb.finish(exit_code=1)\n",
        "    return all_results, all_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OPHhcbZObuu"
      },
      "source": [
        "## Define the base SVAE object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "id": "9Pj8Xn9jNBz1"
      },
      "outputs": [],
      "source": [
        "class SVAE:\n",
        "    def __init__(self,\n",
        "                 recognition=None, decoder=None, prior=None, posterior=None,\n",
        "                 input_dummy=None, latent_dummy=None):\n",
        "        \"\"\"\n",
        "        rec_net, dec_net, prior are all objects that take in parameters\n",
        "        rec_net.apply(params, data) returns Gaussian potentials (parameters)\n",
        "        dec_net.apply(params, latents) returns probability distributions\n",
        "        prior : SVAEPrior\n",
        "        \"\"\"\n",
        "        self.recognition = recognition\n",
        "        self.decoder = decoder\n",
        "        self.prior = prior\n",
        "        self.posterior = posterior\n",
        "        self.input_dummy = input_dummy\n",
        "        self.latent_dummy = latent_dummy\n",
        "\n",
        "    def init(self, key=None):\n",
        "        if key is None:\n",
        "            key = jr.PRNGKey(0)\n",
        "\n",
        "        rec_key, dec_key, prior_key, post_key = jr.split(key, 4)\n",
        "\n",
        "        return {\n",
        "            \"rec_params\": self.recognition.init(rec_key, self.input_dummy),\n",
        "            \"dec_params\": self.decoder.init(dec_key, self.latent_dummy),\n",
        "            \"prior_params\": self.prior.init(prior_key),\n",
        "            \"post_params\": self.posterior.init(post_key)\n",
        "        }\n",
        "\n",
        "    def kl_posterior_prior(self, posterior_params, prior_params, \n",
        "                           samples=None):\n",
        "        posterior = self.posterior.distribution(posterior_params)\n",
        "        prior = self.prior.distribution(prior_params)\n",
        "        if samples is None:\n",
        "            return posterior.kl_divergence(prior)\n",
        "        else:\n",
        "            return np.mean(posterior.log_prob(samples) - prior.log_prob(samples))\n",
        "\n",
        "    def elbo(self, key, data, model_params, sample_kl=False, **params):\n",
        "        rec_params = model_params[\"rec_params\"]\n",
        "        dec_params = model_params[\"dec_params\"]\n",
        "        prior_params = self.prior.get_constrained_params(model_params[\"prior_params\"])\n",
        "\n",
        "        # Mask out a large window of states\n",
        "        mask_size = params.get(\"mask_size\")\n",
        "        T = data.shape[0]\n",
        "        mask = onp.ones((T,))\n",
        "        key, dropout_key = jr.split(key)\n",
        "        if mask_size:\n",
        "            # Potential dropout...!\n",
        "            # Use a trick to generate the mask without indexing with a tracer\n",
        "            start_id = jr.choice(dropout_key, T - mask_size + 1)\n",
        "            mask = np.array(np.arange(T) >= start_id) \\\n",
        "                 * np.array(np.arange(T) < start_id + mask_size)\n",
        "            mask = 1 - mask\n",
        "            if params.get(\"mask_type\") == \"potential\":\n",
        "                # This only works with svaes\n",
        "                potential = self.recognition.apply(rec_params, data)\n",
        "                potential = tree_map(\n",
        "                    lambda t: np.einsum(\"i,i...->i...\", mask[:t.shape[0]], t), potential)\n",
        "            else:\n",
        "                potential = self.recognition.apply(rec_params, \n",
        "                                                   np.einsum(\"t...,t->t...\", data, mask))\n",
        "        else:\n",
        "            # Don't do any masking\n",
        "            potential = self.recognition.apply(rec_params, data)\n",
        "\n",
        "        # Update: it makes more sense that inference is done in the posterior object\n",
        "        posterior_params = self.posterior.infer(prior_params, potential)\n",
        "        \n",
        "        # Take samples under the posterior\n",
        "        num_samples = params.get(\"obj_samples\") or 1\n",
        "        samples = self.posterior.sample(posterior_params, (num_samples,), key)\n",
        "        # and compute average ll\n",
        "\n",
        "        def likelihood_outputs(latent):\n",
        "            likelihood_dist = self.decoder.apply(dec_params, latent)\n",
        "            return likelihood_dist.mean(), likelihood_dist.log_prob(data)\n",
        "\n",
        "        mean, ells = vmap(likelihood_outputs)(samples)\n",
        "        # Take average over samples then sum the rest\n",
        "        ell = np.sum(np.mean(ells, axis=0))\n",
        "        # Compute kl from posterior to prior\n",
        "        if sample_kl:\n",
        "            kl = self.kl_posterior_prior(posterior_params, prior_params, \n",
        "                                         samples=samples)\n",
        "        else:\n",
        "            kl = self.kl_posterior_prior(posterior_params, prior_params)\n",
        "\n",
        "        elbo = ell - kl\n",
        "\n",
        "        return {\n",
        "            \"elbo\": elbo,\n",
        "            \"ell\": ell,\n",
        "            \"kl\": kl,\n",
        "            \"posterior_params\": posterior_params,\n",
        "            \"posterior_samples\": samples,\n",
        "            \"reconstruction\": mean,\n",
        "            \"mask\": mask\n",
        "        }\n",
        "\n",
        "    def compute_objective(self, key, data, model_params, **params):\n",
        "        results = self.elbo(key, data, model_params, **params)\n",
        "        results[\"objective\"] = results[\"elbo\"]\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "cellView": "form",
        "id": "7ckaLRUL1QVb"
      },
      "outputs": [],
      "source": [
        "# @title The DeepLDS object (implements custom kl function)\n",
        "class DeepLDS(SVAE):\n",
        "    def kl_posterior_prior(self, posterior_params, prior_params, \n",
        "                           samples=None):\n",
        "        posterior = self.posterior.distribution(posterior_params)\n",
        "        prior = self.prior.distribution(prior_params)\n",
        "        if samples is None:\n",
        "            Ex = posterior.expected_states\n",
        "            ExxT = posterior.expected_states_squared\n",
        "            ExnxT = posterior.expected_states_next_states\n",
        "            Sigmatt = ExxT - np.einsum(\"ti,tj->tij\", Ex, Ex)\n",
        "            Sigmatnt = ExnxT - np.einsum(\"ti,tj->tji\", Ex[:-1], Ex[1:])\n",
        "\n",
        "            J, L = prior_params[\"J\"], prior_params[\"L\"]\n",
        "\n",
        "            cross_entropy = -prior.log_prob(Ex)\n",
        "            cross_entropy += 0.5 * np.einsum(\"tij,tij->\", J, Sigmatt) \n",
        "            cross_entropy += np.einsum(\"tij,tij->\", L, Sigmatnt)\n",
        "            return cross_entropy - posterior.entropy()\n",
        "            \n",
        "        else:\n",
        "            return np.mean(posterior.log_prob(samples) - prior.log_prob(samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "id": "Nre5PK2MPt2N"
      },
      "outputs": [],
      "source": [
        "# @title SVAE Prior object\n",
        "class SVAEPrior:\n",
        "    def init(self, key):\n",
        "        \"\"\"\n",
        "        Returns the initial prior parameters.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def distribution(self, prior_params):\n",
        "        \"\"\"\n",
        "        Returns a tfp distribution object\n",
        "        Takes constrained params\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def m_step(self, prior_params, posterior, post_params):\n",
        "        \"\"\"\n",
        "        Returns updated prior parameters.\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def sample(self, params, shape, key):\n",
        "        return self.distribution(\n",
        "            self.get_constrained_params(params)).sample(shape, seed=key)\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        return deepcopy(params)\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bss7PlIgQRDp"
      },
      "source": [
        "## Important distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "cellView": "form",
        "id": "FUH3q6kaPyPe"
      },
      "outputs": [],
      "source": [
        "# @title MVN tridiag object (taken from ssm)\n",
        "from tensorflow_probability.python.internal import reparameterization\n",
        "\n",
        "def block_tridiag_mvn_log_normalizer(J_diag, J_lower_diag, h):\n",
        "    \"\"\" TODO\n",
        "    \"\"\"\n",
        "    # extract dimensions\n",
        "    num_timesteps, dim = J_diag.shape[:2]\n",
        "\n",
        "    # Pad the L's with one extra set of zeros for the last predict step\n",
        "    J_lower_diag_pad = np.concatenate((J_lower_diag, np.zeros((1, dim, dim))), axis=0)\n",
        "\n",
        "    def marginalize(carry, t):\n",
        "        Jp, hp, lp = carry\n",
        "\n",
        "        # Condition\n",
        "        Jc = J_diag[t] + Jp\n",
        "        hc = h[t] + hp\n",
        "\n",
        "        # Predict -- Cholesky approach seems unstable!\n",
        "        # sqrt_Jc = np.linalg.cholesky(Jc)\n",
        "        # trm1 = solve_triangular(sqrt_Jc, hc, lower=True)\n",
        "        # trm2 = solve_triangular(sqrt_Jc, J_lower_diag_pad[t].T, lower=True)\n",
        "        # log_Z = 0.5 * dim * np.log(2 * np.pi)\n",
        "        # log_Z += -np.sum(np.log(np.diag(sqrt_Jc)))  # sum these terms only to get approx log|J|\n",
        "        # log_Z += 0.5 * np.dot(trm1.T, trm1)\n",
        "        # Jp = -np.dot(trm2.T, trm2)\n",
        "        # hp = -np.dot(trm2.T, trm1)\n",
        "\n",
        "        # Alternative predict step:\n",
        "        log_Z = 0.5 * dim * np.log(2 * np.pi)\n",
        "        log_Z += -0.5 * np.linalg.slogdet(Jc)[1]\n",
        "        log_Z += 0.5 * np.dot(hc, np.linalg.solve(Jc, hc))\n",
        "        Jp = -np.dot(J_lower_diag_pad[t], np.linalg.solve(Jc, J_lower_diag_pad[t].T))\n",
        "        # Jp = (Jp + Jp.T) * .5   # Manual symmetrization\n",
        "        hp = -np.dot(J_lower_diag_pad[t], np.linalg.solve(Jc, hc))\n",
        "\n",
        "        new_carry = Jp, hp, lp + log_Z\n",
        "        return new_carry, (Jc, hc)\n",
        "\n",
        "    # Initialize\n",
        "    Jp0 = np.zeros((dim, dim))\n",
        "    hp0 = np.zeros((dim,))\n",
        "    (_, _, log_Z), (filtered_Js, filtered_hs) = lax.scan(marginalize, (Jp0, hp0, 0), np.arange(num_timesteps))\n",
        "    return log_Z, (filtered_Js, filtered_hs)\n",
        "\n",
        "class MultivariateNormalBlockTridiag(tfd.Distribution):\n",
        "    \"\"\"\n",
        "    The Gaussian linear dynamical system's posterior distribution over latent states\n",
        "    is a multivariate normal distribution whose _precision_ matrix is\n",
        "    block tridiagonal.\n",
        "\n",
        "        x | y ~ N(\\mu, \\Sigma)\n",
        "\n",
        "    where\n",
        "\n",
        "        \\Sigma^{-1} = J = [[J_{0,0},   J_{0,1},   0,       0,      0],\n",
        "                           [J_{1,0},   J_{1,1},   J_{1,2}, 0,      0],\n",
        "                           [0,         J_{2,1},   J_{2,2}, \\ddots, 0],\n",
        "                           [0,         0,         \\ddots,  \\ddots,  ],\n",
        "\n",
        "    is block tridiagonal, and J_{t, t+1} = J_{t+1, t}^T.\n",
        "\n",
        "    The pdf is\n",
        "\n",
        "        p(x) = exp \\{-1/2 x^T J x + x^T h - \\log Z(J, h) \\}\n",
        "             = exp \\{- 1/2 \\sum_{t=1}^T x_t^T J_{t,t} x_t\n",
        "                     - \\sum_{t=1}^{T-1} x_{t+1}^T J_{t+1,t} x_t\n",
        "                     + \\sum_{t=1}^T x_t^T h_t\n",
        "                     -\\log Z(J, h)\\}\n",
        "\n",
        "    where J = \\Sigma^{-1} and h = \\Sigma^{-1} \\mu = J \\mu.\n",
        "\n",
        "    Using exponential family tricks we know that\n",
        "\n",
        "        E[x_t] = \\grad_{h_t} \\log Z(J, h)\n",
        "        E[x_t x_t^T] = -2 \\grad_{J_{t,t}} \\log Z(J, h)\n",
        "        E[x_{t+1} x_t^T] = -\\grad_{J_{t+1,t}} \\log Z(J, h)\n",
        "\n",
        "    These are the expectations we need for EM.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 precision_diag_blocks,\n",
        "                 precision_lower_diag_blocks,\n",
        "                 linear_potential,\n",
        "                 log_normalizer,\n",
        "                 filtered_precisions,\n",
        "                 filtered_linear_potentials,\n",
        "                 expected_states,\n",
        "                 expected_states_squared,\n",
        "                 expected_states_next_states,\n",
        "                 validate_args=False,\n",
        "                 allow_nan_stats=True,\n",
        "                 name=\"MultivariateNormalBlockTridiag\",\n",
        "             ) -> None:\n",
        "\n",
        "        self._precision_diag_blocks = precision_diag_blocks\n",
        "        self._precision_lower_diag_blocks = precision_lower_diag_blocks\n",
        "        self._linear_potential = linear_potential\n",
        "        self._log_normalizer = log_normalizer\n",
        "        self._filtered_precisions = filtered_precisions\n",
        "        self._filtered_linear_potentials = filtered_linear_potentials\n",
        "        self._expected_states = expected_states\n",
        "        self._expected_states_squared = expected_states_squared\n",
        "        self._expected_states_next_states = expected_states_next_states\n",
        "\n",
        "        # We would detect the dtype dynamically but that would break vmap\n",
        "        # see https://github.com/tensorflow/probability/issues/1271\n",
        "        dtype = np.float32\n",
        "        super(MultivariateNormalBlockTridiag, self).__init__(\n",
        "            dtype=dtype,\n",
        "            validate_args=validate_args,\n",
        "            allow_nan_stats=allow_nan_stats,\n",
        "            reparameterization_type=reparameterization.NOT_REPARAMETERIZED,\n",
        "            parameters=dict(precision_diag_blocks=self._precision_diag_blocks,\n",
        "                            precision_lower_diag_blocks=self._precision_lower_diag_blocks,\n",
        "                            linear_potential=self._linear_potential,\n",
        "                            log_normalizer=self._log_normalizer,\n",
        "                            filtered_precisions=self._filtered_precisions,\n",
        "                            filtered_linear_potentials=self._filtered_linear_potentials,\n",
        "                            expected_states=self._expected_states,\n",
        "                            expected_states_squared=self._expected_states_squared,\n",
        "                            expected_states_next_states=self._expected_states_next_states),\n",
        "            name=name,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _parameter_properties(cls, dtype, num_classes=None):\n",
        "        # pylint: disable=g-long-lambda\n",
        "        return dict(\n",
        "            precision_diag_blocks=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            precision_lower_diag_blocks=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            linear_potential=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "            log_normalizer=tfp.internal.parameter_properties.ParameterProperties(event_ndims=0),\n",
        "            filtered_precisions=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            filtered_linear_potentials=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "            expected_states=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "            expected_states_squared=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "            expected_states_next_states=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def infer(cls,\n",
        "              precision_diag_blocks,\n",
        "              precision_lower_diag_blocks,\n",
        "              linear_potential):\n",
        "        assert precision_diag_blocks.ndim == 3\n",
        "        num_timesteps, dim = precision_diag_blocks.shape[:2]\n",
        "        assert precision_diag_blocks.shape[2] == dim\n",
        "        assert precision_lower_diag_blocks.shape == (num_timesteps - 1, dim, dim)\n",
        "        assert linear_potential.shape == (num_timesteps, dim)\n",
        "\n",
        "        # Run message passing code to get the log normalizer, the filtering potentials,\n",
        "        # and the expected values of x. Technically, the natural parameters are -1/2 J\n",
        "        # so we need to do a little correction of the gradients to get the expectations.\n",
        "        f = value_and_grad(block_tridiag_mvn_log_normalizer, argnums=(0, 1, 2), has_aux=True)\n",
        "        (log_normalizer, (filtered_precisions, filtered_linear_potentials)), grads = \\\n",
        "            f(precision_diag_blocks, precision_lower_diag_blocks, linear_potential)\n",
        "\n",
        "        # Manually symmetrize ExxT due to numerical issues...!!!\n",
        "        # Correct for the -1/2 J -> J implementation\n",
        "        expected_states_squared = - grads[0] - np.swapaxes(grads[0], -2, -1)\n",
        "        expected_states_next_states = -grads[1]\n",
        "        expected_states = grads[2]\n",
        "\n",
        "        return cls(precision_diag_blocks,\n",
        "                   precision_lower_diag_blocks,\n",
        "                   linear_potential,\n",
        "                   log_normalizer,\n",
        "                   filtered_precisions,\n",
        "                   filtered_linear_potentials,\n",
        "                   expected_states,\n",
        "                   expected_states_squared,\n",
        "                   expected_states_next_states)\n",
        "\n",
        "    @classmethod\n",
        "    def infer_from_precision_and_mean(cls,\n",
        "                                      precision_diag_blocks,\n",
        "                                      precision_lower_diag_blocks,\n",
        "                                      mean):\n",
        "        assert precision_diag_blocks.ndim == 3\n",
        "        num_timesteps, dim = precision_diag_blocks.shape[:2]\n",
        "        assert precision_diag_blocks.shape[2] == dim\n",
        "        assert precision_lower_diag_blocks.shape == (num_timesteps - 1, dim, dim)\n",
        "        assert mean.shape == (num_timesteps, dim)\n",
        "\n",
        "        # Convert the mean to the linear potential\n",
        "        linear_potential = np.einsum('tij,tj->ti', precision_diag_blocks, mean)\n",
        "        linear_potential = linear_potential.at[:-1].add(\n",
        "            np.einsum('tji,tj->ti', precision_lower_diag_blocks, mean[1:]))\n",
        "        linear_potential = linear_potential.at[1:].add(\n",
        "            np.einsum('tij,tj->ti', precision_lower_diag_blocks, mean[:-1]))\n",
        "\n",
        "        # Call the constructor above\n",
        "        return cls.infer(precision_diag_blocks,\n",
        "                         precision_lower_diag_blocks,\n",
        "                         linear_potential)\n",
        "\n",
        "    # Properties to get private class variables\n",
        "    @property\n",
        "    def precision_diag_blocks(self):\n",
        "        return self._precision_diag_blocks\n",
        "\n",
        "    @property\n",
        "    def precision_lower_diag_blocks(self):\n",
        "        return self._precision_lower_diag_blocks\n",
        "\n",
        "    @property\n",
        "    def linear_potential(self):\n",
        "        return self._linear_potential\n",
        "\n",
        "    @property\n",
        "    def log_normalizer(self):\n",
        "        return self._log_normalizer\n",
        "\n",
        "    @property\n",
        "    def filtered_precisions(self):\n",
        "        return self._filtered_precisions\n",
        "\n",
        "    @property\n",
        "    def filtered_linear_potentials(self):\n",
        "        return self._filtered_linear_potentials\n",
        "\n",
        "    @property\n",
        "    def filtered_covariances(self):\n",
        "        return inv(self._filtered_precisions)\n",
        "\n",
        "    @property\n",
        "    def filtered_means(self):\n",
        "        # TODO: this is bad numerically\n",
        "        return np.einsum(\"...ij,...j->...i\", self.filtered_covariances, \n",
        "                         self.filtered_linear_potentials)\n",
        "\n",
        "    @property\n",
        "    def expected_states(self):\n",
        "        return self._expected_states\n",
        "\n",
        "    @property\n",
        "    def expected_states_squared(self):\n",
        "        return self._expected_states_squared\n",
        "\n",
        "    @property\n",
        "    def expected_states_next_states(self):\n",
        "        return self._expected_states_next_states\n",
        "\n",
        "    def _log_prob(self, data, **kwargs):\n",
        "        lp = -0.5 * np.einsum('...ti,...tij,...tj->...', data, self._precision_diag_blocks, data)\n",
        "        lp += -np.einsum('...ti,...tij,...tj->...', data[...,1:,:], self._precision_lower_diag_blocks, data[...,:-1,:])\n",
        "        lp += np.einsum('...ti,...ti->...', data, self._linear_potential)\n",
        "        lp -= self.log_normalizer\n",
        "        return lp\n",
        "\n",
        "    def _mean(self):\n",
        "        return self.expected_states\n",
        "\n",
        "    def _covariance(self):\n",
        "        \"\"\"\n",
        "        NOTE: This computes the _marginal_ covariance Cov[x_t] for each t\n",
        "        \"\"\"\n",
        "        Ex = self._expected_states\n",
        "        ExxT = self._expected_states_squared\n",
        "        return ExxT - np.einsum(\"...i,...j->...ij\", Ex, Ex)\n",
        "\n",
        "    def _sample_n(self, n, seed=None):\n",
        "        filtered_Js = self._filtered_precisions\n",
        "        filtered_hs = self._filtered_linear_potentials\n",
        "        J_lower_diag = self._precision_lower_diag_blocks\n",
        "\n",
        "        def sample_single(seed, filtered_Js, filtered_hs, J_lower_diag):\n",
        "\n",
        "            def _sample_info_gaussian(seed, J, h, sample_shape=()):\n",
        "                # TODO: avoid inversion.\n",
        "                # see https://github.com/mattjj/pybasicbayes/blob/master/pybasicbayes/util/stats.py#L117-L122\n",
        "                # L = np.linalg.cholesky(J)\n",
        "                # x = np.random.randn(h.shape[0])\n",
        "                # return scipy.linalg.solve_triangular(L,x,lower=True,trans='T') \\\n",
        "                #     + dpotrs(L,h,lower=True)[0]\n",
        "                cov = np.linalg.inv(J)\n",
        "                loc = np.einsum(\"...ij,...j->...i\", cov, h)\n",
        "                return tfp.distributions.MultivariateNormalFullCovariance(\n",
        "                    loc=loc, covariance_matrix=cov).sample(sample_shape=sample_shape, seed=seed)\n",
        "\n",
        "            def _step(carry, inpt):\n",
        "                x_next, seed = carry\n",
        "                Jf, hf, L = inpt\n",
        "\n",
        "                # Condition on the next observation\n",
        "                Jc = Jf\n",
        "                hc = hf - np.einsum('ni,ij->nj', x_next, L)\n",
        "\n",
        "                # Split the seed\n",
        "                seed, this_seed = jr.split(seed)\n",
        "                x = _sample_info_gaussian(this_seed, Jc, hc)\n",
        "                return (x, seed), x\n",
        "\n",
        "            # Initialize with sample of last timestep and sample in reverse\n",
        "            seed_T, seed = jr.split(seed)\n",
        "            x_T = _sample_info_gaussian(seed_T, filtered_Js[-1], filtered_hs[-1], sample_shape=(n,))\n",
        "            inputs = (filtered_Js[:-1][::-1], filtered_hs[:-1][::-1], J_lower_diag[::-1])\n",
        "            _, x_rev = lax.scan(_step, (x_T, seed), inputs)\n",
        "\n",
        "            # Reverse and concatenate the last time-step's sample\n",
        "            x = np.concatenate((x_rev[::-1], x_T[None, ...]), axis=0)\n",
        "\n",
        "            # Transpose to be (num_samples, num_timesteps, dim)\n",
        "            return np.transpose(x, (1, 0, 2))\n",
        "\n",
        "        # TODO: Handle arbitrary batch shapes\n",
        "        if filtered_Js.ndim == 4:\n",
        "            # batch mode\n",
        "            samples = vmap(sample_single)(seed, filtered_Js, filtered_hs, J_lower_diag)\n",
        "            # Transpose to be (num_samples, num_batches, num_timesteps, dim)\n",
        "            samples = np.transpose(samples, (1, 0, 2, 3))\n",
        "        else:\n",
        "            # non-batch mode\n",
        "            samples = sample_single(seed, filtered_Js, filtered_hs, J_lower_diag)\n",
        "        return samples\n",
        "\n",
        "    # def _sample_n(self, n, seed=None):\n",
        "    #     filtered_Js = self._filtered_precisions\n",
        "    #     filtered_hs = self._filtered_linear_potentials\n",
        "    #     J_lower_diag = self._precision_lower_diag_blocks\n",
        "\n",
        "    #     def sample_single(seed, filtered_Js, filtered_hs, J_lower_diag):\n",
        "\n",
        "    #         # def _sample_info_gaussian(seed, J, h, sample_shape=()):\n",
        "    #         #     # TODO: avoid inversion.\n",
        "    #         #     # see https://github.com/mattjj/pybasicbayes/blob/master/pybasicbayes/util/stats.py#L117-L122\n",
        "    #         #     # L = np.linalg.cholesky(J)\n",
        "    #         #     # x = np.random.randn(h.shape[0])\n",
        "    #         #     # return scipy.linalg.solve_triangular(L,x,lower=True,trans='T') \\\n",
        "    #         #     #     + dpotrs(L,h,lower=True)[0]\n",
        "    #         #     cov = np.linalg.inv(J)\n",
        "    #         #     loc = np.einsum(\"...ij,...j->...i\", cov, h)\n",
        "    #         #     return tfp.distributions.MultivariateNormalFullCovariance(\n",
        "    #         #         loc=loc, covariance_matrix=cov).sample(sample_shape=sample_shape, seed=seed)\n",
        "\n",
        "    #         def _step(carry, inpt):\n",
        "    #             x_next, seed = carry\n",
        "    #             Jf, hf, L = inpt\n",
        "\n",
        "    #             # Condition on the next observation\n",
        "    #             Jc = Jf\n",
        "    #             hc = hf - np.einsum('ni,ij->nj', x_next, L)\n",
        "\n",
        "    #             # Split the seed\n",
        "    #             seed, this_seed = jr.split(seed)\n",
        "    #             x = sample_info_gaussian(this_seed, Jc, hc)\n",
        "    #             return (x, seed), x\n",
        "\n",
        "    #         # Initialize with sample of last timestep and sample in reverse\n",
        "    #         seed_T, seed = jr.split(seed)\n",
        "    #         x_T = sample_info_gaussian(seed_T, filtered_Js[-1], filtered_hs[-1][None] * np.ones((n, 1)))\n",
        "    #         inputs = (filtered_Js[:-1][::-1], filtered_hs[:-1][::-1], J_lower_diag[::-1])\n",
        "    #         _, x_rev = lax.scan(_step, (x_T, seed), inputs)\n",
        "\n",
        "    #         # Reverse and concatenate the last time-step's sample\n",
        "    #         x = np.concatenate((x_rev[::-1], x_T[None, ...]), axis=0)\n",
        "\n",
        "    #         # Transpose to be (num_samples, num_timesteps, dim)\n",
        "    #         return np.transpose(x, (1, 0, 2))\n",
        "\n",
        "    #     # TODO: Handle arbitrary batch shapes\n",
        "    #     if filtered_Js.ndim == 4:\n",
        "    #         # batch mode\n",
        "    #         samples = vmap(sample_single)(seed, filtered_Js, filtered_hs, J_lower_diag)\n",
        "    #         # Transpose to be (num_samples, num_batches, num_timesteps, dim)\n",
        "    #         samples = np.transpose(samples, (1, 0, 2, 3))\n",
        "    #     else:\n",
        "    #         # non-batch mode\n",
        "    #         samples = sample_single(seed, filtered_Js, filtered_hs, J_lower_diag)\n",
        "    #     return samples\n",
        "\n",
        "    def _entropy(self):\n",
        "        \"\"\"\n",
        "        Compute the entropy\n",
        "\n",
        "            H[X] = -E[\\log p(x)]\n",
        "                 = -E[-1/2 x^T J x + x^T h - log Z(J, h)]\n",
        "                 = 1/2 <J, E[x x^T] - <h, E[x]> + log Z(J, h)\n",
        "        \"\"\"\n",
        "        Ex = self._expected_states\n",
        "        ExxT = self._expected_states_squared\n",
        "        ExnxT = self._expected_states_next_states\n",
        "        J_diag = self._precision_diag_blocks\n",
        "        J_lower_diag = self._precision_lower_diag_blocks\n",
        "        h = self._linear_potential\n",
        "\n",
        "        entropy = 0.5 * np.sum(J_diag * ExxT)\n",
        "        entropy += np.sum(J_lower_diag * ExnxT)\n",
        "        entropy -= np.sum(h * Ex)\n",
        "        entropy += self.log_normalizer\n",
        "        return entropy\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        children = (self._precision_diag_blocks,\n",
        "                    self._precision_lower_diag_blocks,\n",
        "                    self._linear_potential,\n",
        "                    self._log_normalizer,\n",
        "                    self._filtered_precisions,\n",
        "                    self._filtered_linear_potentials,\n",
        "                    self._expected_states,\n",
        "                    self._expected_states_squared,\n",
        "                    self._expected_states_next_states)\n",
        "        aux_data = None\n",
        "        return children, aux_data\n",
        "\n",
        "    @classmethod\n",
        "    def tree_unflatten(cls, aux_data, children):\n",
        "        return cls(*children)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "cellView": "form",
        "id": "6D7qIBFpRyH8"
      },
      "outputs": [],
      "source": [
        "# @title Linear Gaussian chain prior\n",
        "def random_rotation(seed, n, theta=None):\n",
        "    key1, key2 = jr.split(seed)\n",
        "\n",
        "    if theta is None:\n",
        "        # Sample a random, slow rotation\n",
        "        theta = 0.5 * np.pi * jr.uniform(key1)\n",
        "\n",
        "    if n == 1:\n",
        "        return jr.uniform(key1) * np.eye(1)\n",
        "\n",
        "    rot = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
        "    out = np.eye(n)\n",
        "    out = out.at[:2, :2].set(rot)\n",
        "    q = np.linalg.qr(jr.uniform(key2, shape=(n, n)))[0]\n",
        "    return q.dot(out).dot(q.T)\n",
        "\n",
        "# Computes ATQ-1A in a way that's guaranteed to be symmetric\n",
        "def inv_quad_form(Q, A):\n",
        "    sqrt_Q = np.linalg.cholesky(Q)\n",
        "    trm = solve_triangular(sqrt_Q, A, lower=True, check_finite=False)\n",
        "    return trm.T @ trm\n",
        "\n",
        "def inv_symmetric(Q):\n",
        "    sqrt_Q = np.linalg.cholesky(Q)\n",
        "    sqrt_Q_inv = np.linalg.inv(sqrt_Q)\n",
        "    return sqrt_Q_inv.T @ sqrt_Q_inv\n",
        "\n",
        "# Converts from (A, b, Q) to (J, L, h)\n",
        "def dynamics_to_tridiag(dynamics_params, T, D):\n",
        "    Q1, m1, A, Q, b = dynamics_params[\"Q1\"], \\\n",
        "        dynamics_params[\"m1\"], dynamics_params[\"A\"], \\\n",
        "        dynamics_params[\"Q\"], dynamics_params[\"b\"]\n",
        "    # diagonal blocks of precision matrix\n",
        "    J = np.zeros((T, D, D))\n",
        "    J = J.at[0].add(inv_symmetric(Q1))\n",
        "\n",
        "    J = J.at[:-1].add(inv_quad_form(Q, A))\n",
        "    J = J.at[1:].add(inv_symmetric(Q))\n",
        "    # lower diagonal blocks of precision matrix\n",
        "    L = -np.linalg.solve(Q, A)\n",
        "    L = np.tile(L[None, :, :], (T - 1, 1, 1))\n",
        "    # linear potential\n",
        "    h = np.zeros((T, D)) \n",
        "    h = h.at[0].add(np.linalg.solve(Q1, m1))\n",
        "    h = h.at[:-1].add(-np.dot(A.T, np.linalg.solve(Q, b)))\n",
        "    h = h.at[1:].add(np.linalg.solve(Q, b))\n",
        "    return { \"J\": J, \"L\": L, \"h\": h }\n",
        "\n",
        "# Helper function: solve a linear regression given expected sufficient statistics\n",
        "def fit_linear_regression(Ex, Ey, ExxT, EyxT, EyyT, En):\n",
        "    big_ExxT = np.row_stack([np.column_stack([ExxT, Ex]),\n",
        "                            np.concatenate( [Ex.T, np.array([En])])])\n",
        "    big_EyxT = np.column_stack([EyxT, Ey])\n",
        "    Cd = np.linalg.solve(big_ExxT, big_EyxT.T).T\n",
        "    C, d = Cd[:, :-1], Cd[:, -1]\n",
        "    R = (EyyT - 2 * Cd @ big_EyxT.T + Cd @ big_ExxT @ Cd.T) / En\n",
        "\n",
        "    # Manually symmetrize R\n",
        "    R = (R + R.T) / 2\n",
        "    return C, d, R\n",
        "\n",
        "# This is a linear Gaussian chain\n",
        "class LinearGaussianChain(SVAEPrior):\n",
        "\n",
        "    # We're not using this at the moment\n",
        "    # COVARIANCE_REGULARIZATION = 1e-3\n",
        "\n",
        "    def __init__(self, latent_dims, seq_len):\n",
        "        self.latent_dims = latent_dims\n",
        "        # The only annoying thing is that we have to specify the sequence length\n",
        "        # ahead of time\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return (self.seq_len, self.latent_dims)\n",
        "\n",
        "    # Must be the full set of constrained parameters!\n",
        "    def distribution(self, params):\n",
        "        J, L, h = params[\"J\"], params[\"L\"], params[\"h\"]\n",
        "        log_Z, J_filtered, h_filtered = params[\"log_Z\"], params[\"J_filtered\"], params[\"h_filtered\"]\n",
        "        Ex, ExxT, ExnxT = params[\"Ex\"], params[\"ExxT\"], params[\"ExnxT\"]\n",
        "        return MultivariateNormalBlockTridiag(J, L, h, \n",
        "            log_Z, J_filtered, h_filtered, Ex, ExxT, ExnxT)\n",
        "\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        params = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"Q1\": np.eye(D),\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": np.eye(D)\n",
        "        }\n",
        "        constrained = self.get_constrained_params(params)\n",
        "        params[\"avg_suff_stats\"] = { \"Ex\": constrained[\"Ex\"], \n",
        "                                    \"ExxT\": constrained[\"ExxT\"], \n",
        "                                    \"ExnxT\": constrained[\"ExnxT\"] }\n",
        "        return params\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        return params\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        p = dynamics_to_tridiag(params, self.seq_len, self.latent_dims)\n",
        "        J, L, h = p[\"J\"], p[\"L\"], p[\"h\"]\n",
        "        dist = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "        p.update({\n",
        "            \"log_Z\": dist.log_normalizer,\n",
        "            \"J_filtered\": dist.filtered_precisions,\n",
        "            \"h_filtered\": dist.filtered_linear_potentials,\n",
        "            \"Ex\": dist.expected_states,\n",
        "            \"ExxT\": dist.expected_states_squared,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    # This is pretty much deprecated since we're using sgd\n",
        "    def m_step(self, prior_params):\n",
        "        suff_stats = prior_params[\"avg_suff_stats\"]\n",
        "        ExxT = suff_stats[\"ExxT\"]\n",
        "        ExnxT = suff_stats[\"ExnxT\"]\n",
        "        Ex = suff_stats[\"Ex\"]\n",
        "        seq_len = Ex.shape[0]\n",
        "        # Update the initials\n",
        "        m1 = Ex[0]\n",
        "        Q1 = ExxT[0] - np.outer(m1, m1)\n",
        "        D = self.latent_dims\n",
        "        A, b, Q = fit_linear_regression(Ex[:-1].sum(axis=0), \n",
        "                                        Ex[1:].sum(axis=0), \n",
        "                                        ExxT[:-1].sum(axis=0), \n",
        "                                        ExnxT.sum(axis=0), \n",
        "                                        ExxT[1:].sum(axis=0), \n",
        "                                        seq_len - 1)\n",
        "        out = { \"m1\": m1, \"Q1\": Q1, \"A\": A, \"b\": b, \"Q\": Q }\n",
        "        out[\"avg_suff_stats\"] = deepcopy(suff_stats)\n",
        "        return out\n",
        "\n",
        "# This is a bit clumsy but it's the best we can do without using some sophisticated way\n",
        "# Of marking the constrained/optimized parameters vs. unconstrained parameters\n",
        "class LieParameterizedLinearGaussianChain(LinearGaussianChain):\n",
        "    def init(self, key):\n",
        "        D = self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        # Equivalent to the unit matrix\n",
        "        Q_flat = np.concatenate([np.ones(D) * inv_softplus(1), np.zeros((D*(D-1)//2))])\n",
        "        params = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"Q1\": Q_flat,\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": Q_flat\n",
        "        }\n",
        "        return params\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        return {\n",
        "            \"m1\": params[\"m1\"],\n",
        "            \"Q1\": lie_params_to_constrained(params[\"Q1\"], self.latent_dims),\n",
        "            \"A\": params[\"A\"],\n",
        "            \"b\": params[\"b\"],\n",
        "            \"Q\": lie_params_to_constrained(params[\"Q\"], self.latent_dims)   \n",
        "        }\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        D = self.latent_dims\n",
        "        p = self.get_dynamics_params(params)\n",
        "        return super().get_constrained_params(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "cellView": "form",
        "id": "6c7K9dJWzyG8"
      },
      "outputs": [],
      "source": [
        "# @title Linear Gaussian chain posteriors\n",
        "\n",
        "# Technically these are not (homogenous) linear Gaussian chains...\n",
        "class LinearGaussianChainPosterior(LinearGaussianChain):\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        params = {\n",
        "            \"J\": np.tile(np.eye(D)[None], (T, 1, 1)),\n",
        "            \"L\": np.zeros((T-1, D, D)),\n",
        "            \"h\": np.zeros((T, D))\n",
        "        }\n",
        "        return self.get_constrained_params(params)\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        J, L, h = params[\"J\"], params[\"L\"], params[\"h\"]\n",
        "        dist = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "        params.update({\n",
        "            \"log_Z\": dist.log_normalizer,\n",
        "            \"J_filtered\": dist.filtered_precisions,\n",
        "            \"h_filtered\": dist.filtered_linear_potentials,\n",
        "            \"Ex\": dist.expected_states,\n",
        "            \"ExxT\": dist.expected_states_squared,\n",
        "            \"ExnxT\": dist.expected_states_next_states\n",
        "        })\n",
        "        return params\n",
        "\n",
        "    def sufficient_statistics(self, params):\n",
        "        return {\n",
        "            \"Ex\": params[\"Ex\"],\n",
        "            \"ExxT\": params[\"ExxT\"],\n",
        "            \"ExnxT\": params[\"ExnxT\"]\n",
        "        }\n",
        "\n",
        "# The SVAE version has an inference function \n",
        "# that combines prior and potential params\n",
        "class LDSSVAEPosterior(LinearGaussianChainPosterior):\n",
        "    def infer(self, prior_params, potential_params):\n",
        "        prior_J, prior_L, prior_h = prior_params[\"J\"], prior_params[\"L\"], prior_params[\"h\"]\n",
        "        params = {\n",
        "            \"J\": prior_J + potential_params[\"J\"],\n",
        "            \"L\": prior_L,\n",
        "            \"h\": prior_h + potential_params[\"h\"]\n",
        "        }\n",
        "        return self.get_constrained_params(params)\n",
        "\n",
        "# The infer function for the DKF version just uses the posterior params \n",
        "class DKFPosterior(LinearGaussianChainPosterior):\n",
        "    def infer(self, prior_params, posterior_params):\n",
        "        return self.get_constrained_params(posterior_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "cellView": "form",
        "id": "Rbe3PsAIHliY"
      },
      "outputs": [],
      "source": [
        "# @title PlaNet type posterior\n",
        "# The infer function for the DKF version just uses the posterior params \n",
        "# TODO: Put the dummies in the params dictionary as well\n",
        "class PlaNetPosterior(DKFPosterior):\n",
        "    def __init__(self, network_params, latent_dims, seq_len):\n",
        "        super().__init__(latent_dims, seq_len)\n",
        "        self.network = StochasticRNNCell.from_params(**network_params)\n",
        "        self.input_dim = network_params[\"input_dim\"]      # u\n",
        "        self.latent_dim = network_params[\"rnn_dim\"]       # h\n",
        "        self.output_dim = network_params[\"output_dim\"]    # x\n",
        "\n",
        "    def init(self, key):\n",
        "        input_dummy = np.zeros((self.input_dim,))\n",
        "        latent_dummy = np.zeros((self.latent_dim,))\n",
        "        output_dummy = np.zeros((self.output_dim,))\n",
        "        rnn_params = self.network.init(key, input_dummy, latent_dummy, output_dummy)\n",
        "        return {\n",
        "            \"rnn_params\": rnn_params,\n",
        "            \"input_dummy\": input_dummy,\n",
        "            \"latent_dummy\": latent_dummy,\n",
        "            \"output_dummy\": output_dummy,\n",
        "        }\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        # All of the information is stored in the second argument already\n",
        "        return params\n",
        "\n",
        "    def distribution(self, params):\n",
        "        return DeepAutoregressiveDynamics(self.network, params)\n",
        "        \n",
        "    # These are just dummies\n",
        "    def sufficient_statistics(self, params):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        return {\n",
        "            \"Ex\": np.zeros((T, D)),\n",
        "            \"ExxT\": np.zeros((T, D, D)),\n",
        "            \"ExnxT\": np.zeros((T-1, D, D))\n",
        "        }\n",
        "\n",
        "# We only need to be able to 1) Evaluate log prob 2) sample\n",
        "# The tricky thing here is evaluating the \n",
        "class DeepAutoregressiveDynamics:\n",
        "\n",
        "    def __init__(self, network, params):\n",
        "        self.cell = network\n",
        "        self.params = params[\"network_params\"]\n",
        "        self.inputs = params[\"network_input\"]\n",
        "        # self.input_dummy = params[\"network_params\"][\"input_dummy\"]\n",
        "        # self.latent_dummy = params[\"network_params\"][\"latent_dummy\"]\n",
        "        # self.output_dummy = params[\"network_params\"][\"output_dummy\"]\n",
        "        self._mean = None\n",
        "        self._covariance = None\n",
        "\n",
        "    def mean(self):\n",
        "        if (self._mean is None):\n",
        "            self.compute_mean_and_cov()\n",
        "        return self._mean\n",
        "\n",
        "    def covariance(self):\n",
        "        if (self._covariance is None):\n",
        "            self.compute_mean_and_cov()\n",
        "        return self._covariance\n",
        "\n",
        "    @property\n",
        "    def filtered_covariances(self):\n",
        "        return self.covariance()\n",
        "\n",
        "    @property\n",
        "    def filtered_means(self):\n",
        "        return self.mean()\n",
        "        \n",
        "    def compute_mean_and_cov(self):\n",
        "        num_samples = 25\n",
        "        samples = self.sample((num_samples,), key_0)\n",
        "        Ex = np.mean(samples, axis=0)\n",
        "        self._mean = Ex\n",
        "        ExxT = np.einsum(\"s...ti,s...tj->s...tij\", samples, samples).mean(axis=0)\n",
        "        self._covariance = ExxT - np.einsum(\"...ti,...tj->...tij\", Ex, Ex)\n",
        "\n",
        "    # TODO: make this work properly with a batched distribution object\n",
        "    def log_prob(self, xs):\n",
        "        params = self.params\n",
        "        def log_prob_single(x_):\n",
        "            def _log_prob_step(carry, i):\n",
        "                h, prev_x = carry\n",
        "                x, u = x_[i], self.inputs[i]\n",
        "                h, (cov, mean) = self.cell.apply(params[\"rnn_params\"], h, prev_x, u)\n",
        "                pred_dist = tfd.MultivariateNormalFullCovariance(loc=mean, \n",
        "                                                            covariance_matrix=cov)\n",
        "                log_prob = pred_dist.log_prob(x)\n",
        "                carry = h, x\n",
        "                return carry, log_prob\n",
        "            # Assuming these are zero arrays already\n",
        "            init = (params[\"latent_dummy\"], params[\"output_dummy\"])\n",
        "            _, log_probs = scan(_log_prob_step, init, np.arange(x_.shape[0]))\n",
        "            return np.sum(log_probs, axis=0)\n",
        "        return vmap(log_prob_single)(xs)\n",
        "\n",
        "    # TODO: make this work with a batched distribution object\n",
        "    # Only supports rank 0 and 1 sample shapes\n",
        "    # Output: ([num_samples,] [batch_size,] seq_len, event_dim)\n",
        "    def sample(self, sample_shape, seed):\n",
        "        def _sample_single(key, params, inputs):\n",
        "            def _sample_step(carry, u):\n",
        "                key, h, x = carry\n",
        "                key, new_key = jr.split(key)\n",
        "                h, (cov, mean) = self.cell.apply(params[\"rnn_params\"], h, x, u)\n",
        "                sample = jr.multivariate_normal(key, mean, cov)\n",
        "                carry = new_key, h, sample\n",
        "                output = sample\n",
        "                return carry, output\n",
        "\n",
        "            init = (key, params[\"latent_dummy\"], params[\"output_dummy\"])\n",
        "            _, sample = scan(_sample_step, init, inputs)\n",
        "            return sample\n",
        "\n",
        "        if (len(self.inputs.shape) == 2):\n",
        "            if (len(sample_shape) == 0):\n",
        "                return _sample_single(seed, self.params, self.inputs)\n",
        "            else:\n",
        "                assert (len(sample_shape) == 1)\n",
        "                return vmap(_sample_single, in_axes=(0, None, None))(jr.split(seed, sample_shape[0]),\n",
        "                                            self.params, self.inputs)\n",
        "        else:\n",
        "            # This is a batched distribution object\n",
        "            assert(len(self.inputs.shape) == 3)\n",
        "            batch_size = self.inputs.shape[0]\n",
        "            if (len(sample_shape) == 0):\n",
        "                return vmap(_sample_single)(\n",
        "                            jr.split(seed, batch_size), \n",
        "                            self.params,\n",
        "                            self.inputs\n",
        "                        )\n",
        "            else:\n",
        "                assert (len(sample_shape) == 1)\n",
        "                return vmap(\n",
        "                        lambda s:vmap(_sample_single)(\n",
        "                            jr.split(s, batch_size), \n",
        "                            self.params,\n",
        "                            self.inputs\n",
        "                        )\n",
        "                    )(jr.split(seed, sample_shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "cellView": "form",
        "id": "qDsz18BcyDcf"
      },
      "outputs": [],
      "source": [
        "# @title LDS object (might wanna refactor this)\n",
        "\n",
        "# Takes a linear Gaussian chain as its base\n",
        "class LDS(LinearGaussianChain):\n",
        "    def __init__(self, latent_dims, seq_len, base=None, posterior=None):\n",
        "        super().__init__(latent_dims, seq_len)\n",
        "        self.posterior = posterior or LDSSVAEPosterior(latent_dims, seq_len)\n",
        "        self.base = base or LinearGaussianChain(latent_dims, seq_len) # Slightly redundant...\n",
        "\n",
        "    # Takes unconstrained params\n",
        "    def sample(self, params, shape, key):\n",
        "        latents = self.base.sample(params, shape, key)\n",
        "        sample_shape = latents.shape[:-1]\n",
        "        key, _ = jr.split(key)\n",
        "        C, d, R = params[\"C\"], params[\"d\"], params[\"R\"]\n",
        "        obs_noise = tfd.MultivariateNormalFullCovariance(loc=d, covariance_matrix=R)\\\n",
        "            .sample(sample_shape=sample_shape, seed=key)\n",
        "        obs = np.einsum(\"ij,...tj->...ti\", C, latents) + obs_noise\n",
        "        return latents, obs\n",
        "\n",
        "    # Should work with any batch dimension\n",
        "    def log_prob(self, params, states, data):\n",
        "        latent_dist = self.base.distribution(self.base.get_constrained_params(params))\n",
        "        latent_ll = latent_dist.log_prob(states)\n",
        "        C, d, R = params[\"C\"], params[\"d\"], params[\"R\"]\n",
        "        # Gets around batch dimensions\n",
        "        noise = tfd.MultivariateNormalFullCovariance(loc=d, covariance_matrix=R)\n",
        "        obs_ll = noise.log_prob(data - np.einsum(\"ij,...tj->...ti\", C, states))\n",
        "        return latent_ll + obs_ll.sum(axis=-1)\n",
        "\n",
        "    # Assumes single data points\n",
        "    def e_step(self, params, data):\n",
        "        # Shorthand names for parameters\n",
        "        C, d, R = params[\"C\"], params[\"d\"], params[\"R\"]\n",
        "\n",
        "        J = np.dot(C.T, np.linalg.solve(R, C))\n",
        "        J = np.tile(J[None, :, :], (self.seq_len, 1, 1))\n",
        "        # linear potential\n",
        "        h = np.dot(data - d, np.linalg.solve(R, C))\n",
        "\n",
        "        return self.posterior.infer(self.base.get_constrained_params(params), {\"J\": J, \"h\": h})\n",
        "        \n",
        "    # Also assumes single data points\n",
        "    def marginal_log_likelihood(self, params, data):\n",
        "        posterior = self.posterior.distribution(self.e_step(params, data))\n",
        "        states = posterior.mean()\n",
        "        prior_ll = self.log_prob(params, states, data)\n",
        "        posterior_ll = posterior.log_prob(states)\n",
        "        # This is numerically unstable!\n",
        "        lps = prior_ll - posterior_ll\n",
        "        return lps\n",
        "\n",
        "class ParallelLDS(LDS):\n",
        "    def __init__(self, latent_dims, seq_len):\n",
        "        self.latent_dims = latent_dims\n",
        "        self.seq_len = seq_len\n",
        "        self.posterior = ParallelLDSSVAEPosterior(latent_dims, seq_len)\n",
        "        self.base = ParallelLinearGaussianChain(latent_dims, seq_len)\n",
        "\n",
        "    def marginal_log_likelihood(self, params, data):\n",
        "        posterior = self.posterior.distribution(self.e_step(params, data))\n",
        "        states = posterior.mean()\n",
        "        prior_ll = self.log_prob(params, states, data)\n",
        "        posterior_ll = posterior.log_prob(states)\n",
        "        lps = posterior._log_normalizer + np.sum(MVN(loc=params[\"d\"], \n",
        "                            covariance_matrix=params[\"R\"]).log_prob(data))\n",
        "        return lps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLwPpc4SVRTE"
      },
      "source": [
        "## Parallel version of the distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "cellView": "form",
        "id": "WHqamMarWJXx"
      },
      "outputs": [],
      "source": [
        "# @title Implementation of parallel Kalman filtering/smoothing (taken from Dynamax)\n",
        "import jax.scipy as jsc\n",
        "from tensorflow_probability.substrates.jax.distributions \\\n",
        "    import MultivariateNormalFullCovariance as MVN\n",
        "from jax.scipy.linalg import solve\n",
        "\n",
        "def psd_solve(A,b):\n",
        "    \"\"\"A wrapper for coordinating the linalg solvers used in the library for psd matrices.\"\"\"\n",
        "    A = A + 1e-6\n",
        "    return np.linalg.solve(A,b)\n",
        "\n",
        "def _make_filtering_elements_with_emission_potentials(params, potentials):\n",
        "\n",
        "    F = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "    Q1 = params[\"Q1\"]\n",
        "    m1 = params[\"m1\"]\n",
        "    dim = Q.shape[0]\n",
        "\n",
        "    def _first_filtering_element(J_obs, h_obs):\n",
        "        IpQ1J = np.eye(dim) + Q1 @ J_obs\n",
        "        C = solve(IpQ1J, Q1)\n",
        "        b = C @ (h_obs + psd_solve(Q1, m1))\n",
        "        A = np.zeros_like(F)\n",
        "\n",
        "        IpQJ = np.eye(dim) + Q @ J_obs\n",
        "        IpJQ = np.eye(dim) + J_obs @ Q\n",
        "\n",
        "        eta = F.T @ solve(IpJQ, h_obs)\n",
        "        J = F.T @ J_obs @ solve(IpQJ, F)\n",
        "\n",
        "        # We need to take into account the fact that J might be non-invertible\n",
        "        logZ_tilde = 0.5 * np.linalg.slogdet(IpQ1J)[1] - 0.5 * h_obs.T @ Q1 @ solve(IpQ1J, h_obs)\n",
        "        \n",
        "        # mu_obs = solve(J_obs, h_obs)\n",
        "        # Sigma_obs = inv(J_obs)\n",
        "        # logZ_tilde = -MVN(loc=np.zeros(dim), covariance_matrix=Q+Sigma_obs).log_prob(mu_obs)\n",
        "        \n",
        "        # Note: logZ = logZ_tilde - log(n_t) where n_t is an unknown constant\n",
        "        # that is cancelled when we compute the posterior log likelihood\n",
        "\n",
        "        J = (J + J.T) * .5 # Manual symmetrization\n",
        "        C = (C + C.T) * .5\n",
        "\n",
        "        return A, b, C, J, eta, logZ_tilde\n",
        "\n",
        "    def _generic_filtering_element(J_obs, h_obs):\n",
        "        # C = (Q^{-1} + J_{obs})^{-1} = (I + QJ_{obs})^{-1}Q\n",
        "        IpQJ = np.eye(dim) + Q @ J_obs\n",
        "        IpJQ = np.eye(dim) + J_obs @ Q\n",
        "        C = solve(IpQJ, Q)\n",
        "        b = C @ h_obs\n",
        "        A = solve(IpQJ, F)\n",
        "\n",
        "        # eta = F^T(I+J_{obs}Q)^{-1}h_{obs}\n",
        "        # J = F^T(Q+J_{obs}^{-1})^{-1}F = F^TJ_{obs}(I +QJ_{obs})^{-1}F\n",
        "        # This is the same update as in the associative operator...!\n",
        "        eta = F.T @ solve(IpJQ, h_obs)\n",
        "        J = F.T @ J_obs @ solve(IpQJ, F) # This can be replaced with A\n",
        "\n",
        "        # We need to take into account the fact that J might be non-invertible\n",
        "        logZ_tilde = 0.5 * np.linalg.slogdet(IpQJ)[1] - 0.5 * h_obs.T @ Q @ solve(IpJQ, h_obs)\n",
        "\n",
        "        # Note: logZ = logZ_tilde - log(n_t) where n_t is an unknown constant \n",
        "        # that is cancelled when we compute the posterior log likelihood\n",
        "\n",
        "        J = (J + J.T) * .5 # Manual symmetrization\n",
        "        C = (C + C.T) * .5\n",
        "\n",
        "        return A, b, C, J, eta, logZ_tilde\n",
        "\n",
        "    J_obs, h_obs = potentials[\"J\"], potentials[\"h\"]\n",
        "\n",
        "    first_elems = _first_filtering_element(J_obs[0], h_obs[0])\n",
        "    generic_elems = vmap(_generic_filtering_element)(J_obs[1:], h_obs[1:])\n",
        "    combined_elems = tuple(np.concatenate((first_elm[None,...], gen_elm))\n",
        "                           for first_elm, gen_elm in zip(first_elems, generic_elems))\n",
        "    return combined_elems\n",
        "\n",
        "def lgssm_filter_with_emission_potentials(params, potentials):\n",
        "\n",
        "    initial_elements = _make_filtering_elements_with_emission_potentials(params, potentials)\n",
        "\n",
        "    @vmap\n",
        "    def filtering_operator(elem1, elem2):\n",
        "        A1, b1, C1, J1, eta1, logZ1 = elem1\n",
        "        A2, b2, C2, J2, eta2, logZ2 = elem2\n",
        "        dim = A1.shape[0]\n",
        "        I = np.eye(dim)\n",
        "\n",
        "        I_C1J2 = I + C1 @ J2\n",
        "        temp = jsc.linalg.solve(I_C1J2.T, A2.T).T\n",
        "        A = temp @ A1\n",
        "        b = temp @ (b1 + C1 @ eta2) + b2\n",
        "        C = temp @ C1 @ A2.T + C2\n",
        "        C = (C + C.T) * .5  # Add manual symmetrization\n",
        "\n",
        "        I_J2C1 = I + J2 @ C1\n",
        "        temp = jsc.linalg.solve(I_J2C1.T, A1).T\n",
        "\n",
        "        eta = temp @ (eta2 - J2 @ b1) + eta1\n",
        "        J = temp @ J2 @ A1 + J1\n",
        "        J = (J + J.T) * .5 # Manual symmetrization\n",
        "\n",
        "        # mu = jsc.linalg.solve(J2, eta2)\n",
        "        # t2 = - eta2 @ mu + (b1 - mu) @ jsc.linalg.solve(I_J2C1, (J2 @ b1 - eta2))\n",
        "\n",
        "        mu = psd_solve(C1, b1)\n",
        "        t1 = (b1 @ mu - (eta2 + mu) @ np.linalg.solve(I_C1J2, C1 @ eta2 + b1))\n",
        "\n",
        "        logZ = (logZ1 + logZ2 + 0.5 * np.linalg.slogdet(I_C1J2)[1] + 0.5 * t1)\n",
        "\n",
        "        return A, b, C, J, eta, logZ\n",
        "\n",
        "    _, filtered_means, filtered_covs, _, _, logZ = lax.associative_scan(\n",
        "                                                filtering_operator, initial_elements)\n",
        "\n",
        "    return {\n",
        "        \"marginal_logliks\": -logZ,\n",
        "        \"marginal_loglik\": -logZ[-1],\n",
        "        \"filtered_means\": filtered_means, \n",
        "        \"filtered_covariances\": filtered_covs\n",
        "    }\n",
        "\n",
        "def _make_associative_smoothing_elements(params, filtered_means, filtered_covariances):\n",
        "    \"\"\"Preprocess filtering output to construct input for smoothing assocative scan.\"\"\"\n",
        "\n",
        "    F = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "\n",
        "    def _last_smoothing_element(m, P):\n",
        "        return np.zeros_like(P), m, P\n",
        "\n",
        "    def _generic_smoothing_element(params, m, P):\n",
        "\n",
        "        Pp = F @ P @ F.T + Q\n",
        "\n",
        "        E  = psd_solve(Pp, F @ P).T\n",
        "        g  = m - E @ F @ m\n",
        "        L  = P - E @ Pp @ E.T\n",
        "        return E, g, L\n",
        "\n",
        "    last_elems = _last_smoothing_element(filtered_means[-1], filtered_covariances[-1])\n",
        "    generic_elems = vmap(_generic_smoothing_element, (None, 0, 0))(\n",
        "        params, filtered_means[:-1], filtered_covariances[:-1]\n",
        "        )\n",
        "    combined_elems = tuple(np.append(gen_elm, last_elm[None,:], axis=0)\n",
        "                           for gen_elm, last_elm in zip(generic_elems, last_elems))\n",
        "    return combined_elems\n",
        "\n",
        "def _make_associative_sampling_elements(params, key, filtered_means, filtered_covariances):\n",
        "    \"\"\"Preprocess filtering output to construct input for smoothing assocative scan.\"\"\"\n",
        "\n",
        "    F = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "\n",
        "    def _last_sampling_element(key, m, P):\n",
        "        return np.zeros_like(P), MVN(m, P).sample(seed=key)\n",
        "\n",
        "    def _generic_sampling_element(params, key, m, P):\n",
        "\n",
        "        Pp = F @ P @ F.T + Q\n",
        "\n",
        "        FP = F @ P\n",
        "        E  = psd_solve(Pp, FP).T\n",
        "        g  = m - E @ F @ m\n",
        "        L  = P - E @ Pp @ E.T\n",
        "\n",
        "        L = (L + L.T) * .5\n",
        "\n",
        "        h = MVN(g, L).sample(seed=key)\n",
        "        return E, h\n",
        "\n",
        "    num_timesteps = len(filtered_means)\n",
        "    dims = filtered_means.shape[-1]\n",
        "    keys = jr.split(key, num_timesteps)\n",
        "    last_elems = _last_sampling_element(keys[-1], filtered_means[-1], \n",
        "                                        filtered_covariances[-1])\n",
        "    generic_elems = vmap(_generic_sampling_element, (None, 0, 0, 0))(\n",
        "        params, keys[:-1], filtered_means[:-1], filtered_covariances[:-1]\n",
        "        )\n",
        "    combined_elems = tuple(np.append(gen_elm, last_elm[None,:], axis=0)\n",
        "                           for gen_elm, last_elm in zip(generic_elems, last_elems))\n",
        "    return combined_elems\n",
        "\n",
        "def lgssm_smoother_with_emission_potentials(params, potentials):\n",
        "    \"\"\"A parallel version of the lgssm smoothing algorithm.\n",
        "    See S. Särkkä and Á. F. García-Fernández (2021) - https://arxiv.org/abs/1905.13002.\n",
        "    Note: This function does not yet handle `inputs` to the system.\n",
        "    \"\"\"\n",
        "    filtered_posterior = lgssm_filter_with_emission_potentials(params, potentials)\n",
        "    filtered_means = filtered_posterior[\"filtered_means\"]\n",
        "    filtered_covs = filtered_posterior[\"filtered_covariances\"]\n",
        "    initial_elements = _make_associative_smoothing_elements(params, filtered_means, filtered_covs)\n",
        "\n",
        "    @vmap\n",
        "    def smoothing_operator(elem1, elem2):\n",
        "        E1, g1, L1 = elem1\n",
        "        E2, g2, L2 = elem2\n",
        "\n",
        "        E = E2 @ E1\n",
        "        g = E2 @ g1 + g2\n",
        "        L = E2 @ L1 @ E2.T + L2\n",
        "        \n",
        "        L = (L + L.T) * .5\n",
        "\n",
        "        return E, g, L\n",
        "\n",
        "    _, smoothed_means, smoothed_covs, *_ = lax.associative_scan(\n",
        "                                                smoothing_operator, initial_elements, reverse=True\n",
        "                                                )\n",
        "    return {\n",
        "        \"marginal_loglik\": filtered_posterior[\"marginal_loglik\"],\n",
        "        \"filtered_means\": filtered_means,\n",
        "        \"filtered_covariances\": filtered_covs,\n",
        "        \"smoothed_means\": smoothed_means,\n",
        "        \"smoothed_covariances\": smoothed_covs\n",
        "    }\n",
        "\n",
        "def post_log_prob_with_emission_potentials(x, params, J_obs, h_obs, log_p_tilde_y, T=None):\n",
        "    A = params[\"A\"]\n",
        "    Q = params[\"Q\"]\n",
        "    Q1 = params[\"Q1\"]\n",
        "    m1 = params[\"m1\"]\n",
        "\n",
        "    T = T or x.shape[0]\n",
        "    x = x[:T]\n",
        "    J_obs = J_obs[:T]\n",
        "    h_obs = h_obs[:T]\n",
        "\n",
        "    ll = np.sum(MVN(loc=np.einsum(\"ij,tj->ti\", A, x[:-1]), \n",
        "        covariance_matrix=Q[None]).log_prob(x[1:]))\n",
        "    ll += MVN(loc=m1, covariance_matrix=Q1).log_prob(x[0])\n",
        "    # Add the observation potentials\n",
        "    ll += - 0.5 * np.einsum(\"ti,tij,tj->\", x, J_obs, x) + np.sum(x * h_obs)\n",
        "    ll -= log_p_tilde_y[T-1]\n",
        "    return ll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "cellView": "form",
        "id": "TseqN6gWVZZt"
      },
      "outputs": [],
      "source": [
        "# @title Parallel linear Gaussian state space model object\n",
        "class ParallelLGSSM(tfd.Distribution):\n",
        "    def __init__(self,\n",
        "                 initial_mean,\n",
        "                 initial_covariance,\n",
        "                 dynamics_matrix,\n",
        "                 dynamics_bias,\n",
        "                 dynamics_noise_covariance,\n",
        "                 emissions_precisions,\n",
        "                 emissions_linear_potentials,\n",
        "                 log_normalizer,\n",
        "                 filtered_means,\n",
        "                 filtered_covariances,\n",
        "                 smoothed_means,\n",
        "                 smoothed_covariances,\n",
        "                 validate_args=False,\n",
        "                 allow_nan_stats=True,\n",
        "                 name=\"ParallelLGSSM\",\n",
        "             ) -> None:\n",
        "        # Dynamics\n",
        "        self._initial_mean = initial_mean\n",
        "        self._initial_covariance = initial_covariance\n",
        "        self._dynamics_matrix = dynamics_matrix\n",
        "        self._dynamics_bias = dynamics_bias\n",
        "        self._dynamics_noise_covariance = dynamics_noise_covariance\n",
        "        # Emissions\n",
        "        self._emissions_precisions = emissions_precisions\n",
        "        self._emissions_linear_potentials = emissions_linear_potentials\n",
        "        # Filtered\n",
        "        self._log_normalizer = log_normalizer\n",
        "        self._filtered_means = filtered_means\n",
        "        self._filtered_covariances = filtered_covariances\n",
        "        # Smoothed\n",
        "        self._smoothed_means = smoothed_means\n",
        "        self._smoothed_covariances = smoothed_covariances\n",
        "\n",
        "        # We would detect the dtype dynamically but that would break vmap\n",
        "        # see https://github.com/tensorflow/probability/issues/1271\n",
        "        dtype = np.float32\n",
        "        super(ParallelLGSSM, self).__init__(\n",
        "            dtype=dtype,\n",
        "            validate_args=validate_args,\n",
        "            allow_nan_stats=allow_nan_stats,\n",
        "            reparameterization_type=reparameterization.NOT_REPARAMETERIZED,\n",
        "            parameters=dict(initial_mean=self._initial_mean,\n",
        "                            initial_covariance=self._initial_covariance,\n",
        "                            dynamics_matrix=self._dynamics_matrix,\n",
        "                            dynamics_bias=self._dynamics_bias,\n",
        "                            dynamics_noise_covariance=self._dynamics_noise_covariance,\n",
        "                            emissions_precisions=self._emissions_precisions,\n",
        "                            emissions_linear_potentials=self._emissions_linear_potentials,\n",
        "                            log_normalizer=self._log_normalizer,\n",
        "                            filtered_means=self._filtered_means,\n",
        "                            filtered_covariances=self._filtered_covariances,\n",
        "                            smoothed_means=self._smoothed_means,\n",
        "                            smoothed_covariances=self._smoothed_covariances),\n",
        "            name=name,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _parameter_properties(cls, dtype, num_classes=None):\n",
        "        # pylint: disable=g-long-lambda\n",
        "        return dict(initial_mean=tfp.internal.parameter_properties.ParameterProperties(event_ndims=1),\n",
        "                    initial_covariance=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    dynamics_matrix=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    dynamics_bias=tfp.internal.parameter_properties.ParameterProperties(event_ndims=1),\n",
        "                    dynamics_noise_covariance=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    emissions_precisions=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "                    emissions_linear_potentials=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    log_normalizer=tfp.internal.parameter_properties.ParameterProperties(event_ndims=0),\n",
        "                    filtered_means=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    filtered_covariances=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3),\n",
        "                    smoothed_means=tfp.internal.parameter_properties.ParameterProperties(event_ndims=2),\n",
        "                    smoothed_covariances=tfp.internal.parameter_properties.ParameterProperties(event_ndims=3)\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def infer_from_dynamics_and_potential(cls, dynamics_params, emissions_potentials):\n",
        "        \n",
        "        smoothed = lgssm_smoother_with_emission_potentials(\n",
        "            dynamics_params, emissions_potentials)\n",
        "        \n",
        "        return cls(dynamics_params[\"m1\"],\n",
        "                   dynamics_params[\"Q1\"],\n",
        "                   dynamics_params[\"A\"],\n",
        "                   dynamics_params[\"b\"],\n",
        "                   dynamics_params[\"Q\"],\n",
        "                   emissions_potentials[\"J\"],\n",
        "                   emissions_potentials[\"h\"],\n",
        "                   smoothed[\"marginal_loglik\"],\n",
        "                   smoothed[\"filtered_means\"],\n",
        "                   smoothed[\"filtered_covariances\"],\n",
        "                   smoothed[\"smoothed_means\"],\n",
        "                   smoothed[\"smoothed_covariances\"])\n",
        "        \n",
        "    # Properties to get private class variables\n",
        "    @property\n",
        "    def log_normalizer(self):\n",
        "        return self._log_normalizer\n",
        "\n",
        "    @property\n",
        "    def filtered_means(self):\n",
        "        return self._filtered_means\n",
        "\n",
        "    @property\n",
        "    def filtered_covariances(self):\n",
        "        return self._filtered_covariances\n",
        "\n",
        "    @property\n",
        "    def smoothed_means(self):\n",
        "        return self._smoothed_means\n",
        "\n",
        "    @property\n",
        "    def smoothed_covariances(self):\n",
        "        return self._smoothed_covariances\n",
        "\n",
        "    def _mean(self):\n",
        "        return self.smoothed_means\n",
        "\n",
        "    def _covariance(self):\n",
        "        return self.smoothed_covariances\n",
        "    \n",
        "    # TODO: currently this function does not depend on the dynamics bias\n",
        "    def _log_prob(self, data, **kwargs):\n",
        "        A = self._dynamics_matrix #params[\"A\"]\n",
        "        Q = self._dynamics_noise_covariance #params[\"Q\"]\n",
        "        Q1 = self._initial_covariance #params[\"Q1\"]\n",
        "        m1 = self._initial_mean #params[\"m1\"]\n",
        "\n",
        "        num_batch_dims = len(data.shape) - 2\n",
        "\n",
        "        ll = np.sum(\n",
        "            MVN(loc=np.einsum(\"ij,...tj->...ti\", A, data[...,:-1,:]), \n",
        "                covariance_matrix=Q).log_prob(data[...,1:,:])\n",
        "            )\n",
        "        ll += MVN(loc=m1, covariance_matrix=Q1).log_prob(data[...,0,:])\n",
        "\n",
        "        # Add the observation potentials\n",
        "        ll += - 0.5 * np.einsum(\"...ti,tij,...tj->...\", data, self._emissions_precisions, data) \\\n",
        "              + np.einsum(\"...ti,ti->...\", data, self._emissions_linear_potentials)\n",
        "        # Add the log normalizer\n",
        "        ll -= self._log_normalizer\n",
        "\n",
        "        return ll\n",
        "\n",
        "    # This sample function seems to be problematic...?\n",
        "    # def _sample_n(self, n, seed=None):\n",
        "\n",
        "    #     F = self._dynamics_matrix\n",
        "    #     b = self._dynamics_bias\n",
        "    #     Q = self._dynamics_noise_covariance\n",
        "        \n",
        "    #     def sample_single(\n",
        "    #         key,\n",
        "    #         filtered_means,\n",
        "    #         filtered_covariances\n",
        "    #     ):\n",
        "\n",
        "    #         num_timesteps = filtered_means.shape[0]\n",
        "\n",
        "    #         def _condition_on(m, P, H, d, R, y):\n",
        "    #             # Compute the Kalman gain\n",
        "    #             S = R + H @ P @ H.T\n",
        "    #             K = psd_solve(S, H @ P).T\n",
        "    #             Sigma_cond = P - K @ S @ K.T\n",
        "    #             mu_cond = m[None] + np.einsum(\"ij,nj->ni\", K, y - (d + H @ m)[None])\n",
        "    #             return mu_cond, Sigma_cond\n",
        "\n",
        "    #         # Sample backward in time\n",
        "    #         def _step(carry, args):\n",
        "    #             next_state = carry\n",
        "    #             key, filtered_mean, filtered_cov, t = args\n",
        "\n",
        "    #             # Condition on next state\n",
        "    #             smoothed_mean, smoothed_cov = _condition_on(filtered_mean, filtered_cov, F, b, Q, next_state)\n",
        "    #             state = MVN(smoothed_mean, smoothed_cov[None]).sample(seed=key)\n",
        "    #             return state, state\n",
        "\n",
        "    #         # Initialize the last state\n",
        "    #         key, this_key = jr.split(key, 2)\n",
        "    #         last_state = MVN(filtered_means[-1], filtered_covariances[-1]).sample(\n",
        "    #             seed=this_key, sample_shape=(n,))\n",
        "\n",
        "    #         args = (\n",
        "    #             jr.split(key, num_timesteps - 1),\n",
        "    #             filtered_means[:-1][::-1],\n",
        "    #             filtered_covariances[:-1][::-1],\n",
        "    #             np.arange(num_timesteps - 2, -1, -1),\n",
        "    #         )\n",
        "    #         _, reversed_states = lax.scan(_step, last_state, args)\n",
        "    #         states = np.concatenate([reversed_states[::-1], last_state[None]], axis=0)\n",
        "    #         # Transpose to be (num_samples, num_timesteps, dim)\n",
        "    #         return np.transpose(states, (1, 0, 2))\n",
        "\n",
        "    #     # TODO: Handle arbitrary batch shapes\n",
        "    #     if self._filtered_covariances.ndim == 4:\n",
        "    #         # batch mode\n",
        "    #         samples = vmap(sample_single)(seed, self._filtered_means, self._filtered_covariances)\n",
        "    #         # Transpose to be (num_samples, num_batches, num_timesteps, dim)\n",
        "    #         samples = np.transpose(samples, (1, 0, 2, 3))\n",
        "    #     else:\n",
        "    #         # non-batch mode\n",
        "    #         samples = sample_single(seed, self._filtered_means, self._filtered_covariances)\n",
        "    #     return samples\n",
        "\n",
        "    def _sample_n(self, n, seed=None):\n",
        "\n",
        "        F = self._dynamics_matrix\n",
        "        b = self._dynamics_bias\n",
        "        Q = self._dynamics_noise_covariance\n",
        "        \n",
        "        def sample_single(\n",
        "            key,\n",
        "            filtered_means,\n",
        "            filtered_covariances\n",
        "        ):\n",
        "\n",
        "            initial_elements = _make_associative_sampling_elements(\n",
        "                { \"A\": F, \"b\": b, \"Q\": Q }, key, filtered_means, filtered_covariances)\n",
        "\n",
        "            @vmap\n",
        "            def sampling_operator(elem1, elem2):\n",
        "                E1, h1 = elem1\n",
        "                E2, h2 = elem2\n",
        "\n",
        "                E = E2 @ E1\n",
        "                h = E2 @ h1 + h2\n",
        "                return E, h\n",
        "\n",
        "            _, sample = \\\n",
        "                lax.associative_scan(sampling_operator, initial_elements, reverse=True)\n",
        "                \n",
        "            return sample\n",
        "\n",
        "        # TODO: Handle arbitrary batch shapes\n",
        "        if self._filtered_covariances.ndim == 4:\n",
        "            # batch mode\n",
        "            samples = vmap(vmap(sample_single, in_axes=(None, 0, 0)), in_axes=(0, None, None))\\\n",
        "                (jr.split(seed, n), self._filtered_means, self._filtered_covariances)\n",
        "            # Transpose to be (num_samples, num_batches, num_timesteps, dim)\n",
        "            # samples = np.transpose(samples, (1, 0, 2, 3))\n",
        "        else:\n",
        "            # non-batch mode\n",
        "            samples = vmap(sample_single, in_axes=(0, None, None))\\\n",
        "                (jr.split(seed, n), self._filtered_means, self._filtered_covariances)\n",
        "        return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "6Tr0fcboYPVQ"
      },
      "outputs": [],
      "source": [
        "# @ Parallel versions of the same priors and posteriors\n",
        "\n",
        "class ParallelLinearGaussianChain(SVAEPrior):\n",
        "\n",
        "    # We're not using this at the moment\n",
        "    # COVARIANCE_REGULARIZATION = 1e-3\n",
        "\n",
        "    def __init__(self, latent_dims, seq_len):\n",
        "        self.latent_dims = latent_dims\n",
        "        # The only annoying thing is that we have to specify the sequence length\n",
        "        # ahead of time\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return (self.seq_len, self.latent_dims)\n",
        "\n",
        "    # Must be the full set of constrained parameters!\n",
        "    def distribution(self, p):\n",
        "        m1, Q1, A, b, Q, J_obs, h_obs = p[\"m1\"], p[\"Q1\"], p[\"A\"], p[\"b\"], p[\"Q\"], p[\"J_obs\"], p[\"h_obs\"]\n",
        "        log_Z, mu_filtered, Sigma_filtered = p[\"log_Z\"], p[\"mu_filtered\"], p[\"Sigma_filtered\"]\n",
        "        mu_smoothed, Sigma_smoothed = p[\"mu_smoothed\"], p[\"Sigma_smoothed\"]\n",
        "        return ParallelLGSSM(m1, Q1, A, b, Q, J_obs, h_obs, \n",
        "                             log_Z, mu_filtered, Sigma_filtered, \n",
        "                             mu_smoothed, Sigma_smoothed)\n",
        "\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        params = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"Q1\": np.eye(D),\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": np.eye(D)\n",
        "        }\n",
        "        # constrained = self.get_constrained_params(params)\n",
        "        # It's some extra work to compute the sufficient stats...\n",
        "        # params[\"avg_suff_stats\"] = { \"Ex\": constrained[\"Ex\"], \n",
        "        #                             \"ExxT\": constrained[\"ExxT\"], \n",
        "        #                             \"ExnxT\": constrained[\"ExnxT\"] }\n",
        "        return params\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        return params\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        p.update({\n",
        "            \"J_obs\": np.zeros((T, D, D)),\n",
        "            \"h_obs\": np.zeros((T, D))\n",
        "        })\n",
        "        dist = ParallelLGSSM.infer_from_dynamics_and_potential(params, \n",
        "                                    {\"J\": p[\"J_obs\"], \"h\": p[\"h_obs\"]})\n",
        "        \n",
        "        p.update({\n",
        "            \"log_Z\": dist.log_normalizer,\n",
        "            \"mu_filtered\": dist.filtered_means,\n",
        "            \"Sigma_filtered\": dist.filtered_covariances,\n",
        "            \"mu_smoothed\": dist.smoothed_means,\n",
        "            \"Sigma_smoothed\": dist.smoothed_covariances\n",
        "        })\n",
        "        return p\n",
        "\n",
        "class ParallelLieParameterizedLinearGaussianChain(ParallelLinearGaussianChain):\n",
        "    def init(self, key):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        key_A, key = jr.split(key, 2)\n",
        "        # Equivalent to the unit matrix\n",
        "        Q_flat = np.concatenate([np.ones(D) * inv_softplus(1), np.zeros((D*(D-1)//2))])\n",
        "        params = {\n",
        "            \"m1\": np.zeros(D),\n",
        "            \"Q1\": Q_flat,\n",
        "            \"A\": random_rotation(key_A, D, theta=np.pi/20),\n",
        "            \"b\": np.zeros(D),\n",
        "            \"Q\": Q_flat\n",
        "        }\n",
        "        return params\n",
        "\n",
        "    def get_dynamics_params(self, params):\n",
        "        T, D = self.seq_len, self.latent_dims\n",
        "        return {\n",
        "            \"m1\": params[\"m1\"],\n",
        "            \"Q1\": lie_params_to_constrained(params[\"Q1\"], self.latent_dims),\n",
        "            \"A\": params[\"A\"],\n",
        "            \"b\": params[\"b\"],\n",
        "            \"Q\": lie_params_to_constrained(params[\"Q\"], self.latent_dims)\n",
        "        }\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        D = self.latent_dims\n",
        "        p = self.get_dynamics_params(params)\n",
        "        return super().get_constrained_params(p)\n",
        "\n",
        "# Super simple because all the machinary is already taken care of\n",
        "class ParallelLDSSVAEPosterior(ParallelLinearGaussianChain):\n",
        "    def init(self, key):\n",
        "        return super().get_constrained_params(super().init(key))\n",
        "\n",
        "    def infer(self, prior_params, potential_params):\n",
        "        p = copy.deepcopy(prior_params)\n",
        "        p[\"J_obs\"] = potential_params[\"J\"]\n",
        "        p[\"h_obs\"] = potential_params[\"h\"]\n",
        "\n",
        "        dist = ParallelLGSSM.infer_from_dynamics_and_potential(prior_params, \n",
        "                                    {\"J\": p[\"J_obs\"], \"h\": p[\"h_obs\"]})\n",
        "        p.update({\n",
        "            \"log_Z\": dist.log_normalizer,\n",
        "            \"mu_filtered\": dist.filtered_means,\n",
        "            \"Sigma_filtered\": dist.filtered_covariances,\n",
        "            \"mu_smoothed\": dist.smoothed_means,\n",
        "            \"Sigma_smoothed\": dist.smoothed_covariances\n",
        "        })\n",
        "        return p\n",
        "\n",
        "    def get_constrained_params(self, params):\n",
        "        p = copy.deepcopy(params)\n",
        "        return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtvHU_dlOklC"
      },
      "source": [
        "## Define neural network architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "cellView": "form",
        "id": "40wvAwfoOn_E"
      },
      "outputs": [],
      "source": [
        "# @title Neural network utils\n",
        "\n",
        "PRNGKey = Any\n",
        "Shape = Iterable[int]\n",
        "Dtype = Any  # this could be a real type?\n",
        "Array = Any\n",
        "\n",
        "# Note: the last layer output does not have a relu activation!\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Define a simple fully connected MLP with ReLU activations.\n",
        "    \"\"\"\n",
        "    features: Sequence[int]\n",
        "    kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.he_normal()\n",
        "    bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.zeros\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for feat in self.features[:-1]:\n",
        "            x = nn.relu(nn.Dense(feat, \n",
        "                kernel_init=self.kernel_init,\n",
        "                bias_init=self.bias_init,)(x))\n",
        "        x = nn.Dense(self.features[-1], \n",
        "            kernel_init=self.kernel_init, \n",
        "            bias_init=self.bias_init)(x)\n",
        "        return x\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    \"\"\"\n",
        "    A layer which passes the input through unchanged.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "class Static(nn.Module):\n",
        "    \"\"\"\n",
        "    A layer which just returns some static parameters.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "    kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.lecun_normal()\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        kernel = self.param('kernel',\n",
        "                            self.kernel_init,\n",
        "                            (self.features, ))\n",
        "        return kernel\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"A simple CNN model.\"\"\"\n",
        "    input_rank : int = None   \n",
        "    output_dim : int = None\n",
        "    layer_params : Sequence[dict] = None\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for params in self.layer_params:\n",
        "            x = nn.relu(Conv(**params)(x))\n",
        "        # No activations at the output\n",
        "        x = nn.Dense(features=self.output_dim)(x.flatten())\n",
        "        return x\n",
        "\n",
        "class DCNN(nn.Module):\n",
        "    \"\"\"A simple DCNN model.\"\"\"   \n",
        "\n",
        "    input_shape: Sequence[int] = None\n",
        "    layer_params: Sequence[dict] = None\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        input_features = onp.prod(onp.array(self.input_shape))\n",
        "        x = nn.Dense(features=input_features)(x)\n",
        "        x = x.reshape(self.input_shape)\n",
        "        # Note that the last layer doesn't have an activation\n",
        "        for params in self.layer_params:\n",
        "            x = ConvTranspose(**params)(nn.relu(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "id": "Ja-LJ88rybCV"
      },
      "outputs": [],
      "source": [
        "# @title Potential networks (outputs potentials on single observations)\n",
        "class PotentialNetwork(nn.Module):\n",
        "    def __call__(self, inputs):\n",
        "        J, h = self._generate_distribution_parameters(inputs)\n",
        "        if (len(J.shape) == 3):\n",
        "            seq_len, latent_dims, _ = J.shape\n",
        "            # lower diagonal blocks of precision matrix\n",
        "            L = np.zeros((seq_len-1, latent_dims, latent_dims))\n",
        "        elif (len(J.shape) == 4):\n",
        "            batch_size, seq_len, latent_dims, _ = J.shape\n",
        "            # lower diagonal blocks of precision matrix\n",
        "            L = np.zeros((batch_size, seq_len-1, latent_dims, latent_dims))\n",
        "        else:\n",
        "            L = np.zeros(tuple())\n",
        "        return {\"J\": J, \"L\": L, \"h\": h}\n",
        "\n",
        "    def _generate_distribution_parameters(self, inputs):\n",
        "        if (len(inputs.shape) == self.input_rank + 2):\n",
        "            # We have both a batch dimension and a time dimension\n",
        "            # and we have to vmap over both...!\n",
        "            return vmap(vmap(self._call_single, 0), 0)(inputs)\n",
        "        elif (len(inputs.shape) == self.input_rank + 1):\n",
        "            return vmap(self._call_single)(inputs)\n",
        "        elif (len(inputs.shape) == self.input_rank):\n",
        "            return self._call_single(inputs)\n",
        "        else:\n",
        "            # error\n",
        "            return None\n",
        "\n",
        "    def _call_single(self, inputs):\n",
        "        pass\n",
        "\n",
        "# A new, more general implementation of the Gaussian recognition network\n",
        "# Uses mean parameterization which works better empirically\n",
        "class GaussianRecognition(PotentialNetwork):\n",
        "\n",
        "    use_diag : int = None\n",
        "    input_rank : int = None\n",
        "    latent_dims : int = None\n",
        "    trunk_fn : nn.Module = None\n",
        "    head_mean_fn : nn.Module = None\n",
        "    head_log_var_fn : nn.Module = None\n",
        "    eps : float = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_params(cls, input_rank=1, input_dim=None, output_dim=None, \n",
        "                    trunk_type=\"Identity\", trunk_params=None, \n",
        "                    head_mean_type=\"MLP\", head_mean_params=None,\n",
        "                    head_var_type=\"MLP\", head_var_params=None, diagonal_covariance=False,\n",
        "                    cov_init=1, eps=1e-3): \n",
        "\n",
        "        if trunk_type == \"Identity\":\n",
        "            trunk_params = { \"features\": input_dim }\n",
        "        if head_mean_type == \"MLP\":\n",
        "            head_mean_params[\"features\"] += [output_dim]\n",
        "        if head_var_type == \"MLP\":\n",
        "            if (diagonal_covariance):\n",
        "                head_var_params[\"features\"] += [output_dim]\n",
        "            else:\n",
        "                head_var_params[\"features\"] += [output_dim * (output_dim + 1) // 2]\n",
        "            head_var_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_var_params[\"bias_init\"] = nn.initializers.constant(cov_init)\n",
        "\n",
        "        trunk_fn = globals()[trunk_type](**trunk_params)\n",
        "        head_mean_fn = globals()[head_mean_type](**head_mean_params)\n",
        "        head_log_var_fn = globals()[head_var_type](**head_var_params)\n",
        "\n",
        "        return cls(diagonal_covariance, input_rank, output_dim, trunk_fn, \n",
        "                   head_mean_fn, head_log_var_fn, eps)\n",
        "\n",
        "    def _call_single(self, inputs):\n",
        "        # Apply the trunk.\n",
        "        trunk_output = self.trunk_fn(inputs)\n",
        "        # Get the mean.\n",
        "        mu = self.head_mean_fn(trunk_output)\n",
        "        # Get the covariance parameters and build a full matrix from it.\n",
        "        var_output_flat = self.head_log_var_fn(trunk_output)\n",
        "        if self.use_diag:\n",
        "            Sigma = np.diag(softplus(var_output_flat) + self.eps)\n",
        "        else:\n",
        "            Sigma = lie_params_to_constrained(var_output_flat, self.latent_dims, self.eps)\n",
        "        h = np.linalg.solve(Sigma, mu)\n",
        "        J = np.linalg.inv(Sigma)\n",
        "        # lower diagonal blocks of precision matrix\n",
        "        return (J, h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "id": "gmeCrsbCy96H"
      },
      "outputs": [],
      "source": [
        "# @title Posterior networks (outputs full posterior for entire sequence)\n",
        "# Outputs Gaussian distributions for the entire sequence at once\n",
        "class PosteriorNetwork(PotentialNetwork):\n",
        "    def __call__(self, inputs):\n",
        "        J, L, h = self._generate_distribution_parameters(inputs)\n",
        "        return {\"J\": J, \"L\": L, \"h\": h}\n",
        "\n",
        "    def _generate_distribution_parameters(self, inputs):\n",
        "        is_batched = (len(inputs.shape) == self.input_rank+2)\n",
        "        if is_batched:\n",
        "            return vmap(self._call_single, in_axes=0)(inputs)\n",
        "        else:\n",
        "            assert(len(inputs.shape) == self.input_rank+1)\n",
        "            return self._call_single(inputs)\n",
        "\n",
        "class GaussianBiRNN(PosteriorNetwork):\n",
        "    \n",
        "    input_rank : int = None\n",
        "    rnn_dim : int = None\n",
        "    output_dim : int = None\n",
        "    forward_RNN : nn.Module = None\n",
        "    backward_RNN : nn.Module = None\n",
        "    input_fn : nn.Module = None\n",
        "    trunk_fn: nn.Module = None\n",
        "    head_mean_fn : nn.Module = None\n",
        "    head_log_var_fn : nn.Module = None\n",
        "    head_dyn_fn : nn.Module = None\n",
        "    eps : float = None\n",
        "        \n",
        "    @classmethod\n",
        "    def from_params(cls, input_rank=1, cell_type=nn.GRUCell,\n",
        "                    input_dim=None, rnn_dim=None, output_dim=None, \n",
        "                    input_type=\"MLP\", input_params=None,\n",
        "                    trunk_type=\"Identity\", trunk_params=None, \n",
        "                    head_mean_type=\"MLP\", head_mean_params=None,\n",
        "                    head_var_type=\"MLP\", head_var_params=None,\n",
        "                    head_dyn_type=\"MLP\", head_dyn_params=None,\n",
        "                    cov_init=1, eps=1e-4): \n",
        "\n",
        "        forward_RNN = nn.scan(cell_type, variable_broadcast=\"params\", \n",
        "                                             split_rngs={\"params\": False})()\n",
        "        backward_RNN = nn.scan(cell_type, variable_broadcast=\"params\", \n",
        "                                               split_rngs={\"params\": False}, reverse=True)()\n",
        "        if trunk_type == \"Identity\":\n",
        "            trunk_params = { \"features\": rnn_dim }\n",
        "        if input_type == \"MLP\":\n",
        "            input_params[\"features\"] += [rnn_dim]\n",
        "        if head_mean_type == \"MLP\":\n",
        "            head_mean_params[\"features\"] += [output_dim]\n",
        "        if head_var_type == \"MLP\":\n",
        "            head_var_params[\"features\"] += [output_dim * (output_dim + 1) // 2]\n",
        "            head_var_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_var_params[\"bias_init\"] = nn.initializers.constant(cov_init)\n",
        "        if head_dyn_type == \"MLP\":\n",
        "            head_dyn_params[\"features\"] += [output_dim ** 2,]\n",
        "            head_dyn_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_dyn_params[\"bias_init\"] = nn.initializers.zeros\n",
        "\n",
        "        trunk_fn = globals()[trunk_type](**trunk_params)\n",
        "        input_fn = globals()[input_type](**input_params)\n",
        "        head_mean_fn = globals()[head_mean_type](**head_mean_params)\n",
        "        head_log_var_fn = globals()[head_var_type](**head_var_params)\n",
        "        head_dyn_fn = globals()[head_dyn_type](**head_dyn_params)\n",
        "\n",
        "        return cls(input_rank, rnn_dim, output_dim, \n",
        "                   forward_RNN, backward_RNN, \n",
        "                   input_fn, trunk_fn, \n",
        "                   head_mean_fn, head_log_var_fn, head_dyn_fn, eps)\n",
        "\n",
        "    # Applied the BiRNN to a single sequence of inputs\n",
        "    def _call_single(self, inputs):\n",
        "\n",
        "        output_dim = self.output_dim\n",
        "        \n",
        "        inputs = vmap(self.input_fn)(inputs)\n",
        "        init_carry_forward = np.zeros((self.rnn_dim,))\n",
        "        _, out_forward = self.forward_RNN(init_carry_forward, inputs)\n",
        "        init_carry_backward = np.zeros((self.rnn_dim,))\n",
        "        _, out_backward = self.backward_RNN(init_carry_backward, inputs)\n",
        "        # Concatenate the forward and backward outputs\n",
        "        out_combined = np.concatenate([out_forward, out_backward], axis=-1)\n",
        "        \n",
        "        # Get the mean.\n",
        "        # vmap over the time dimension\n",
        "        mu = vmap(self.head_mean_fn)(out_combined)\n",
        "        # Get the variance output and reshape it.\n",
        "        # vmap over the time dimension\n",
        "        var_output_flat = vmap(self.head_log_var_fn)(out_combined)\n",
        "        Sigma = vmap(lie_params_to_constrained, in_axes=(0, None, None))\\\n",
        "            (var_output_flat, output_dim, self.eps)\n",
        "\n",
        "        h = vmap(np.linalg.solve, in_axes=(0, 0))(Sigma, mu)\n",
        "        J = np.linalg.inv(Sigma)\n",
        "\n",
        "        seq_len = J.shape[0]\n",
        "        # lower diagonal blocks of precision matrix\n",
        "        L = np.zeros((seq_len-1, output_dim, output_dim))\n",
        "        return (J, L, h)\n",
        "\n",
        "# Also uses the mean parameterization\n",
        "class ConditionalGaussianBiRNN(GaussianBiRNN):\n",
        "    # Applied the BiRNN to a single sequence of inputs\n",
        "    def _call_single(self, inputs):\n",
        "        output_dim = self.output_dim\n",
        "        \n",
        "        inputs = vmap(self.input_fn)(inputs)\n",
        "        init_carry_forward = np.zeros((self.rnn_dim,))\n",
        "        _, out_forward = self.forward_RNN(init_carry_forward, inputs)\n",
        "        init_carry_backward = np.zeros((self.rnn_dim,))\n",
        "        _, out_backward = self.backward_RNN(init_carry_backward, inputs)\n",
        "        # Concatenate the forward and backward outputs\n",
        "        out_combined = np.concatenate([out_forward, out_backward], axis=-1)\n",
        "        \n",
        "        # Get the mean.\n",
        "        # vmap over the time dimension\n",
        "        b = vmap(self.head_mean_fn)(out_combined)\n",
        "\n",
        "        # Get the variance output and reshape it.\n",
        "        # vmap over the time dimension\n",
        "        var_output_flat = vmap(self.head_log_var_fn)(out_combined)\n",
        "\n",
        "        # Here we could just parameterize Q_inv instead of Q\n",
        "        # but we parameterize Q to be more in line with the SVAE recognition\n",
        "        # which empirically works better with mean instead of natural parameterization\n",
        "        Q = vmap(lie_params_to_constrained, in_axes=(0, None, None))\\\n",
        "            (var_output_flat, output_dim, self.eps)\n",
        "        Q_inv = inv(Q)\n",
        "\n",
        "        dynamics_flat = vmap(self.head_dyn_fn)(out_combined)\n",
        "        A = dynamics_flat.reshape((-1, output_dim, output_dim))\n",
        "\n",
        "        L_diag = np.einsum(\"til,tlj->tij\", -Q_inv[1:], A[1:])\n",
        "        ATQinvA = np.einsum(\"tji,tjl,tlk->tik\", A[1:], Q_inv[1:], A[1:])\n",
        "        ATQinvb = np.einsum(\"tli,tlj,tj->ti\", A[1:], Q_inv[1:], b[1:])\n",
        "        # Here the J matrices are full matrices\n",
        "        J_diag = Q_inv.at[:-1].add(ATQinvA)\n",
        "\n",
        "        h = np.einsum(\"tij,tj->ti\", Q_inv, b).at[:-1].add(ATQinvb)\n",
        "\n",
        "        return (J_diag, L_diag, h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "cellView": "form",
        "id": "caU8H-hMEB_f"
      },
      "outputs": [],
      "source": [
        "# @title Special architectures for PlaNet\n",
        "class PlaNetRecognitionWrapper:\n",
        "    def __init__(self, rec_net):\n",
        "        self.rec_net = rec_net\n",
        "\n",
        "    def init(self, key, *inputs):\n",
        "        return self.rec_net.init(key, *inputs)\n",
        "    \n",
        "    def apply(self, params, x):\n",
        "        return {\n",
        "            \"network_input\": self.rec_net.apply(params[\"rec_params\"], x)[\"h\"],\n",
        "            \"network_params\": params[\"post_params\"],\n",
        "        }\n",
        "\n",
        "class StochasticRNNCell(nn.Module):\n",
        "\n",
        "    output_dim : int = None\n",
        "    rnn_cell : nn.Module = None\n",
        "    trunk_fn: nn.Module = None\n",
        "    head_mean_fn : nn.Module = None\n",
        "    head_log_var_fn : nn.Module = None\n",
        "    eps : float = None\n",
        "        \n",
        "    @classmethod\n",
        "    def from_params(cls, cell_type=nn.GRUCell,\n",
        "                    rnn_dim=None, output_dim=None, \n",
        "                    trunk_type=\"Identity\", trunk_params=None, \n",
        "                    head_mean_type=\"MLP\", head_mean_params=None,\n",
        "                    head_var_type=\"MLP\", head_var_params=None,\n",
        "                    cov_init=1, eps=1e-4, **kwargs): \n",
        "\n",
        "        rnn_cell = cell_type()\n",
        "\n",
        "        if trunk_type == \"Identity\":\n",
        "            trunk_params = { \"features\": rnn_dim }\n",
        "        if head_mean_type == \"MLP\":\n",
        "            head_mean_params[\"features\"] += [output_dim]\n",
        "        if head_var_type == \"MLP\":\n",
        "            head_var_params[\"features\"] += [output_dim * (output_dim + 1) // 2]\n",
        "            head_var_params[\"kernel_init\"] = nn.initializers.zeros\n",
        "            head_var_params[\"bias_init\"] = nn.initializers.constant(cov_init)\n",
        "\n",
        "        trunk_fn = globals()[trunk_type](**trunk_params)\n",
        "        head_mean_fn = globals()[head_mean_type](**head_mean_params)\n",
        "        head_log_var_fn = globals()[head_var_type](**head_var_params)\n",
        "\n",
        "        return cls(output_dim, rnn_cell, trunk_fn, head_mean_fn, head_log_var_fn, eps)\n",
        "\n",
        "    # h: latent state that's carried to the next\n",
        "    # x: last sample\n",
        "    # u: input at this timestep\n",
        "    def __call__(self, h, x, u):\n",
        "        h, out = self.rnn_cell(h, np.concatenate([x, u]))\n",
        "        out = self.trunk_fn(out)\n",
        "        mean, cov_flat = self.head_mean_fn(out), self.head_log_var_fn(out)\n",
        "        cov = lie_params_to_constrained(cov_flat, self.output_dim, self.eps)\n",
        "        return h, (cov, mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {
        "cellView": "form",
        "id": "_DU0-NEWx_Il"
      },
      "outputs": [],
      "source": [
        "# @title Emission network (outputs distribution instead of parameters)\n",
        "\n",
        "# This is largely for convenience\n",
        "class GaussianEmission(GaussianRecognition):\n",
        "    def __call__(self, inputs):\n",
        "        J, h = self._generate_distribution_parameters(inputs)\n",
        "        # TODO: inverting J is pretty bad numerically, perhaps save Cholesky instead?\n",
        "        if (len(J.shape) == 3):\n",
        "            Sigma = vmap(inv)(J)\n",
        "            mu = np.einsum(\"tij,tj->ti\", Sigma, h)\n",
        "        elif (len(J.shape) == 2):\n",
        "            Sigma = inv(J)\n",
        "            mu = np.linalg.solve(J, h)\n",
        "        else:\n",
        "            # Error\n",
        "            return None\n",
        "        return tfd.MultivariateNormalFullCovariance(\n",
        "            loc=mu, covariance_matrix=Sigma)\n",
        "        \n",
        "class GaussianDCNNEmission(PotentialNetwork):\n",
        "\n",
        "    input_rank : int = None\n",
        "    network : nn.Module = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_params(cls, **params):\n",
        "        network = DCNN(**params)\n",
        "        return cls(1, network)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        out = self._generate_distribution_parameters(inputs)\n",
        "        mu = out[\"mu\"]\n",
        "        # Adding a constant to prevent the model from getting too crazy\n",
        "        sigma = out[\"sigma\"] + 1e-4\n",
        "        return tfd.Normal(loc=mu, scale=sigma)\n",
        "\n",
        "    def _call_single(self, x):\n",
        "        out_raw = self.network(x)\n",
        "        mu_raw, sigma_raw = np.split(out_raw, 2, axis=-1)\n",
        "        mu = sigmoid(mu_raw)\n",
        "        sigma = softplus(sigma_raw)\n",
        "        # sigma = np.ones_like(mu) * 0.1\n",
        "        return { \"mu\": mu, \"sigma\": sigma }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xevru2BSSSZ"
      },
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "28XHvF41SVWK"
      },
      "outputs": [],
      "source": [
        "# @title Visualization/animation helpers\n",
        "\n",
        "# Returns a random projection matrix from ND to 2D\n",
        "def random_projection(seed, N):\n",
        "    key1, key2 = jr.split(seed, 2)\n",
        "    v1 = jr.normal(key1, (N,))\n",
        "    v2 = jr.normal(key2, (N,))\n",
        "\n",
        "    v1 /= np.linalg.norm(v1)\n",
        "    v2 -= v1 * np.dot(v1, v2)\n",
        "    v2 /= np.linalg.norm(v2)\n",
        "\n",
        "    return np.stack([v1, v2])\n",
        "\n",
        "def get_gaussian_draw_params(mu, Sigma, proj_seed=None):\n",
        "\n",
        "    Sigma = (Sigma + Sigma.T) * .5\n",
        "\n",
        "    if (mu.shape[0] > 2):\n",
        "        P = random_projection(proj_seed, mu.shape[0])\n",
        "        mu = P @ mu\n",
        "    angles = np.hstack([np.arange(0, 2*np.pi, 0.01), 0])\n",
        "    circle = np.vstack([np.sin(angles), np.cos(angles)])\n",
        "    min_eig = np.min(eigh(Sigma)[0])\n",
        "    eps = 1e-6\n",
        "    if (min_eig <= eps): Sigma += np.eye(Sigma.shape[0]) * eps\n",
        "    L = np.linalg.cholesky(Sigma)\n",
        "    u, svs, vt = svd(P @ L)\n",
        "    ellipse = np.dot(u * svs, circle) * 2\n",
        "    return (mu[0], mu[1]), (ellipse[0, :] + mu[0], ellipse[1, :] + mu[1])\n",
        "\n",
        "def plot_gaussian_2D(mu, Sigma, proj_seed=None, ax=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Helper function to plot 2D Gaussian contours\n",
        "    \"\"\"\n",
        "    (px, py), (exs, eys) = get_gaussian_draw_params(mu, Sigma, proj_seed)\n",
        "\n",
        "    ax = plt.gca() if ax is None else ax\n",
        "    point = ax.plot(px, py, marker='D', **kwargs)\n",
        "    line, = ax.plot(exs, eys, **kwargs)\n",
        "    return (point, line)\n",
        "\n",
        "def get_artists(ax, mus, Sigmas, proj_seed, num_pts, **draw_params):\n",
        "    point_artists = []\n",
        "    line_artists = []\n",
        "\n",
        "    for j in range(num_pts):\n",
        "        mean_params, cov_params = get_gaussian_draw_params(mus[j], \n",
        "                                                           Sigmas[j], \n",
        "                                                           proj_seed)\n",
        "        point = ax.plot(mean_params[0], mean_params[1], marker='D', \n",
        "                        color=colors[j], **draw_params)[0]\n",
        "        line = ax.plot(cov_params[0], cov_params[1], \n",
        "                       color=colors[j], **draw_params)[0]\n",
        "        point_artists.append(point)\n",
        "        line_artists.append(line)\n",
        "    return point_artists, line_artists\n",
        "\n",
        "def update_draw_params(point_artists, line_artists, mus, Sigmas, proj_seed, num_pts):\n",
        "    for i in range(num_pts):\n",
        "        mean_params, cov_params = get_gaussian_draw_params(mus[i], Sigmas[i], proj_seed)\n",
        "        point_artists[i].set_data(mean_params[0], mean_params[1])\n",
        "        line_artists[i].set_data(cov_params[0], cov_params[1])\n",
        "\n",
        "# Some animation helpers\n",
        "def animate_gaussians(inf_mus, inf_Sigmas, \n",
        "                      tgt_mus, tgt_Sigmas, \n",
        "                      true_mus, true_Sigmas,\n",
        "                      num_pts,\n",
        "                      proj_seed=None, x_lim=None, y_lim=None, **kwargs):\n",
        "    proj_seed = jr.PRNGKey(0) if proj_seed is None else proj_seed\n",
        "    print(\"Animating Gaussian blobs...!\")\n",
        "    # First set up the figure, the axis, and the plot element we want to animate\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlim(-10, 10)\n",
        "    ax.set_ylim(-10, 10)\n",
        "    plt.close()\n",
        "    \n",
        "\n",
        "    tgt_points, tgt_lines = get_artists(ax, tgt_mus[0], tgt_Sigmas[0], proj_seed, num_pts,\n",
        "                                        alpha=0.3, linestyle=\"dotted\")\n",
        "    inf_points, inf_lines = get_artists(ax, inf_mus[0], inf_Sigmas[0], proj_seed, num_pts)\n",
        "    true_points, true_lines = get_artists(ax, true_mus, true_Sigmas, \n",
        "                                          proj_seed, num_pts, alpha=0.2)\n",
        "\n",
        "    artists = tgt_points + tgt_lines + inf_points + inf_lines + true_points + true_lines\n",
        "\n",
        "    T = len(inf_mus)\n",
        "\n",
        "    # animation function. This is called sequentially  \n",
        "    def animate(i):\n",
        "        update_draw_params(tgt_points, tgt_lines, tgt_mus[i], tgt_Sigmas[i], proj_seed, num_pts)\n",
        "        update_draw_params(inf_points, inf_lines, inf_mus[i], inf_Sigmas[i], proj_seed, num_pts)\n",
        "        clear_output(wait=True)\n",
        "        print(\"Processing frame #{}/{}\".format(i+1, T))\n",
        "        return artists\n",
        "    \n",
        "    if x_lim is not None:\n",
        "        ax.set_xlim(x_lim)\n",
        "    if y_lim is not None:\n",
        "        ax.set_ylim(y_lim)\n",
        "\n",
        "    anim = animation.FuncAnimation(fig, animate, \n",
        "                                frames=T, interval=50, blit=True)\n",
        "    print(\"Frames created! Displaying animation in output cell...\")\n",
        "    # Note: below is the part which makes it work on Colab\n",
        "    rc('animation', html='jshtml')\n",
        "    return anim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "EUG5JgVpTkC3"
      },
      "outputs": [],
      "source": [
        "# @title Helper for computing posterior marginals\n",
        "def get_emission_matrices(dec_params):\n",
        "    eps = 1e-4\n",
        "    dec_mean_params = dec_params[\"params\"][\"head_mean_fn\"][\"Dense_0\"]\n",
        "    dec_cov_params = dec_params[\"params\"][\"head_log_var_fn\"][\"Dense_0\"]\n",
        "    C_, d_ = dec_mean_params[\"kernel\"].T, dec_mean_params[\"bias\"]\n",
        "    R_ = np.diag(softplus(dec_cov_params[\"bias\"]) + eps)\n",
        "    return { \"C\": C_, \"d\": d_, \"R\": R_ }\n",
        "\n",
        "def get_marginals_and_targets(seed, data, num_points, model, \n",
        "                              past_params, true_model_params):\n",
        "    N, T = data.shape[:2]\n",
        "    rand_sample = jr.permutation(seed, onp.arange(N * T))[:num_points]\n",
        "    trials = rand_sample // T\n",
        "    times = rand_sample % T\n",
        "\n",
        "    # Compute a linear transformation in the latent space that will attempt to \n",
        "    # align the learned posterior to the true posterior\n",
        "    C, d = true_model_params[\"C\"], true_model_params[\"d\"]\n",
        "    CTC = C.T @ C\n",
        "    C_pinv = np.linalg.solve(CTC, C.T)\n",
        "\n",
        "    def align_latents(mus, Sigmas, p):\n",
        "        emissions_matrices = get_emission_matrices(p)\n",
        "        C_, d_ = emissions_matrices[\"C\"], emissions_matrices[\"d\"]\n",
        "        P = C_pinv @ C_\n",
        "        mus = np.einsum(\"ij,nj->ni\", P, mus) + (C_pinv @ (d_ - d))[None,:]\n",
        "        Sigmas = np.einsum(\"ij,njk,kl->nil\", P, Sigmas, P.T)\n",
        "        return mus, Sigmas\n",
        "\n",
        "    def posterior_mean_and_cov(post_params, t):\n",
        "        dist = model.posterior.distribution(post_params)\n",
        "        return dist.mean()[t], dist.covariance()[t]\n",
        "\n",
        "    def gaussian_posterior_mean_and_cov(post_params, t):\n",
        "        dist = true_lds.posterior.distribution(post_params)\n",
        "        return dist.mean()[t], dist.covariance()[t]\n",
        "\n",
        "    index_into_leaves = lambda l: l[trials]\n",
        "    \n",
        "    inf_mus = []\n",
        "    inf_Sigmas = []\n",
        "    tgt_mus = []\n",
        "    tgt_Sigmas = []\n",
        "    \n",
        "    true_lds = LDS(model.prior.latent_dims, T)\n",
        "    true_post_params = vmap(true_lds.e_step, in_axes=(None, 0))\\\n",
        "        (true_model_params, data[trials])\n",
        "    true_mus, true_Sigmas = vmap(gaussian_posterior_mean_and_cov)(true_post_params, times)\n",
        "\n",
        "    # TODO: this is temporary! Only for testing parallel KF!\n",
        "    base = LieParameterizedLinearGaussianChain(model.prior.latent_dims, T)\n",
        "    model_lds = LDS(model.prior.latent_dims, T, base=base)\n",
        "\n",
        "    for i in range(len(past_params)):\n",
        "        model_params = past_params[i]\n",
        "        post_params = jax.tree_util.tree_map(index_into_leaves, \n",
        "                                             model_params[\"post_params\"])\n",
        "        dec_params = model_params[\"dec_params\"]\n",
        "        prior_params = deepcopy(model_params[\"prior_params\"])\n",
        "        # Compute posterior marginals\n",
        "        mus, Sigmas = vmap(posterior_mean_and_cov)(post_params, times)\n",
        "        mus, Sigmas = align_latents(mus, Sigmas, dec_params)\n",
        "        inf_mus.append(mus)\n",
        "        inf_Sigmas.append(Sigmas)\n",
        "\n",
        "        # Infer true posterior under current model params\n",
        "        prior_params.update(get_emission_matrices(dec_params))\n",
        "        tgt_post_params = vmap(model_lds.e_step, in_axes=(None, 0))(prior_params, data[trials])\n",
        "        mus, Sigmas = vmap(gaussian_posterior_mean_and_cov)(tgt_post_params, times)\n",
        "        mus, Sigmas = align_latents(mus, Sigmas, dec_params)\n",
        "\n",
        "        tgt_mus.append(mus)\n",
        "        tgt_Sigmas.append(Sigmas)\n",
        "    return inf_mus, inf_Sigmas, tgt_mus, tgt_Sigmas, true_mus, true_Sigmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "aTYdrgHMdJct"
      },
      "outputs": [],
      "source": [
        "# Trying to figure out the proper way of plotting the projection\n",
        "# Of high dimensional Gaussians\n",
        "# key = jr.split(key)[1]\n",
        "# dim = 5\n",
        "# Q = jr.uniform(key, shape=(dim, dim))\n",
        "# A = Q @ Q.T\n",
        "# L = cholesky(A)\n",
        "# P = random_projection(key_0, dim)\n",
        "# plot_gaussian_2D(np.zeros(dim), A, key_0)\n",
        "# points = jr.normal(key_0, (400, dim))\n",
        "# points /= np.sum(points ** 2, axis=1, keepdims=True) ** .5\n",
        "# points = P @ L @ points.T\n",
        "# plt.scatter(points[0, :], points[1, :], s=1)\n",
        "# u, svs, vt = svd(P @ L)\n",
        "# plt.scatter(u[0][0] * svs[0], u[1][0] * svs[0])\n",
        "# plt.scatter(u[0][1] * svs[1], u[1][1] * svs[1])\n",
        "# # plt.scatter(u[0][0] * np.sqrt(svs[0]), u[1][0] * np.sqrt(svs[0]))\n",
        "# # plt.scatter(u[0][1] * svs[1], u[1][1] * svs[1])\n",
        "# # P = random_projection(key_0, 3)\n",
        "# angles = np.hstack([np.arange(0, 2*np.pi, 0.01), 0])\n",
        "# circle = np.vstack([np.sin(angles), np.cos(angles)])\n",
        "# ellipse = np.dot(u * svs, circle) * 2\n",
        "# plt.plot(ellipse[0,:], ellipse[1,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ImmaouPD-G"
      },
      "source": [
        "## Define training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "cellView": "form",
        "id": "cPqoyXb6PgpV"
      },
      "outputs": [],
      "source": [
        "# @title Trainer object \n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    model: a pytree node\n",
        "    loss (key, params, model, data, **train_params) -> (loss, aux)\n",
        "        Returns a loss (a single float) and an auxillary output (e.g. posterior)\n",
        "    init (key, model, data, **train_params) -> (params, opts)\n",
        "        Returns the initial parameters and optimizers to go with those parameters\n",
        "    update (params, grads, opts, model, aux, **train_params) -> (params, opts)\n",
        "        Returns updated parameters, optimizers\n",
        "    \"\"\"\n",
        "    def __init__(self, model, \n",
        "                 train_params=None, \n",
        "                 init=None, \n",
        "                 loss=None, \n",
        "                 val_loss=None,\n",
        "                 update=None,\n",
        "                 initial_params=None):\n",
        "        # Trainer state\n",
        "        self.params = initial_params\n",
        "        self.model = model\n",
        "        self.past_params = []\n",
        "\n",
        "        if train_params is None:\n",
        "            train_params = dict()\n",
        "\n",
        "        self.train_params = train_params\n",
        "\n",
        "        if init is not None:\n",
        "            self.init = init\n",
        "        if loss is not None:\n",
        "            self.loss = loss\n",
        "\n",
        "        self.val_loss = val_loss or self.loss\n",
        "        if update is not None: \n",
        "            self.update = update\n",
        "\n",
        "    # @partial(jit, static_argnums=(0,))\n",
        "    def train_step(self, key, params, data, opt_states):\n",
        "        model = self.model\n",
        "        results = \\\n",
        "            jax.value_and_grad(\n",
        "                lambda params: partial(self.loss, **self.train_params)(key, model, data, params), has_aux=True)(params)\n",
        "        (loss, aux), grads = results\n",
        "        params, opts = self.update(params, grads, self.opts, opt_states, model, aux, **self.train_params)\n",
        "        return params, opts, (loss, aux), grads\n",
        "\n",
        "    # @partial(jit, static_argnums=(0,))\n",
        "    def val_step(self, key, params, data):\n",
        "        return self.val_loss(key, self.model, data, params, **self.train_params)\n",
        "\n",
        "    # def test_step(self, key, params, model, data):\n",
        "    #     loss_out = self.loss(key, params, model, data)\n",
        "    #     return loss_out\n",
        "\n",
        "    \"\"\"\n",
        "    Callback: a function that takes training iterations and relevant parameter\n",
        "        And logs to WandB\n",
        "    \"\"\"\n",
        "    def train(self, data_dict, max_iters, \n",
        "              callback=None, val_callback=None, \n",
        "              summary=None, key=None,\n",
        "              early_stop_start=5000, \n",
        "              max_lose_streak=1000):\n",
        "\n",
        "        if key is None:\n",
        "            key = jr.PRNGKey(0)\n",
        "\n",
        "        model = self.model\n",
        "        train_data = data_dict[\"train_data\"]\n",
        "        batch_size = self.train_params.get(\"batch_size\") or train_data.shape[0]\n",
        "        num_batches = train_data.shape[0] // batch_size\n",
        "\n",
        "        init_key, key = jr.split(key, 2)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.params, self.opts, self.opt_states = self.init(init_key, model, \n",
        "                                                       train_data[:batch_size], \n",
        "                                                       self.params,\n",
        "                                                       **self.train_params)\n",
        "        self.train_losses = []\n",
        "        self.test_losses = []\n",
        "        self.val_losses = []\n",
        "        self.past_params = []\n",
        "\n",
        "        pbar = trange(max_iters)\n",
        "        pbar.set_description(\"[jit compling...]\")\n",
        "        \n",
        "        mask_start = self.train_params.get(\"mask_start\")\n",
        "        if (mask_start):\n",
        "            mask_size = self.train_params[\"mask_size\"]\n",
        "            self.train_params[\"mask_size\"] = 0\n",
        "\n",
        "        train_step = jit(self.train_step)\n",
        "        val_step = jit(self.val_step)\n",
        "\n",
        "        best_loss = None\n",
        "        best_itr = 0\n",
        "        val_loss = None\n",
        "\n",
        "        for itr in pbar:\n",
        "            train_key, val_key, key = jr.split(key, 3)\n",
        "\n",
        "            batch_id = itr % num_batches\n",
        "            batch_start = batch_id * batch_size\n",
        "\n",
        "            self.params, self.opt_states, loss_out, grads = \\\n",
        "                train_step(train_key, self.params, \n",
        "                           train_data[batch_start:batch_start+batch_size], self.opt_states)\n",
        "\n",
        "            loss, aux = loss_out\n",
        "            self.train_losses.append(loss)\n",
        "            pbar.set_description(\"LP: {:.3f}\".format(loss))\n",
        "\n",
        "            if batch_id == num_batches - 1:\n",
        "                # We're at the end of an epoch\n",
        "                # We could randomly shuffle the data\n",
        "                # train_data = jr.permutation(key, train_data)\n",
        "                if (self.train_params.get(\"use_validation\")):\n",
        "                    val_loss_out = val_step(val_key, self.params, data_dict[\"val_data\"])\n",
        "                    if (val_callback): val_callback(self, val_loss_out, data_dict)\n",
        "                    val_loss, _ = val_loss_out\n",
        "                    \n",
        "            if not self.train_params.get(\"use_validation\") or val_loss is None:\n",
        "                curr_loss = loss\n",
        "            else:\n",
        "                curr_loss = val_loss\n",
        "\n",
        "            if itr >= early_stop_start:\n",
        "                if best_loss is None or curr_loss < best_loss:\n",
        "                    best_itr = itr\n",
        "                    best_loss = curr_loss\n",
        "                if curr_loss > best_loss and itr - best_itr > max_lose_streak:\n",
        "                    print(\"Early stopping!\")\n",
        "                    break\n",
        "\n",
        "            if (callback): callback(self, loss_out, data_dict, grads)\n",
        "\n",
        "            # Record parameters\n",
        "            record_params = self.train_params.get(\"record_params\")\n",
        "            if record_params and record_params(itr):\n",
        "                curr_params = deepcopy(self.params)\n",
        "                curr_params[\"iteration\"] = itr\n",
        "                self.past_params.append(curr_params)\n",
        "\n",
        "            if (mask_start and itr == mask_start):\n",
        "                self.train_params[\"mask_size\"] = mask_size\n",
        "                train_step = jit(self.train_step)\n",
        "                val_step = jit(self.val_step)\n",
        "\n",
        "        if summary:\n",
        "            summary(self)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "id": "KmTBoMwgArBw"
      },
      "outputs": [],
      "source": [
        "# @title Logging to WandB\n",
        "\n",
        "def visualize_lds(trainer, data_dict, aux):\n",
        "    data = data_dict[\"train_data\"]\n",
        "    true_model_params = data_dict[\"lds_params\"]\n",
        "    model = trainer.model\n",
        "    params = [trainer.params]\n",
        "    num_pts = 10\n",
        "    # We want to visualize the posterior marginals\n",
        "    inf_mus, inf_Sigmas, tgt_mus, tgt_Sigmas, true_mus, true_Sigmas = \\\n",
        "        get_marginals_and_targets(key_0, data, num_pts, model, params, true_model_params)\n",
        "    # Create the axis\n",
        "    fig, ax = plt.subplots()\n",
        "    # Plot each of the groups\n",
        "    tgt_points, tgt_lines = get_artists(ax, tgt_mus[0], tgt_Sigmas[0], key_0, num_pts,\n",
        "                                        alpha=0.3, linestyle=\"dotted\", label=\"current target\")\n",
        "    inf_points, inf_lines = get_artists(ax, inf_mus[0], inf_Sigmas[0], key_0, num_pts, label=\"inferred\")\n",
        "    true_points, true_lines = get_artists(ax, true_mus, true_Sigmas, \n",
        "                                          key_0, num_pts, alpha=0.2, label=\"true target\")\n",
        "    # The legend is too large and blocks most of the plot\n",
        "    # plt.legend()\n",
        "    # Relimit the axes\n",
        "    ax.relim()\n",
        "    # update ax.viewLim using the new dataLim\n",
        "    ax.autoscale_view()\n",
        "    fig.canvas.draw()\n",
        "    img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
        "    post_img = wandb.Image(img, caption=\"Posterior marginals versus targets\")\n",
        "    plt.close()\n",
        "    return {\n",
        "        \"Posterior marginals\": post_img\n",
        "    }\n",
        "\n",
        "def visualize_pendulum(trainer, aux):\n",
        "    # This assumes single sequence has shape (100, 24, 24, 1)\n",
        "    recon = aux[\"reconstruction\"][0][0]\n",
        "    # Show the sequence as a block of images\n",
        "    stacked = recon.reshape(10, 24 * 10, 24)\n",
        "    imgrid = stacked.swapaxes(0, 1).reshape(24 * 10, 24 * 10)\n",
        "    recon_img = wandb.Image(onp.array(imgrid), caption=\"Sample Reconstruction\")\n",
        "\n",
        "    fig = plt.figure()\n",
        "    mask = aux[\"mask\"][0]\n",
        "    post_sample = aux[\"posterior_samples\"][0][0]\n",
        "    top, bot = np.max(post_sample) + 5, np.min(post_sample) - 5\n",
        "    left, right = 0, post_sample.shape[0]\n",
        "    plt.imshow(mask[None], cmap=\"gray\", alpha=.4, vmin=0, vmax=1,\n",
        "               extent=(left, right, top, bot))\n",
        "    plt.plot(post_sample)\n",
        "    fig.canvas.draw()\n",
        "    img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
        "    post_img = wandb.Image(img, caption=\"Posterior Sample\")\n",
        "    plt.close()\n",
        "    return {\n",
        "        \"Reconstruction\": recon_img, \n",
        "        \"Posterior Sample\": post_img\n",
        "    }\n",
        "\n",
        "def get_group_name(run_params):\n",
        "    p = run_params\n",
        "    run_type = \"\" if p[\"inference_method\"] in [\"EM\", \"GT\", \"SMC\"] else \"_\" + p[\"run_type\"]\n",
        "    if p[\"dataset\"] == \"pendulum\":\n",
        "        dataset_summary = \"pendulum\"\n",
        "    elif p[\"dataset\"] == \"lds\":\n",
        "        d = p[\"dataset_params\"]\n",
        "        dataset_summary = \"lds_dims_{}_{}_noises_{}_{}\".format(\n",
        "            d[\"latent_dims\"], d[\"emission_dims\"], \n",
        "            d[\"dynamics_cov\"], d[\"emission_cov\"])\n",
        "    else:\n",
        "        dataset_summary = \"???\"\n",
        "\n",
        "    model_summary = \"_{}d_latent_\".format(p[\"latent_dims\"]) + p[\"inference_method\"]\n",
        "\n",
        "    group_tag = p.get(\"group_tag\") or \"\"\n",
        "    if group_tag != \"\": group_tag += \"_\"\n",
        "\n",
        "    group_name = (group_tag +\n",
        "        dataset_summary\n",
        "        + model_summary\n",
        "        + run_type\n",
        "    )\n",
        "    return group_name\n",
        "\n",
        "def validation_log_to_wandb(trainer, loss_out, data_dict):\n",
        "    p = trainer.train_params\n",
        "    if not p.get(\"log_to_wandb\"): return\n",
        "    \n",
        "    project_name = p[\"project_name\"]\n",
        "    group_name = get_group_name(p)\n",
        "\n",
        "    obj, aux = loss_out\n",
        "    elbo = -obj\n",
        "    kl = np.mean(aux[\"kl\"])\n",
        "    ell = np.mean(aux[\"ell\"])\n",
        "\n",
        "    model = trainer.model\n",
        "    prior = model.prior\n",
        "    D = prior.latent_dims\n",
        "    prior_params = trainer.params[\"prior_params\"]\n",
        "\n",
        "    visualizations = {}\n",
        "    if p[\"dataset\"] == \"pendulum\":\n",
        "        visualizations = visualize_pendulum(trainer, aux)\n",
        "        pred_ll = np.mean(aux[\"prediction_ll\"])\n",
        "        visualizations = {\n",
        "            \"Validation reconstruction\": visualizations[\"Reconstruction\"], \n",
        "            \"Validation posterior sample\": visualizations[\"Posterior Sample\"],\n",
        "            \"Validation prediction log likelihood\": pred_ll\n",
        "        }\n",
        "        \n",
        "    to_log = {\"Validation ELBO\": elbo, \"Validation KL\": kl, \"Validation likelihood\": ell,}\n",
        "    to_log.update(visualizations)\n",
        "    wandb.log(to_log)\n",
        "\n",
        "def log_to_wandb(trainer, loss_out, data_dict, grads):\n",
        "    p = trainer.train_params\n",
        "    if not p.get(\"log_to_wandb\"): return\n",
        "    \n",
        "    project_name = p[\"project_name\"]\n",
        "    group_name = get_group_name(p)\n",
        "\n",
        "    itr = len(trainer.train_losses) - 1\n",
        "    if len(trainer.train_losses) == 1:\n",
        "        wandb.init(project=project_name, group=group_name, config=p)\n",
        "        pprint(p)\n",
        "\n",
        "    obj, aux = loss_out\n",
        "    elbo = -obj\n",
        "    kl = np.mean(aux[\"kl\"])\n",
        "    ell = np.mean(aux[\"ell\"])\n",
        "\n",
        "    model = trainer.model\n",
        "    prior = model.prior\n",
        "    D = prior.latent_dims\n",
        "    prior_params = trainer.params[\"prior_params\"]\n",
        "    if (p.get(\"use_natural_grad\")):\n",
        "        Q = prior_params[\"Q\"]\n",
        "        A = prior_params[\"A\"]\n",
        "    else:\n",
        "        Q = lie_params_to_constrained(prior_params[\"Q\"], D)\n",
        "        A = prior_params[\"A\"]\n",
        "\n",
        "    eigs = eigh(Q)[0]\n",
        "    Q_cond_num = np.max(eigs) / np.min(eigs)\n",
        "    svs = svd(A)[1]\n",
        "    max_sv, min_sv = np.max(svs), np.min(svs)\n",
        "    A_cond_num = max_sv / min_sv\n",
        "\n",
        "    # Also log the prior params gradients\n",
        "    # prior_grads = grads[\"prior_params\"][\"sgd_params\"]\n",
        "    # prior_grads_norm = np.linalg.norm(\n",
        "    #     jax.tree_util.tree_leaves(tree_map(np.linalg.norm, prior_grads)))\n",
        "\n",
        "    visualizations = {}\n",
        "    if (itr % p[\"plot_interval\"] == 0):\n",
        "        if p[\"dataset\"] == \"lds\" and p.get(\"visualize_training\"):\n",
        "            visualizations = visualize_lds(trainer, data_dict, aux)\n",
        "        elif p[\"dataset\"] == \"pendulum\" and p.get(\"visualize_training\"):\n",
        "            visualizations = visualize_pendulum(trainer, aux)\n",
        "\n",
        "        # fig = plt.figure()\n",
        "        # prior_sample = prior.sample(prior_params, shape=(1,), key=jr.PRNGKey(0))[0]\n",
        "        # plt.plot(prior_sample)\n",
        "        # fig.canvas.draw()\n",
        "        # img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
        "        # prior_img = wandb.Image(img, caption=\"Prior Sample\")\n",
        "        # plt.close()\n",
        "        # visualizations[\"Prior sample\"] = prior_img\n",
        "    # Also log the learning rates\n",
        "    lr = p[\"learning_rate\"] \n",
        "    lr = lr if isinstance(lr, float) else lr(itr)\n",
        "    prior_lr = p[\"prior_learning_rate\"] \n",
        "    prior_lr = prior_lr if isinstance(prior_lr, float) else prior_lr(itr)\n",
        "\n",
        "    to_log = { \"ELBO\": elbo, \"KL\": kl, \"Likelihood\": ell, # \"Prior graident norm\": prior_grads_norm,\n",
        "               \"Max singular value of A\": max_sv, \"Min singular value of A\": min_sv,\n",
        "               \"Condition number of A\": A_cond_num, \"Condition number of Q\": Q_cond_num,\n",
        "               \"Learning rate\": lr, \"Prior learning rate\": prior_lr }\n",
        "    to_log.update(visualizations)\n",
        "    wandb.log(to_log)\n",
        "\n",
        "def save_params_to_wandb(trainer):\n",
        "    file_name = \"parameters.pkl\"\n",
        "    with open(file_name, \"wb\") as f:\n",
        "        pkl.dump(trainer.past_params, f)\n",
        "        wandb.save(file_name, policy=\"now\")\n",
        "\n",
        "def on_error(data_dict, model_dict):\n",
        "    save_params_to_wandb(model_dict[\"trainer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "ve7zlI0-P8tP"
      },
      "outputs": [],
      "source": [
        "# @title Specifics of SVAE training\n",
        "def svae_init(key, model, data, initial_params=None, **train_params):\n",
        "    init_params = model.init(key)\n",
        "    if (initial_params): init_params.update(initial_params)\n",
        "    \n",
        "    if (train_params[\"inference_method\"] == \"planet\"):\n",
        "        init_params[\"rec_params\"] = {\n",
        "            \"rec_params\": init_params[\"rec_params\"],\n",
        "            \"post_params\": init_params[\"post_params\"]\n",
        "        }\n",
        "    # Expand the posterior parameters by batch size\n",
        "    init_params[\"post_params\"] = vmap(lambda _: init_params[\"post_params\"])(data)\n",
        "    init_params[\"post_samples\"] = np.zeros((data.shape[0], \n",
        "                                            train_params.get(\"obj_samples\") or 1) \n",
        "                                             + model.posterior.shape)\n",
        "\n",
        "    learning_rate = train_params[\"learning_rate\"]\n",
        "    rec_opt = opt.adam(learning_rate=learning_rate)\n",
        "    rec_opt_state = rec_opt.init(init_params[\"rec_params\"])\n",
        "    dec_opt = opt.adam(learning_rate=learning_rate)\n",
        "    dec_opt_state = dec_opt.init(init_params[\"dec_params\"])\n",
        "\n",
        "    if (train_params.get(\"use_natural_grad\")):\n",
        "        prior_lr = None\n",
        "        prior_opt = None\n",
        "        prior_opt_state = None\n",
        "    else:\n",
        "        # Add the option of using an gradient optimizer for prior parameters\n",
        "        prior_lr = train_params.get(\"prior_learning_rate\") or learning_rate\n",
        "        prior_opt = opt.adam(learning_rate=prior_lr)\n",
        "        prior_opt_state = prior_opt.init(init_params[\"prior_params\"])\n",
        "\n",
        "    return (init_params, \n",
        "            (rec_opt, dec_opt, prior_opt), \n",
        "            (rec_opt_state, dec_opt_state, prior_opt_state))\n",
        "    \n",
        "def svae_loss(key, model, data_batch, model_params, **train_params):\n",
        "    batch_size = data_batch.shape[0]\n",
        "    # Axes specification for vmap\n",
        "    # For purposes of this work (AISTATS), we're just going to ignore this\n",
        "    params_in_axes = None\n",
        "    # params_in_axes = dict.fromkeys(model_params.keys(), None)\n",
        "    # params_in_axes[\"post_samples\"] = 0\n",
        "    result = vmap(partial(model.compute_objective, **train_params), \n",
        "                  in_axes=(0, 0, params_in_axes))(jr.split(key, batch_size), data_batch, model_params)\n",
        "    objs = result[\"objective\"]\n",
        "    post_params = result[\"posterior_params\"]\n",
        "    post_samples = result[\"posterior_samples\"]\n",
        "    # Need to compute sufficient stats if we want the natural gradient update\n",
        "    if (train_params.get(\"use_natural_grad\")):\n",
        "        post_suff_stats = vmap(model.posterior.sufficient_statistics)(post_params)\n",
        "        expected_post_suff_stats = tree_map(\n",
        "            lambda l: np.mean(l,axis=0), post_suff_stats)\n",
        "        result[\"sufficient_statistics\"] = expected_post_suff_stats\n",
        "    return -np.mean(objs), result\n",
        "\n",
        "def predict_forward(x, A, b, T):\n",
        "    def _step(carry, t):\n",
        "        carry = A @ carry + b\n",
        "        return carry, carry\n",
        "    return scan(_step, x, np.arange(T))[1]\n",
        "\n",
        "# Note: this is for pendulum data only\n",
        "def svae_val_loss(key, model, data_batch, model_params, **train_params):  \n",
        "    N, T = data_batch.shape[:2]\n",
        "    # We only care about the first 100 timesteps\n",
        "    T = T // 2\n",
        "    D = model.prior.latent_dims\n",
        "\n",
        "    # obs_data, pred_data = data_batch[:,:T//2], data_batch[:,T//2:]\n",
        "    obs_data = data_batch[:,:T]\n",
        "    obj, out_dict = svae_loss(key, model, obs_data, model_params, **train_params)\n",
        "    # Compute the prediction accuracy\n",
        "    prior_params = model_params[\"prior_params\"] \n",
        "    # Instead of this, we want to evaluate the expected log likelihood of the future observations\n",
        "    # under the posterior given the current set of observations\n",
        "    # So E_{q(x'|y)}[p(y'|x')] where the primes represent the future\n",
        "    post_params = out_dict[\"posterior_params\"]\n",
        "\n",
        "    posterior = model.posterior.distribution(post_params)\n",
        "    # J = posterior.filtered_precisions\n",
        "    # h = posterior.filtered_linear_potentials\n",
        "    Sigma_filtered = posterior.filtered_covariances # inv(J)\n",
        "    mu_filtered = posterior.filtered_means # np.einsum(\"...ij,...j->...i\", Sigma_filtered, h)\n",
        "    horizon = train_params[\"prediction_horizon\"] or 5\n",
        "\n",
        "    def _prediction_lls(data_id, key):\n",
        "        num_windows = T-horizon-1\n",
        "        pred_lls = vmap(_sliding_window_prediction_lls, in_axes=(None, None, None, 0, 0))(\n",
        "            mu_filtered[data_id], Sigma_filtered[data_id], obs_data[data_id],\n",
        "            np.arange(num_windows), jr.split(key, num_windows))\n",
        "        return pred_lls.mean()\n",
        "\n",
        "    def _sliding_window_prediction_lls(mu, Sigma, data, t, key):\n",
        "        # Build the posterior object on the future latent states \n",
        "        # (\"the posterior predictive distribution\")\n",
        "        # Convert unconstrained params to constrained dynamics parameters\n",
        "        prior_params_constrained = model.prior.get_dynamics_params(prior_params)\n",
        "        dynamics_params = {\n",
        "            \"m1\": mu[t],\n",
        "            \"Q1\": Sigma[t],\n",
        "            \"A\": prior_params_constrained[\"A\"],\n",
        "            \"b\": prior_params_constrained[\"b\"],\n",
        "            \"Q\": prior_params_constrained[\"Q\"]\n",
        "        }\n",
        "        tridiag_params = dynamics_to_tridiag(dynamics_params, horizon+1, D) # Note the +1\n",
        "        J, L, h = tridiag_params[\"J\"], tridiag_params[\"L\"], tridiag_params[\"h\"]\n",
        "        pred_posterior = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "        # Sample from it and evaluate the log likelihood\n",
        "        x_pred = pred_posterior.sample(seed=key)[1:]\n",
        "        likelihood_dist = model.decoder.apply(model_params[\"dec_params\"], x_pred)\n",
        "        return likelihood_dist.log_prob(\n",
        "            lax.dynamic_slice(data, (t+1, 0, 0, 0), (horizon,) + data.shape[1:])).sum(axis=(1, 2, 3))\n",
        "\n",
        "    # pred_lls = vmap(_prediction_lls)(\n",
        "    #     out_dict[\"posterior_params\"], data_batch, jr.split(key, N))\n",
        "    pred_lls = vmap(_prediction_lls)(np.arange(N), jr.split(key, N))\n",
        "    out_dict[\"prediction_ll\"] = pred_lls\n",
        "    return obj, out_dict\n",
        "\n",
        "def svae_update(params, grads, opts, opt_states, model, aux, **train_params):\n",
        "    rec_opt, dec_opt, prior_opt = opts\n",
        "    rec_opt_state, dec_opt_state, prior_opt_state = opt_states\n",
        "    rec_grad, dec_grad = grads[\"rec_params\"], grads[\"dec_params\"]\n",
        "    updates, rec_opt_state = rec_opt.update(rec_grad, rec_opt_state)\n",
        "    params[\"rec_params\"] = opt.apply_updates(params[\"rec_params\"], updates)\n",
        "    params[\"post_params\"] = aux[\"posterior_params\"]\n",
        "    params[\"post_samples\"] = aux[\"posterior_samples\"]\n",
        "    if train_params[\"run_type\"] == \"model_learning\":\n",
        "        # Update decoder\n",
        "        updates, dec_opt_state = dec_opt.update(dec_grad, dec_opt_state)\n",
        "        params[\"dec_params\"] = opt.apply_updates(params[\"dec_params\"], updates)\n",
        "\n",
        "        old_Q = deepcopy(params[\"prior_params\"][\"Q\"])\n",
        "        old_b = deepcopy(params[\"prior_params\"][\"b\"])\n",
        "\n",
        "        # Update prior parameters\n",
        "        if (train_params.get(\"use_natural_grad\")):\n",
        "            # Here we interpolate the sufficient statistics instead of the parameters\n",
        "            suff_stats = aux[\"sufficient_statistics\"]\n",
        "            lr = params.get(\"prior_learning_rate\") or 1\n",
        "            avg_suff_stats = params[\"prior_params\"][\"avg_suff_stats\"]\n",
        "            # Interpolate the sufficient statistics\n",
        "            params[\"prior_params\"][\"avg_suff_stats\"] = tree_map(lambda x,y : (1 - lr) * x + lr * y, \n",
        "                avg_suff_stats, suff_stats)\n",
        "            params[\"prior_params\"] = model.prior.m_step(params[\"prior_params\"])\n",
        "        else:\n",
        "            updates, prior_opt_state = prior_opt.update(grads[\"prior_params\"], prior_opt_state)\n",
        "            params[\"prior_params\"] = opt.apply_updates(params[\"prior_params\"], updates)\n",
        "        \n",
        "        if (train_params.get(\"constrain_prior\")):\n",
        "            # Revert Q and b to their previous values\n",
        "            params[\"prior_params\"][\"Q\"] = old_Q\n",
        "            params[\"prior_params\"][\"b\"] = old_b\n",
        "\n",
        "        if (train_params.get(\"constrain_dynamics\")):\n",
        "            # Scale A so that its maximum singular value does not exceed 1\n",
        "            params[\"prior_params\"][\"A\"] = truncate_singular_values(params[\"prior_params\"][\"A\"])\n",
        "            # params[\"prior_params\"][\"A\"] = scale_singular_values(params[\"prior_params\"][\"A\"])\n",
        "\n",
        "    return params, (rec_opt_state, dec_opt_state, prior_opt_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {
        "id": "8AdW1WXA4n1U"
      },
      "outputs": [],
      "source": [
        "# @title Model initialization and trainer\n",
        "def init_model(run_params, data_dict):\n",
        "    p = deepcopy(run_params)\n",
        "    d = p[\"dataset_params\"]\n",
        "    latent_dims = p[\"latent_dims\"]\n",
        "    input_shape = data_dict[\"train_data\"].shape[1:]\n",
        "    num_timesteps = input_shape[0]\n",
        "    data = data_dict[\"train_data\"]\n",
        "    seed = p[\"seed\"]\n",
        "    seed_model, seed_elbo, seed_ems, seed_rec = jr.split(seed, 4)\n",
        "\n",
        "    run_type = p[\"run_type\"]\n",
        "    recnet_class = globals()[p[\"recnet_class\"]]\n",
        "    decnet_class = globals()[p[\"decnet_class\"]]\n",
        "\n",
        "    if p[\"inference_method\"] in [\"cdkf\", \"dkf\"]:\n",
        "        posterior = DKFPosterior(latent_dims, num_timesteps)\n",
        "    elif p[\"inference_method\"] == \"planet\":\n",
        "        posterior = PlaNetPosterior(p[\"posterior_architecture\"],\n",
        "                                    latent_dims, num_timesteps)\n",
        "    elif p[\"inference_method\"] == \"svae\":\n",
        "        # The parallel Kalman stuff only applies to SVAE\n",
        "        # Since RNN based methods are inherently sequential\n",
        "        if (p.get(\"use_parallel_kf\")):\n",
        "            posterior = ParallelLDSSVAEPosterior(latent_dims, num_timesteps)\n",
        "        else:\n",
        "            posterior = LDSSVAEPosterior(latent_dims, num_timesteps)\n",
        "        \n",
        "    rec_net = recnet_class.from_params(**p[\"recnet_architecture\"])\n",
        "    dec_net = decnet_class.from_params(**p[\"decnet_architecture\"])\n",
        "    if p[\"inference_method\"] == \"planet\":\n",
        "        # Wrap the recognition network\n",
        "        rec_net = PlaNetRecognitionWrapper(rec_net)\n",
        "\n",
        "    if (p.get(\"use_natural_grad\")):\n",
        "        if (p.get(\"use_parallel_kf\")):\n",
        "            prior = ParallelLinearGaussianChain(latent_dims, num_timesteps)\n",
        "        else:\n",
        "            prior = LinearGaussianChain(latent_dims, num_timesteps)\n",
        "    else:\n",
        "        if (p.get(\"use_parallel_kf\")):\n",
        "            prior = ParallelLieParameterizedLinearGaussianChain(latent_dims, num_timesteps)\n",
        "        else:\n",
        "            prior = LieParameterizedLinearGaussianChain(latent_dims, num_timesteps)\n",
        "\n",
        "    model = DeepLDS(\n",
        "        recognition=rec_net,\n",
        "        decoder=dec_net,\n",
        "        prior=prior,\n",
        "        posterior=posterior,\n",
        "        input_dummy=np.zeros(input_shape),\n",
        "        latent_dummy=np.zeros((num_timesteps, latent_dims))\n",
        "    )\n",
        "    \n",
        "    # TODO: Let's get the full linear version working first before moving on\n",
        "    # assert(run_params[\"run_type\"] == \"full_linear\")\n",
        "    if (run_type == \"inference_only\"):\n",
        "        p = data_dict[\"lds_params\"]\n",
        "        prior_params = { \"A\": p[\"A\"], \"b\": p[\"b\"], \n",
        "                        \"Q\": p[\"Q\"], \"m1\": p[\"m1\"], \"Q1\": p[\"Q1\"],\n",
        "                        \"avg_suff_stats\": p[\"avg_suff_stats\"]}\n",
        "        dec_params = fd.FrozenDict(\n",
        "            {\n",
        "                \"params\": {\n",
        "                    \"head_log_var_fn\":{\n",
        "                        \"Dense_0\":{\n",
        "                            \"bias\": inv_softplus(np.diag(p[\"R\"])),\n",
        "                            \"kernel\": np.zeros_like(p[\"C\"]).T\n",
        "                        }\n",
        "                    },\n",
        "                    \"head_mean_fn\":{\n",
        "                        \"Dense_0\":{\n",
        "                            \"bias\": p[\"d\"],\n",
        "                            \"kernel\": p[\"C\"].T\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            })\n",
        "        initial_params = { \"prior_params\": prior_params, \"dec_params\": dec_params }\n",
        "    else:\n",
        "        initial_params = None\n",
        "\n",
        "    # emission_params = emission.init(seed_ems, np.ones((num_latent_dims,)))\n",
        "    # Define the trainer object here\n",
        "    trainer = Trainer(model, train_params=run_params, init=svae_init, \n",
        "                      loss=svae_loss, \n",
        "                      val_loss=svae_val_loss, \n",
        "                      update=svae_update, initial_params=initial_params)\n",
        "\n",
        "    return {\n",
        "        # We don't actually need to include model here\n",
        "        # 'cause it's included in the trainer object\n",
        "        \"model\": model,\n",
        "        # \"emission_params\": emission_params\n",
        "        \"trainer\": trainer\n",
        "    }\n",
        "\n",
        "def start_trainer(model_dict, data_dict, run_params):\n",
        "    trainer = model_dict[\"trainer\"]\n",
        "    trainer.train(data_dict,\n",
        "                  max_iters=run_params[\"max_iters\"],\n",
        "                  key=run_params[\"seed\"],\n",
        "                  callback=log_to_wandb, val_callback=validation_log_to_wandb,\n",
        "                  summary=save_params_to_wandb)\n",
        "    return (trainer.model, trainer.params, trainer.train_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NSj6XUpPZRG"
      },
      "source": [
        "## Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {
        "id": "ZeG1j7HgqWya"
      },
      "outputs": [],
      "source": [
        "# @title Sample from LDS\n",
        "def sample_lds_dataset(run_params):    \n",
        "    d = run_params[\"dataset_params\"]\n",
        "    \n",
        "    global data_dict\n",
        "    if data_dict is not None \\\n",
        "        and \"dataset_params\" in data_dict \\\n",
        "        and str(data_dict[\"dataset_params\"]) == str(fd.freeze(d)):\n",
        "        print(\"Using existing data.\")\n",
        "        print(\"Data MLL: \", data_dict[\"marginal_log_likelihood\"])\n",
        "        return data_dict\n",
        "\n",
        "    data_dict = {}\n",
        "\n",
        "    seed = d[\"seed\"]\n",
        "    emission_dims = d[\"emission_dims\"]\n",
        "    latent_dims = d[\"latent_dims\"]\n",
        "    emission_cov = d[\"emission_cov\"]\n",
        "    dynamics_cov = d[\"dynamics_cov\"]\n",
        "    num_timesteps = d[\"num_timesteps\"]\n",
        "    num_trials = d[\"num_trials\"]\n",
        "    seed_m1, seed_C, seed_d, seed_A, seed_sample = jr.split(seed, 5)\n",
        "\n",
        "    R = emission_cov * np.eye(emission_dims)\n",
        "    Q = dynamics_cov * np.eye(latent_dims)\n",
        "    C = jr.normal(seed_C, shape=(emission_dims, latent_dims))\n",
        "    d = jr.normal(seed_d, shape=(emission_dims,))\n",
        "\n",
        "    # Here we let Q1 = Q\n",
        "    lds = ParallelLDS(latent_dims, num_timesteps)\n",
        "    \n",
        "    params = {\n",
        "            \"m1\": jr.normal(key=seed_m1, shape=(latent_dims,)),\n",
        "            \"Q1\": Q,\n",
        "            \"Q\": Q,\n",
        "            \"A\": random_rotation(seed_A, latent_dims, theta=np.pi/20),\n",
        "            \"b\": np.zeros(latent_dims),\n",
        "            \"R\": R,\n",
        "            \"C\": C,\n",
        "            \"d\": d,\n",
        "        }\n",
        "    constrained = lds.get_constrained_params(params)\n",
        "    params[\"avg_suff_stats\"] = { \"Ex\": constrained[\"Ex\"], \n",
        "                                \"ExxT\": constrained[\"ExxT\"], \n",
        "                                \"ExnxT\": constrained[\"ExnxT\"] }\n",
        "\n",
        "    states, data = lds.sample(params, \n",
        "                              shape=(num_trials,), \n",
        "                              key=seed_sample)\n",
        "    \n",
        "    mll = vmap(lds.marginal_log_likelihood, in_axes=(None, 0))(params, data)\n",
        "    mll = np.mean(mll, axis=0)\n",
        "    print(\"Data MLL: \", mll)\n",
        "    \n",
        "    seed_val, _ = jr.split(seed_sample)\n",
        "    val_states, val_data = lds.sample(params, \n",
        "                              shape=(num_trials,), \n",
        "                              key=seed_val)\n",
        "\n",
        "    data_dict[\"generative_model\"] = lds\n",
        "    data_dict[\"marginal_log_likelihood\"] = mll\n",
        "    data_dict[\"train_data\"] = data\n",
        "    data_dict[\"train_states\"] = states\n",
        "    data_dict[\"val_data\"] = val_data\n",
        "    data_dict[\"val_states\"] = val_states\n",
        "    data_dict[\"dataset_params\"] = fd.freeze(run_params[\"dataset_params\"])\n",
        "    data_dict[\"lds_params\"] = params\n",
        "    return data_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "cellView": "form",
        "id": "MKj_UmfS6JdM"
      },
      "outputs": [],
      "source": [
        "# @title Testing the correctness of LDS m-step\n",
        "# seed = jr.PRNGKey(0)\n",
        "# emission_dims = 5\n",
        "# latent_dims = 3\n",
        "# emission_cov = 10.\n",
        "# dynamics_cov = .1\n",
        "# num_timesteps = 100\n",
        "# seed, seed_C, seed_d, seed_sample = jr.split(seed, 4)\n",
        "# R = emission_cov * np.eye(emission_dims)\n",
        "# Q = dynamics_cov * np.eye(latent_dims)\n",
        "# C = jr.normal(seed_C, shape=(emission_dims, latent_dims))\n",
        "# d = jr.normal(seed_d, shape=(emission_dims,))\n",
        "# # Here we let Q1 = Q\n",
        "# lds = LDS(latent_dims, num_timesteps)\n",
        "# params = lds.init(seed)\n",
        "# params.update(\n",
        "#     {\n",
        "#         \"Q1\": Q,\n",
        "#         \"Q\": Q,\n",
        "#         \"R\": R,\n",
        "#         \"C\": C,\n",
        "#         \"d\": d,\n",
        "#     }\n",
        "# )\n",
        "# states, data = lds.sample(params, \n",
        "#                           shape=(50,), \n",
        "#                           key=seed_sample)\n",
        "# post_params = vmap(lds.e_step, in_axes=(None, 0))(params, data)\n",
        "# posterior = LinearGaussianChainPosterior(latent_dims, num_timesteps)\n",
        "# suff_stats = vmap(posterior.sufficient_statistics)(post_params)\n",
        "# expected_suff_stats = jax.tree_util.tree_map(\n",
        "#         lambda l: np.mean(l,axis=0), suff_stats)\n",
        "# inferred_params = lds.m_step(params, expected_suff_stats)\n",
        "# for key in [\"m1\", \"Q1\", \"A\", \"Q\", \"b\"]:\n",
        "#     print(inferred_params[key] - params[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "cellView": "form",
        "id": "Qzp5Fb0CGtqk"
      },
      "outputs": [],
      "source": [
        "# @title Code for the pendulum dataset (~128 mb)\n",
        "\n",
        "\n",
        "# Modeling Irregular Time Series with Continuous Recurrent Units (CRUs)\n",
        "# Copyright (c) 2022 Robert Bosch GmbH\n",
        "# This program is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Affero General Public License as published\n",
        "# by the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "#\n",
        "# This program is distributed in the hope that it will be useful,\n",
        "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "# GNU Affero General Public License for more details.\n",
        "#\n",
        "# You should have received a copy of the GNU Affero General Public License\n",
        "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
        "#\n",
        "# This source code is derived from Pytorch RKN Implementation (https://github.com/ALRhub/rkn_share)\n",
        "# Copyright (c) 2021 Philipp Becker (Autonomous Learning Robots Lab @ KIT)\n",
        "# licensed under MIT License\n",
        "# cf. 3rd-party-licenses.txt file in the root directory of this source tree.\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "def add_img_noise(imgs, first_n_clean, random, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75, t_uu=1.0):\n",
        "    \"\"\"\n",
        "    :param imgs: Images to add noise to\n",
        "    :param first_n_clean: Keep first_n_images clean to allow the filter to burn in\n",
        "    :param random: np.random.RandomState used for sampling\n",
        "    :param r: \"correlation (over time) factor\" the smaller the more the noise is correlated\n",
        "    :param t_ll: lower bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_lu: upper bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_ul: lower bound of the interval the upper bound for each sequence is sampled from\n",
        "    :param t_uu: upper bound of the interval the upper bound for each sequence is sampled from\n",
        "    :return: noisy images, factors used to create them\n",
        "    \"\"\"\n",
        "\n",
        "    assert t_ll <= t_lu <= t_ul <= t_uu, \"Invalid bounds for noise generation\"\n",
        "    if len(imgs.shape) < 5:\n",
        "        imgs = onp.expand_dims(imgs, -1)\n",
        "    batch_size, seq_len = imgs.shape[:2]\n",
        "    factors = onp.zeros([batch_size, seq_len])\n",
        "    factors[:, 0] = random.uniform(low=0.0, high=1.0, size=batch_size)\n",
        "    for i in range(seq_len - 1):\n",
        "        factors[:, i + 1] = onp.clip(factors[:, i] + random.uniform(\n",
        "            low=-r, high=r, size=batch_size), a_min=0.0, a_max=1.0)\n",
        "\n",
        "    t1 = random.uniform(low=t_ll, high=t_lu, size=(batch_size, 1))\n",
        "    t2 = random.uniform(low=t_ul, high=t_uu, size=(batch_size, 1))\n",
        "\n",
        "    factors = (factors - t1) / (t2 - t1)\n",
        "    factors = onp.clip(factors, a_min=0.0, a_max=1.0)\n",
        "    factors = onp.reshape(factors, list(factors.shape) + [1, 1, 1])\n",
        "    factors[:, :first_n_clean] = 1.0\n",
        "    noisy_imgs = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        if imgs.dtype == onp.uint8:\n",
        "            noise = random.uniform(low=0.0, high=255, size=imgs.shape[1:])\n",
        "            noisy_imgs.append(\n",
        "                (factors[i] * imgs[i] + (1 - factors[i]) * noise).astype(onp.uint8))\n",
        "        else:\n",
        "            noise = random.uniform(low=0.0, high=1.1, size=imgs.shape[1:])\n",
        "            noisy_imgs.append(factors[i] * imgs[i] + (1 - factors[i]) * noise)\n",
        "\n",
        "    return onp.squeeze(onp.concatenate([onp.expand_dims(n, 0) for n in noisy_imgs], 0)), factors\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "def add_img_noise4(imgs, first_n_clean, random, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75, t_uu=1.0):\n",
        "    \"\"\"\n",
        "    :param imgs: Images to add noise to\n",
        "    :param first_n_clean: Keep first_n_images clean to allow the filter to burn in\n",
        "    :param random: np.random.RandomState used for sampling\n",
        "    :param r: \"correlation (over time) factor\" the smaller the more the noise is correlated\n",
        "    :param t_ll: lower bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_lu: upper bound of the interval the lower bound for each sequence is sampled from\n",
        "    :param t_ul: lower bound of the interval the upper bound for each sequence is sampled from\n",
        "    :param t_uu: upper bound of the interval the upper bound for each sequence is sampled from\n",
        "    :return: noisy images, factors used to create them\n",
        "    \"\"\"\n",
        "\n",
        "    half_x = int(imgs.shape[2] / 2)\n",
        "    half_y = int(imgs.shape[3] / 2)\n",
        "    assert t_ll <= t_lu <= t_ul <= t_uu, \"Invalid bounds for noise generation\"\n",
        "    if len(imgs.shape) < 5:\n",
        "        imgs = np.expand_dims(imgs, -1)\n",
        "    batch_size, seq_len = imgs.shape[:2]\n",
        "    factors = np.zeros([batch_size, seq_len, 4])\n",
        "    factors[:, 0] = random.uniform(low=0.0, high=1.0, size=(batch_size, 4))\n",
        "    for i in range(seq_len - 1):\n",
        "        factors[:, i + 1] = np.clip(factors[:, i] + random.uniform(\n",
        "            low=-r, high=r, size=(batch_size, 4)), a_min=0.0, a_max=1.0)\n",
        "\n",
        "    t1 = random.uniform(low=t_ll, high=t_lu, size=(batch_size, 1, 4))\n",
        "    t2 = random.uniform(low=t_ul, high=t_uu, size=(batch_size, 1, 4))\n",
        "\n",
        "    factors = (factors - t1) / (t2 - t1)\n",
        "    factors = np.clip(factors, a_min=0.0, a_max=1.0)\n",
        "    factors = np.reshape(factors, list(factors.shape) + [1, 1, 1])\n",
        "    factors[:, :first_n_clean] = 1.0\n",
        "    noisy_imgs = []\n",
        "    qs = []\n",
        "    for i in range(batch_size):\n",
        "        if imgs.dtype == np.uint8:\n",
        "            qs.append(detect_pendulums(imgs[i], half_x, half_y))\n",
        "            noise = random.uniform(low=0.0, high=255, size=[\n",
        "                                   4, seq_len, half_x, half_y, imgs.shape[-1]]).astype(np.uint8)\n",
        "            curr = np.zeros(imgs.shape[1:], dtype=np.uint8)\n",
        "            curr[:, :half_x, :half_y] = (factors[i, :, 0] * imgs[i, :, :half_x, :half_y] + (\n",
        "                1 - factors[i, :, 0]) * noise[0]).astype(np.uint8)\n",
        "            curr[:, :half_x, half_y:] = (factors[i, :, 1] * imgs[i, :, :half_x, half_y:] + (\n",
        "                1 - factors[i, :, 1]) * noise[1]).astype(np.uint8)\n",
        "            curr[:, half_x:, :half_y] = (factors[i, :, 2] * imgs[i, :, half_x:, :half_y] + (\n",
        "                1 - factors[i, :, 2]) * noise[2]).astype(np.uint8)\n",
        "            curr[:, half_x:, half_y:] = (factors[i, :, 3] * imgs[i, :, half_x:, half_y:] + (\n",
        "                1 - factors[i, :, 3]) * noise[3]).astype(np.uint8)\n",
        "        else:\n",
        "            noise = random.uniform(low=0.0, high=1.0, size=[\n",
        "                                   4, seq_len, half_x, half_y, imgs.shape[-1]])\n",
        "            curr = np.zeros(imgs.shape[1:])\n",
        "            curr[:, :half_x, :half_y] = factors[i, :, 0] * imgs[i, :,\n",
        "                                                                :half_x, :half_y] + (1 - factors[i, :, 0]) * noise[0]\n",
        "            curr[:, :half_x, half_y:] = factors[i, :, 1] * imgs[i, :,\n",
        "                                                                :half_x, half_y:] + (1 - factors[i, :, 1]) * noise[1]\n",
        "            curr[:, half_x:, :half_y] = factors[i, :, 2] * imgs[i, :,\n",
        "                                                                half_x:, :half_y] + (1 - factors[i, :, 2]) * noise[2]\n",
        "            curr[:, half_x:, half_y:] = factors[i, :, 3] * imgs[i, :,\n",
        "                                                                half_x:, half_y:] + (1 - factors[i, :, 3]) * noise[3]\n",
        "        noisy_imgs.append(curr)\n",
        "\n",
        "    factors_ext = np.concatenate([np.squeeze(factors), np.zeros(\n",
        "        [factors.shape[0], factors.shape[1], 1])], -1)\n",
        "    q = np.concatenate([np.expand_dims(q, 0) for q in qs], 0)\n",
        "    f = np.zeros(q.shape)\n",
        "    for i in range(f.shape[0]):\n",
        "        for j in range(f.shape[1]):\n",
        "            for k in range(3):\n",
        "                f[i, j, k] = factors_ext[i, j, q[i, j, k]]\n",
        "\n",
        "    return np.squeeze(np.concatenate([np.expand_dims(n, 0) for n in noisy_imgs], 0)), f\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "def detect_pendulums(imgs, half_x, half_y):\n",
        "    qs = [imgs[:, :half_x, :half_y], imgs[:, :half_x, half_y:],\n",
        "          imgs[:, half_x:, :half_y], imgs[:, half_x:, half_y:]]\n",
        "\n",
        "    r_cts = np.array(\n",
        "        [np.count_nonzero(q[:, :, :, 0] > 5, axis=(-1, -2)) for q in qs]).T\n",
        "    g_cts = np.array(\n",
        "        [np.count_nonzero(q[:, :, :, 1] > 5, axis=(-1, -2)) for q in qs]).T\n",
        "    b_cts = np.array(\n",
        "        [np.count_nonzero(q[:, :, :, 2] > 5, axis=(-1, -2)) for q in qs]).T\n",
        "\n",
        "    cts = np.concatenate([np.expand_dims(c, 1)\n",
        "                         for c in [r_cts, g_cts, b_cts]], 1)\n",
        "\n",
        "    q_max = np.max(cts, -1)\n",
        "    q = np.argmax(cts, -1)\n",
        "    q[q_max < 10] = 4\n",
        "    return q\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "class Pendulum:\n",
        "\n",
        "    MAX_VELO_KEY = 'max_velo'\n",
        "    MAX_TORQUE_KEY = 'max_torque'\n",
        "    MASS_KEY = 'mass'\n",
        "    LENGTH_KEY = 'length'\n",
        "    GRAVITY_KEY = 'g'\n",
        "    FRICTION_KEY = 'friction'\n",
        "    DT_KEY = 'dt'\n",
        "    SIM_DT_KEY = 'sim_dt'\n",
        "    TRANSITION_NOISE_TRAIN_KEY = 'transition_noise_train'\n",
        "    TRANSITION_NOISE_TEST_KEY = 'transition_noise_test'\n",
        "\n",
        "    OBSERVATION_MODE_LINE = \"line\"\n",
        "    OBSERVATION_MODE_BALL = \"ball\"\n",
        "\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def __init__(self,\n",
        "                 img_size,\n",
        "                 observation_mode,\n",
        "                 generate_actions=False,\n",
        "                 transition_noise_std=0.0,\n",
        "                 observation_noise_std=0.0,\n",
        "                 pendulum_params=None,\n",
        "                 seed=0):\n",
        "\n",
        "        assert observation_mode == Pendulum.OBSERVATION_MODE_BALL or observation_mode == Pendulum.OBSERVATION_MODE_LINE\n",
        "        # Global Parameters\n",
        "        self.state_dim = 2\n",
        "        self.action_dim = 1\n",
        "        self.img_size = img_size\n",
        "        self.observation_dim = img_size ** 2\n",
        "        self.observation_mode = observation_mode\n",
        "\n",
        "        self.random = onp.random.RandomState(seed)\n",
        "\n",
        "        # image parameters\n",
        "        self.img_size_internal = 128\n",
        "        self.x0 = self.y0 = 64\n",
        "        self.plt_length = 55 if self.observation_mode == Pendulum.OBSERVATION_MODE_LINE else 50\n",
        "        self.plt_width = 8\n",
        "\n",
        "        self.generate_actions = generate_actions\n",
        "\n",
        "        # simulation parameters\n",
        "        if pendulum_params is None:\n",
        "            pendulum_params = self.pendulum_default_params()\n",
        "        self.max_velo = pendulum_params[Pendulum.MAX_VELO_KEY]\n",
        "        self.max_torque = pendulum_params[Pendulum.MAX_TORQUE_KEY]\n",
        "        self.dt = pendulum_params[Pendulum.DT_KEY]\n",
        "        self.mass = pendulum_params[Pendulum.MASS_KEY]\n",
        "        self.length = pendulum_params[Pendulum.LENGTH_KEY]\n",
        "        self.inertia = self.mass * self.length**2 / 3\n",
        "        self.g = pendulum_params[Pendulum.GRAVITY_KEY]\n",
        "        self.friction = pendulum_params[Pendulum.FRICTION_KEY]\n",
        "        self.sim_dt = pendulum_params[Pendulum.SIM_DT_KEY]\n",
        "\n",
        "        self.observation_noise_std = observation_noise_std\n",
        "        self.transition_noise_std = transition_noise_std\n",
        "\n",
        "        self.tranisition_covar_mat = onp.diag(\n",
        "            np.array([1e-8, self.transition_noise_std**2, 1e-8, 1e-8]))\n",
        "        self.observation_covar_mat = onp.diag(\n",
        "            [self.observation_noise_std**2, self.observation_noise_std**2])\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def sample_data_set(self, num_episodes, episode_length, full_targets):\n",
        "        states = onp.zeros((num_episodes, episode_length, self.state_dim))\n",
        "        actions = self._sample_action(\n",
        "            (num_episodes, episode_length, self.action_dim))\n",
        "        states[:, 0, :] = self._sample_init_state(num_episodes)\n",
        "        t = onp.zeros((num_episodes, episode_length))\n",
        "\n",
        "        for i in range(1, episode_length):\n",
        "            states[:, i, :], dt = self._get_next_states(\n",
        "                states[:, i - 1, :], actions[:, i - 1, :])\n",
        "            t[:, i:] += dt\n",
        "        states[..., 0] -= onp.pi\n",
        "\n",
        "        if self.observation_noise_std > 0.0:\n",
        "            observation_noise = self.random.normal(loc=0.0,\n",
        "                                                   scale=self.observation_noise_std,\n",
        "                                                   size=states.shape)\n",
        "        else:\n",
        "            observation_noise = onp.zeros(states.shape)\n",
        "\n",
        "        targets = self.pendulum_kinematic(states)\n",
        "        if self.observation_mode == Pendulum.OBSERVATION_MODE_LINE:\n",
        "            noisy_states = states + observation_noise\n",
        "            noisy_targets = self.pendulum_kinematic(noisy_states)\n",
        "        elif self.observation_mode == Pendulum.OBSERVATION_MODE_BALL:\n",
        "            noisy_targets = targets + observation_noise\n",
        "        imgs = self._generate_images(noisy_targets[..., :2])\n",
        "\n",
        "        return imgs, targets[..., :(4 if full_targets else 2)], states, noisy_targets[..., :(4 if full_targets else 2)], t/self.dt\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    @staticmethod\n",
        "    def pendulum_default_params():\n",
        "        return {\n",
        "            Pendulum.MAX_VELO_KEY: 8,\n",
        "            Pendulum.MAX_TORQUE_KEY: 10,\n",
        "            Pendulum.MASS_KEY: 1,\n",
        "            Pendulum.LENGTH_KEY: 1,\n",
        "            Pendulum.GRAVITY_KEY: 9.81,\n",
        "            Pendulum.FRICTION_KEY: 0,\n",
        "            Pendulum.DT_KEY: 0.05,\n",
        "            Pendulum.SIM_DT_KEY: 1e-4\n",
        "        }\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _sample_action(self, shape):\n",
        "        if self.generate_actions:\n",
        "            return self.random.uniform(-self.max_torque, self.max_torque, shape)\n",
        "        else:\n",
        "            return np.zeros(shape=shape)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _transition_function(self, states, actions):\n",
        "        dt = self.dt\n",
        "        n_steps = dt / self.sim_dt\n",
        "\n",
        "        if n_steps != np.round(n_steps):\n",
        "            #print(n_steps, 'Warning from Pendulum: dt does not match up')\n",
        "            n_steps = np.round(n_steps)\n",
        "\n",
        "        c = self.g * self.length * self.mass / self.inertia\n",
        "        for i in range(0, int(n_steps)):\n",
        "            velNew = states[..., 1:2] + self.sim_dt * (c * np.sin(states[..., 0:1])\n",
        "                                                       + actions / self.inertia\n",
        "                                                       - states[..., 1:2] * self.friction)\n",
        "            states = onp.concatenate(\n",
        "                (states[..., 0:1] + self.sim_dt * velNew, velNew), axis=1)\n",
        "        return states, dt\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _get_next_states(self, states, actions):\n",
        "        actions = np.maximum(-self.max_torque,\n",
        "                             np.minimum(actions, self.max_torque))\n",
        "\n",
        "        states, dt = self._transition_function(states, actions)\n",
        "        if self.transition_noise_std > 0.0:\n",
        "            states[:, 1] += self.random.normal(loc=0.0,\n",
        "                                               scale=self.transition_noise_std,\n",
        "                                               size=[len(states)])\n",
        "\n",
        "        states[:, 0] = ((states[:, 0]) % (2 * np.pi))\n",
        "        return states, dt\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def get_ukf_smothing(self, obs):\n",
        "        batch_size, seq_length = obs.shape[:2]\n",
        "        succ = np.zeros(batch_size, dtype=np.bool)\n",
        "        means = np.zeros([batch_size, seq_length, 4])\n",
        "        covars = np.zeros([batch_size, seq_length, 4, 4])\n",
        "        fail_ct = 0\n",
        "        for i in range(batch_size):\n",
        "            if i % 10 == 0:\n",
        "                print(i)\n",
        "            try:\n",
        "                means[i], covars[i] = self.ukf.filter(obs[i])\n",
        "                succ[i] = True\n",
        "            except:\n",
        "                fail_ct += 1\n",
        "        print(fail_ct / batch_size, \"failed\")\n",
        "\n",
        "        return means[succ], covars[succ], succ\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _sample_init_state(self, nr_epochs):\n",
        "        return onp.concatenate((self.random.uniform(0, 2 * np.pi, (nr_epochs, 1)), np.zeros((nr_epochs, 1))), 1)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def add_observation_noise(self, imgs, first_n_clean, r=0.2, t_ll=0.1, t_lu=0.4, t_ul=0.6, t_uu=0.9):\n",
        "        return add_img_noise(imgs, first_n_clean, self.random, r, t_ll, t_lu, t_ul, t_uu)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _get_task_space_pos(self, joint_states):\n",
        "        task_space_pos = onp.zeros(list(joint_states.shape[:-1]) + [2])\n",
        "        task_space_pos[..., 0] = np.sin(joint_states[..., 0]) * self.length\n",
        "        task_space_pos[..., 1] = np.cos(joint_states[..., 0]) * self.length\n",
        "        return task_space_pos\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _generate_images(self, ts_pos):\n",
        "        imgs = onp.zeros(shape=list(ts_pos.shape)[\n",
        "                        :-1] + [self.img_size, self.img_size], dtype=np.uint8)\n",
        "        for seq_idx in range(ts_pos.shape[0]):\n",
        "            for idx in range(ts_pos.shape[1]):\n",
        "                imgs[seq_idx, idx] = self._generate_single_image(\n",
        "                    ts_pos[seq_idx, idx])\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _generate_single_image(self, pos):\n",
        "        x1 = pos[0] * (self.plt_length / self.length) + self.x0\n",
        "        y1 = pos[1] * (self.plt_length / self.length) + self.y0\n",
        "        img = Image.new('F', (self.img_size_internal,\n",
        "                        self.img_size_internal), 0.0)\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        if self.observation_mode == Pendulum.OBSERVATION_MODE_LINE:\n",
        "            draw.line([(self.x0, self.y0), (x1, y1)],\n",
        "                      fill=1.0, width=self.plt_width)\n",
        "        elif self.observation_mode == Pendulum.OBSERVATION_MODE_BALL:\n",
        "            x_l = x1 - self.plt_width\n",
        "            x_u = x1 + self.plt_width\n",
        "            y_l = y1 - self.plt_width\n",
        "            y_u = y1 + self.plt_width\n",
        "            draw.ellipse((x_l, y_l, x_u, y_u), fill=1.0)\n",
        "\n",
        "        img = img.resize((self.img_size, self.img_size),\n",
        "                         resample=Image.ANTIALIAS)\n",
        "        img_as_array = onp.asarray(img)\n",
        "        img_as_array = onp.clip(img_as_array, 0, 1)\n",
        "        return 255.0 * img_as_array\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def _kf_transition_function(self, state, noise):\n",
        "        nSteps = self.dt / self.sim_dt\n",
        "\n",
        "        if nSteps != np.round(nSteps):\n",
        "            print('Warning from Pendulum: dt does not match up')\n",
        "            nSteps = np.round(nSteps)\n",
        "\n",
        "        c = self.g * self.length * self.mass / self.inertia\n",
        "        for i in range(0, int(nSteps)):\n",
        "            velNew = state[1] + self.sim_dt * \\\n",
        "                (c * np.sin(state[0]) - state[1] * self.friction)\n",
        "            state = onp.array([state[0] + self.sim_dt * velNew, velNew])\n",
        "        state[0] = state[0] % (2 * np.pi)\n",
        "        state[1] = state[1] + noise[1]\n",
        "        return state\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def pendulum_kinematic_single(self, js):\n",
        "        theta, theat_dot = js\n",
        "        x = np.sin(theta)\n",
        "        y = np.cos(theta)\n",
        "        x_dot = theat_dot * y\n",
        "        y_dot = theat_dot * -x\n",
        "        return onp.array([x, y, x_dot, y_dot]) * self.length\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def pendulum_kinematic(self, js_batch):\n",
        "        theta = js_batch[..., :1]\n",
        "        theta_dot = js_batch[..., 1:]\n",
        "        x = np.sin(theta)\n",
        "        y = np.cos(theta)\n",
        "        x_dot = theta_dot * y\n",
        "        y_dot = theta_dot * -x\n",
        "        return onp.concatenate([x, y, x_dot, y_dot], axis=-1)\n",
        "\n",
        "    # taken from https://github.com/ALRhub/rkn_share/ and not modified\n",
        "    def inverse_pendulum_kinematics(self, ts_batch):\n",
        "        x = ts_batch[..., :1]\n",
        "        y = ts_batch[..., 1:2]\n",
        "        x_dot = ts_batch[..., 2:3]\n",
        "        y_dot = ts_batch[..., 3:]\n",
        "        val = x / y\n",
        "        theta = np.arctan2(x, y)\n",
        "        theta_dot_outer = 1 / (1 + val**2)\n",
        "        theta_dot_inner = (x_dot * y - y_dot * x) / y**2\n",
        "        return onp.concatenate([theta, theta_dot_outer * theta_dot_inner], axis=-1)\n",
        "\n",
        "\n",
        "# taken from https://github.com/ALRhub/rkn_share/ and modified\n",
        "def generate_pendulums(file_path, task, \n",
        "                       num_train_trials=200, num_test_trials=100,\n",
        "                       impute_rate=0.5, seq_len=100, file_tag=\"\"):\n",
        "    \n",
        "    if task == 'interpolation':\n",
        "        pend_params = Pendulum.pendulum_default_params()\n",
        "        pend_params[Pendulum.FRICTION_KEY] = 0.1\n",
        "        n = seq_len\n",
        "\n",
        "        pendulum = Pendulum(24, observation_mode=Pendulum.OBSERVATION_MODE_LINE,\n",
        "                            transition_noise_std=0.1, observation_noise_std=1e-5,\n",
        "                            seed=42, pendulum_params=pend_params)\n",
        "        rng = pendulum.random\n",
        "\n",
        "        train_obs, _, _, _, train_ts = pendulum.sample_data_set(\n",
        "            num_train_trials, n, full_targets=False)\n",
        "        train_obs = onp.expand_dims(train_obs, -1)\n",
        "        train_targets = train_obs.copy()\n",
        "        train_obs_valid = rng.rand(\n",
        "            train_obs.shape[0], train_obs.shape[1], 1) > impute_rate\n",
        "        train_obs_valid[:, :5] = True\n",
        "        train_obs[onp.logical_not(onp.squeeze(train_obs_valid))] = 0\n",
        "\n",
        "        test_obs, _, _, _, test_ts = pendulum.sample_data_set(\n",
        "            num_test_trials, n, full_targets=False)\n",
        "        test_obs = onp.expand_dims(test_obs, -1)\n",
        "        test_targets = test_obs.copy()\n",
        "        test_obs_valid = rng.rand(\n",
        "            test_obs.shape[0], test_obs.shape[1], 1) > impute_rate\n",
        "        test_obs_valid[:, :5] = True\n",
        "        test_obs[onp.logical_not(onp.squeeze(test_obs_valid))] = 0\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            os.makedirs(file_path)\n",
        "        onp.savez_compressed(os.path.join(file_path, f\"pend_interpolation_ir{impute_rate}\"),\n",
        "                            train_obs=train_obs, train_targets=train_targets, train_obs_valid=train_obs_valid, train_ts=train_ts,\n",
        "                            test_obs=test_obs, test_targets=test_targets, test_obs_valid=test_obs_valid, test_ts=test_ts)\n",
        "    \n",
        "    elif task == 'regression':\n",
        "        pend_params = Pendulum.pendulum_default_params()\n",
        "        pend_params[Pendulum.FRICTION_KEY] = 0.1\n",
        "        pend_params[Pendulum.DT_KEY] = 0.01\n",
        "        n = seq_len\n",
        "\n",
        "        pendulum = Pendulum(24, observation_mode=Pendulum.OBSERVATION_MODE_LINE,\n",
        "                            transition_noise_std=0.1, observation_noise_std=1e-5,\n",
        "                            seed=42, pendulum_params=pend_params)\n",
        "\n",
        "        train_obs, train_targets, _, _, train_ts = pendulum.sample_data_set(\n",
        "            num_train_trials, n, full_targets=False)\n",
        "        # train_obs, _ = pendulum.add_observation_noise(train_obs, first_n_clean=5, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75,\n",
        "        #                                               t_uu=1.0)\n",
        "        train_obs = onp.expand_dims(train_obs, -1)\n",
        "\n",
        "        test_obs, test_targets, _, _, test_ts = pendulum.sample_data_set(\n",
        "            num_test_trials, n, full_targets=False)\n",
        "        # test_obs, _ = pendulum.add_observation_noise(test_obs, first_n_clean=5, r=0.2, t_ll=0.0, t_lu=0.25, t_ul=0.75,\n",
        "        #                                              t_uu=1.0)\n",
        "        test_obs = onp.expand_dims(test_obs, -1)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            os.makedirs(file_path)\n",
        "        onp.savez_compressed(os.path.join(file_path, f\"pend_regression\"+file_tag+\".npz\"),\n",
        "                            train_obs=train_obs, train_targets=train_targets, train_ts=train_ts,\n",
        "                            test_obs=test_obs, test_targets=test_targets, test_ts=test_ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Roo84N4NN9-"
      },
      "source": [
        "The full dataset is (2000, 100, 24, 24, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "id": "RaD7Wl1f9mWx"
      },
      "outputs": [],
      "source": [
        "# @title Create the pendulum dataset (uncomment this block!)\n",
        "# Takes about 2 minutes\n",
        "# generate_pendulums(\"pendulum\", \"regression\", seq_len=200)\n",
        "# generate_pendulums(\"pendulum\", \"regression\", \n",
        "#                    num_train_trials=0, num_test_trials=100, \n",
        "#                    seq_len=400, file_tag=\"_longer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "cellView": "form",
        "id": "5Op7ol3UKP0g"
      },
      "outputs": [],
      "source": [
        "# @title Load the pendulum dataset\n",
        "def load_pendulum(run_params, log=False):    \n",
        "    d = run_params[\"dataset_params\"]\n",
        "    train_trials = d[\"train_trials\"]\n",
        "    val_trials = d[\"val_trials\"]\n",
        "    noise_scale = d[\"emission_cov\"] ** 0.5\n",
        "    key_train, key_val, key_pred = jr.split(d[\"seed\"], 3)\n",
        "\n",
        "    data = np.load(\"pendulum/pend_regression.npz\")\n",
        "\n",
        "    def _process_data(data, key):\n",
        "        processed = data[:, ::2] / 255.0\n",
        "        processed += jr.normal(key=key, shape=processed.shape) * noise_scale\n",
        "        return np.clip(processed, 0, 1)\n",
        "\n",
        "    # Take subset, subsample every 2 frames, normalize to [0, 1]\n",
        "    train_data = _process_data(data[\"train_obs\"][:train_trials], key_train)\n",
        "    # val_data = _process_data(data[\"test_obs\"][:val_trials], key_val)\n",
        "    val_data = _process_data(\n",
        "        np.load(\"pendulum/pend_regression_longer.npz\")[\"test_obs\"][:val_trials], key_pred)\n",
        "\n",
        "    print(\"Full dataset:\", data[\"train_obs\"].shape)\n",
        "    print(\"Subset:\", train_data.shape)\n",
        "    return {\n",
        "        \"train_data\": train_data,\n",
        "        \"val_data\": val_data,\n",
        "        # \"val_data_pred\": val_data_pred\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU2AZWhAPjux"
      },
      "source": [
        "# Run experiments!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "_MograqHxqqs"
      },
      "outputs": [],
      "source": [
        "if (\"data_dict\" not in globals()): data_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "id": "DXDxYDcDrCBG"
      },
      "outputs": [],
      "source": [
        "# @title Define network architecture parameters\n",
        "\n",
        "linear_recnet_architecture = {\n",
        "    \"diagonal_covariance\": False,\n",
        "    \"input_rank\": 1,\n",
        "    \"head_mean_params\": { \"features\": [] },\n",
        "    \"head_var_params\": { \"features\": [] },\n",
        "    \"eps\": 1e-3,\n",
        "    \"cov_init\": 2,\n",
        "}\n",
        "\n",
        "BiRNN_recnet_architecture = {\n",
        "    \"input_rank\": 1,\n",
        "    \"input_type\": \"MLP\",\n",
        "    \"input_params\":{ \"features\": [20,] },\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"head_dyn_params\": { \"features\": [20,] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "planet_posterior_architecture = {\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "linear_decnet_architecture = {\n",
        "    \"diagonal_covariance\": True,\n",
        "    \"input_rank\": 1,\n",
        "    \"head_mean_params\": { \"features\": [] },\n",
        "    \"head_var_params\": { \"features\": [] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "CNN_layers = [\n",
        "            {\"features\": 32, \"kernel_size\": (3, 3), \"strides\": (1, 1) },\n",
        "            {\"features\": 64, \"kernel_size\": (3, 3), \"strides\": (2, 2) },\n",
        "            {\"features\": 32, \"kernel_size\": (3, 3), \"strides\": (2, 2) }\n",
        "]\n",
        "\n",
        "CNN_recnet_architecture = {\n",
        "    \"input_rank\": 3,\n",
        "    \"trunk_type\": \"CNN\",\n",
        "    \"trunk_params\": {\n",
        "        \"layer_params\": CNN_layers\n",
        "    },\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "CNN_BiRNN_recnet_architecture = {\n",
        "    \"input_rank\": 3,\n",
        "    \"input_type\": \"CNN\",\n",
        "    \"input_params\":{\n",
        "        \"layer_params\": CNN_layers\n",
        "    },\n",
        "    \"head_mean_params\": { \"features\": [20, 20] },\n",
        "    \"head_var_params\": { \"features\": [20, 20] },\n",
        "    \"head_dyn_params\": { \"features\": [20,] },\n",
        "    \"eps\": 1e-4,\n",
        "    \"cov_init\": 1,\n",
        "}\n",
        "\n",
        "DCNN_decnet_architecture = {\n",
        "    \"input_shape\": (6, 6, 32),\n",
        "    \"layer_params\": [\n",
        "        { \"features\": 64, \"kernel_size\": (3, 3), \"strides\": (2, 2) },\n",
        "        { \"features\": 32, \"kernel_size\": (3, 3), \"strides\": (2, 2) },\n",
        "        { \"features\": 2, \"kernel_size\": (3, 3) }\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "id": "92Lh9HVoqqZO"
      },
      "outputs": [],
      "source": [
        "# @title Run parameter expanders\n",
        "def get_lr(params, max_iters):\n",
        "    base_lr = params[\"base_lr\"]\n",
        "    prior_base_lr = params[\"prior_base_lr\"]\n",
        "    lr = base_lr\n",
        "    prior_lr = prior_base_lr\n",
        "    pprint(params)\n",
        "    if params[\"lr_decay\"]:\n",
        "        print(\"Using learning rate decay!\")\n",
        "        lr = opt.exponential_decay(init_value=base_lr, \n",
        "                                     transition_steps=max_iters,\n",
        "                                     decay_rate=0.99, \n",
        "                                   transition_begin=.8*max_iters, staircase=False)\n",
        "        # This is kind of a different scheme but whatever...\n",
        "        if params[\"prior_lr_warmup\"]:\n",
        "            prior_lr = opt.cosine_onecycle_schedule(max_iters, prior_base_lr, 0.5)\n",
        "    else:\n",
        "        lr = base_lr\n",
        "        if params[\"prior_lr_warmup\"]: \n",
        "            prior_lr = opt.linear_schedule(0, prior_base_lr, .2 * max_iters, 0)\n",
        "    return lr, prior_lr\n",
        "\n",
        "def expand_lds_parameters(params):\n",
        "    num_timesteps = params.get(\"num_timesteps\") or 200\n",
        "    train_trials = { \"small\": 10, \"medium\": 100, \"large\": 1000 }\n",
        "    batch_sizes = {\"small\": 10, \"medium\": 10, \"large\": 20 }\n",
        "    emission_noises = { \"small\": 10., \"medium\": 1., \"large\": .1 }\n",
        "    dynamics_noises = { \"small\": 0.01, \"medium\": .1, \"large\": .1 }\n",
        "    max_iters = 8000\n",
        "\n",
        "    # Modify all the architectures according to the parameters given\n",
        "    D, H, N = params[\"latent_dims\"], params[\"rnn_dims\"], params[\"emission_dims\"]\n",
        "    inf_params = {}\n",
        "    if (params[\"inference_method\"] == \"svae\"):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(linear_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "    elif (params[\"inference_method\"] == \"dkf\"):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianBiRNN\"\n",
        "        architecture = deepcopy(BiRNN_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "        architecture[\"rnn_dim\"] = H\n",
        "    elif (params[\"inference_method\"] == \"cdkf\"):\n",
        "        inf_params[\"recnet_class\"] = \"ConditionalGaussianBiRNN\"\n",
        "        architecture = deepcopy(BiRNN_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "        architecture[\"rnn_dim\"] = H\n",
        "    elif (params[\"inference_method\"] == \"planet\"):\n",
        "        # Here we're considering the filtering setting as default\n",
        "        # Most likely we're not even going to use this so it should be fine\n",
        "        # If we want smoothing then we can probably just replace these with the\n",
        "        # BiRNN version\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(linear_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = H\n",
        "        post_arch = deepcopy(planet_posterior_architecture)\n",
        "        post_arch[\"input_dim\"] = H\n",
        "        post_arch[\"rnn_dim\"] = H\n",
        "        post_arch[\"output_dim\"] = D\n",
        "        inf_params[\"posterior_architecture\"] = post_arch\n",
        "        inf_params[\"sample_kl\"] = True # PlaNet doesn't have built-in suff-stats\n",
        "    else:\n",
        "        print(\"Inference method not found: \" + params[\"inference_method\"])\n",
        "        assert(False)\n",
        "    decnet_architecture = deepcopy(linear_decnet_architecture)\n",
        "    decnet_architecture[\"output_dim\"] = N\n",
        "    inf_params[\"decnet_class\"] = \"GaussianEmission\"\n",
        "    inf_params[\"decnet_architecture\"] = decnet_architecture\n",
        "    inf_params[\"recnet_architecture\"] = architecture\n",
        "\n",
        "    lr, prior_lr = get_lr(params, max_iters)\n",
        "\n",
        "    extended_params = {\n",
        "        \"project_name\": \"SVAE-LDS-Final\",\n",
        "        \"log_to_wandb\": True,\n",
        "        \"dataset\": \"lds\",\n",
        "        \"dataset_params\": {\n",
        "            \"seed\": key_0,\n",
        "            \"num_trials\": train_trials[params[\"dataset_size\"]],\n",
        "            \"num_timesteps\": num_timesteps,\n",
        "            \"emission_cov\": emission_noises[params[\"snr\"]],\n",
        "            \"dynamics_cov\": dynamics_noises[params[\"snr\"]],\n",
        "            \"latent_dims\": D,\n",
        "            \"emission_dims\": N,\n",
        "        },\n",
        "        # Implementation choice\n",
        "        \"use_parallel_kf\": False,\n",
        "        # Training specifics\n",
        "        \"max_iters\": max_iters,\n",
        "        \"elbo_samples\": 1,\n",
        "        \"sample_kl\": False,\n",
        "        \"batch_size\": batch_sizes[params[\"dataset_size\"]],\n",
        "        \"record_params\": lambda i: i % 100 == 0,\n",
        "        \"plot_interval\": 100,\n",
        "        \"learning_rate\": lr, \n",
        "        \"prior_learning_rate\": prior_lr\n",
        "    }\n",
        "    extended_params.update(inf_params)\n",
        "    # This allows us to override ANY of the above...!\n",
        "    extended_params.update(params)\n",
        "    return extended_params\n",
        "\n",
        "def expand_pendulum_parameters(params):\n",
        "    train_trials = { \"small\": 20, \"medium\": 100, \"large\": 2000 }\n",
        "    batch_sizes = {\"small\": 10, \"medium\": 10, \"large\": 40 }\n",
        "    # Not a very good validation split (mostly because we're doing one full batch for val)\n",
        "    val_trials = { \"small\": 4, \"medium\": 20, \"large\": 200 }\n",
        "    noise_scales = { \"small\": 1., \"medium\": .1, \"large\": .01 }\n",
        "    max_iters = 20000\n",
        "\n",
        "    # Modify all the architectures according to the parameters given\n",
        "    D, H = params[\"latent_dims\"], params[\"rnn_dims\"]\n",
        "    inf_params = {}\n",
        "    if (params[\"inference_method\"] == \"svae\"):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(CNN_recnet_architecture)\n",
        "        architecture[\"trunk_params\"][\"output_dim\"] = D\n",
        "        architecture[\"output_dim\"] = D\n",
        "    elif (params[\"inference_method\"] == \"dkf\"):\n",
        "        inf_params[\"recnet_class\"] = \"GaussianBiRNN\"\n",
        "        architecture = deepcopy(CNN_BiRNN_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "        architecture[\"rnn_dim\"] = H\n",
        "        architecture[\"input_params\"][\"output_dim\"] = H\n",
        "    elif (params[\"inference_method\"] == \"cdkf\"):\n",
        "        inf_params[\"recnet_class\"] = \"ConditionalGaussianBiRNN\"\n",
        "        architecture = deepcopy(CNN_BiRNN_recnet_architecture)\n",
        "        architecture[\"output_dim\"] = D\n",
        "        architecture[\"rnn_dim\"] = H\n",
        "        architecture[\"input_params\"][\"output_dim\"] = H\n",
        "    elif (params[\"inference_method\"] == \"planet\"):\n",
        "        # Here we're considering the filtering setting as default\n",
        "        # Most likely we're not even going to use this so it should be fine\n",
        "        # If we want smoothing then we can probably just replace these with the\n",
        "        # BiRNN version\n",
        "        inf_params[\"recnet_class\"] = \"GaussianRecognition\"\n",
        "        architecture = deepcopy(CNN_recnet_architecture)\n",
        "        architecture[\"trunk_params\"][\"output_dim\"] = H\n",
        "        architecture[\"output_dim\"] = H\n",
        "        post_arch = deepcopy(planet_posterior_architecture)\n",
        "        post_arch[\"input_dim\"] = H\n",
        "        post_arch[\"rnn_dim\"] = H\n",
        "        post_arch[\"output_dim\"] = D\n",
        "        inf_params[\"posterior_architecture\"] = post_arch\n",
        "        inf_params[\"sample_kl\"] = True # PlaNet doesn't have built-in suff-stats\n",
        "    else:\n",
        "        print(\"Inference method not found: \" + params[\"inference_method\"])\n",
        "        assert(False)\n",
        "    inf_params[\"recnet_architecture\"] = architecture\n",
        "    \n",
        "    lr, prior_lr = get_lr(params, max_iters)\n",
        "\n",
        "    extended_params = {\n",
        "        \"project_name\": \"SVAE-Pendulum-Final\",\n",
        "        \"log_to_wandb\": True,\n",
        "        \"dataset\": \"pendulum\",\n",
        "        # Must be model learning\n",
        "        \"run_type\": \"model_learning\",\n",
        "        \"decnet_class\": \"GaussianDCNNEmission\",\n",
        "        \"decnet_architecture\": DCNN_decnet_architecture,\n",
        "        \"dataset_params\": {\n",
        "            \"seed\": key_0,\n",
        "            \"train_trials\": train_trials[params[\"dataset_size\"]],\n",
        "            \"val_trials\": val_trials[params[\"dataset_size\"]],\n",
        "            \"emission_cov\": noise_scales[params[\"snr\"]]\n",
        "        },\n",
        "        # Implementation choice\n",
        "        \"use_parallel_kf\": False,\n",
        "        # Training specifics\n",
        "        \"max_iters\": max_iters,\n",
        "        \"elbo_samples\": 1,\n",
        "        \"sample_kl\": False,\n",
        "        \"batch_size\": batch_sizes[params[\"dataset_size\"]],\n",
        "        \"record_params\": lambda i: i % 100 == 0,\n",
        "        \"plot_interval\": 200,\n",
        "        \"mask_type\": \"potential\" if params[\"inference_method\"] == \"svae\" else \"data\",\n",
        "        \"learning_rate\": lr, \n",
        "        \"prior_learning_rate\": prior_lr,\n",
        "        \"use_validation\": True,\n",
        "        \"constrain_dynamics\": True,\n",
        "        \"prediction_horizon\": 5,\n",
        "    }\n",
        "    extended_params.update(inf_params)\n",
        "    # This allows us to override ANY of the above...!\n",
        "    extended_params.update(params)\n",
        "    return extended_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "id": "Q5saocWweLzt"
      },
      "outputs": [],
      "source": [
        "# @title LDS run parameters\n",
        "run_params = {\n",
        "    # Most important: inference method\n",
        "    \"inference_method\": \"svae\",\n",
        "    \"use_parallel_kf\": True,\n",
        "    # Relevant dimensions\n",
        "    \"latent_dims\": 3,\n",
        "    \"rnn_dims\": 10,\n",
        "    \"seed\": jr.PRNGKey(0),\n",
        "    \"dataset_size\": \"small\", # \"small\", \"medium\", \"large\"\n",
        "    \"snr\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "    \"use_natural_grad\": False,\n",
        "    \"constrain_prior\": True,\n",
        "    \"constrain_dynamics\": True, # Truncates/scales the singular values of A\n",
        "    # We set it to true for since it's extra work to compute the sufficient stats \n",
        "    # from smoothed potentials in the current parallel KF\n",
        "    \"sample_kl\": True,\n",
        "    \"base_lr\": 1e-3,\n",
        "    \"prior_base_lr\": 1e-3,\n",
        "    \"prior_lr_warmup\": True,\n",
        "    \"lr_decay\": False,\n",
        "    \"group_tag\": \"\",\n",
        "    # The only LDS-specific entries\n",
        "    \"emission_dims\": 5,\n",
        "    \"num_timesteps\": 1000,\n",
        "    \"run_type\": \"model_learning\", # \"inference_only\"\n",
        "    \"log_to_wandb\": True,\n",
        "    \"visualize_training\": False\n",
        "}\n",
        "\n",
        "run_variations = {\n",
        "    \"max_iters\": [10, 10, 10],\n",
        "    \"snr\": [\"small\", \"medium\", \"large\"],\n",
        "    \"inference_method\": [\"planet\", \"dkf\", \"planet\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cf360b64524a4e77a5d91a2eb1ee5141",
            "8fda8ee2014e4722b130cbd2425ecf93",
            "121ba4af374245e8a3fc2c8e453e28e3",
            "fea301e7bfae442f8f32898af5761ab4",
            "719d9e9ce429414ba58ff81e5bc41079",
            "eef26dac7df34694ad278fa583fcc6f7",
            "2b4d2cc7b16a4bbba1a331fbc4a6cd5a",
            "dafb4f8933004052b74cba5e238c9480",
            "e1e2620eebdb4a53a621b703b8089cc5",
            "15e4d13f8aba48c3afa1c7ceceadbb04",
            "832496646ce0471991a3478a3f004489",
            "c4c4829fb582458a9ac5a430fd8b082a",
            "3311502ea4694629840c722cfb57edce",
            "f6d8ef3d8af74f6d94c3fc38197f598b",
            "1c6b6ab8d2094c8d826ded69da0dfbe4",
            "b4a4c8d429034737b6673e23ceababd6"
          ]
        },
        "id": "cqhimlvNt39G",
        "outputId": "29b666b1-9cba-43e9-aa8e-0af83c4fea69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of runs: 1\n",
            "Base paramerters:\n",
            "{'base_lr': 0.001,\n",
            " 'constrain_dynamics': True,\n",
            " 'constrain_prior': True,\n",
            " 'dataset_size': 'small',\n",
            " 'emission_dims': 5,\n",
            " 'group_tag': '',\n",
            " 'inference_method': 'svae',\n",
            " 'latent_dims': 3,\n",
            " 'log_to_wandb': True,\n",
            " 'lr_decay': False,\n",
            " 'num_timesteps': 1000,\n",
            " 'prior_base_lr': 0.001,\n",
            " 'prior_lr_warmup': True,\n",
            " 'rnn_dims': 10,\n",
            " 'run_type': 'model_learning',\n",
            " 'sample_kl': True,\n",
            " 'seed': DeviceArray([0, 0], dtype=uint32),\n",
            " 'snr': 'medium',\n",
            " 'use_natural_grad': False,\n",
            " 'use_parallel_kf': True,\n",
            " 'visualize_training': False}\n",
            "##########################################\n",
            "Starting run #0\n",
            "##########################################\n",
            "{'base_lr': 0.001,\n",
            " 'constrain_dynamics': True,\n",
            " 'constrain_prior': True,\n",
            " 'dataset_size': 'small',\n",
            " 'emission_dims': 5,\n",
            " 'group_tag': '',\n",
            " 'inference_method': 'svae',\n",
            " 'latent_dims': 3,\n",
            " 'log_to_wandb': True,\n",
            " 'lr_decay': False,\n",
            " 'num_timesteps': 1000,\n",
            " 'prior_base_lr': 0.001,\n",
            " 'prior_lr_warmup': True,\n",
            " 'rnn_dims': 10,\n",
            " 'run_type': 'model_learning',\n",
            " 'sample_kl': True,\n",
            " 'seed': DeviceArray([0, 0], dtype=uint32),\n",
            " 'snr': 'medium',\n",
            " 'use_natural_grad': False,\n",
            " 'use_parallel_kf': True,\n",
            " 'visualize_training': False}\n",
            "Loading dataset!\n",
            "Data MLL:  -7978.6846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LP: 366835.438:   0%|          | 0/8000 [01:00<?, ?it/s]   "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668628150000586, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf360b64524a4e77a5d91a2eb1ee5141"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230110_020914-c94xuuud</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/matthew9671/SVAE-LDS-Final/runs/c94xuuud\" target=\"_blank\">robust-bird-77</a></strong> to <a href=\"https://wandb.ai/matthew9671/SVAE-LDS-Final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LP: 363898.344:   0%|          | 2/8000 [01:02<57:26:12, 25.85s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'base_lr': 0.001,\n",
            " 'batch_size': 10,\n",
            " 'constrain_dynamics': True,\n",
            " 'constrain_prior': True,\n",
            " 'dataset': 'lds',\n",
            " 'dataset_params': {'dynamics_cov': 0.1,\n",
            "                    'emission_cov': 1.0,\n",
            "                    'emission_dims': 5,\n",
            "                    'latent_dims': 3,\n",
            "                    'num_timesteps': 1000,\n",
            "                    'num_trials': 10,\n",
            "                    'seed': DeviceArray([0, 0], dtype=uint32)},\n",
            " 'dataset_size': 'small',\n",
            " 'decnet_architecture': {'cov_init': 1,\n",
            "                         'diagonal_covariance': True,\n",
            "                         'eps': 0.0001,\n",
            "                         'head_mean_params': {'features': []},\n",
            "                         'head_var_params': {'features': []},\n",
            "                         'input_rank': 1,\n",
            "                         'output_dim': 5},\n",
            " 'decnet_class': 'GaussianEmission',\n",
            " 'elbo_samples': 1,\n",
            " 'emission_dims': 5,\n",
            " 'group_tag': '',\n",
            " 'inference_method': 'svae',\n",
            " 'latent_dims': 3,\n",
            " 'learning_rate': 0.001,\n",
            " 'log_to_wandb': True,\n",
            " 'lr_decay': False,\n",
            " 'max_iters': 8000,\n",
            " 'num_timesteps': 1000,\n",
            " 'plot_interval': 100,\n",
            " 'prior_base_lr': 0.001,\n",
            " 'prior_learning_rate': <function polynomial_schedule.<locals>.schedule at 0x7ef256b45310>,\n",
            " 'prior_lr_warmup': True,\n",
            " 'project_name': 'SVAE-LDS-Final',\n",
            " 'recnet_architecture': {'cov_init': 2,\n",
            "                         'diagonal_covariance': False,\n",
            "                         'eps': 0.001,\n",
            "                         'head_mean_params': {'features': []},\n",
            "                         'head_var_params': {'features': []},\n",
            "                         'input_rank': 1,\n",
            "                         'output_dim': 3},\n",
            " 'recnet_class': 'GaussianRecognition',\n",
            " 'record_params': <function expand_lds_parameters.<locals>.<lambda> at 0x7ef256b45820>,\n",
            " 'rnn_dims': 10,\n",
            " 'run_type': 'model_learning',\n",
            " 'sample_kl': True,\n",
            " 'seed': DeviceArray([0, 0], dtype=uint32),\n",
            " 'snr': 'medium',\n",
            " 'use_natural_grad': False,\n",
            " 'use_parallel_kf': True,\n",
            " 'visualize_training': False}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LP: 8359.796: 100%|██████████| 8000/8000 [13:20<00:00,  9.99it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='119.275 MB of 119.275 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0,…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1e2620eebdb4a53a621b703b8089cc5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Condition number of A</td><td>▁▂▅▇█▆▃▂▂▃▄▄▅▅▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂</td></tr><tr><td>Condition number of Q</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ELBO</td><td>▁▅▆▇▇▇██████████████████████████████████</td></tr><tr><td>KL</td><td>▃▆██▇▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Learning rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Likelihood</td><td>▁▅▇▇▇███████████████████████████████████</td></tr><tr><td>Max singular value of A</td><td>▃▇▅▅▄▅▁▂▃▅▄▃▅▃█▇▅▇▆▆▆▇▆▅█▅▆▆▅▆▆▂▄▁▅▇▂▃▆█</td></tr><tr><td>Min singular value of A</td><td>█▆▄▂▁▃▆▇▆▅▅▄▄▄▄▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇</td></tr><tr><td>Prior learning rate</td><td>▁▂▃▄▄▅▇▇████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Condition number of A</td><td>1.0207</td></tr><tr><td>Condition number of Q</td><td>1.0</td></tr><tr><td>ELBO</td><td>-8359.7959</td></tr><tr><td>KL</td><td>1223.02576</td></tr><tr><td>Learning rate</td><td>0.001</td></tr><tr><td>Likelihood</td><td>-7136.77051</td></tr><tr><td>Max singular value of A</td><td>0.99986</td></tr><tr><td>Min singular value of A</td><td>0.97958</td></tr><tr><td>Prior learning rate</td><td>0.001</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">robust-bird-77</strong>: <a href=\"https://wandb.ai/matthew9671/SVAE-LDS-Final/runs/c94xuuud\" target=\"_blank\">https://wandb.ai/matthew9671/SVAE-LDS-Final/runs/c94xuuud</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230110_020914-c94xuuud/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "jax.config.update(\"jax_enable_x64\", False)\n",
        "\n",
        "results = experiment_scheduler(run_params, \n",
        "                    #  run_variations=run_variations,\n",
        "                     dataset_getter=sample_lds_dataset, \n",
        "                     model_getter=init_model, \n",
        "                     train_func=start_trainer,\n",
        "                     params_expander=expand_lds_parameters,\n",
        "                     on_error=on_error,\n",
        "                     continue_on_error=True\n",
        "                     )\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "JjtwI49pGk7U"
      },
      "outputs": [],
      "source": [
        "# @title Pendulum run params\n",
        "run_params = {\n",
        "    # Most important: inference method\n",
        "    \"inference_method\": \"svae\",#\"svae\",\n",
        "    \"use_parallel_kf\": True,\n",
        "    # Relevant dimensions\n",
        "    \"latent_dims\": 5,\n",
        "    \"rnn_dims\": 10,\n",
        "    \"seed\": jr.PRNGKey(0),\n",
        "    \"dataset_size\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "    \"snr\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "    \"use_natural_grad\": False,\n",
        "    \"constrain_prior\": True,\n",
        "    \"base_lr\": 1e-2,\n",
        "    \"prior_base_lr\": 1e-2,\n",
        "    \"prior_lr_warmup\": True,\n",
        "    \"lr_decay\": False,\n",
        "    \"group_tag\": \"test_parallel_scan\",\n",
        "    # The only pendulum-specific entry, will be overridden by params expander\n",
        "    \"mask_size\": 40,\n",
        "    # \"plot_interval\": 1,\n",
        "    \"mask_start\": 0,#1000,\n",
        "    \"sample_kl\": True,\n",
        "    \"log_to_wandb\": True\n",
        "}\n",
        "\n",
        "# methods = {\n",
        "#     # \"inference_method\": [\"svae\", \"cdkf\", \"planet\", \"dkf\"],\n",
        "#     # \"mask_start\": [0, 2000, 2000, 2000]\n",
        "#     \"inference_method\": [\"planet\", \"svae\"],\n",
        "#     \"mask_start\": [2000, 0],\n",
        "#     \"use_natural_grad\": [False, False],\n",
        "#     \"constrain_prior\": [True, True]\n",
        "# }\n",
        "\n",
        "seeds = {\n",
        "    \"seed\": [jr.PRNGKey(i) for i in range(3)]\n",
        "}\n",
        "\n",
        "run_variations = seeds#dict_product(seeds, methods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "7cd8Vsd5q9Dl"
      },
      "outputs": [],
      "source": [
        "# @title Run the pendulum experiments\n",
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "jax.config.update(\"jax_enable_x64\", False)\n",
        "\n",
        "results = experiment_scheduler(run_params, \n",
        "                    #  run_variations=run_variations,\n",
        "                     dataset_getter=load_pendulum, \n",
        "                     model_getter=init_model, \n",
        "                     train_func=start_trainer,\n",
        "                     params_expander=expand_pendulum_parameters,\n",
        "                     on_error=on_error,\n",
        "                     continue_on_error=True)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZwKD63PsK9so"
      },
      "outputs": [],
      "source": [
        "# Params used previously\n",
        "run_params = {\n",
        "    # Most important: inference method\n",
        "    \"inference_method\": \"svae\",\n",
        "    \"use_parallel_kf\": True,\n",
        "    # Relevant dimensions\n",
        "    \"latent_dims\": 3,\n",
        "    \"rnn_dims\": 10,\n",
        "    \"seed\": jr.PRNGKey(0),\n",
        "    \"dataset_size\": \"small\", # \"small\", \"medium\", \"large\"\n",
        "    \"snr\": \"medium\", # \"small\", \"medium\", \"large\"\n",
        "    \"use_natural_grad\": False,\n",
        "    \"constrain_prior\": True,\n",
        "    # We set it to true for since it's extra work to compute the sufficient stats \n",
        "    # from smoothed potentials in the current parallel KF\n",
        "    \"sample_kl\": True,\n",
        "    \"base_lr\": 1e-2,\n",
        "    \"prior_base_lr\": 0, #1e-3, # Debugging: set prior lr to 0\n",
        "    \"prior_lr_warmup\": True,\n",
        "    \"lr_decay\": False,\n",
        "    \"group_tag\": \"\",\n",
        "    # The only LDS-specific entries\n",
        "    \"emission_dims\": 5,\n",
        "    \"run_type\": \"model_learning\", # \"inference_only\"\n",
        "    \"log_to_wandb\": True,\n",
        "    # \"max_iters\": 200,\n",
        "}\n",
        "\n",
        "results = experiment_scheduler(run_params, \n",
        "                    #  run_variations=run_variations,\n",
        "                     dataset_getter=sample_lds_dataset, \n",
        "                     model_getter=init_model, \n",
        "                     train_func=start_trainer,\n",
        "                     params_expander=expand_lds_parameters,\n",
        "                     on_error=on_error,\n",
        "                     continue_on_error=True\n",
        "                     )\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xXi0h-N5qMmv"
      },
      "outputs": [],
      "source": [
        "prior_params = copy.deepcopy(results[1][0][\"trainer\"].params[\"prior_params\"])\n",
        "# post_params = copy.deepcopy(results[1][0][\"trainer\"].params[\"post_params\"])\n",
        "rec_params = copy.deepcopy(results[1][0][\"trainer\"].params[\"rec_params\"])\n",
        "model = results[1][0][\"model\"]\n",
        "posterior = model.posterior\n",
        "alt_posterior = LDSSVAEPosterior(3, seq_len=200)\n",
        "prior = ParallelLieParameterizedLinearGaussianChain(3, 200)# model.prior\n",
        "alt_prior = LieParameterizedLinearGaussianChain(3, 200)\n",
        "constrained_params = model.prior.get_constrained_params(prior_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q6U_up9AvC58"
      },
      "outputs": [],
      "source": [
        "# m1 = samples[0][0]\n",
        "# prior_params[\"Q\"] = np.array([-10, -10, -10, 0, 0, 0])\n",
        "# prior_params[\"m1\"] = m1\n",
        "# prior_params[\"Q1\"] = prior_params[\"Q\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r4of2dkBrQWr"
      },
      "outputs": [],
      "source": [
        "samples = prior.sample(prior_params, (1,), key_0)\n",
        "plt.plot(samples[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rzrObIG3rgyf"
      },
      "outputs": [],
      "source": [
        "plt.plot(alt_prior.sample(prior_params, (1,), key_0)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y9EyjEmbo2yt"
      },
      "outputs": [],
      "source": [
        "print(prior.distribution(prior.get_constrained_params(prior_params)).log_prob(samples[0]))\n",
        "print(alt_prior.distribution(alt_prior.get_constrained_params(prior_params)).log_prob(samples[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AjgZ8TveqctX"
      },
      "outputs": [],
      "source": [
        "data = data_dict[\"train_data\"][0]\n",
        "potential = vmap(model.recognition.apply, in_axes=(None, 0))(rec_params, data)\n",
        "post_params = posterior.infer(constrained_params, potential)\n",
        "# alt_post_params = alt_posterior.infer(alt_prior.get_constrained_params(prior_params), potential)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LVXofatGsHlv"
      },
      "outputs": [],
      "source": [
        "post_dist = posterior.distribution(post_params)\n",
        "post_sample = posterior.sample(post_params, shape=(1,), key=key_0)\n",
        "alt_post_dist = alt_posterior.distribution(alt_post_params)\n",
        "alt_post_sample = alt_post_dist.sample(seed=key_0)\n",
        "print(alt_post_dist.log_prob(post_sample))\n",
        "print(post_dist.log_prob(post_sample))\n",
        "print(prior.distribution(prior.get_constrained_params(prior_params)).log_prob(post_sample))\n",
        "print(alt_prior.distribution(alt_prior.get_constrained_params(prior_params)).log_prob(post_sample))\n",
        "plt.plot(alt_post_sample - post_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CvcblvND14Lh"
      },
      "outputs": [],
      "source": [
        "plt.plot(post_sample[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N8dGeHGMyQBE"
      },
      "outputs": [],
      "source": [
        "model.kl_posterior_prior(post_params, constrained_params, samples=post_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "umLIjpMc1oN5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ASs5866Y0gaw"
      },
      "outputs": [],
      "source": [
        "model.elbo(key_0, data, results[1][0][\"trainer\"].params, sample_kl=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzngmpgUw4cb"
      },
      "source": [
        "# Pendulum Diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XbEGFOLU2i4"
      },
      "source": [
        "- [x] Visualize the samples from the predictive posterior\n",
        "- [x] Implement a shorter horizon prediction \n",
        "- [x] See how the svae performs as prediction horizon gets longer\n",
        "- [x] What about the other frameworks? -- best SVAE beats the cDKF...!\n",
        "- [x] Test the linear decoding for the entire dataset\n",
        "- [x] Package the diagnostics code into helper functions\n",
        "- [x] Also include the linear decoding accuracies (MSE) analysis\n",
        "\n",
        "TODOS:\n",
        "- [ ] Investigate why the sliding window prediction is still not giving us what we what\n",
        "- [ ] Pick one best run for all methods and plot the uncertainty estimates learned by them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "5mX32LKg6xXj"
      },
      "outputs": [],
      "source": [
        "# @title Load the run from WandB\n",
        "data_dict = {}\n",
        "\n",
        "def load_run(project_path, run_name):\n",
        "    api = wandb.Api()\n",
        "    \n",
        "    try:\n",
        "        os.remove(\"parameters.pkl\")\n",
        "    except:\n",
        "        pass\n",
        "    with open(wandb.restore(\"parameters.pkl\", project_path+run_name).name, \"rb\") as f:\n",
        "        d = pkl.load(f)\n",
        "    params = d[-1]\n",
        "\n",
        "    # Get the configs for that specific run\n",
        "    run = api.run(project_path+run_name)\n",
        "    run_params = deepcopy(run.config)\n",
        "    # Get the dataset and the model object\n",
        "\n",
        "    # run_params[\"seed\"] = np.array(run_params[\"seed\"], dtype=np.uint32)\n",
        "    # For some unknown reason we're getting an int instead of a PRNGKey object \n",
        "    run_params[\"seed\"] = jr.PRNGKey(run_params[\"seed\"])\n",
        "    # For some old runs the architecture logged is incorrect\n",
        "    # del(run_params[\"recnet_architecture\"][\"head_mean_params\"][\"features\"][-1])\n",
        "    # del(run_params[\"recnet_architecture\"][\"head_var_params\"][\"features\"][-1])\n",
        "    run_params[\"dataset_params\"][\"seed\"] = np.array(\n",
        "        run_params[\"dataset_params\"][\"seed\"], dtype=np.uint32)\n",
        "    \n",
        "    global data_dict\n",
        "\n",
        "    data_dict = load_pendulum(run_params)\n",
        "    model_dict = init_model(run_params, data_dict)\n",
        "    model = model_dict[\"model\"]\n",
        "    return run_params, params, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LA1iJuvv4o39"
      },
      "outputs": [],
      "source": [
        "def predict_multiple(run_params, model_params, model, data, key, num_samples=6):\n",
        "    out = model.elbo(key, data, model_params, **run_params)\n",
        "    post_params = out[\"posterior_params\"]\n",
        "    posterior = model.posterior.distribution(post_params)\n",
        "    T, D = model.prior.seq_len // 2, model.prior.latent_dims\n",
        "    # Get the final mean and covariance\n",
        "    mu, Sigma = posterior.mean()[T-1], posterior.covariance()[T-1]\n",
        "    # Build the posterior object on the future latent states \n",
        "    # (\"the posterior predictive distribution\")\n",
        "    # Convert unconstrained params to constrained dynamics parameters\n",
        "    prior_params_constrained = model.prior.get_dynamics_params(model_params[\"prior_params\"])\n",
        "    dynamics_params = {\n",
        "        \"m1\": mu,\n",
        "        \"Q1\": Sigma,\n",
        "        \"A\": prior_params_constrained[\"A\"],\n",
        "        \"b\": prior_params_constrained[\"b\"],\n",
        "        \"Q\": prior_params_constrained[\"Q\"]\n",
        "    }\n",
        "\n",
        "    tridiag_params = dynamics_to_tridiag(dynamics_params, T+1, D) # Note the +1\n",
        "    J, L, h = tridiag_params[\"J\"], tridiag_params[\"L\"], tridiag_params[\"h\"]\n",
        "    pred_posterior = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "    # Sample from it and evaluate the log likelihood\n",
        "    x_preds = pred_posterior.sample(seed=key, sample_shape=(num_samples,))[:,1:]\n",
        "\n",
        "    def pred_ll(x_pred):\n",
        "        likelihood_dist = model.decoder.apply(model_params[\"dec_params\"], x_pred)\n",
        "        return likelihood_dist.log_prob(data[T:])\n",
        "\n",
        "    pred_lls = vmap(pred_ll)(x_preds)\n",
        "    # This assumes the pendulum dataset\n",
        "    pred_lls = pred_lls.sum(axis=(2, 3, 4))\n",
        "    pred_lls = pred_lls.mean(axis=0)\n",
        "    return posterior.mean(), x_preds, pred_lls\n",
        "\n",
        "def evaluate_run(project_path, run_name, key=None):\n",
        "\n",
        "    key = key_0 if key is None else key\n",
        "\n",
        "    run_params, model_params, model = load_run(project_path, run_name)\n",
        "    run_params = deepcopy(run_params)\n",
        "    run_params[\"mask_size\"] = 0\n",
        "\n",
        "    train_data = data_dict[\"train_data\"][:20,:100]\n",
        "    Ex, x_preds, pred_lls = vmap(predict_multiple, in_axes=(None, None, None, 0, None, None))\\\n",
        "        (run_params, model_params, model, train_data, key, 10)\n",
        "\n",
        "    # Also visualize a sample prediction\n",
        "    # out_dist = model.decoder.apply(model_params[\"dec_params\"], np.concatenate([Ex[0, :50], x_preds[0, 0]]))\n",
        "    # y_decoded = out_dist.mean()\n",
        "    # plt.figure()\n",
        "    # plot_img_grid(y_decoded)\n",
        "\n",
        "    targets = np.load(\"pendulum/pend_regression.npz\")[\"train_targets\"][:20]\n",
        "    states = targets[:,::2]\n",
        "    train_thetas = np.arctan2(states[:,:,0], states[:,:,1])\n",
        "    train_omegas = train_thetas[:,1:]-train_thetas[:,:-1]\n",
        "    thetas = train_thetas.flatten()\n",
        "    omegas = train_omegas.flatten()\n",
        "    D = model.prior.latent_dims\n",
        "    xs_theta = Ex.reshape((-1, D))\n",
        "    xs_omega = Ex[:,1:].reshape((-1, D))\n",
        "    W_theta, _, _, _ = np.linalg.lstsq(xs_theta, thetas)\n",
        "    W_omega, _, _, _ = np.linalg.lstsq(xs_omega, omegas)\n",
        "\n",
        "    test_targets = np.load(\"pendulum/pend_regression_longer.npz\")[\"test_targets\"][:20, ::2][:, :100]\n",
        "    test_data = data_dict[\"val_data\"][:, :100]#np.load(\"pendulum/pend_regression.npz\")[\"test_obs\"][:, ::2]\n",
        "    thetas = np.arctan2(test_targets[:,:,0], test_targets[:,:,1])\n",
        "    omegas = thetas[:,1:]-thetas[:,:-1]\n",
        "\n",
        "    def encode(data):\n",
        "        out = model.elbo(jr.PRNGKey(0), data, model_params, **run_params)\n",
        "        post_params = out[\"posterior_params\"]\n",
        "        post_dist = model.posterior.distribution(post_params)\n",
        "        return post_dist.mean()\n",
        "\n",
        "    Ex_test = vmap(encode)(test_data)\n",
        "    pred_thetas = np.einsum(\"i,...i->...\", W_theta, Ex_test)\n",
        "    theta_mse = np.mean((pred_thetas - thetas) ** 2)\n",
        "    pred_omegas = np.einsum(\"i,...i->...\", W_omega, Ex_test[:,1:])\n",
        "    omega_mse = np.mean((pred_omegas - omegas) ** 2)\n",
        "    # test_id = 0 #if \"test_id\" not in globals() else test_id + 1\n",
        "    # plt.plot(Ex_test[test_id] @ W_theta, label=\"decoded\")\n",
        "    # plt.plot(thetas[test_id], label=\"true\")\n",
        "    # plt.title(\"Linear decoding of pendulum angle (test sequence #{})\".format(test_id))\n",
        "    # plt.legend()\n",
        "    # plt.figure()\n",
        "    # plt.plot(Ex_test[test_id] @ W_omega, label=\"decoded\")\n",
        "    # plt.plot(omegas[test_id], label=\"true\")\n",
        "    # plt.title(\"Linear decoding of pendulum velocity (test sequence #{})\".format(test_id))\n",
        "    # plt.legend()\n",
        "\n",
        "    return {\n",
        "        # \"prior_sample\":,\n",
        "        \"long_horizon_pred_lls\": pred_lls,\n",
        "        \"predictions\": x_preds,\n",
        "        # \"sliding_window_pred_lls\":,\n",
        "        \"w_theta\": W_theta,\n",
        "        \"w_omega\": W_omega,\n",
        "        \"theta_mse\": theta_mse,\n",
        "        \"omega_mse\": omega_mse,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JxCueh0pIQK_"
      },
      "outputs": [],
      "source": [
        "# result = evaluate_run(project_path, \"v6sbb9xh\", key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ns5x_wrtKc1W"
      },
      "outputs": [],
      "source": [
        "# result = evaluate_run(project_path, \"cdt4gir1\", key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8foTbjOSMiq2"
      },
      "outputs": [],
      "source": [
        "# _ = evaluate_run(project_path, \"dgudjrut\", key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rsHe74D5FFVK"
      },
      "outputs": [],
      "source": [
        "# del all_results\n",
        "if (\"all_results\" not in globals()): all_results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_uVctTR3JcBN"
      },
      "outputs": [],
      "source": [
        "api = wandb.Api()\n",
        "latent_dims = 5\n",
        "runs = api.runs(\"matthew9671/SVAE-Pendulum-ICML-3\", filters={\"State\": \"finished\", \n",
        "                                                             \"config.inference_method\": \"svae\",\n",
        "                                                             \"config.latent_dims\": latent_dims})\n",
        "svae_runs = []\n",
        "for run in runs:\n",
        "    svae_runs.append(run.id)\n",
        "runs = api.runs(\"matthew9671/SVAE-Pendulum-ICML-3\", filters={\"State\": \"finished\", \n",
        "                                                             \"config.inference_method\": \"cdkf\",\n",
        "                                                             \"config.latent_dims\": latent_dims})\n",
        "cdkf_runs = []\n",
        "for run in runs:\n",
        "    cdkf_runs.append(run.id)\n",
        "runs = api.runs(\"matthew9671/SVAE-Pendulum-ICML-3\", filters={\"State\": \"finished\", \n",
        "                                                             \"config.inference_method\": \"planet\",\n",
        "                                                             \"config.latent_dims\": latent_dims})\n",
        "planet_runs = []\n",
        "for run in runs:\n",
        "    planet_runs.append(run.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PKPe7m3kD70V"
      },
      "outputs": [],
      "source": [
        "ax = plt.subplot()\n",
        "project_path = \"matthew9671/SVAE-Pendulum-ICML-3/\"\n",
        "# svae_runs = [\"v6sbb9xh\", \"xpf9s9ie\", \"1jxw27wp\", \"yo3fprzr\"]\n",
        "# cdkf_runs = [\"ik6i6igs\"] #\"cdt4gir1\", \"mpc58ktj\", \"dgudjrut\", \"4nctqeby\", \"cc8s8xxs\"]\n",
        "key = key_0\n",
        "for run_name in svae_runs + cdkf_runs:# + planet_runs:\n",
        "    print(\"Loading run \" + run_name)\n",
        "    key = jr.split(key)[1]\n",
        "    if (run_name not in all_results):\n",
        "        all_results[run_name] = evaluate_run(project_path, run_name, key)\n",
        "    result = all_results[run_name]\n",
        "    mean_pred_lls = result[\"long_horizon_pred_lls\"].mean(axis=0)\n",
        "    if (mean_pred_lls.mean() < -100 and run_name in svae_runs):\n",
        "        print(\"Run \" + run_name + \" gives really bad likelihoods!\")\n",
        "    if (run_name in svae_runs):\n",
        "        ax.plot(mean_pred_lls, color=\"blue\", alpha=.3)\n",
        "    elif (run_name in cdkf_runs):\n",
        "        ax.plot(mean_pred_lls, color=\"red\", alpha=.3)\n",
        "    elif (run_name in planet_runs):\n",
        "        # pass\n",
        "        ax.plot(mean_pred_lls, color=\"green\", alpha=.3)\n",
        "ax.plot(0, label=\"svae\", color=\"blue\")\n",
        "ax.plot(0, label=\"cdkf\", color=\"red\")\n",
        "ax.plot(0, label=\"planet\", color=\"green\")\n",
        "ax.set_title(\"Prediction log likelihood vs. horizon\")\n",
        "# ax.set_ylim(-100, 150)\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FjH4oyyKkE-D"
      },
      "outputs": [],
      "source": [
        "svae_thetas = []\n",
        "svae_omegas = []\n",
        "for run_name in svae_runs:\n",
        "    svae_thetas.append(all_results[run_name][\"theta_mse\"])\n",
        "    svae_omegas.append(all_results[run_name][\"omega_mse\"])\n",
        "cdkf_thetas = []\n",
        "cdkf_omegas = []\n",
        "for run_name in cdkf_runs:\n",
        "    cdkf_thetas.append(all_results[run_name][\"theta_mse\"])\n",
        "    cdkf_omegas.append(all_results[run_name][\"omega_mse\"])\n",
        "planet_thetas = []\n",
        "planet_omegas = []\n",
        "for run_name in planet_runs:\n",
        "    planet_thetas.append(all_results[run_name][\"theta_mse\"])\n",
        "    planet_omegas.append(all_results[run_name][\"omega_mse\"])\n",
        "\n",
        "svae_thetas = np.array(svae_thetas)\n",
        "cdkf_thetas = np.array(cdkf_thetas)\n",
        "planet_thetas = np.array(planet_thetas)\n",
        "svae_omegas = np.array(svae_omegas) * 100\n",
        "cdkf_omegas = np.array(cdkf_omegas) * 100\n",
        "planet_omegas = np.array(planet_omegas) * 100\n",
        "bar_width = .2\n",
        "\n",
        "plt.scatter(np.zeros_like(svae_thetas)-bar_width/2, svae_thetas, s=2, color=\"blue\")\n",
        "plt.bar(-bar_width/2, svae_thetas.mean(), width=bar_width, color=\"blue\", alpha=.3, label=\"svae\")\n",
        "plt.scatter(np.zeros_like(cdkf_thetas)+bar_width/2, cdkf_thetas, s=2, color=\"red\")\n",
        "plt.bar(bar_width/2, cdkf_thetas.mean(), width=bar_width, color=\"red\", alpha=.3, label=\"cdkf\")\n",
        "plt.scatter(np.zeros_like(planet_thetas)+bar_width*3/2, planet_thetas, s=2, color=\"green\")\n",
        "plt.bar(bar_width*3/2, planet_thetas.mean(), width=bar_width, color=\"green\", alpha=.3, label=\"planet\")\n",
        "\n",
        "plt.scatter(np.zeros_like(svae_omegas)+1-bar_width/2, svae_omegas, s=2, color=\"blue\")\n",
        "plt.bar(1-bar_width/2, svae_omegas.mean(), width=bar_width, color=\"blue\", alpha=.3)\n",
        "plt.scatter(np.zeros_like(cdkf_omegas)+1+bar_width/2, cdkf_omegas, s=2, color=\"red\")\n",
        "plt.bar(1+bar_width/2, cdkf_omegas.mean(), width=bar_width, color=\"red\", alpha=.3)\n",
        "plt.scatter(np.zeros_like(planet_omegas)+1+bar_width*3/2, planet_omegas, s=2, color=\"green\")\n",
        "plt.bar(1+bar_width*3/2, planet_omegas.mean(), width=bar_width, color=\"green\", alpha=.3)\n",
        "plt.title(\"MSE for linear decoding of true pendulum state\")\n",
        "plt.xticks([0, 1], [\"theta\", \"omega\"])\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iychu_ZEV6Z7"
      },
      "outputs": [],
      "source": [
        "# @title Turns out the correlation between runs comes from the fluctuating averge pixel intensity of the image\n",
        "i = 1 # data id\n",
        "result = all_results[\"v6sbb9xh\"]\n",
        "plt.plot(result[\"long_horizon_pred_lls\"][i])\n",
        "# plt.plot(result[\"predictions\"][i][0])\n",
        "result = all_results[\"xpf9s9ie\"]\n",
        "plt.plot(result[\"long_horizon_pred_lls\"][i])\n",
        "# plt.plot(result[\"predictions\"][i][0])\n",
        "obs_mean = data_dict[\"train_data\"][i,50:100].sum(axis=(1, 2, 3))\n",
        "plt.plot((obs_mean - obs_mean.mean()) * -5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m7tG1ohb4wl-"
      },
      "outputs": [],
      "source": [
        "# @title SVAE run names\n",
        "# run_name = \"391tsihg\" # 5d\n",
        "# run_name = \"vs2dkdje\" # 3d\n",
        "# run_name = \"zp5manco\" # 2d\n",
        "# 5d sinusoidal\n",
        "# run_name = \"0q0c0hbw\" \n",
        "# run_name = \"0vogdb8f\"\n",
        "# run_name = \"5b8cefgf\" # ICML-2 hopeful-sweep\n",
        "# run_name = \"7ntuood6\" # ICML-2 dainty-sweep (best prediction I've seen so far)\n",
        "# run_name = \"xpf9s9ie\" # ICML-3 good-sweep-21\n",
        "run_name = \"v6sbb9xh\" # ICML-3 rose-sweep-20 (good prediction)\n",
        "# \"1jxw27wp\" # ICML-3 solar-sweep\n",
        "# \"yo3fprzr\" # ICML-3 dry-sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LJi79Ias4IP_"
      },
      "outputs": [],
      "source": [
        "# @title CDKF run names\n",
        "# run_name = \"cy1j7jyg\" # ICML-2 fancy-sweep\n",
        "run_name = \"dgudjrut\" # ICML-3 quiet-sweep\n",
        "run_name = \"4nctqeby\" # ICML-3 honest-sweep\n",
        "# \"cdt4gir1\" tough-sweep\n",
        "# \"mpc58ktj\" young-sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Okamp0foh-Yg"
      },
      "outputs": [],
      "source": [
        "# If sampling from the prior becomes problematic, run this to truncate the singular values of A\n",
        "# prior_params[\"A\"] = truncate_singular_values(prior_params[\"A\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hlSFRedYCyUR"
      },
      "outputs": [],
      "source": [
        "# @title Sample from the prior and visualize its decoding\n",
        "key = key_0\n",
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "prior_sample = model.prior.sample(prior_params, shape=(1,), key=key)[0]\n",
        "plt.plot(prior_sample)\n",
        "plt.figure()\n",
        "# plot_pcs(prior_sample, 2)\n",
        "out_dist = model.decoder.apply(dec_params, prior_sample)\n",
        "y_decoded = out_dist.mean()\n",
        "plt.figure()\n",
        "plot_img_grid(y_decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TPgdXO-fDWue"
      },
      "outputs": [],
      "source": [
        "key = jr.split(key)[0]\n",
        "data_id = 3#jr.choice(key, 100)\n",
        "data = data_dict[\"train_data\"][data_id]\n",
        "# states = targets[data_id]\n",
        "# angles = np.arctan2(states[:,0], states[:,1])\n",
        "plot_img_grid(data)\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "sZcQduzMC5m-"
      },
      "outputs": [],
      "source": [
        "# @title Visualize the mean predicted trajectory from the model\n",
        "# This might be the wrong thing to do, because the prior dynamics might not be accurate for\n",
        "# specific observation sequences\n",
        "temp_params = deepcopy(run_params)\n",
        "temp_params[\"mask_size\"] = 0\n",
        "out = model.elbo(jr.PRNGKey(0), data, params, **temp_params)\n",
        "post_params = out[\"posterior_params\"]\n",
        "post_dist = model.posterior.distribution(post_params)\n",
        "Ex = post_dist.mean()\n",
        "A = prior_params[\"A\"]\n",
        "b = prior_params[\"b\"]\n",
        "T = 100\n",
        "Ex_pred = predict_forward(Ex[T//2-1], A, b, T//2)\n",
        "hs = plt.plot(Ex)\n",
        "hs_ = plt.plot(np.arange(T//2-1, T), np.concatenate([Ex[T//2-1][None], Ex_pred]), linestyle=\":\")\n",
        "plt.title(\"Posterior and predictions\")\n",
        "for i in range(len(hs)):\n",
        "    hs_[i].set_color(hs[i].get_color())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SDmP8-yldd8U"
      },
      "outputs": [],
      "source": [
        "out_dist = model.decoder.apply(dec_params, np.concatenate([Ex[:T//2], Ex_pred]))\n",
        "y_decoded = out_dist.mean()\n",
        "plt.figure()\n",
        "plot_img_grid(y_decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "0aed57nAXHoP"
      },
      "outputs": [],
      "source": [
        "# @title Visualize multiple possible future paths predicted by the model...!\n",
        "\n",
        "\n",
        "jax.config.update(\"jax_debug_nans\", False)\n",
        "train_data = data_dict[\"train_data\"][:20,:100]\n",
        "x_preds, svae_pred_lls = vmap(predict_multiple, in_axes=(0, None, None))\\\n",
        "    (train_data, key_0, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JhKaOqzTZHRW"
      },
      "outputs": [],
      "source": [
        "# D = model.prior.latent_dims\n",
        "# offset = 100\n",
        "\n",
        "# plt.figure(figsize=(20, 10))\n",
        "# for i in range(D):\n",
        "#     plt.plot(Ex[:,i] + i * offset, color=colors[i])\n",
        "#     plt.plot(np.arange(50, 100), x_preds[data_id,:,:,i].T + i * offset, color=colors[i], linestyle=\":\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEaCclVd0AZv"
      },
      "source": [
        "## Look at how well the physical state can be decoded from the latent representations linearly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MnfLNMgc-eRl"
      },
      "outputs": [],
      "source": [
        "temp_params = deepcopy(run_params)\n",
        "temp_params[\"mask_size\"] = 0\n",
        "\n",
        "targets = np.load(\"pendulum/pend_regression.npz\")[\"train_targets\"][:100]\n",
        "\n",
        "def encode(data):\n",
        "    out = model.elbo(jr.PRNGKey(0), data, params, **temp_params)\n",
        "    post_params = out[\"posterior_params\"]\n",
        "    post_dist = model.posterior.distribution(post_params)\n",
        "    return post_dist.mean()\n",
        "\n",
        "all_latents_train = vmap(encode)(data_dict[\"train_data\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Qsufn5KZYUN1"
      },
      "outputs": [],
      "source": [
        "states = targets[:,::2]\n",
        "train_thetas = np.arctan2(states[:,:,0], states[:,:,1])\n",
        "train_omegas = train_thetas[:,1:]-train_thetas[:,:-1]\n",
        "thetas = train_thetas.flatten()\n",
        "omegas = train_omegas.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6ofB0o2F_qOS"
      },
      "outputs": [],
      "source": [
        "D = 5\n",
        "xs_theta = all_latents_train.reshape((-1, D))\n",
        "xs_omega = all_latents_train[:,1:].reshape((-1, D))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "es8t-Udfasa8"
      },
      "outputs": [],
      "source": [
        "W_theta, _, _, _ = np.linalg.lstsq(xs_theta, thetas)\n",
        "W_omega, _, _, _ = np.linalg.lstsq(xs_omega, omegas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LPZckeRTDEbx"
      },
      "outputs": [],
      "source": [
        "test_id = 0 if \"test_id\" not in globals() else test_id + 1\n",
        "plt.plot(all_latents_train[test_id] @ W_theta, label=\"decoded\")\n",
        "plt.plot(train_thetas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum angle (train sequence #{})\".format(test_id))\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(all_latents_train[test_id] @ W_omega, label=\"decoded\")\n",
        "plt.plot(train_omegas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum velocity (train sequence #{})\".format(test_id))\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EQAY3_BZA4Px"
      },
      "outputs": [],
      "source": [
        "test_targets = np.load(\"pendulum/pend_regression_longer.npz\")[\"test_targets\"][:20, ::2][:, :100]\n",
        "test_data = data_dict[\"val_data\"][:, :100]#np.load(\"pendulum/pend_regression.npz\")[\"test_obs\"][:, ::2]\n",
        "thetas = np.arctan2(test_targets[:,:,0], test_targets[:,:,1])\n",
        "omegas = thetas[:,1:]-thetas[:,:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Gmcd1UabBcZg"
      },
      "outputs": [],
      "source": [
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "all_latents_test = vmap(encode)(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0QOzPyLqZ78y"
      },
      "outputs": [],
      "source": [
        "test_id = 0 if \"test_id\" not in globals() else test_id + 1\n",
        "plt.plot(all_latents_test[test_id] @ W_theta, label=\"decoded\")\n",
        "plt.plot(thetas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum angle (test sequence #{})\".format(test_id))\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(all_latents_test[test_id] @ W_omega, label=\"decoded\")\n",
        "plt.plot(omegas[test_id], label=\"true\")\n",
        "plt.title(\"Linear decoding of pendulum velocity (test sequence #{})\".format(test_id))\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxjbHKWcPSmd"
      },
      "source": [
        "## Evaluate the sliding window prediction log likelihoods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DvNpnf-wPXsk"
      },
      "outputs": [],
      "source": [
        "def prediction_lls(post_params):\n",
        "    posterior = model.posterior.distribution(post_params)\n",
        "    # Get the final mean and covariance\n",
        "    mu, Sigma = posterior.mean()[-1], posterior.covariance()[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SAuuK-FGRUW2"
      },
      "outputs": [],
      "source": [
        "obj, out_dict = svae_loss(key, model, data_dict[\"train_data\"][:10], params, **temp_params)\n",
        "post_params = out_dict[\"posterior_params\"]\n",
        "posterior = model.posterior.distribution(post_params)\n",
        "J = posterior.filtered_precisions\n",
        "h = posterior.filtered_linear_potentials\n",
        "Sigma_filtered = inv(J)\n",
        "mu_filtered = np.einsum(\"...ij,...j->...i\", Sigma_filtered, h)\n",
        "data_batch = data_dict[\"train_data\"]\n",
        "horizon = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eZ1y0BMfSNWf"
      },
      "outputs": [],
      "source": [
        "def pred_ll(data_id, key):\n",
        "    num_windows = T-horizon-1\n",
        "    pred_lls = vmap(sliding_window_prediction_lls, in_axes=(None, None, None, 0, 0))(\n",
        "        mu_filtered[data_id], Sigma_filtered[data_id], data_batch[data_id],\n",
        "        np.arange(num_windows), jr.split(key, num_windows))\n",
        "    return pred_lls.mean(axis=0)\n",
        "\n",
        "def sliding_window_prediction_lls(mu, Sigma, data, t, key):\n",
        "    # Build the posterior object on the future latent states \n",
        "    # (\"the posterior predictive distribution\")\n",
        "    # Convert unconstrained params to constrained dynamics parameters\n",
        "    prior_params_constrained = model.prior.get_dynamics_params(prior_params)\n",
        "    dynamics_params = {\n",
        "        \"m1\": mu[t],\n",
        "        \"Q1\": Sigma[t],\n",
        "        \"A\": prior_params_constrained[\"A\"],\n",
        "        \"b\": prior_params_constrained[\"b\"],\n",
        "        \"Q\": prior_params_constrained[\"Q\"]\n",
        "    }\n",
        "    tridiag_params = dynamics_to_tridiag(dynamics_params, horizon+1, D) # Note the +1\n",
        "    J, L, h = tridiag_params[\"J\"], tridiag_params[\"L\"], tridiag_params[\"h\"]\n",
        "    pred_posterior = MultivariateNormalBlockTridiag.infer(J, L, h)\n",
        "    # Sample from it and evaluate the log likelihood\n",
        "    x_pred = pred_posterior.sample(seed=key)[1:]\n",
        "    likelihood_dist = model.decoder.apply(dec_params, x_pred)\n",
        "    return likelihood_dist.log_prob(\n",
        "        lax.dynamic_slice(data, (t+1, 0, 0, 0), (horizon,) + data.shape[1:])).sum(axis=(1, 2, 3))\n",
        "\n",
        "num_windows = T-horizon-1\n",
        "pred_lls = vmap(pred_ll)(np.arange(10), jr.split(key_0, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IoDd5GdpUAqi"
      },
      "outputs": [],
      "source": [
        "plt.plot(pred_lls.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRM7zBPx4pdE"
      },
      "source": [
        "## Evaluation code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD3BXGWQ6ko_"
      },
      "source": [
        "# What is going on with the dynamics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VtChfl-eG5qm"
      },
      "outputs": [],
      "source": [
        "theta = 2 * np.pi / 100\n",
        "lds_params = {\n",
        "    \"m1\": np.zeros(2),\n",
        "    \"Q1\": np.eye(2),\n",
        "    \"A\": np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]]),\n",
        "    \"Q\": np.eye(2) / 100,\n",
        "    \"b\": np.zeros(2)\n",
        "}\n",
        "prior = LinearGaussianChain(2, 100)\n",
        "prior_dist = prior.distribution(prior.get_constrained_params(lds_params))\n",
        "plt.plot(prior_dist.sample(seed=key_0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dEvsYfef6kdW"
      },
      "outputs": [],
      "source": [
        "temp_params = deepcopy(run_params)\n",
        "temp_params[\"mask_size\"] = 0\n",
        "_, aux = svae_loss(key_0, model, data_dict[\"train_data\"][:10], params, **temp_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qRmFHEyb67Un"
      },
      "outputs": [],
      "source": [
        "pp = deepcopy(params[\"prior_params\"])\n",
        "suff_stats = aux[\"sufficient_statistics\"]\n",
        "pp[\"avg_suff_stats\"] = suff_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8YMXhGbU7RmK"
      },
      "outputs": [],
      "source": [
        "fit_prior_params = model.prior.m_step(pp)\n",
        "# fit_prior_params[\"A\"] = scale_singular_values(fit_prior_params[\"A\"])\n",
        "# fit_prior_params[\"A\"] = truncate_singular_values(fit_prior_params[\"A\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7qU7e1QiDMvy"
      },
      "outputs": [],
      "source": [
        "key = key_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TnFVPlKBBhz0"
      },
      "outputs": [],
      "source": [
        "m1 = Q = fit_prior_params[\"m1\"]\n",
        "Q1 = fit_prior_params[\"Q1\"]\n",
        "Q = fit_prior_params[\"Q\"]\n",
        "A = fit_prior_params[\"A\"]\n",
        "b = fit_prior_params[\"b\"]\n",
        "\n",
        "\n",
        "x = jr.multivariate_normal(key=key, mean=m1, cov=Q1)\n",
        "xs = []\n",
        "for i in range(200):\n",
        "    xs.append(x)\n",
        "    key, _ = jr.split(key)\n",
        "    noise = jr.multivariate_normal(key=key, mean=np.zeros_like(x), cov=Q)\n",
        "    x = A @ x + b + noise\n",
        "\n",
        "plt.plot(np.array(xs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Beq52DGTBr0U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rHEwqDfA8Awl"
      },
      "outputs": [],
      "source": [
        "prior = LinearGaussianChain(model.prior.latent_dims, model.prior.seq_len)\n",
        "prior_dist = prior.distribution(prior.get_constrained_params(fit_prior_params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WT-OnbBX9SC7"
      },
      "outputs": [],
      "source": [
        "Q = fit_prior_params[\"Q\"]\n",
        "A = fit_prior_params[\"A\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uACoUB3T89M_"
      },
      "outputs": [],
      "source": [
        "sample = prior_dist.sample(seed=key_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ISTJrxMD9Mq6"
      },
      "outputs": [],
      "source": [
        "plt.plot(sample)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2OPHhcbZObuu"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cf360b64524a4e77a5d91a2eb1ee5141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8fda8ee2014e4722b130cbd2425ecf93",
              "IPY_MODEL_121ba4af374245e8a3fc2c8e453e28e3"
            ],
            "layout": "IPY_MODEL_fea301e7bfae442f8f32898af5761ab4"
          }
        },
        "8fda8ee2014e4722b130cbd2425ecf93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_719d9e9ce429414ba58ff81e5bc41079",
            "placeholder": "​",
            "style": "IPY_MODEL_eef26dac7df34694ad278fa583fcc6f7",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "121ba4af374245e8a3fc2c8e453e28e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b4d2cc7b16a4bbba1a331fbc4a6cd5a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dafb4f8933004052b74cba5e238c9480",
            "value": 1
          }
        },
        "fea301e7bfae442f8f32898af5761ab4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "719d9e9ce429414ba58ff81e5bc41079": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eef26dac7df34694ad278fa583fcc6f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b4d2cc7b16a4bbba1a331fbc4a6cd5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dafb4f8933004052b74cba5e238c9480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1e2620eebdb4a53a621b703b8089cc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15e4d13f8aba48c3afa1c7ceceadbb04",
              "IPY_MODEL_832496646ce0471991a3478a3f004489"
            ],
            "layout": "IPY_MODEL_c4c4829fb582458a9ac5a430fd8b082a"
          }
        },
        "15e4d13f8aba48c3afa1c7ceceadbb04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3311502ea4694629840c722cfb57edce",
            "placeholder": "​",
            "style": "IPY_MODEL_f6d8ef3d8af74f6d94c3fc38197f598b",
            "value": "119.279 MB of 119.287 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "832496646ce0471991a3478a3f004489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c6b6ab8d2094c8d826ded69da0dfbe4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4a4c8d429034737b6673e23ceababd6",
            "value": 0.9999357858434855
          }
        },
        "c4c4829fb582458a9ac5a430fd8b082a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3311502ea4694629840c722cfb57edce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6d8ef3d8af74f6d94c3fc38197f598b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c6b6ab8d2094c8d826ded69da0dfbe4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4a4c8d429034737b6673e23ceababd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}